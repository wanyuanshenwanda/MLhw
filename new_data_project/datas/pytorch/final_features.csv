number,state,title,author,body,created_at,updated_at,merged_at,merged,comments,review_comments,commits,additions,deletions,changed_files,conversation,closed_at,title_length,body_length,has_test,has_bug,has_feature,has_document,has_improve,has_refactor,files_updated,comment_num,comment_length,total_changes,total_additions_commit,total_deletions_commit,commit_authors_count,commit_count,unique_reviewers,total_comments,first_comment_time,project
163856,open,"Add scaled_mm python API, test",slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163856
* #163855
* #163854

Summary:

* Add `torch.quantization.scaled_mm` as an abstraction around the C++
  methods
* Wraps `torch._scaled_mm_v2` API by default, but user can force use of
  the older `torch._scaled_mm` interface.
* Scaled MM tests now run on the new API

Test Plan:

`pytest test/test_scaled_matmul_cuda.py`

Reviewers:

Subscribers:

Tasks:

Tags:
Signed-off-by: Simon Layton <simonlayton@meta.com>",2025-09-25 13:57:48+00:00,2025-09-25T14:47:43Z,,False,1,0,3,332,43,3,1,,30,504,False,False,False,False,False,False,3,0,0,376,333,43,1,2,,,,pytorch
163855,open,Split Scaled matmul tests into a separate file,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163856
* __->__ #163855
* #163854

Summary:

* Separate scaled_mm tests into a new file

Test Plan:

* `pytest test/test_scaled_matmul_cuda.py`

Reviewers:

Subscribers:

Tasks:

Tags:
Signed-off-by: Simon Layton <simonlayton@meta.com>

cc @albanD",2025-09-25 13:57:44+00:00,2025-09-25T14:47:46Z,,False,3,0,2,1523,1486,3,3,,46,326,False,False,False,False,False,False,3,2,77,3009,1523,1486,1,2,2.0,2.0,2025-09-25T14:01:15Z,pytorch
163854,open,Add _scaled_mm_v2 API,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163856
* #163855
* __->__ #163854

Summary:

* Add new scaled-MM API to future-proof / clean-up existing code.
* Scaling is explicitly described rather than infer
* Swizzling of scaled must now be defined (vs. inferred)
* Adds API support for multi-level scaling
* Refactor dispatch logic to make it easier to add new implementations

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:
Signed-off-by: Simon Layton <simonlayton@meta.com>",2025-09-25 13:57:40+00:00,2025-09-25T14:47:49Z,,False,3,0,1,888,1,3,3,,21,516,False,False,False,False,False,True,3,1,47,889,888,1,1,1,1.0,1.0,2025-09-25T14:40:06Z,pytorch
163850,open,[OpenReg][BE] Replacing explicit prefix/suffix with CMake variables,can-gaa-hou,"As the title states, suffixes like`.dylib` and `lib` can be replaced by `CMAKE_SHARED_LIBRARY_SUFFIX`, and prefixes like `lib` can be replaced by `CMAKE_SHARED_LIBRARY_PREFIX` on Unix or `CMAKE_IMPORT_LIBRARY_PREFIX` on Windows.

cc @albanD
",2025-09-25 11:49:34+00:00,2025-09-25T14:52:44Z,,False,3,0,1,2,4,1,3,,67,241,False,True,False,False,False,False,1,2,643,6,2,4,1,1,2.0,3.0,2025-09-25T13:30:36Z,pytorch
163848,open,"Revert ""enable test_sampled_addmm_zero_sized_cuda for rocm (#121940)""",dnikolaev-amd,"This reverts commit 5494b2a8d38c3ddbeb2d96a5ac990e20ec4c48fd.

Need to skip `test_sparse_csr.py::TestSparseCSRCUDA::test_sampled_addmm_zero_sized_cuda_*` again. Tests are failing now with ""core dumped"" error
```
python test_sparse_csr.py -v -k test_sampled_addmm_zero_sized_cuda_float64

  test_sampled_addmm_zero_sized_cuda_float64 (__main__.TestSparseCSRCUDA) ... /tmp/pytorch/test/test_sparse_csr.py:2503:   c = torch.empty(m, n, dtype=dtype, device=device, layout=torch.sparse_csr)
GPU core dump created: gpucore.186789
:0:rocdevice.cpp            :2992: 4701819131755 us:  Callback: Queue 0x760cdcd00000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016
Aborted (core dumped)
```
These failures are linked to `test_sparse_csr.py::TestSparseCSRCUDA::test_select_SparseBSC_int32_cuda_*` due to incorrect test log parsing. We will be able to close these issues also:

- Fixes https://github.com/pytorch/pytorch/issues/163663
- Fixes https://github.com/pytorch/pytorch/issues/160786
- Fixes https://github.com/pytorch/pytorch/issues/160785
- Fixes https://github.com/pytorch/pytorch/issues/160784


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-25 11:29:41+00:00,2025-09-25T14:47:47Z,,False,2,0,1,2,2,1,2,,69,1282,False,True,False,False,False,False,1,0,0,4,2,2,1,1,1.0,0.0,2025-09-25T14:29:36Z,pytorch
163846,open,[Code Clean] Remove support of python3.9,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163846

As the title stated.",2025-09-25 09:47:13+00:00,2025-09-25T12:59:57Z,,False,1,0,1,1,9,2,1,,40,126,False,False,False,False,False,False,2,0,0,10,1,9,1,1,,,,pytorch
163844,open,Fix specialize_impl from triton.runtime.jit,houseroad,"Summary:
In https://github.com/triton-lang/triton/pull/7771/ , create_specialize_impl is removed. We extend the support using native_specialize_impl.

Otherwise, PyTorch won't work with trunk triton.

Test Plan:
scripts/lufang/llm/launch_qwen3_vl_235b_a22b_thinking_2507_h100.sh

No more error message like
```
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Traceback (most recent call last):
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 924, in identify_mutated_tensors
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 419, in generate_ttir
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     specialization = _get_specialization(ordered_args.values())
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 390, in _get_specialization
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     from triton.runtime.jit import specialize_impl as specialize_impl_orig
(Worker_TP0_EP0 pid=190353) [rank0]:W0924 23:24:48.190000 190353 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] ImportError: cannot import name 'specialize_impl' from 'triton.runtime.jit' (/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inf
erence_platform_sp/llm_predictor_gpu/__service__/service#link-tree/triton/runtime/jit.py)
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Traceback (most recent call last):
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 924, in identify_mutated_tensors
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 419, in generate_ttir
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     specialization = _get_specialization(ordered_args.values())
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 390, in _get_specialization
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]     from triton.runtime.jit import specialize_impl as specialize_impl_orig
(Worker_TP1_EP1 pid=190354) [rank1]:W0924 23:24:48.210000 190354 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] ImportError: cannot import name 'specialize_impl' from 'triton.runtime.jit' (/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inf
erence_platform_sp/llm_predictor_gpu/__service__/service#link-tree/triton/runtime/jit.py)
(Worker_TP5_EP5 pid=190359) [rank5]:W0924 23:24:48.216000 190359 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated
(Worker_TP5_EP5 pid=190359) [rank5]:W0924 23:24:48.216000 190359 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0] Traceback (most recent call last):
(Worker_TP5_EP5 pid=190359) [rank5]:W0924 23:24:48.216000 190359 /data/users/lufang/fbsource/fbcode/caffe2/torch/_higher_order_ops/triton_kernel_wrap.py:948] [0/0]   File ""/data/users/lufang/fbsource/buck-out/v2/gen/fbcode/4e83bca020adbfd7/smart/inference_platform_sp/llm_predictor_gpu/__service__/service#link-tree/to
rch/_higher_order_ops/triton_kernel_wrap.py"", line 924, in identify_mutated_tensors
```

Differential Revision: D83229128


",2025-09-25 07:58:02+00:00,2025-09-25T14:47:48Z,,False,2,0,1,8,0,1,2,,43,6677,False,True,False,False,False,False,1,0,0,8,8,0,1,1,1.0,0.0,2025-09-25T08:03:15Z,pytorch
163843,open,[DTensor] fix uneven _StridedShard,tianyu-l,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163843

Previous uneven `_StridedShard` in https://github.com/pytorch/pytorch/pull/150490 seems failing cases like sharding `tensor = torch.arange(6)` with FSDP 2, TP 2.

This PR attempts to reinvent `_StridedShard`.

I didn't test nested `_StridedShard`, because there shouldn't be any use cases. I think it will become quite messy when it comes to **nested uneven** `_StridedShard`. We are probably going to deprecate it anyway after @zpcore 's work https://github.com/pytorch/pytorch/pull/160266 on ordered sharding, so IMO not worth it to make it too general.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-25 07:02:20+00:00,2025-09-25T14:29:38Z,,False,1,2,2,124,111,5,3,,34,753,False,True,False,False,False,False,5,0,0,237,125,112,1,2,1.0,0.0,2025-09-25T14:14:09Z,pytorch
163842,open,Improve repeat op to a single copy,haifeng-jin,"In #163455 , the `reshape` was not a pure view op.

The `permute` before it created an non-contiguous tensor, which would trigger a data copy during the reshape.

This PR improved the implementation by remove the `urtensor` intermediate tensor completely.
By simply expanding the `xtensor` would achieve the `repeat` effect.

Before this PR, there were two data copies (in `urtensor.copy_` and `urtensor.reshape`).
Now, there is only one data copy in the `.copy_()`.
Reshape would not copy data because it is on a contiguous tensor.

One more note is that we do want at one copy because we want to duplicate the elements for the repeats.
User can inplace modify single elements without afffecting others.",2025-09-25 05:07:40+00:00,2025-09-25T09:00:52Z,,False,1,0,4,19,33,1,1,,34,704,False,False,False,False,True,False,1,0,0,86,36,50,1,4,,,,pytorch
163841,open,Fix CUDA memory usage for CPU only compile,CaoE,"Fixes #150622. Copied from https://github.com/pytorch/pytorch/pull/150669.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-25 04:43:34+00:00,2025-09-25T08:21:26Z,,False,1,0,2,80,35,10,1,,42,291,False,True,False,False,False,False,10,0,0,131,88,43,1,2,,,,pytorch
163840,closed,Test redistriubtion,SherlockNoMad,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163840



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-25 04:35:33+00:00,2025-09-25T04:36:14Z,,False,1,0,1,175,4,1,1,2025-09-25 04:36:14+00:00,19,197,False,False,False,False,False,False,1,0,0,179,175,4,1,1,,,,pytorch
163839,open,[fr] Skip the dtype check for some one to all or all to one collective,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163839

As title, in practice we found that sometimes, the dtype of gather does not match when it comes to output among all ranks, which is a undefined behavior. Same with broadcast and scatter. And they are all completed, so we should not think they are errors, we can skip it.


cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-25 04:34:39+00:00,2025-09-25T07:49:38Z,,False,1,0,2,38,17,2,1,,70,460,False,False,False,False,False,False,2,0,31,57,39,18,1,2,1.0,1.0,2025-09-25T07:49:38Z,pytorch
163838,open,[OpenReg] unify event semantic with device index,wschin,"Fixes #163836.
",2025-09-25 04:24:59+00:00,2025-09-25T06:39:41Z,,False,2,0,1,2,2,1,2,,48,15,False,True,False,False,False,False,1,0,0,4,2,2,1,1,,,,pytorch
163837,open,[a2av] Separate in/out splits into two tensors,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163815
* #163814
* __->__ #163837

Old signature:
`all_to_all_vdev(Tensor input, Tensor(a!) out, Tensor(a!) in_out_splits, str group_name)`
New signature:
`all_to_all_vdev(Tensor input, Tensor(a!) out, Tensor in_splits, Tensor(a!) out_splits_offsets, str group_name)`

i.e. split `in_out_splits` into IN tensor and OUT tensor so that we can define the TORCH_LIBRARY signature better.
Also to be in line with the 2D version.",2025-09-25 04:21:20+00:00,2025-09-25T06:50:11Z,,False,1,0,1,34,26,4,1,,46,514,False,False,False,False,False,False,4,0,0,60,34,26,1,1,,,,pytorch
163835,open,Expose torch.nn.utils.parametrize,hanchchch,"`torch.nn.utils.parametrize` is not imported from `torch/nn/utils/__init__.py`, thus is not exposed and make it hard for code editors to statically analyze the code and provide auto-completion based on the function signature.

<img width=""615"" height=""292"" alt=""Screenshot 2025-09-25 at 12 01 52 PM"" src=""https://github.com/user-attachments/assets/a276f6f0-87f3-4732-943d-2a92ea871974"" />


after the fix:

<img width=""964"" height=""393"" alt=""Screenshot 2025-09-25 at 12 02 16 PM"" src=""https://github.com/user-attachments/assets/ca47f09e-dc4e-4420-a2d2-11669e07471a"" />
",2025-09-25 03:08:03+00:00,2025-09-25T14:47:44Z,,False,5,0,2,2,1,1,5,,33,569,False,True,False,False,False,False,1,4,1085,5,3,2,1,2,3.0,5.0,2025-09-25T03:09:05Z,pytorch
163832,open,Add scaffolding for StableIValue FC/BC,mikaylagawarecki,"Part 2 of plan in https://docs.google.com/document/d/1MaX51H5aEQE5XnOlnZIpf9oCYwzGrTWkgBACxNzsmWE/edit?usp=sharing

- Upgrades `aoti_torch_library_impl_init` to v2 with an `extension_abi_version` kwarg, which should always bake in TORCH_FEATURE_VERSION as the extension_abi_version when a user uses `STABLE_TORCH_LIBRARY_IMPL`
- Plumbs `extension_abi_version` to 
        - `from_ivalue`
        - `to_ivalue`
        - `From`
        - `To`
        - Decoupled `from_internal` and `to_internal` (used by `from_ivalue` and `to_ivalue` from `from` and `to` (used by boxed kernel for extension)
- (likely won't land) Proof of concept to show how we will handle FC/BC for StableIValue in the case that one of our structs in headeronly/stable changes
        - Created a `Dummy` type in torch/headeronly
        - simulated an ABI breaking change to it (keeping the old version around in a separate namespace)
        - TORCH_FEATURE_VERSION to toggle between which namespace gets inlined
        - Shows how FromImpl/ToImpl should be updated (preprocessor macros based on TORCH_FEATURE_VERSION + runtime dispatch based on extension_abi_version for FC/BC)
     

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163832
* #163683



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-25 01:39:21+00:00,2025-09-25T03:14:29Z,,False,2,0,2,602,65,20,2,,38,1312,False,False,True,True,False,False,20,0,0,667,602,65,1,2,,,,pytorch
163831,open,"use sym_numel, to allow fake tensors to work",vinayp17,"Fixes #[163759](https://github.com/pytorch/pytorch/issues/163759)

Replace `numel` with `sym_numel`. Tested with example in issue and it works now . ",2025-09-25 01:22:36+00:00,2025-09-25T14:07:35Z,,False,3,1,2,44,2,2,4,,44,149,False,True,False,False,False,False,2,1,118,46,44,2,1,2,2.0,2.0,2025-09-25T01:34:44Z,pytorch
163830,open,"Revert ""[RELAND] Always build USE_DISTRIBUTED (#160449) and Make dist…",wdvr,"…ributed modules importable even when backend not built (#159889) (#162594)""

This reverts commit 09cb34c1dce8fe1b880bbf3115d8ddad3401d871.

Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-25 01:18:56+00:00,2025-09-25T14:43:48Z,,False,6,0,1,458,778,52,6,,70,309,False,True,False,False,False,False,52,5,1630,1236,458,778,1,1,3.0,5.0,2025-09-25T01:20:04Z,pytorch
163829,open,[Inductor][ATen][FP8] Relax stride check for block-wise scaling when scaling dimension is 1,jananisriram,"Summary: Relax stride check for block-wise scaling (1x128, 128x128) when a dimension of the scaling factor is 1. When the scaling tensor has a dimension of size 1, the stride is effectively ""meaningless"" to PyTorch, i.e. PyTorch decides to replace its stride with a default of `[1, 1]`. However, the old stride check required the stride to match one of the scaling dimensions. Here, we relax the stride check when the effective stride is 1 in order to allow for cases in which `K <= 128` and `N <= 128`.

Test Plan:
```
pytest -s -v test/test_matmul_cuda.py::TestFP8MatmulCUDA::test_scaled_mm_vs_emulated_block_wise_float32_lhs_block_1_rhs_block_128_cuda   2>&1 | tee ~/personal/stride_check.log
```

Differential Revision: D83023706


",2025-09-25 01:15:14+00:00,2025-09-25T02:52:13Z,,False,3,0,1,26,6,1,3,,91,736,False,False,False,False,False,False,1,0,0,32,26,6,1,1,,,,pytorch
163828,open,[Inductor-FX] Remove python_slow grid mode,blaine-rister,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-25 00:58:45+00:00,2025-09-25T10:20:37Z,,False,4,0,3,46,63,3,4,,42,223,False,True,False,False,False,False,3,0,0,149,66,83,1,3,,,,pytorch
163827,open,[dde] is_channels_last_strides_2d_s4 -> is_channels_last_strides_2d_s4_false to support unbacked shapes,ColinPeppler,"Adds is_channels_last_strides_2d_s4_or_false which will return False if it can't statically determine is_channels_last_strides_2d_s4.


Summary:
```
  File ""/caffe2/test/inductor/__unbacked_symints__/unbacked_symints#link-tree/torch/_dynamo/utils.py"", line 3547, in run_node
    return node.target(*args, **kwargs)  # type: ignore[operator]
...
  File ""<invalid>"", line 0, in at::_ops::pixel_shuffle::call(at::Tensor const&, long)
...
  File ""<invalid>"", line 0, in at::TensorBase::suggest_memory_format(bool) const
  File ""<invalid>"", line 0, in c10::TensorImpl::is_strides_like_default(c10::MemoryFormat) const
  File ""<invalid>"", line 0, in c10::SymbolicShapeMeta::init_is_channels_last() const
  File ""<invalid>"", line 0, in bool c10::is_channels_last_strides_2d_s4<c10::SymInt>(c10::ArrayRef<c10::SymInt>, c10::ArrayRef<c10::SymInt>)
...
 File ""torch/fx/experimental/symbolic_shapes.py"", line 7593, in _evaluate_expr
    raise self._make_data_dependent_error(

torch._dynamo.exc.UserError: Could not guard on data-dependent expression u0 + 1 > 1

from user code:
   File ""test/inductor/test_unbacked_symints.py"", line 618, in fn
    return torch.nn.functional.pixel_shuffle(repeated, upscale_factor=2)
```

Differential Revision: D83094252


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-25 00:58:04+00:00,2025-09-25T07:00:48Z,,False,2,0,1,30,8,2,2,,103,1448,False,False,False,False,False,False,2,0,0,38,30,8,1,1,,,,pytorch
163826,open,WIP: Testing glossary directive.,AlannaBurke,"Fixes #ISSUE_NUMBER
",2025-09-25 00:41:05+00:00,2025-09-25T02:52:25Z,,False,1,0,1,38,85,3,1,,32,20,False,True,False,False,False,False,3,0,0,123,38,85,1,1,,,,pytorch
163825,closed,[a2av] Separate in/out splits into two tensors,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163815
* #163814
* __->__ #163825

",2025-09-25 00:33:31+00:00,2025-09-25T04:27:41Z,,False,1,0,1,34,26,4,1,2025-09-25 04:27:41+00:00,46,126,False,False,False,False,False,False,4,0,0,60,34,26,1,1,,,,pytorch
163824,open,DebugMode supports_higher_order_operators=True,SherlockNoMad,"Make DebugMode supports HOP

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-25 00:31:35+00:00,2025-09-25T14:54:19Z,,False,3,0,3,11,0,2,3,,46,130,False,True,False,False,False,False,2,2,493,63,37,26,1,3,3.0,2.0,2025-09-25T05:38:21Z,pytorch
163823,open,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-25 00:26:07+00:00,2025-09-25T05:21:29Z,,False,4,0,1,1,1,1,4,,46,156,False,False,False,False,False,False,1,3,1023,2,1,1,1,1,2.0,3.0,2025-09-25T00:26:08Z,pytorch
163822,open,Handle DDE in infer_size_impl,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163822
* #163652

hit this while running VLLM with unbacked for model Qwen/Qwen2-1.5B-Instruct
",2025-09-25 00:25:33+00:00,2025-09-25T10:34:59Z,,False,3,4,5,35,10,1,7,,29,181,False,False,False,False,False,False,1,2,253,107,66,41,1,5,2.0,2.0,2025-09-25T00:27:39Z,pytorch
163820,open,[WIP][dtensor] avoid shape recompilations on DTensorSpec,pianpwk,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-25 00:16:01+00:00,2025-09-25T05:46:05Z,,False,1,0,1,54,1,3,1,,56,123,False,True,False,False,False,False,3,0,0,55,54,1,1,1,,,,pytorch
163819,open,add ABI stable method for updating constant buffer,yushangdi,"Fixes #ISSUE_NUMBER


Example usage:

```
        // Load constants. Create random constants here.
        auto* fc1_w = new slim::SlimTensor(slim::empty({16, 10}, c10::kFloat, c10::Device(c10::kCUDA, 0)));
        fc1_w->fill_(1.0);

        auto* fc1_b = new slim::SlimTensor(slim::empty({16}, c10::kFloat, c10::Device(c10::kCUDA, 0)));
        fc1_b->fill_(1.0);

        auto* fc2_w = new slim::SlimTensor(slim::empty({1, 16}, c10::kFloat, c10::Device(c10::kCUDA, 0)));
        fc2_w->fill_(1.0);

        auto* fc2_b = new slim::SlimTensor(slim::empty({1}, c10::kFloat, c10::Device(c10::kCUDA, 0)));
        fc2_b->fill_(1.0);

        // Build pairs
        std::vector<AOTInductorConstantPair> constants{
            {""fc1_weight"", fc1_w},
            {""fc1_bias"",   fc1_b},
            {""fc2_weight"", fc2_w},
            {""fc2_bias"",   fc2_b},
        };
        std::cout << ""initialized rand map\n"";
        
        if (!update_user_managed_constant_buffer_abi) {
            std::cerr << ""Function pointer not initialized!\n"";
        }


        // Call runtime (pass raw pointer + size)
        update_user_managed_constant_buffer_abi( 
            container_handle,
            constants.data(),
            constants.size(),
            /*use_inactive=*/false,
            /*validate_full_update=*/true);
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-25 00:10:41+00:00,2025-09-25T01:07:16Z,,False,1,0,1,34,0,2,1,,50,1527,False,True,False,False,False,False,2,0,0,34,34,0,1,1,,,,pytorch
163818,open,"[dynamo, 3.14] fix BUILD_TUPLE with 0 args",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163818
* #163796
* #163292
* #163191
* #163110
* #163109
* #163009
* #161839
* #161555
* #161838

",2025-09-25 00:04:03+00:00,2025-09-25T04:01:37Z,,False,2,0,1,26,21,5,2,,42,184,False,True,False,False,False,False,5,0,0,47,26,21,1,1,,,,pytorch
163817,open,[ROCm][DO NOT MERGE] Dummy PR to run CI for sparse tests,dnikolaev-amd,"Fixes https://github.com/pytorch/pytorch/issues/163663
Fixes https://github.com/pytorch/pytorch/issues/160786
Fixes https://github.com/pytorch/pytorch/issues/160785
Fixes https://github.com/pytorch/pytorch/issues/160784


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-24 23:38:42+00:00,2025-09-25T05:34:04Z,,False,1,0,1,0,0,0,1,,56,338,False,True,False,False,False,False,0,0,0,0,0,0,1,0,,,,pytorch
163815,open,Hierarchical A2A: intra-node dispatch,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163815
* #163814
* #163837

Mimic'ing the second step of dedup A2A --
an a2av_2d that runs intra-node, concurrently, on a 2D mesh.

To be as close to MoE as possible, the code:
- expands token sequence by `topk_per_node` factor;
- converts `topk_idx_intranode` into `splits` via histogram.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 23:26:08+00:00,2025-09-25T06:50:09Z,,False,1,0,3,74,2,1,1,,37,490,False,False,False,False,False,False,1,0,0,127,102,25,1,3,,,,pytorch
163814,open,Hierarchical A2A: inter-node dispatch,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163815
* __->__ #163814
* #163837

Mimic'ing the first step of dedup A2A --
- run histogram on `topk_node_idx`, get splits
- sort token sequence with `topk_node_idx`, expanding it by `topk_node` times
- perform a 1D a2a that runs rail-wise, concurrently on a 2D mesh.",2025-09-24 23:26:04+00:00,2025-09-25T06:01:20Z,,False,1,0,3,77,0,1,1,,37,358,False,False,False,False,False,False,1,0,0,115,96,19,1,3,,,,pytorch
163813,closed,[a2av] Separate in/out splits into two tensors,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163815
* #163814
* __->__ #163813

Old signature:
`all_to_all_vdev(Tensor input, Tensor(a!) out, Tensor(a!) in_out_splits, str group_name)`
New signature:
`all_to_all_vdev(Tensor input, Tensor(a!) out, Tensor in_splits, Tensor(a!) out_splits_offsets, str group_name)`

i.e. split `in_out_splits` into IN tensor and OUT tensor so that we can define the TORCH_LIBRARY signature better.
Also to be in line with the 2D version.",2025-09-24 23:25:59+00:00,2025-09-25T04:27:53Z,,False,1,0,1,34,26,4,1,2025-09-25 04:27:53+00:00,46,514,False,False,False,False,False,False,4,0,0,60,34,26,1,1,,,,pytorch
163812,open,[torchfuzz] make generated code much more concise and cleaner,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163812
* #163743

```
import torch

torch._dynamo.config.capture_scalar_outputs = True
torch.manual_seed(42)

def fuzzed_program(arg_0, arg_1, arg_2):
    var_node_3 = arg_0 # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_4 = torch.full((1,), (-0.29262632146522655-0.7687848816195035j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_2 = torch.ops.aten.add(var_node_3, var_node_4) # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_6 = arg_1 # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_7 = arg_2 # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_5 = torch.ops.aten.add(var_node_6, var_node_7) # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_1 = torch.ops.aten.add(var_node_2, var_node_5) # size=(1,), stride=(1,), dtype=complex128, device=cuda
    var_node_0 = var_node_1.item() # dtype=complex128
    return var_node_0

arg_0 = torch.as_strided(torch.randn(1).to(torch.complex128), (1,), (1,))
arg_1 = torch.as_strided(torch.randn(1).to(torch.complex128), (1,), (1,))
arg_2 = torch.as_strided(torch.randn(1).to(torch.complex128), (1,), (1,))

args = (arg_0, arg_1, arg_2)
result_original = fuzzed_program(*args)
print('✅ eager success')
compiled_program = torch.compile(fuzzed_program, fullgraph=False, dynamic=True)
result_compiled = compiled_program(*args)
print('✅ compile success')
```",2025-09-24 23:08:42+00:00,2025-09-25T06:50:05Z,,False,1,0,5,29,50,2,1,,61,1526,False,False,False,False,False,False,2,0,0,17278,8615,8663,1,5,,,,pytorch
163810,closed,[PGO] ignore extra PGO key if warm/cold cache present,pianpwk,"Summary: avoids PGO profile merges

Test Plan: test_pgo

Differential Revision: D83200714




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 22:59:35+00:00,2025-09-25T07:17:10Z,,False,4,0,1,16,78,2,4,2025-09-25 07:16:08+00:00,53,264,False,False,False,False,False,False,2,1,476,94,16,78,1,1,2.0,1.0,2025-09-25T02:58:30Z,pytorch
163809,open,[export] _detect_attribute_assignment gives warning instead of raising ValueError,yiming0416,"Summary:
LSTM was not exportable with non-strict export as it failed at `_detect_attribute_assignment`

This is because the `_flat_weights` attribute in LSTM is a list of registered parameters and will be updated by the `_update_flat_weights` method in `forward`.

However, in `_detect_attribute_assignment`, we manually restore the state of the module by `mod.__dict__.update(snapshot)`. Therefore, it should be fine to turn the `ValueError` into a warning so that RNN models are exportable with non-strict export.

Added test to verify that there is no lifted tensor constant and no fake tensor leakage.

Test Plan: buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_export_rnn_variants_with_warning

Differential Revision: D83196971


",2025-09-24 22:39:33+00:00,2025-09-25T03:41:01Z,,False,4,0,1,50,5,2,4,,81,747,False,False,False,False,False,False,2,0,0,55,50,5,1,1,,,,pytorch
163808,open,[AOTI] Update AOTInductor tutorial,desertfire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163808

Summary: Remove the BC breaking warning. Add inductor_config to the example code.",2025-09-24 22:35:07+00:00,2025-09-25T00:01:36Z,,False,1,0,2,4,5,1,1,,34,175,False,False,False,False,False,False,1,0,0,9,4,5,1,2,,,,pytorch
163807,open,graph break on tolist if capture_scalar_outputs is false,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163807

address https://github.com/pytorch/pytorch/issues/163798

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 22:33:46+00:00,2025-09-25T05:32:29Z,,False,2,1,2,18,12,2,3,,56,322,False,False,False,False,False,False,2,0,0,36,21,15,1,2,1.0,0.0,2025-09-25T01:07:35Z,pytorch
163806,open,[ContextParallel][benchmark] Add sparsity benchmark over different load-balance policies,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163806
* #163617
* #163053
* #161062



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 22:33:26+00:00,2025-09-25T04:21:28Z,,False,2,0,2,469,11,6,2,,88,227,False,False,False,False,False,False,6,0,0,678,568,110,1,2,,,,pytorch
163805,closed,[BE] Introduce `CONDA_ROOT_DIR`,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162310
* #162862
* __->__ #163341
* #163339

Which equal to `%CONDA_PARENT_DIR%/Miniconda3`, and replace this pattern with `%CONDA_ROOT_DIR%` throughout the codebase",2025-09-24 22:30:42+00:00,2025-09-24T22:42:17Z,2025-09-24T22:42:17Z,True,1,0,1,6,6,3,1,2025-09-24 22:42:17+00:00,31,244,False,False,False,False,False,False,3,0,0,12,6,6,1,1,1.0,0.0,2025-09-24T22:42:09Z,pytorch
163804,closed,Move ROCM trunk wheel builds to 3.10,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162310
* #162862
* #163341
* __->__ #163339

This code is a delicious spaghetti: Sometimes python version is defined in jinja template (see https://github.com/pytorch/pytorch/pull/162297 ) sometimes in shell script (see https://github.com/pytorch/pytorch/pull/162877 ), but this time around it's in a python file (and there is another one called `generate_binary_build_matrix.py` that defines `FULL_PYTHON_VERSIONS`)

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-24 22:25:45+00:00,2025-09-24T22:41:56Z,2025-09-24T22:41:55Z,True,1,0,1,8,8,2,1,2025-09-24 22:41:56+00:00,36,613,False,False,False,False,False,False,2,0,0,16,8,8,1,1,1.0,0.0,2025-09-24T22:41:16Z,pytorch
163803,open,[Inductor] add a new config fallback_embedding_bag_byte_unpack,hl475,"Differential Revision: D82988783

introduce an inductor config fallback_embedding_bag_byte_unpack so we can have options to not let inductor decompose the op


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 22:25:31+00:00,2025-09-25T10:20:36Z,,False,7,0,1,33,0,3,7,,62,361,False,False,False,False,False,False,3,1,42,33,33,0,1,1,2.0,1.0,2025-09-24T22:31:34Z,pytorch
163802,closed,[BE] Update Python min version to 3.10 (#162310),Camyll,"Pull Request resolved: https://github.com/pytorch/pytorch/pull/162310
Approved by: https://github.com/atalman, https://github.com/Skylion007, https://github.com/ZainRizvi
ghstack dependencies: #162862

fixes https://github.com/pytorch/pytorch/issues/161167
",2025-09-24 22:24:04+00:00,2025-09-24T22:48:19Z,2025-09-24T22:48:19Z,True,1,0,1,3,4,2,1,2025-09-24 22:48:19+00:00,48,257,False,True,False,False,False,False,2,0,0,7,3,4,1,1,1.0,0.0,2025-09-24T22:40:41Z,pytorch
163800,open,[cherrypick] [CI] Move Windows build/tests to Python-3.10 #162862,Camyll,"cherry pick of #162862 
for milestone: https://github.com/pytorch/pytorch/issues/161167 ",2025-09-24 22:09:11+00:00,2025-09-25T14:41:43Z,,False,1,0,1,33,12,8,1,,65,88,False,False,False,False,False,False,8,0,0,45,33,12,1,1,1.0,0.0,2025-09-25T14:41:43Z,pytorch
163799,closed,[PGO] distinguish sticky PGO put,pianpwk,"Summary: put_remote_code_state vs. put_extra_remote_code_state

Test Plan: test_pgo

Differential Revision: D83195687




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 22:06:49+00:00,2025-09-25T07:00:30Z,,False,5,0,1,10,5,1,5,2025-09-25 06:59:27+00:00,32,292,False,False,False,False,False,False,1,1,476,15,10,5,1,1,2.0,1.0,2025-09-25T02:58:12Z,pytorch
163797,closed,[CI] Install libuv for Win testing,malfet,"Current working theory why https://hud.pytorch.org/pytorch/pytorch/commit/f0078941cf4e9cfa1a464d0d12d999926fdd8cc5 caused a regression, are because Windows CI no longer could be build with distributed, as it could not find libuv",2025-09-24 21:45:01+00:00,2025-09-25T01:11:19Z,,False,3,0,1,1,1,1,3,2025-09-25 01:10:17+00:00,34,228,False,False,False,False,False,False,1,2,788,2,1,1,1,1,3.0,2.0,2025-09-24T21:45:34Z,pytorch
163796,open,"[dynamo, 3.14] fix stack ref copy error",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* __->__ #163796
* #163292
* #163191
* #163110
* #163109
* #163009
* #161839
* #161555
* #161838

",2025-09-24 21:16:10+00:00,2025-09-25T01:51:26Z,,False,2,0,1,13,13,1,2,,39,184,False,True,False,False,False,False,1,0,0,26,13,13,1,1,,,,pytorch
163795,open,Add H100 runner to be recognized in actionlint,janeyx99,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163782
* __->__ #163795

",2025-09-24 20:46:31+00:00,2025-09-24T22:32:58Z,,False,1,4,1,1,0,1,5,,46,104,False,False,False,False,False,False,1,0,48,1,1,0,1,1,2.0,1.0,2025-09-24T20:50:33Z,pytorch
163794,open,[doc] Add AOTInductor intermediate debug printer OSS user manual,YUNQIUGUO,"Summary: Add a OSS user manual for AOTI intermediate debug printer so we can link it in the Pytorch conference poster.

Test Plan: N/A

Differential Revision: D83171374


cc @svekars @sekyondaMeta @AlannaBurke",2025-09-24 20:40:24+00:00,2025-09-24T23:46:21Z,,False,7,2,1,79,0,3,9,,64,209,False,True,False,True,False,False,3,0,0,79,79,0,1,1,,,,pytorch
163793,closed,fix backend registration link in torch.compile code block,msaroufim,"Fixes https://github.com/pytorch/pytorch/issues/119272

cc @mlazos",2025-09-24 20:34:36+00:00,2025-09-24T20:52:49Z,,False,1,0,1,1,1,1,1,2025-09-24 20:52:49+00:00,57,66,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
163792,open,[aoti] Save compute information,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163792
* #163779

Metadata looks like:
```
{
  'AOTI_DEVICE_KEY': 'cpu',
  'AOTI_PLATFORM': 'linux',
  'AOTI_MACHINE': 'x86_64',
  'AOTI_CPU_ISA': 'AVX512',
  'AOTI_COMPUTE_CAPABILITY': '90'
}
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 20:34:35+00:00,2025-09-25T02:23:55Z,,False,1,1,2,75,12,3,2,,31,485,False,False,False,False,False,False,3,0,29,87,75,12,1,2,3.0,1.0,2025-09-24T20:36:01Z,pytorch
163790,closed,[CD] Simplify NVIDIA driver installation step (#163349),atalman,"Undo changes introduced in https://github.com/pytorch/pytorch/pull/160956 as driver has been updated to 580 for both fleets

Fixes https://github.com/pytorch/pytorch/issues/163342 Pull Request resolved: https://github.com/pytorch/pytorch/pull/163349 Approved by: https://github.com/seemethere

",2025-09-24 20:29:19+00:00,2025-09-25T14:40:58Z,2025-09-25T14:40:57Z,True,1,0,1,0,2,1,1,2025-09-25 14:40:57+00:00,55,294,False,True,False,False,False,False,1,0,0,2,0,2,1,1,1.0,0.0,2025-09-24T22:48:59Z,pytorch
163788,closed,Use cuda nvrtc so file based on cuda version used by torch (#163642),atalman,"Fixes https://github.com/pytorch/pytorch/issues/162367

Pull Request resolved: https://github.com/pytorch/pytorch/pull/163642
Approved by: https://github.com/msaroufim
",2025-09-24 20:24:30+00:00,2025-09-25T14:40:09Z,2025-09-25T14:40:09Z,True,1,0,1,14,4,1,1,2025-09-25 14:40:09+00:00,68,168,False,True,False,False,False,False,1,0,0,18,14,4,1,1,1.0,0.0,2025-09-24T20:28:32Z,pytorch
163787,closed,[CI] Run CUDA-13 binary builds on trunk,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163787

There are numerous other workflows that could be used to catch CUDA-12
build regression (our CI builds are almost identical to CD ones), but not many CUDA-13 builds around, so https://github.com/pytorch/pytorch/issues/163342 are really hard to detect in CI",2025-09-24 20:06:42+00:00,2025-09-25T00:59:22Z,,False,3,0,1,13,13,2,3,2025-09-25 00:58:19+00:00,39,350,False,False,False,False,False,False,2,2,623,26,13,13,1,1,4.0,3.0,2025-09-24T20:17:15Z,pytorch
163784,closed,[DO NOT LAND] Make _flat_weights in RNN ParameterList,yiming0416,"Summary: Removed `_update_flat_weights()` in the `forward`. Let's see what CI says.

Test Plan: buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_lstm_export

Differential Revision: D83170973


",2025-09-24 19:14:09+00:00,2025-09-25T13:27:04Z,,False,4,0,1,30,23,2,4,2025-09-24 22:15:00+00:00,53,204,False,False,False,False,False,False,2,2,352,53,30,23,1,1,2.0,3.0,2025-09-24T19:50:54Z,pytorch
163783,closed,[ROCm][CI] adjust tf32 tolerance for test_compile_kernel_advanced,ethanwee1,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-24 18:57:49+00:00,2025-09-24T19:40:20Z,,False,3,0,1,1,1,1,3,2025-09-24 19:39:18+00:00,65,138,False,True,False,False,False,False,1,2,811,2,1,1,1,1,2.0,2.0,2025-09-24T18:58:20Z,pytorch
163782,open,Add smoke tests to verify that stable ABI FA3 wheel runs w/ newer torch,janeyx99,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163782
* #163795

",2025-09-24 18:34:48+00:00,2025-09-24T22:33:04Z,,False,1,1,4,274,0,2,2,,71,104,False,False,False,False,False,False,2,0,0,383,329,54,1,4,1.0,0.0,2025-09-24T19:17:35Z,pytorch
163781,open,[Triton] [Inductor] Prune template selection based on decompose_k,njriasan,"Summary:

Triton templates tend to perform very poorly on large K, hence the introduction of decompose_k. As a result, when decompose_k is selected will disable exploring the Triton templates. We may want to consider an override in the future. 

Note: Based on the timing results it may be desirable to better refine/prune the decompose k decisions.

Testing:

Tested by looking at the autotune/compilation time using a single shape in TritonBench.
`TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 ENABLE_PERSISTENT_TMA_MATMUL=1 python run --op gemm --rep 1000 --sleep 1.0 --m 512 --n 512 --k 300000 --only pt2_matmul_maxautotune`
Before this change:
`SingleProcess AUTOTUNE benchmarking takes 13.5368 seconds and 0.1595 seconds precompiling for 38 choices`
With this change:
`SingleProcess AUTOTUNE benchmarking takes 9.9626 seconds and 0.0020 seconds precompiling for 11 choices`



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-24 18:30:18+00:00,2025-09-25T14:10:47Z,,False,7,0,4,24,12,3,7,,65,1083,False,False,False,False,False,False,3,6,1849,46,29,17,1,4,3.0,8.0,2025-09-24T18:34:34Z,pytorch
163780,open,[PyTorch Pinned Allocator] Add support of reserved pinned memory segment,banitag1,"Summary:
This diff adds the feature of allocating a large pinned memory segment upfront based on the provided config. This large segment is then used to serve all the small pinned memory requests to avoid expensive device level APIs (slow paths).

Example:

PYTORCH_CUDA_ALLOC_CONF=pinned_reserve_segment_size_mb:2048

This reserves a 2GB pinned memory segment for the process and then all incoming small requests are just served from this segment and no cudaHostAlloc/cudaHostRegister apis are being called.

Differential Revision: D83169722
",2025-09-24 18:05:46+00:00,2025-09-24T20:20:55Z,,False,3,0,3,181,32,10,3,,72,543,False,False,True,False,False,False,10,0,0,213,181,32,1,3,,,,pytorch
163779,open,[aoti] Load metadata w/o loading package,angelayi,"Add a function to load the metadata stored in aoti without needing to load the .so. This can be used to store what platform we are compiling the .so on which we can check before loading the .so.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163792
* __->__ #163779



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 18:02:31+00:00,2025-09-25T02:20:16Z,,False,4,1,1,148,9,4,5,,40,503,False,False,False,False,False,False,4,3,1454,157,148,9,1,1,5.0,3.0,2025-09-24T18:19:29Z,pytorch
163778,closed,Remove Python 3.9 for Triton builds,atalman,"Related to https://github.com/pytorch/pytorch/issues/161167
",2025-09-24 18:00:45+00:00,2025-09-24T20:20:49Z,,False,3,0,1,2,5,1,3,2025-09-24 20:19:47+00:00,35,60,False,False,False,False,False,False,1,2,798,7,2,5,1,1,3.0,2.0,2025-09-24T18:22:42Z,pytorch
163777,open,[PyTorch Pinned Allocator] Pinned memory stats and perf fixes around allocating blocks,banitag1,"Summary: This diff adds bucket stats for pinned memory and also a perf fix to not check for sizes when background thread is enabled

Differential Revision: D83162186


",2025-09-24 18:00:10+00:00,2025-09-24T20:01:12Z,,False,3,0,1,20,28,1,3,,86,168,False,True,False,False,False,False,1,0,0,48,20,28,1,1,,,,pytorch
163776,closed,[ROCm] Increase binary build timeout to 5 hours (300 minutes),jithunnair-amd,"Despite narrowing down the [FBGEMM_GENAI build to gfx942](https://github.com/pytorch/pytorch/pull/162648), the nightly builds still timed out because they [didn't get enough time to finish the post-PyTorch-build steps](https://github.com/pytorch/pytorch/actions/runs/17969771026/job/51109432897).

This PR increases timeout for ROCm builds for both [libtorch ](https://github.com/pytorch/pytorch/actions/runs/17969771026)and [manywheel](https://github.com/pytorch/pytorch/actions/runs/17969771041), because both of those are close to the 4hr mark currently.

This PR is a more ROCm-targeted version of https://github.com/pytorch/pytorch/pull/162880 (which is for release/2.9 branch).

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-24 17:56:39+00:00,2025-09-24T23:03:14Z,,False,3,0,2,20,0,4,3,2025-09-24 23:02:12+00:00,61,785,False,False,False,False,False,False,4,2,854,55852,39117,16735,2,2,2.0,2.0,2025-09-24T18:20:41Z,pytorch
163775,closed,Update pyrefly configuration file,maggiemoss,"Related to: https://github.com/pytorch/pytorch/issues/163283

This simply updates the existing pyrefly configuration and opts out additional directories. Running `pyrefly check` with this setup will result in ~100 errors reported. 
",2025-09-24 17:54:32+00:00,2025-09-24T23:15:45Z,,False,5,0,1,67,16,1,5,2025-09-24 23:14:42+00:00,33,232,False,False,False,False,False,False,1,3,501,83,67,16,1,1,4.0,3.0,2025-09-24T18:41:30Z,pytorch
163774,open,Add fused linear cross entropy CPU implementation,krocki,"  Summary:
  - add a fused CPU implementation of linear cross entropy with vocabulary/batch chunking so large logits never fully materialize
  - expose torch.nn.functional.linear_cross_entropy; the CPU path uses the fused kernel while other devices fall back to linear + cross_entropy (keeping behavior identical)
  - add unit tests covering forward/backward parity (auto/vocab/batch), reductions, ignore_index, label smoothing, and gradcheck

  Addresses: #124480
  GPU/Triton support will follow in a separate PR; please note this is the first installment toward #124480.

  Release Notes:
  - [New Feature] torch.nn.functional.linear_cross_entropy now provides a fused CPU implementation with chunking heuristics to avoid materializing large logit tensors.

  Test Plan:
  - python test/nn/test_linear_cross_entropy.py -v
  
  Expected output:

```
  test_batch_chunking (main.TestLinearCrossEntropyCPU.test_batch_chunking) ... ok
  test_forward_backward_matches_reference_auto (main.TestLinearCrossEntropyCPU.test_forward_backward_matches_reference_auto) ... ok
  test_gradcheck (main.TestLinearCrossEntropyCPU.test_gradcheck) ... ok
  test_parameter_validation (main.TestLinearCrossEntropyCPU.test_parameter_validation) ... ok
  test_reduction_and_options (main.TestLinearCrossEntropyCPU.test_reduction_and_options) ... ok
  test_vocab_chunking (main.TestLinearCrossEntropyCPU.test_vocab_chunking) ... ok
```
",2025-09-24 17:42:24+00:00,2025-09-25T01:07:48Z,,False,4,0,3,1008,0,5,4,,49,1414,False,False,True,False,False,False,5,1,601,1250,1129,121,1,2,1.0,1.0,2025-09-24T20:00:30Z,pytorch
163773,open,Avoid fast path mask left-align check in compiled TransformerEncoder,jbschlosser,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163773

Fixes #163640

This PR avoids a mask left align check in the case that we're operating under torch.compile / torch.export. Originally, I planned to make a more invasive change to auto-disable the fast path entirely underneath torch.compile / torch.export, but I realized during testing that the fast path wasn't actually causing compile issues outside of the narrow issue identified here.",2025-09-24 17:23:15+00:00,2025-09-24T21:30:01Z,,False,1,2,1,28,0,2,3,,68,482,False,True,False,False,False,False,2,0,36,28,28,0,1,1,2.0,1.0,2025-09-24T20:51:28Z,pytorch
163772,open,Support of AllPermute for redistribution,zpcore,"***Not Ready For Review!***
***Not Ready For Review!***
***Not Ready For Review!***

### Summary
Introduce the `AllPermute` collective operation as mentioned in https://arxiv.org/pdf/2112.01075 ""section 2.6 Collective operations"".  

### What is AllPermute?
AllPermute can transform any 𝜏1 to 𝜏2 if their local and global shapes match. For example:
Given mesh and size {X:4, Y:4, Z:16}, we have
- example 1: [32{X,Y}}512, 128] -> [32{Y,X}512, 128]
- example 2: [128{Y}512, 32{X}128] -> [128{X}512, 32{Y}128]
- example 3: [32{X,Y}512, 128] -> [32{Z}512, 128]
Note: annotation borrowed from https://arxiv.org/pdf/2112.01075 ""section 2.1 Distributed array types""

### Why we need AllPermute?
With AllPermute, we can eliminate some AllGather ops during redistribution. This plays an important role in reducing the memory overhead. In theory, at most one AllPermute is needed to redistribute from any 𝜏1 to 𝜏2. The `AllPermute` can be performed as the final step, or moved before the last `AllGather` to minimize the amount of data relocated between shards in the `AllPermute`.


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #163772
* #162294
* #160903
* #160266



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 17:08:22+00:00,2025-09-24T20:21:22Z,,False,2,0,2,484,161,4,2,,40,1303,False,False,False,False,False,False,4,0,0,38626,25082,13544,1,2,,,,pytorch
163771,open,[PyTorch CCA] Add an API to get expandable segment sizes,banitag1,"Summary: This diffs add an API to query expandable segment size for each stream so that we can use this info to warmup the segment in advance, so we dont incur any performance penalty during steady state inference for new CUDA memory allocations.

Differential Revision: D76447308


",2025-09-24 16:38:16+00:00,2025-09-24T18:50:17Z,,False,3,0,1,65,0,5,3,,56,283,False,False,False,False,False,False,5,0,0,65,65,0,1,1,,,,pytorch
163770,open,Fix various bugs in subclass input in export,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163770

Differential Revision: [D83156489](https://our.internmc.facebook.com/intern/diff/D83156489)",2025-09-24 15:37:33+00:00,2025-09-24T18:37:37Z,,False,2,9,1,91,13,3,11,,44,185,False,True,False,False,False,False,3,1,244,104,91,13,1,1,3.0,2.0,2025-09-24T15:38:34Z,pytorch
163769,closed,"Reapply ""Make functionalization `ViewMeta` serializable with pickle. (#143712)"" ",Lucaskabela,"### Summary:
NOTE: This is a re-export of https://github.com/pytorch/pytorch/pull/161994 ; the changes between these two PRs is exclusively to the buck/build files 

(Summary from #161994 )
Attempted rebase of https://github.com/pytorch/pytorch/pull/143712. 

This reverts commit 6c713ccb5e0df227dd5b630057cbccd373cbe7d6.

cc voznesenskym penguinwu EikanWang jgong5 Guobing-Chen XiaobingSuper zhuhaozhe blzheng wenzhe-nrv jiayisunx chenyang78 kadeng chauhang amjames Lucaskabela

imported-using-ghimport

Test Plan: Imported from OSS

Differential Revision: D81524507

Pulled By: Lucaskabela


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",2025-09-24 15:23:22+00:00,2025-09-25T10:28:45Z,,False,10,0,1,979,423,38,10,2025-09-25 10:27:42+00:00,80,751,False,False,False,False,False,False,38,2,956,1402,979,423,1,1,3.0,3.0,2025-09-24T15:28:19Z,pytorch
163768,closed,[CD] Add statically linked windows libraries to exclude list,atalman,"Fixes: https://github.com/pytorch/pytorch/issues/159514

Seeing following in the Wheel build logs:
```
Linking CXX static library lib\kineto.lib
Linking CXX static library lib\dnnl.lib
....
```

These files are around 800MB uncompressed and 109MB compressed, hence provide ~50% size reduction for Windows CPU builds.

Test Plan: Build Pytorch Windows binary. Build vision, audio and torchcodec with this binary. Smoke test.",2025-09-24 15:05:01+00:00,2025-09-25T14:04:23Z,,False,5,1,4,12,1,1,6,2025-09-25 14:03:20+00:00,60,423,False,True,False,False,False,False,1,4,1192,23,17,6,1,4,4.0,5.0,2025-09-24T15:31:26Z,pytorch
163767,open,IGNORE: Testing build times,zxiiro,"Fixes #ISSUE_NUMBER
",2025-09-24 15:00:29+00:00,2025-09-25T14:47:45Z,,False,1,0,1,85,85,1,1,,27,20,False,True,False,False,False,False,1,0,0,170,85,85,1,1,,,,pytorch
163766,closed,[CD] CUDA 13.0 fix preload logic to include nvidia/cu13/lib/,pytorchbot,"Preload logic no longer works with CUDA 13.0
See the installation path:
```
ls /home/ubuntu/.venv/lib/python3.10/site-packages/nvidia/cu13/lib/
libcheckpoint.so   libcudadevrt.a      libcufft.so.12   libcufile_rdma.so.1  libcusolver.so.12    libnvJitLink.so.13  libnvperf_target.so            libnvrtc.alt.so.13    libpcsamplingutil.so
libcublas.so.13    libcudart.so.13     libcufftw.so.12  libcupti.so.13       libcusolverMg.so.12  libnvblas.so.13     libnvrtc-builtins.alt.so.13.0  libnvrtc.so.13
libcublasLt.so.13  libcudart_static.a  libcufile.so.0   libcurand.so.10      libcusparse.so.12    libnvperf_host.so   libnvrtc-builtins.so.13.0      libnvtx3interop.so.1

ls /home/ubuntu/.venv/lib/python3.10/site-packages/nvidia/
cu13  cudnn  cusparselt  nccl  nvshmem
```

Test using script from : https://github.com/pytorch/pytorch/issues/162367
```
Kernel test passed!
```",2025-09-24 14:52:58+00:00,2025-09-25T14:38:16Z,2025-09-25T14:38:16Z,True,1,0,1,15,4,1,1,2025-09-25 14:38:16+00:00,60,875,False,True,False,False,False,False,1,0,4,19,15,4,1,1,1.0,1.0,2025-09-24T18:24:08Z,pytorch
163764,closed,[Cherry-Pick] [CD] CUDA 13 specific followup changes. Remove sm50-70 From CUDA 12.6 and CUDA 12.8 builds (#162455),atalman,"Follow up for CUDA 13 bring up https://github.com/pytorch/pytorch/issues/159779 sm50-70 should not be added to sbsa build arch list, as previous archs had no support for arm. remove platform_machine from PYTORCH_EXTRA_INSTALL_REQUIREMENTS

Pull Request resolved: https://github.com/pytorch/pytorch/pull/162455
Approved by: https://github.com/atalman
",2025-09-24 14:43:18+00:00,2025-09-25T14:37:52Z,2025-09-25T14:37:52Z,True,2,2,2,95,95,6,4,2025-09-25 14:37:52+00:00,114,350,False,False,False,False,False,False,6,0,0,252,126,126,2,2,3.0,0.0,2025-09-24T17:48:59Z,pytorch
163763,open,Fix THP_PyObject_VirtualFree return type,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163763

# Motivation
`void THP_PyObject_VirtualFree` should have no return value; otherwise, it would raise a build warning 
```bash
C:\Users\guangyey\pytorch\torch\csrc\dynamo\cpython_defs.c(264): warning C4098: 'THP_PyObject_VirtualFree': 'void' function returning a value
```
# Additional Context
Refer to
https://github.com/python/cpython/blob/c4f21d7c7c415a85a975fb878a1e578c12969d82/Include/cpython/objimpl.h#L59-L68
PyObjectArenaAllocator::free is defined with `void` return type.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 14:39:09+00:00,2025-09-25T15:03:27Z,,False,5,0,1,1,1,1,5,,40,745,False,True,False,False,False,False,1,4,977,2,1,1,1,1,5.0,6.0,2025-09-24T15:04:25Z,pytorch
163762,closed,Pass through CMake compiler/linker args to setup.py,directhex,"Presumably if the arguments were worth giving to CMake, they were worth giving to setup.py's compile calls too. For example, if your specific environment requires use of a `-fuse-ld`  flag or `--target=` flag.",2025-09-24 13:53:50+00:00,2025-09-24T22:23:58Z,,False,4,0,1,4,0,1,4,2025-09-24 22:23:58+00:00,51,209,False,False,False,False,False,False,1,3,397,4,4,0,1,1,2.0,3.0,2025-09-24T13:54:42Z,pytorch
163760,open,[TESTING] Increased the padding bench multiplier to make it more strict,iupaikov-amd,"Increased the padding bench multiplier to make it more strict. This can help with rank variation using pattern matcher.

This is a testing PR, no need to merge, we will see if this helps with performance on our side.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 13:35:56+00:00,2025-09-24T18:38:27Z,,False,2,0,2,6,5,2,2,,71,420,False,False,False,False,False,False,2,0,0,11,6,5,1,2,,,,pytorch
163758,open,Update torch-xpu-ops commit pin,CuiYifeng,"Update the torch-xpu-ops commit to [intel/torch-xpu-ops@229e8b](https://github.com/intel/torch-xpu-ops/commit/229e8ba104e3b0a6e88c36effefa3ea914b25673), includes:

- Revert tracking of Work status for FlightRecorder in ProcessGroupXCCL to fix memory leak
- Enable SYCL warnings on Linux
- Fix accuracy issues with CTC loss
- Enable aten::nonzero_static on XPU backend
- Stop recursive calculations in polynomial kernels if tensor has NaNs",2025-09-24 12:52:05+00:00,2025-09-25T10:00:52Z,,False,1,0,1,1,1,1,1,,31,438,False,True,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-24T20:19:02Z,pytorch
163757,open,Better error handling in torch/csrc/jit/ir/*,licy666,"Refactor error handling to use TORCH_CHECK for improved clarity in constants and scope management

Fixes some parts of ISSUE #148114



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-24 12:29:41+00:00,2025-09-25T14:20:40Z,,False,4,0,1,9,18,3,4,,44,183,False,True,False,False,True,True,3,3,176,27,9,18,1,1,1.0,3.0,2025-09-24T12:30:07Z,pytorch
163755,open,[inductor] Improve bound on the number of dims to match for the block,kundaMwiza,"- Removes redundant broadcast code when `len(kernel.range_tree_nodes)` is much larger than `len(range_tree.nodes)`. For example:
```python
# before, the broadcast is to [1, 1, XBLOCK, R0_BLOCK]
tmp0 = tl.reshape(tl.broadcast_to(tl.load(block_ptr0, boundary_check=[2], padding_option='zero', eviction_policy='evict_last')[:, None, :, :], [(511 + XBLOCK) // 512, ((1) * ((1) <= ((511 + XBLOCK) // 512)) + ((511 + XBLOCK) // 512) * (((511 + XBLOCK) // 512) < (1))), ((512) * ((512) <= (XBLOCK)) + (XBLOCK) * ((XBLOCK) < (512))), R0_BLOCK]), [XBLOCK, R0_BLOCK])
# after
tmp0 = tl.reshape(tl.load(block_ptr0, boundary_check=[2], padding_option='zero', eviction_policy='evict_last'), [XBLOCK, R0_BLOCK])
```
- Fix: also save range_tree_nodes per subgraph 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 12:13:24+00:00,2025-09-24T17:45:49Z,,False,3,0,2,12,4,3,3,,69,952,False,True,False,False,True,False,3,2,98,16,12,4,1,2,1.0,2.0,2025-09-24T12:55:14Z,pytorch
163754,open,refactor bucketing,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163754
* #163215

Preparatory refactory

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 12:02:08+00:00,2025-09-24T14:33:53Z,,False,1,0,1,220,166,1,1,,18,328,False,False,False,False,False,True,1,0,0,386,220,166,1,1,1.0,0.0,2025-09-24T13:17:26Z,pytorch
163752,open,[inductor][templates] Distinguish between kernel input nodes and codegen input nodes,kundaMwiza,"If there is a single autotuner choice, the wrong type of input node is used to instantiate `TritonTemplateBuffer` through `TritonTemplateCaller.output_node`. This PR distinguishes the input nodes used in `AlgorithmSelectorCache.__call__` between the actual inputs passed to the kernel at runtime, vs the possibly viewed inputs that influence scheduling behaviour (e.g. `MemoryDeps`) and codegen. See the added unit test for more detail.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 11:17:04+00:00,2025-09-24T14:01:00Z,,False,3,1,1,75,4,2,4,,84,640,False,False,False,False,False,False,2,2,154,79,75,4,1,1,1.0,2.0,2025-09-24T11:18:04Z,pytorch
163751,closed,OpInfo(addmm): more tests for the Lt interface,nikitaved,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163751

",2025-09-24 10:33:49+00:00,2025-09-24T10:35:04Z,,False,2,0,1,2,0,1,2,2025-09-24 10:34:39+00:00,46,106,False,False,False,False,False,False,1,0,0,2,2,0,1,1,,,,pytorch
163750,open,Releases multicast object before releasing mapped buffers in CUDASymmetricMemory,syed-ahmed,"Fixes: https://github.com/pytorch/pytorch/issues/162429. In B200, cuMulticastUnbind can error if the mapped buffers are free'd before the multicast object is free'd. The only documentation I could find is here: https://github.com/NVIDIA/nccl/blob/e11d7f77c126561e35909407a5bd1461a437322b/src/transport/nvls.cc#L113.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163750
* #163575

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 08:07:02+00:00,2025-09-24T17:13:52Z,,False,3,0,1,1,1,1,3,,80,522,False,True,False,True,False,False,1,2,14403,2,1,1,1,1,2.0,3.0,2025-09-24T08:26:54Z,pytorch
163749,open,[dynamo] [fix] fix weakref proxy in dynamo symbolic covert when triggering a graph break ,shaoyuyoung,"Similar to #161508
The repro can be found in my test (so I didn't draft an issue).
```python
import torch
import weakref
import torch._dynamo as dynamo
from torch._dynamo.symbolic_convert import code_context


class Dummy:
    pass


def target_fn(x: torch.Tensor) -> torch.Tensor:
    dynamo.graph_break()
    return x + 1



dummy = Dummy()
proxy = weakref.proxy(dummy)
code_context.get_context(target_fn.__code__)[""orig_graphmodule""] = (lambda: proxy)
f_compiled = torch.compile(target_fn)
y = f_compiled(torch.tensor(1))
torch.testing.assert_close(y, torch.tensor(2))
```
err log
```
File ""/root/ysy/pytorch/torch/_dynamo/symbolic_convert.py"", line 2708, in create_call_resume_at
  code_context.get_context(new_code)[""orig_graphmodule""] = weakref.ref(
                                                           ^^^^^^^^^^^^
torch._dynamo.exc.InternalTorchDynamoError: TypeError: cannot create weak reference to 'weakref.ProxyType' object
```

I think we should add a check for `context[""orig_graphmodule""]` to avoid repeatedly creating weak references. Because this will throw the above `TypeError:`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 08:06:07+00:00,2025-09-24T08:37:13Z,,False,2,2,3,20,1,2,4,,89,1275,False,True,False,False,False,False,2,1,42,23,21,2,1,3,2.0,1.0,2025-09-24T08:08:17Z,pytorch
163748,open,[Tools] Adapting the Hypothesis library (version 5.x) for use with the PyTorch framework,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163748

Starting from version 5.x, the Hypothesis library removed the timeout setting and only retained the deadline.",2025-09-24 07:29:16+00:00,2025-09-25T15:01:15Z,,False,6,2,2,18,6,1,8,,88,215,False,False,False,False,False,False,1,5,1006,6266,764,5502,1,2,4.0,6.0,2025-09-24T07:32:36Z,pytorch
163747,open,[Inductor] Check if profiling before using record_function in CompiledFxGraph,azahed98,"The call to `record_function` adds overhead even if profiling is disabled, which can as much as double the total runtime overhead of a compiled function. #163566 aims to make `record_function` more efficient, but doesn't fully eliminate overhead. This change adds a check if profiling is active before using `record_function`, which avoids this issue all together.

`TestExecutionTrace.test_execution_trace_with_pt2` in https://github.com/pytorch/pytorch/blob/main/test/profiler/test_execution_trace.py#L372 already checks that the `record_function` region is tracked during profiling.

Comparison of the `benchmarks/dynamo/microbenchmarks/overheads.py ` results:

Before Change:
```
requires_grad=False
compiled 56.9us (warmup=10.7s)

requires_grad=True
compiled 99.4us (warmup=0.2s)

inference_mode()
compiled 55.7us (warmup=0.1s)
```

After Change:
```
requires_grad=False
eager    6.9us (warmup=0.0s)
compiled 23.9us (warmup=22.3s)

requires_grad=True
eager    8.7us (warmup=0.0s)
compiled 56.8us (warmup=0.1s)

inference_mode()
eager    6.3us (warmup=0.0s)
compiled 22.2us (warmup=0.1s)
```

TODO: Add runtime overhead benchmark to the CI?

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-24 06:57:21+00:00,2025-09-25T07:52:43Z,,False,1,0,2,7,3,1,1,,77,1355,False,False,False,False,False,False,1,0,59,10,7,3,1,2,1.0,1.0,2025-09-25T07:52:43Z,pytorch
163746,open,fixes import error 'functionalize' from functorch,RiyaP-QA,"Fixes #163637 


cc @zou3519 @Chillee @samdow @kshitij12345",2025-09-24 06:34:00+00:00,2025-09-24T16:58:21Z,,False,3,0,1,1,1,1,3,,49,59,False,True,False,False,False,False,1,2,79,2,1,1,1,1,1.0,2.0,2025-09-24T06:34:58Z,pytorch
163745,open,[ROCm] Transformer/SDPA unit test parity,xinyazhang,"## Major Changes

* Efficient Attention on ROCM requires last dimensions of input tensors align with 16 bytes.
  - Unlike FA, ME does not pad input tensors in `scaled_dot_product_attention` and hence this is required. 
* Fix `atomic_counter` handling in varlen FA API
* Unskips a few unit tests.

Fixes #157120
Fixes #157121
Fixes #157122
Fixes #157167
Fixes #155217
Fixes #157043
Fixes #157060


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-24 05:34:57+00:00,2025-09-25T11:19:41Z,,False,1,0,14,29,30,6,1,,40,513,False,True,False,False,False,False,6,0,0,9252,2790,6462,1,14,1.0,0.0,2025-09-24T22:14:34Z,pytorch
163744,open,Update ruff to 0.13.1,cyyever,"Update ruff to 0.13.1 so that we can remove `UP038` from `pyproject.toml` because it has been removed from supported rules of ruff.
There are some fixes, the most notable one is [(PYI059)](https://docs.astral.sh/ruff/rules/generic-not-last-base-class/#generic-not-last-base-class-pyi059)
```
Checks for classes inheriting from typing.Generic[] where Generic[] is not the last base class in the bases tuple.

```

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 05:07:31+00:00,2025-09-24T11:19:33Z,,False,4,0,1,12,13,8,4,,21,622,False,True,False,True,False,False,8,3,896,25,12,13,1,1,4.0,3.0,2025-09-24T05:49:21Z,pytorch
163743,open,[torchfuzz] simplify codegen and runner,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163812
* __->__ #163743

much less code. a followup PR will make these repro files even smaller.
small is important since it reduces the time for users to understand
what the repro is doing. here's a sample:

```
(/home/bobren/local/a/pytorch-env) [21:34] devgpu009:/home/bobren/local/a/pytorch/tools/experimental/dynamic_shapes/torchfuzz [130] python fuzzer.py --seed 42
Running single fuzz_and_execute...
Using seed: 42, max_depth: 10
Running generated program...
Selected CUDA_VISIBLE_DEVICES=2
=== Program Output ===
✅ eager success
✅ compile success

===============================
=== Program Source ===
import torch
import sys
import os
fuzzer_dir = r'/home/bobren/local/a/pytorch/tools/experimental/dynamic_shapes/torchfuzz'
if fuzzer_dir not in sys.path:
    sys.path.insert(0, fuzzer_dir)
from tensor_fuzzer import fuzz_scalar, fuzz_tensor_simple, ScalarSpec, TensorSpec

def fuzzed_program(arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, arg_11, arg_12, arg_13, arg_14, arg_15, arg_16, arg_17, arg_18, arg_19, arg_20, arg_21, arg_22, arg_23, arg_24, arg_25, arg_26):
    # Node node_4: arg (depth 6)
    var_node_4 = arg_0 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_7: constant (depth 4)
    var_node_7 = torch.full((1,), (-0.8353595860703585-0.8384634248041143j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_8: arg (depth 4)
    var_node_8 = arg_1 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_6: tensor_pointwise (depth 5)
    var_node_6 = torch.ops.aten.mul(var_node_7, var_node_8) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_9: constant (depth 5)
    var_node_9 = torch.full((1,), (-0.32478860712861235+0.033909682598544454j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_5: tensor_pointwise (depth 6)
    var_node_5 = torch.ops.aten.mul(var_node_6, var_node_9) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_3: tensor_pointwise (depth 7)
    var_node_3 = torch.ops.aten.sub(var_node_4, var_node_5) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_11: arg (depth 6)
    var_node_11 = arg_2 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_18: constant (depth 0)
    var_node_18 = torch.full((1,), (0.12855308616305575+1.5268033634325642j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_19: arg (depth 0)
    var_node_19 = arg_3 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_17: tensor_pointwise (depth 1)
    var_node_17 = torch.ops.aten.mul(var_node_18, var_node_19) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_21: arg (depth 0)
    var_node_21 = arg_4 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_22: arg (depth 0)
    var_node_22 = arg_5 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_20: tensor_pointwise (depth 1)
    var_node_20 = torch.ops.aten.sub(var_node_21, var_node_22) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_16: tensor_pointwise (depth 2)
    var_node_16 = torch.ops.aten.add(var_node_17, var_node_20) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_25: arg (depth 0)
    var_node_25 = arg_6 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_26: arg (depth 0)
    var_node_26 = arg_7 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_24: tensor_pointwise (depth 1)
    var_node_24 = torch.ops.aten.add(var_node_25, var_node_26) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_27: constant (depth 1)
    var_node_27 = torch.full((1,), (-0.6315711191260084+1.342004076501214j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_23: tensor_pointwise (depth 2)
    var_node_23 = torch.ops.aten.mul(var_node_24, var_node_27) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_15: tensor_pointwise (depth 3)
    var_node_15 = torch.ops.aten.mul(var_node_16, var_node_23) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_28: constant (depth 3)
    var_node_28 = torch.full((1,), (1.064498531874825-0.37289464356501284j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_14: tensor_pointwise (depth 4)
    var_node_14 = torch.ops.aten.mul(var_node_15, var_node_28) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_30: arg (depth 3)
    var_node_30 = arg_8 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_32: arg (depth 2)
    var_node_32 = arg_9 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_33: constant (depth 2)
    var_node_33 = torch.full((1,), (1.5815627438573372+0.5124667911691704j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_31: tensor_pointwise (depth 3)
    var_node_31 = torch.ops.aten.div(var_node_32, var_node_33) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_29: tensor_pointwise (depth 4)
    var_node_29 = torch.ops.aten.div(var_node_30, var_node_31) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_13: tensor_pointwise (depth 5)
    var_node_13 = torch.ops.aten.div(var_node_14, var_node_29) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_39: arg (depth 0)
    var_node_39 = arg_10 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_40: constant (depth 0)
    var_node_40 = torch.full((1,), (-0.5987350493494642-0.5711360569376475j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_38: tensor_pointwise (depth 1)
    var_node_38 = torch.ops.aten.mul(var_node_39, var_node_40) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_41: arg (depth 1)
    var_node_41 = arg_11 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_37: tensor_pointwise (depth 2)
    var_node_37 = torch.ops.aten.add(var_node_38, var_node_41) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_42: constant (depth 2)
    var_node_42 = torch.full((1,), (0.7246044564672116-0.5930730980273312j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_36: tensor_pointwise (depth 3)
    var_node_36 = torch.ops.aten.mul(var_node_37, var_node_42) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_43: constant (depth 3)
    var_node_43 = torch.full((1,), (-0.7582976293117148+1.1880929376258396j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_35: tensor_pointwise (depth 4)
    var_node_35 = torch.ops.aten.mul(var_node_36, var_node_43) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_45: constant (depth 3)
    var_node_45 = torch.full((1,), (1.0896212896322774+0.3124038130417098j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_46: arg (depth 3)
    var_node_46 = arg_12 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_44: tensor_pointwise (depth 4)
    var_node_44 = torch.ops.aten.add(var_node_45, var_node_46) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_34: tensor_pointwise (depth 5)
    var_node_34 = torch.ops.aten.div(var_node_35, var_node_44) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_12: tensor_pointwise (depth 6)
    var_node_12 = torch.ops.aten.div(var_node_13, var_node_34) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_10: tensor_pointwise (depth 7)
    var_node_10 = torch.ops.aten.mul(var_node_11, var_node_12) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_2: tensor_pointwise (depth 8)
    var_node_2 = torch.ops.aten.div(var_node_3, var_node_10) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_48: constant (depth 7)
    var_node_48 = torch.full((1,), (-1.047745491289218+0.279447315087422j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_54: arg (depth 2)
    var_node_54 = arg_13 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_55: arg (depth 2)
    var_node_55 = arg_14 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_53: tensor_pointwise (depth 3)
    var_node_53 = torch.ops.aten.div(var_node_54, var_node_55) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_56: arg (depth 3)
    var_node_56 = arg_15 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_52: tensor_pointwise (depth 4)
    var_node_52 = torch.ops.aten.div(var_node_53, var_node_56) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_59: arg (depth 2)
    var_node_59 = arg_16 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_60: arg (depth 2)
    var_node_60 = arg_17 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_58: tensor_pointwise (depth 3)
    var_node_58 = torch.ops.aten.div(var_node_59, var_node_60) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_61: constant (depth 3)
    var_node_61 = torch.full((1,), (-0.7386327586576402-0.027025998767172658j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_57: tensor_pointwise (depth 4)
    var_node_57 = torch.ops.aten.add(var_node_58, var_node_61) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_51: tensor_pointwise (depth 5)
    var_node_51 = torch.ops.aten.sub(var_node_52, var_node_57) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_64: arg (depth 3)
    var_node_64 = arg_18 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_67: arg (depth 1)
    var_node_67 = arg_19 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_68: constant (depth 1)
    var_node_68 = torch.full((1,), (-0.6840241429755998+1.327637020136433j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_66: tensor_pointwise (depth 2)
    var_node_66 = torch.ops.aten.mul(var_node_67, var_node_68) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_69: arg (depth 2)
    var_node_69 = arg_20 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_65: tensor_pointwise (depth 3)
    var_node_65 = torch.ops.aten.sub(var_node_66, var_node_69) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_63: tensor_pointwise (depth 4)
    var_node_63 = torch.ops.aten.sub(var_node_64, var_node_65) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_70: arg (depth 4)
    var_node_70 = arg_21 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_62: tensor_pointwise (depth 5)
    var_node_62 = torch.ops.aten.sub(var_node_63, var_node_70) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_50: tensor_pointwise (depth 6)
    var_node_50 = torch.ops.aten.mul(var_node_51, var_node_62) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_76: constant (depth 1)
    var_node_76 = torch.full((1,), (1.864651314238342+0.27066487315113186j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_77: arg (depth 1)
    var_node_77 = arg_22 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_75: tensor_pointwise (depth 2)
    var_node_75 = torch.ops.aten.mul(var_node_76, var_node_77) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_78: arg (depth 2)
    var_node_78 = arg_23 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_74: tensor_pointwise (depth 3)
    var_node_74 = torch.ops.aten.add(var_node_75, var_node_78) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_79: arg (depth 3)
    var_node_79 = arg_24 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_73: tensor_pointwise (depth 4)
    var_node_73 = torch.ops.aten.mul(var_node_74, var_node_79) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_80: arg (depth 4)
    var_node_80 = arg_25 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_72: tensor_pointwise (depth 5)
    var_node_72 = torch.ops.aten.mul(var_node_73, var_node_80) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_82: constant (depth 4)
    var_node_82 = torch.full((1,), (1.6341547018841247+0.3096989611326181j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_84: constant (depth 3)
    var_node_84 = torch.full((1,), (0.9609065596935821+0.2920229825681946j), dtype=torch.complex128) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_85: arg (depth 3)
    var_node_85 = arg_26 # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_83: tensor_pointwise (depth 4)
    var_node_83 = torch.ops.aten.add(var_node_84, var_node_85) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_81: tensor_pointwise (depth 5)
    var_node_81 = torch.ops.aten.sub(var_node_82, var_node_83) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_71: tensor_pointwise (depth 6)
    var_node_71 = torch.ops.aten.sub(var_node_72, var_node_81) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_49: tensor_pointwise (depth 7)
    var_node_49 = torch.ops.aten.mul(var_node_50, var_node_71) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_47: tensor_pointwise (depth 8)
    var_node_47 = torch.ops.aten.add(var_node_48, var_node_49) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_1: tensor_pointwise (depth 9)
    var_node_1 = torch.ops.aten.add(var_node_2, var_node_47) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_0: torch.ops.aten.item (depth 10)
    var_node_0 = var_node_1.item() # dtype=complex128

    # Final result from root node
    return var_node_0

arg_0 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10042)
arg_1 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10043)
arg_2 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10044)
arg_3 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10045)
arg_4 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10046)
arg_5 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10047)
arg_6 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10048)
arg_7 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10049)
arg_8 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10050)
arg_9 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10051)
arg_10 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10052)
arg_11 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10053)
arg_12 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10054)
arg_13 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10055)
arg_14 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10056)
arg_15 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10057)
arg_16 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10058)
arg_17 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10059)
arg_18 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10060)
arg_19 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10061)
arg_20 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10062)
arg_21 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10063)
arg_22 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10064)
arg_23 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10065)
arg_24 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10066)
arg_25 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10067)
arg_26 = fuzz_tensor_simple((1,), (1,), torch.complex128, seed=10068)
import torch
import sys
torch._dynamo.config.capture_scalar_outputs = True

args = (arg_0, arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8, arg_9, arg_10, arg_11, arg_12, arg_13, arg_14, arg_15, arg_16, arg_17, arg_18, arg_19, arg_20, arg_21, arg_22, arg_23, arg_24, arg_25, arg_26)
result_original = fuzzed_program(*args)
print('✅ eager success')
sys.exit(1)
compiled_program = torch.compile(fuzzed_program, fullgraph=False, dynamic=True)
result_compiled = compiled_program(*args)
print('✅ compile success')

======================
Program exited with code: 1
```",2025-09-24 04:37:17+00:00,2025-09-25T06:50:03Z,,False,1,0,7,164,317,3,1,,39,17073,False,False,False,False,False,False,3,0,0,17758,8780,8978,1,7,,,,pytorch
163742,closed,Shortcut redistribution when num_shards == 1,SherlockNoMad,"Redistribution doesn't need collectives when num_shards == 1 on a mesh dimension. 
Only placement update is needed, local_tensor remains unchanged. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 04:19:59+00:00,2025-09-24T23:50:14Z,,False,6,11,6,27,1,2,17,2025-09-24 23:49:12+00:00,44,251,False,False,False,False,False,False,2,5,1741,104,65,39,1,6,3.0,6.0,2025-09-24T04:21:51Z,pytorch
163740,closed,[Triton] [Inductor] Set default configs for Blackwell Matmul Template,njriasan,"Summary: Sets the default configs for the Blackwell Matmul Templates.

Test Plan: NFC

Differential Revision: D83116342




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-24 04:10:13+00:00,2025-09-24T08:18:41Z,,False,4,0,1,12,4,1,4,2025-09-24 08:17:38+00:00,69,333,False,False,False,False,False,False,1,2,560,16,12,4,1,1,3.0,3.0,2025-09-24T04:28:31Z,pytorch
163739,closed,[torchfuzz] print out tensor descriptor as comments in codegen,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163812
* #163743
* __->__ #163739
* #163698
* #163560
* #163558
* #163557
* #163556
* #163555
* #163554
* #163553
* #163547

eg.

```
    # Node node_12: tensor_pointwise (depth 6)
    var_node_12 = torch.ops.aten.mul(var_node_13, var_node_34) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_10: tensor_pointwise (depth 7)
    var_node_10 = torch.ops.aten.div(var_node_11, var_node_12) # size=(1,), stride=(1,), dtype=complex128, device=cuda

    # Node node_2: tensor_pointwise (depth 8)
    var_node_2 = torch.ops.aten.div(var_node_3, var_node_10) # size=(1,), stride=(1,), dtype=complex128, device=cuda
```",2025-09-24 03:57:04+00:00,2025-09-25T01:30:35Z,,False,3,0,4,37,2,2,3,2025-09-25 01:29:32+00:00,62,713,False,False,False,False,False,False,2,2,493,1105,570,535,1,4,3.0,2.0,2025-09-24T23:21:05Z,pytorch
163738,open,[WIP][precompile] Set fake_mode of base tensor in fx graph pickler,yiming0416,"Summary:
When unpickling a fake tensor in fx graph pickler. It only sets the fake mode of the current tensor's metadata to the one that is consistent with pickler's `unpickle_state`. However, it doesn't set the fake mode of a tensor's base tensor when that tensor is a view.

This will cause an issue when dumping and loading the following graph
```
class GraphModule(torch.nn.Module):
    def forward(self, s77: ""Sym(s77)"", L_x_: ""f32[s77, 8]""):
        l_x_ = L_x_
        chunk = l_x_.chunk(2, dim = -1);  l_x_ = None
        y: ""f32[s77, 4]"" = chunk[0];  chunk = None
        y_repeat: ""f32[s77, 8]"" = y.repeat_interleave(2, dim = -1);  y = None
        return (y_repeat,)
```
because `repeat_interleave` will create an intermediate fake tensor of size `[s77, 2, 4]` and it will become the base of the node `y_repeat`'s `meta['val']`.

This causes issues during the deserialization phase when applying AOT precompile to DeepSeek in vLLM.

Test Plan:
This has been tested in vLLM with DeepSeek.

As for unittest, ideally it should be `test_aot_compile_repeat_interleave` with mark_dynamic turned on. However, that's leading to some other pickle issues.

```
python test/dynamo/test_aot_compile.py -k test_aot_compile_repeat_interleave
```

I have yet to figure out a more appropriate unittest. But a proof-of-concept demo would be the following:
```
import inspect
import sympy
import torch
from torch.fx._graph_pickler import GraphPickler
from torch.fx.experimental.symbolic_shapes import ShapeEnv
from torch._subclasses import FakeTensorMode
from torch.fx._graph_pickler import GraphPickler, Options
from unittest.mock import patch

class M(torch.nn.Module):
    def forward(self, x):
        chunk = x.chunk(2, dim=-1)
        y = chunk[0]
        y_repeat = y.repeat_interleave(2, dim=-1)
        return y_repeat

def my_custom_backend(gm, example_inputs):
    global gm_global
    gm_global = gm
    return gm.forward

m = M()
m_opt = torch.compile(m, backend=my_custom_backend, fullgraph=True)

sample_inputs = (torch.randn(2, 8),)
torch._dynamo.mark_dynamic(sample_inputs[0], [0])
opt_out = m_opt(*sample_inputs)

graph_reducer_override = GraphPickler.reducer_override

def _graph_reducer_override(self, obj):
    if (inspect.isclass(obj) and issubclass(obj, sympy.Function)
            and hasattr(obj, ""_torch_unpickler"")):
        return obj._torch_unpickler, (obj._torch_handler_name, )
    if isinstance(obj, FakeTensorMode):
        return type(None), ()
    return graph_reducer_override(self, obj)

with patch.object(GraphPickler, ""reducer_override"", _graph_reducer_override):
    pickled_gm = GraphPickler.dumps(gm_global, Options(ops_filter=None))

fake_mode = FakeTensorMode(shape_env=ShapeEnv())
loaded_gm = GraphPickler.loads(pickled_gm, fake_mode)
```

Differential Revision: D83112599




cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 03:54:20+00:00,2025-09-25T05:01:15Z,,False,5,0,1,66,1,2,5,,66,2992,False,False,False,False,False,False,2,0,0,67,66,1,1,1,,,,pytorch
163737,closed,"Revert ""[inductor] Fix issue with scalar arg handling""",jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163377
* __->__ #163737

This reverts commit a8cd437183142e17ba6fc8d7b5e9dcee462d7904.

See https://github.com/pytorch/pytorch/pull/163481#issuecomment-3326310774

This PR might also cause issues with cudagraphs.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 03:46:58+00:00,2025-09-24T07:34:19Z,,False,3,0,1,3,104,6,3,2025-09-24 07:33:16+00:00,54,506,False,True,False,False,False,False,6,2,493,107,3,104,1,1,3.0,2.0,2025-09-24T03:48:21Z,pytorch
163736,open,dtensor prototype,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163736
* #163406

",2025-09-24 03:46:55+00:00,2025-09-25T06:39:39Z,,False,2,0,2,354,185,42,2,,17,104,False,False,False,False,False,False,42,0,0,89502,62842,26660,1,2,,,,pytorch
163735,open,more dense operators,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* __->__ #163735
* #163734
* #163733
* #163732
* #163436
* #163406

",2025-09-24 03:46:51+00:00,2025-09-24T05:33:46Z,,False,2,0,1,740,61,29,2,,20,154,False,False,False,False,False,False,29,0,0,801,740,61,1,1,,,,pytorch
163734,open,more layout operators,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* #163735
* __->__ #163734
* #163733
* #163732
* #163436
* #163406

",2025-09-24 03:46:47+00:00,2025-09-24T05:33:30Z,,False,2,0,1,1201,1,10,2,,21,154,False,False,False,False,False,False,10,0,0,1202,1201,1,1,1,,,,pytorch
163733,open,support start and count cli args,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* #163735
* #163734
* __->__ #163733
* #163732
* #163436
* #163406

",2025-09-24 03:46:43+00:00,2025-09-24T05:33:31Z,,False,2,0,1,12,19,1,2,,32,154,False,False,False,False,False,False,1,0,0,31,12,19,1,1,,,,pytorch
163732,open,variable inputs,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* #163735
* #163734
* #163733
* __->__ #163732
* #163436
* #163406

",2025-09-24 03:46:39+00:00,2025-09-24T05:33:37Z,,False,2,0,1,29,1,8,2,,15,154,False,False,False,False,False,False,8,0,0,30,29,1,1,1,,,,pytorch
163731,closed,[WIP] Implement aten_bilinear,Copilot,"Thanks for assigning this issue to me. I'm starting to work on it and will keep this PR's description up to date as I form a plan and make progress.

Original issue description:

> Implement the aten_binear function at https://github.com/microsoft/onnxscript/blob/e67eeefc8bc2b120bab79a8d04f303690ddc4bc0/onnxscript/function_libs/torch_lib/ops/core.py#L1192
> 
> Operator schema: `bilinear(Tensor input1, Tensor input2, Tensor weight, Tensor? bias=None) -> Tensor` 
> 
> Docs:
> 
> ```
> Applies a bilinear transformation to the incoming data: :math:`y = x_1^T A x_2 + b`.
> 
>     Args:
>         in1_features: size of each first input sample, must be > 0
>         in2_features: size of each second input sample, must be > 0
>         out_features: size of each output sample, must be > 0
>         bias: If set to ``False``, the layer will not learn an additive bias.
>             Default: ``True``
> 
>     Shape:
>         - Input1: :math:`(*, H_\text{in1})` where :math:`H_\text{in1}=\text{in1\_features}` and
>           :math:`*` means any number of additional dimensions including none. All but the last dimension
>           of the inputs should be the same.
>         - Input2: :math:`(*, H_\text{in2})` where :math:`H_\text{in2}=\text{in2\_features}`.
>         - Output: :math:`(*, H_\text{out})` where :math:`H_\text{out}=\text{out\_features}`
>           and all but the last dimension are the same shape as the input.
> 
>     Attributes:
>         weight: the learnable weights of the module of shape
>             :math:`(\text{out\_features}, \text{in1\_features}, \text{in2\_features})`.
>             The values are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
>             :math:`k = \frac{1}{\text{in1\_features}}`
>         bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
>                 If :attr:`bias` is ``True``, the values are initialized from
>                 :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
>                 :math:`k = \frac{1}{\text{in1\_features}}`
> ```


Fixes #163730.

<!-- START COPILOT CODING AGENT TIPS -->
---

💡 You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",2025-09-24 03:35:32+00:00,2025-09-24T03:39:06Z,,False,2,0,1,0,0,0,2,2025-09-24 03:36:28+00:00,29,2380,False,True,True,True,False,False,0,0,0,0,0,0,1,0,,,,pytorch
163729,open,[Inductor-FX] Support unbacked symbol definitions,blaine-rister,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 03:29:16+00:00,2025-09-25T05:01:16Z,,False,1,0,23,273,31,4,1,,49,223,False,True,False,False,False,False,4,0,0,30489,22714,7775,1,23,,,,pytorch
163728,closed,[Code Clean] Remove deadcodes about Python3.9 [8/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163728
* #163646
* #163645
* #163644
* #163643
* #163629
* #163627
* #163626

As the title stated.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-24 03:01:48+00:00,2025-09-25T05:13:53Z,,False,5,0,2,3,56,6,5,2025-09-25 05:12:50+00:00,51,412,False,False,False,False,False,False,6,3,516,61,4,57,1,2,4.0,4.0,2025-09-24T03:34:18Z,pytorch
163727,open,[inductor] kernel template choice serialization,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163727
* #163726

# why

- enable distributed autotuning to pass choices directly

# what

- break out information piece of KTC from the rest, and enable
  serialization creation/restoration

# testing

- simple tests added

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-24 02:28:39+00:00,2025-09-24T21:01:48Z,,False,1,0,2,241,0,2,1,,47,498,False,False,False,False,False,False,2,0,0,385,325,60,1,2,,,,pytorch
163726,open,[inductor] template registry,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163727
* __->__ #163726

# why

find templates by uid and enable information passing about which
templates is used

# what

- expand registry for heuristics to also track templates
- register all templates on creation
- register extern kernels (pseudo templates) on creation

- this also now enforces that uid is unique

# testing

existing tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-24 02:28:35+00:00,2025-09-24T21:01:50Z,,False,1,0,2,71,1,3,1,,28,614,False,False,False,False,False,False,3,0,0,215,154,61,1,2,,,,pytorch
163725,closed,[inductor] kernel template choice serialization,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163725
* #163724

# why

- enable distributed autotuning to pass choices directly

# what

- break out information piece of KTC from the rest, and enable
  serialization creation/restoration

# testing

- simple tests added

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-24 02:18:49+00:00,2025-09-24T02:28:01Z,,False,2,0,2,241,0,2,2,2025-09-24 02:28:01+00:00,47,498,False,False,False,False,False,False,2,0,0,10849,7735,3114,2,2,,,,pytorch
163724,closed,[inductor] template registry,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163725
* __->__ #163724

# why

find templates by uid and enable information passing about which
templates is used

# what

- expand registry for heuristics to also track templates
- register all templates on creation
- register extern kernels (pseudo templates) on creation

- this also now enforces that uid is unique

# testing

existing tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-24 02:18:45+00:00,2025-09-24T02:28:07Z,,False,2,0,2,70,1,3,2,2025-09-24 02:28:07+00:00,28,614,False,False,False,False,False,False,3,0,0,10679,7564,3115,2,2,,,,pytorch
163723,open,[Inductor][CPP] Fix the test case of test_linear_reuse_kernels,CaoE,"Fixes #163491.
Add tolerances to make `test_linear_reuse_kernels` more stable.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 02:09:30+00:00,2025-09-25T01:19:26Z,,False,1,0,1,4,2,1,1,,62,281,False,True,False,False,False,False,1,0,0,6,4,2,1,1,,,,pytorch
163720,open,[dynamo][WIP] Add most recent bytecode to graph break with developer initiation,fxdawnn,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163720

#162858 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-24 01:39:28+00:00,2025-09-24T18:25:26Z,,False,3,0,1,19,0,1,3,,79,274,False,False,False,False,False,False,1,1,40,19,19,0,1,1,1.0,1.0,2025-09-24T01:45:10Z,pytorch
163719,closed,[c10d] P2P tensors must be dense,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163719

Fixes #161324
by adding `is_non_overlapping_and_dense` check.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 01:18:43+00:00,2025-09-24T06:59:09Z,,False,3,0,2,26,1,2,3,2025-09-24 06:58:06+00:00,32,270,False,True,False,False,False,False,2,2,493,33,29,4,1,2,3.0,2.0,2025-09-24T02:24:58Z,pytorch
163718,closed,[AOTI] Add verbose error information for extract file,xuhancn,"This PR optimize `extract_file` functions:
1. `normalize_path_separator` the dest path for Windows.
2. Add verbose error message:
a. On Linux, add mz_zip error string.
b. On Windows, add mz_zip error string and Windows error code.

For the UT `test_package_user_managed_weight`:
<img width=""1910"" height=""442"" alt=""image"" src=""https://github.com/user-attachments/assets/6a63eda1-70ce-40fb-9681-adc955463884"" />

It still have issue with error code `32`, checked https://learn.microsoft.com/en-us/windows/win32/debug/system-error-codes--0-499- and find the verbose is `ERROR_SHARING_VIOLATION`.

It is a little complex to debug, I will continue to working on it in further PR.


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 ",2025-09-24 01:14:33+00:00,2025-09-24T19:42:34Z,,False,3,0,6,20,3,1,3,2025-09-24 19:27:32+00:00,53,810,False,True,False,False,False,False,1,2,493,53,35,18,2,6,3.0,2.0,2025-09-24T17:06:41Z,pytorch
163715,open,Always produce kernel_info.json,yushangdi,"Summary: Always produce kernel_info.json so zoomer can use this json to populate GPU traces

Test Plan: CI

Differential Revision: D82762435




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-24 00:46:25+00:00,2025-09-25T04:47:14Z,,False,3,0,1,9,9,1,3,,31,346,False,False,False,False,False,False,1,0,0,18,9,9,1,1,1.0,0.0,2025-09-24T16:59:27Z,pytorch
163714,open,[opaque obj] Allow non-effectful scriptobjs,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163714
* #163284
* #163278
* #163277
* #163279

",2025-09-24 00:40:42+00:00,2025-09-25T01:07:51Z,,False,1,2,4,117,12,3,3,,43,134,False,False,False,False,False,False,3,0,0,161,133,28,1,4,2.0,0.0,2025-09-24T17:21:41Z,pytorch
163712,closed,[dist] handle discontiguous allgather/reducescatter inputs,ngimel,"Fixes #163483 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-24 00:26:56+00:00,2025-09-24T19:39:49Z,,False,8,3,2,47,8,3,11,2025-09-24 19:38:47+00:00,58,117,False,True,False,False,False,False,3,7,1910,59,49,10,1,2,5.0,7.0,2025-09-24T00:29:05Z,pytorch
163711,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-24 00:25:25+00:00,2025-09-24T04:32:42Z,,False,3,0,1,1,1,1,3,2025-09-24 04:31:39+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-24T00:25:26Z,pytorch
163710,closed,"Revert ""Remove test conditions for CUDA<12 (#163495)""",Camyll,"This reverts commit 5d749ceb92c2c28bcfbdf918b4ab99b1a91fcb50.

Fixes trunk. Manual revert due to merge conflict
",2025-09-24 00:05:03+00:00,2025-09-24T05:00:33Z,,False,7,0,1,66,6,4,7,2025-09-24 05:00:33+00:00,53,112,False,True,False,False,False,False,4,6,2120,72,66,6,1,1,6.0,7.0,2025-09-24T00:08:12Z,pytorch
163708,open,Change to pytorch,dzmitry-huba,"Testing permissions


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 23:55:20+00:00,2025-09-24T01:07:14Z,,False,2,0,1,1,0,1,2,,17,123,False,False,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
163707,open,[inductor] fix: 'get_raw_stream' undefined,dolpm,"Summary:
ran into this when precompiling baidu/ERNIE-4.5-21B-A3B-PT

codegen after fix:
```py
import triton
import triton.language as tl
from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_raw_stream
with torch.cuda._DeviceGuard(0):
    stream0 = get_raw_stream(0)
...
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 23:38:41+00:00,2025-09-25T15:01:25Z,,False,2,0,1,12,0,1,2,,42,554,False,True,False,False,False,False,1,1,33,12,12,0,1,1,1.0,1.0,2025-09-25T15:01:19Z,pytorch
163706,closed,changed,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163706



cc @ezyang @EikanWang @jgong5 @wenzhe-nrv",2025-09-23 23:31:28+00:00,2025-09-23T23:34:54Z,,False,2,0,1,4,0,1,2,2025-09-23 23:34:53+00:00,7,137,False,False,False,False,False,False,1,1,13,4,4,0,1,1,1.0,1.0,2025-09-23T23:34:53Z,pytorch
163705,closed,Do not generate guards on unbacked SymInt inputs,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163705

We should not generate guards on unbacked symbols, i found that we do some recompilations 
with my changes in [](https://github.com/pytorch/pytorch/pull/163652) and also found the following error messages
on current state of pytorch when running the program with the graph below.

TODO:
Still figuring out what is the right approach here. 
We need to know what guards are side effects of torch checks and which ones are things that did not go through a torch 
check yet. (like striding relations...)

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv",2025-09-23 23:24:14+00:00,2025-09-24T19:20:46Z,,False,6,4,13,143,4,2,10,2025-09-24 19:19:20+00:00,48,636,False,False,False,False,False,False,2,5,1721,239,189,50,1,13,3.0,5.0,2025-09-24T00:36:30Z,pytorch
163704,closed,Record redistribute_local_tensor in DebugMode,SherlockNoMad,"Explicit redistribute_local_tensor API call could also results in communication, record it! 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 23:00:10+00:00,2025-09-24T16:58:03Z,,False,12,3,5,145,113,5,15,2025-09-24 16:11:28+00:00,45,195,False,True,False,False,False,False,5,11,3416,296,164,132,1,5,4.0,12.0,2025-09-23T23:33:41Z,pytorch
163703,closed,Update pytorch.org links in docs/conf.py,pytorchbot,"Update links in conf.py to docs.pytorch.org

Fixes #ISSUE_NUMBER


cc @sekyondaMeta @AlannaBurke",2025-09-23 22:53:48+00:00,2025-09-24T22:44:44Z,2025-09-24T22:44:43Z,True,1,0,1,2,2,1,1,2025-09-24 22:44:43+00:00,40,96,False,True,False,True,False,False,1,0,0,4,2,2,1,1,1.0,0.0,2025-09-24T22:44:12Z,pytorch
163700,open,Add fake_impl for _native_multi_head_attention,ydwu4,"

Test Plan: See added test in test_export.py

Differential Revision: D83099187




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 22:18:15+00:00,2025-09-24T19:46:32Z,,False,2,0,1,138,1,3,2,,46,254,False,False,False,False,False,False,3,0,0,139,138,1,1,1,1.0,0.0,2025-09-24T16:44:19Z,pytorch
163698,closed,[torchfuzz] refactor multi_process_fuzzer to be more readable,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163812
* #163743
* #163739
* __->__ #163698
* #163560
* #163558
* #163557
* #163556
* #163555
* #163554
* #163553
* #163547

",2025-09-23 22:03:11+00:00,2025-09-24T23:33:41Z,,False,3,0,2,112,95,1,3,2025-09-24 23:32:37+00:00,61,204,False,False,False,False,False,True,1,2,493,207,112,95,1,2,3.0,2.0,2025-09-24T23:20:38Z,pytorch
163697,open,[NFC] fixed mistake in comment,aartbik,"I used ""floor"" instead of ""ceil"", so fix it. Also fixed other typo.


",2025-09-23 21:59:47+00:00,2025-09-24T22:01:44Z,,False,4,0,1,2,2,1,4,,30,70,False,True,False,False,False,False,1,3,912,4,2,2,1,1,2.0,4.0,2025-09-24T15:34:10Z,pytorch
163696,closed,[Inductor] Update DeviceAssert op to behave like store,karthickai,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163696

Updated the DeviceAssert operation to match the behavior of Store, it will fixes the issue mentioned in [this PR](https://github.com/pytorch/pytorch/pull/163023) and updated testcases as Elias [suggested](https://github.com/pytorch/pytorch/pull/160677#discussion_r2353834646).



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos @choijon5 ",2025-09-23 21:38:44+00:00,2025-09-24T23:37:02Z,,False,3,4,2,59,93,6,7,2025-09-24 23:35:59+00:00,54,594,False,True,False,False,False,False,6,2,493,156,61,95,1,2,3.0,2.0,2025-09-24T08:06:26Z,pytorch
163695,closed,Add analytics ID to cpp docs,pytorchbot,cc @sekyondaMeta @AlannaBurke,2025-09-23 21:34:11+00:00,2025-09-24T22:45:17Z,2025-09-24T22:45:17Z,True,1,0,1,1,0,1,1,2025-09-24 22:45:17+00:00,28,29,False,False,False,True,False,False,1,0,0,1,1,0,1,1,1.0,0.0,2025-09-24T22:45:10Z,pytorch
163694,open,[MPS] [Sparse] unique_dim and sparse broadcast,Isalia20,"Implements unique_dim, sparse broadcast ops and adds dtypes for mps for tests where we expect to fail, otherwise they would always fail due to being run in double precision

cc @kulinseth @malfet @DenisVieriu97 @jhavukainen",2025-09-23 21:32:04+00:00,2025-09-25T13:38:38Z,,False,3,14,4,126,6,3,17,,46,223,False,False,False,False,False,False,3,0,0,10789,7544,3245,1,4,3.0,0.0,2025-09-24T17:05:08Z,pytorch
163693,closed,[ROCm][CI] skip TestCudaPrimaryCtx.test_set_device_0,ethanwee1,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-23 21:31:38+00:00,2025-09-23T22:16:16Z,,False,3,0,2,4,1,1,3,2025-09-23 22:15:13+00:00,52,138,False,True,False,False,False,False,1,2,802,7,5,2,1,2,2.0,2.0,2025-09-23T22:02:49Z,pytorch
163691,open,Add attention benchmarking to operator microbenchmarks,jainapurva,"Fixes #ISSUE_NUMBER
",2025-09-23 21:22:38+00:00,2025-09-23T23:01:42Z,,False,2,0,1,471,0,9,2,,54,20,False,True,False,False,False,False,9,0,0,471,471,0,1,1,,,,pytorch
163685,open,[inductor] Fix unbounded number of substitutions when equality checks contain Max expr,sevenEng,"## Issue

From an internal use case, we found that if we have an equality rule like:

```
Max(15, u0) == s0 * Max(15, u0)
```

This would lead to wrong substitution rule being generated in the substitution table, the result would be the process got stuck in the substitution loop as if it hangs indefinitely, as it's doing the following substitutions:

```
Max(15, u0) 
--> s0 * Max(15, u0)
--> s0 ** 2 * Max(15, u0)
--> s0 ** 3 * Max(15, u0)
--> s0 ** 4 * Max(15, u0)
...
```


The root cause is with SymPy expression comparison: as `Max` is [not inside the op class table](https://github.com/sympy/sympy/blob/1.14/sympy/core/basic.py#L50-L86), it'll take the [UNKNOWN](https://github.com/sympy/sympy/blob/1.14/sympy/core/basic.py#L120) order, and considered bigger than any other types of expressions. 


## Fix
1. Added a breaking-out from the substitution while-loop to warn about any exccessive substitutions, what threshold should be used here and how to pass it are open to suggestion, using a hard-coded static value to be simple for now
2. Enhanced the sympy expression comparison logic, so that we first check if one expr ""has"" the other one or not, to help work around the issue with `Max` here

## Testing

- with the unittiest alone --> unittest stuck
- with the unittest and while-loop breakout, we could see tests finished with warning ""**Substitution limit reached**"":
```
test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpu::test_unbounded_expr_substitutions_cpu W0923 13:00:37.864000 46140 /data/users/q1l1/pytorch/torch/_export/__init__.py:70] +============================+
W0923 13:00:37.864000 46140 /data/users/q1l1/pytorch/torch/_export/__init__.py:71] |     !!!   WARNING   !!!    |
W0923 13:00:37.865000 46140 /data/users/q1l1/pytorch/torch/_export/__init__.py:72] +============================+
W0923 13:00:37.865000 46140 /data/users/q1l1/pytorch/torch/_export/__init__.py:73] torch._export.aot_compile()/torch._export.aot_load() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export())/torch._inductor.aoti_load_package() instead.
stats [('calls_captured', 5), ('unique_graphs', 1)]
inductor [('extern_calls', 2)]
graph_break []
aten_mm_info [('aten.mm_Max(15, u0)_16_64', 1)]
PASSED [5.6947s]
test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleGpu::test_unbounded_expr_substitutions_cuda W0923 13:00:39.633000 46140 /data/users/q1l1/pytorch/torch/_inductor/sizevars.py:765] [0/0] Substitution limit (30) reached w/ u1**30*Max(15, u0)
W0923 13:00:39.679000 46140 /data/users/q1l1/pytorch/torch/_inductor/sizevars.py:765] [0/0] Substitution limit (30) reached w/ 64*u1**30*Max(15, u0)
stats [('calls_captured', 5), ('unique_graphs', 1)]
inductor [('extern_calls', 2), ('benchmarking.InductorBenchmarker.benchmark_gpu', 2), ('async_compile_cache_miss', 1)]
graph_break []
aten_mm_info [('aten.mm_Max(15, u0)_16_64', 1)]
PASSED [5.6278s]
test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleMps::test_unbounded_expr_substitutions_mps SKIPPED [0.0002s]

============================ 2 passed, 1 skipped, 870 deselected in 19.66s ============================
```

- with the unittest + comparison logic enhanced, we don't see the warning any more:
```
Running 3 items in this shard

test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpu::test_unbounded_expr_substitutions_cpu W0923 13:15:39.560000 290812 /data/users/q1l1/pytorch/torch/_export/__init__.py:70] +============================+
W0923 13:15:39.561000 290812 /data/users/q1l1/pytorch/torch/_export/__init__.py:71] |     !!!   WARNING   !!!    |
W0923 13:15:39.561000 290812 /data/users/q1l1/pytorch/torch/_export/__init__.py:72] +============================+
W0923 13:15:39.562000 290812 /data/users/q1l1/pytorch/torch/_export/__init__.py:73] torch._export.aot_compile()/torch._export.aot_load() is being deprecated, please switch to directly calling torch._inductor.aoti_compile_and_package(torch.export.export())/torch._inductor.aoti_load_package() instead.
stats [('calls_captured', 5), ('unique_graphs', 1)]
inductor [('extern_calls', 2)]
graph_break []
aten_mm_info [('aten.mm_Max(15, u0)_16_64', 1)]
PASSED [6.6093s]
test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleGpu::test_unbounded_expr_substitutions_cuda stats [('calls_captured', 5), ('unique_graphs', 1)]
inductor [('extern_calls', 2), ('benchmarking.InductorBenchmarker.benchmark_gpu', 2), ('async_compile_cache_miss', 1)]
graph_break []
aten_mm_info [('aten.mm_Max(15, u0)_16_64', 1)]
PASSED [6.0502s]
test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleMps::test_unbounded_expr_substitutions_mps SKIPPED [0.0002s]

============================ 2 passed, 1 skipped, 870 deselected in 21.99s ============================
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 20:35:37+00:00,2025-09-25T03:14:20Z,,False,2,0,2,52,4,2,2,,86,5020,False,True,False,False,False,False,2,0,0,56,52,4,1,2,,,,pytorch
163683,open,Add scaffolding for aoti_torch_call_dispatcher BC/FC with native ops,mikaylagawarecki,"Part 1 of plan in https://docs.google.com/document/d/1MaX51H5aEQE5XnOlnZIpf9oCYwzGrTWkgBACxNzsmWE/edit?usp=sharing

- Upgrade `aoti_torch_call_dispatcher` to v2 with an `extension_abi_version`
- Allow registration of StableIValue stack  --> IValue stack adapters for schema changes

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163832
* __->__ #163683

",2025-09-23 20:28:12+00:00,2025-09-25T02:25:31Z,,False,2,0,2,534,5,10,2,,68,387,False,False,False,True,False,False,10,0,0,539,534,5,1,2,,,,pytorch
163682,closed,Update pytorch.org links in docs/conf.py,svekars,"Update links in conf.py to docs.pytorch.org

Fixes #ISSUE_NUMBER


cc @sekyondaMeta @AlannaBurke",2025-09-23 20:28:05+00:00,2025-09-23T22:53:50Z,,False,5,0,1,2,2,1,5,2025-09-23 21:40:15+00:00,40,96,False,True,False,True,False,False,1,4,1242,4,2,2,1,1,5.0,5.0,2025-09-23T20:43:18Z,pytorch
163681,closed,cd: Move arm64 to linux.arm64.r7g.12xlarge.memory,seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163681

This should reduce the amount of build time we have by a lot by just
throwing more hardware at the problem.

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-09-23 20:27:15+00:00,2025-09-24T04:07:16Z,,False,3,0,1,29,29,2,3,2025-09-24 04:06:12+00:00,49,263,False,False,False,False,False,False,2,2,807,58,29,29,1,1,4.0,4.0,2025-09-23T20:35:49Z,pytorch
163680,closed,"Add scaled_mm python API, split _scaled_mm tests",slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163680
* #163679

Summary:

* Add `torch.quantization.scaled_mm` as an abstraction around the C++
methods.
* Wraps both `torch._scaled_mm` and `torch._scaled_mm_v2` APIs
* Split scaled-MM tests from general matmul tests into scaled-specific
  file.

Test Plan:

`pytest test/test_scaled_matmul_cuda.py`

Reviewers:

Subscribers:

Tasks:

Tags:
Signed-off-by: Simon Layton <simonlayton@meta.com>",2025-09-23 20:26:49+00:00,2025-09-25T13:56:47Z,,False,8,0,1,2136,1487,4,8,2025-09-25 13:56:47+00:00,48,480,False,False,False,False,False,False,4,6,681,3623,2136,1487,1,1,3.0,6.0,2025-09-24T13:08:43Z,pytorch
163679,closed,Add _scaled_mm_v2 API,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163680
* __->__ #163679

Summary:

* Add new scaled-MM API to future-proof / clean-up existing code.
* Scaling is explicitly described rather than infer
* Swizzling of scaled must now be defined (vs. inferred)
* Adds API support for multi-level scaling
* Refactor dispatch logic to make it easier to add new implementations

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:
Signed-off-by: Simon Layton <simonlayton@meta.com>",2025-09-23 20:26:45+00:00,2025-09-25T13:56:47Z,,False,4,0,1,888,1,3,4,2025-09-25 13:56:47+00:00,21,506,False,False,False,False,False,True,3,1,47,889,888,1,1,1,1.0,1.0,2025-09-24T12:51:32Z,pytorch
163677,closed,[Flex] Fix silent correctness w/ backpropping grads,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163677

Fixes #https://github.com/pytorch/pytorch/issues/162228 

# Summary

Majority of our tests are only compiling flex-attention in isolation. This means that for fake tensor propagation the input primals and all captured buffers dont do any intermediate computation below autograd.  As a result result the by happen chance match the `require_grad`ness of the eager implementation and this check  will pass. However if score_mod is a the result of some other intermediate fake tensor prop then it is not guaranteed to have accurate req_gradness, which was happening here.

TLDR is that this was a boot and suspenders that was actually harmful and we should just let the joint graph handle creating the correct joint graph 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-09-23 20:12:37+00:00,2025-09-24T02:13:26Z,,False,3,0,1,30,1,2,3,2025-09-24 02:12:23+00:00,51,1048,False,True,False,False,False,False,2,2,493,31,30,1,1,1,3.0,2.0,2025-09-23T20:14:02Z,pytorch
163676,open,Add Static Dispatch Kernels,kqfu,"Summary: Add a few missing static dispatch kernels for remote_ro.

Test Plan: Tested with scripts in D83028841.

Differential Revision: D83028503


",2025-09-23 20:09:38+00:00,2025-09-25T05:01:28Z,,False,5,0,1,29,0,1,5,,27,148,False,False,False,False,False,False,1,0,0,29,29,0,1,1,1.0,0.0,2025-09-23T21:01:37Z,pytorch
163673,open,Preserve user annotation in graph,SherlockNoMad,"```
import torch
import torch.fx.traceback as fx_traceback
import torch.export


class M(torch.nn.Module):
    def forward(self, x):
        with fx_traceback.annotate({""pp_stage"": 0}):
            with fx_traceback.annotate({""fdsp_bucket"": 0}):
                x = x + 1
            x = x - 2
            with fx_traceback.annotate({""cuda_stream"": 2, ""fsdp_bucket"": 1}):
                x = x * 2
        x = x / 3
        return x

m = M()

with fx_traceback.preserve_node_meta():
    ep = torch.export.export(m, (torch.randn(10),))

for node in ep.graph.nodes:
    if node.op == ""call_function"":
        print(f""{node.target}, {node.meta.get(""custom"", {})}"")


```

prints 

```
aten.add.Tensor, {'pp_stage': 0, 'fdsp_bucket': 0}
aten.sub.Tensor, {'pp_stage': 0}
aten.mul.Tensor, {'pp_stage': 0, 'cuda_stream': 2, 'fsdp_bucket': 1}
aten.div.Tensor, {}
```


TODOs:
- run_decomposition is failing
- Need to test with the new full graph capture + aot_export_joint apis
- Need to make the annotation propagate through autograd engine to reach the bw nodes. Sample impl here: https://github.com/pytorch/pytorch/pull/83558
- Edward want to restrict the key in custom field to be top-level singleton objects only
- also need to take care of metadata merging when passes are fusing nodes

Thanks @angelayi  for contributing the dynamo fixes.

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 20:04:17+00:00,2025-09-25T15:01:20Z,,False,8,11,8,106,0,5,19,,33,1517,False,True,False,False,False,False,5,6,1669,232,169,63,1,8,7.0,7.0,2025-09-23T20:12:03Z,pytorch
163671,open,Fix double dispatch to Python for detach?,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163671

I am not very sure what I'm doing here, but this appears to fix #71725. I would very much appreciate feedback about whether this is the right thing to do here.

I manually ran the repro.py attached to #71725 to verify the fix, as well as noting that DTensor.detach got faster. ~~I would also appreciate feedback about where a conversion of that repro into a regression test should live, because I am not particularly familiar with autograd or view ops broadly.~~ I will update TestPythonDispatch::test_detach_appears_twice_when_called_once to be a regression test.",2025-09-23 18:33:30+00:00,2025-09-24T18:54:44Z,,False,4,5,2,28,35,7,9,,41,658,False,True,False,False,False,False,7,3,484,63,28,35,1,2,3.0,4.0,2025-09-23T20:09:37Z,pytorch
163669,closed,"Support for amin, amax, and aminmax",srsuryadev,"Support for amin, amax, and aminmax

Test Plan: E2E tests in the stack with benchmark suite passes.

Differential Revision: D83016894




cc @egienvalue",2025-09-23 18:02:17+00:00,2025-09-23T23:46:50Z,,False,17,0,1,3,3,1,17,2025-09-23 23:45:46+00:00,35,152,False,False,False,False,False,False,1,6,1758,6,3,3,1,1,4.0,7.0,2025-09-23T19:58:27Z,pytorch
163667,open,Remove torch.distributed.tensor.OpSchema.has_symints,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163667
* #162990
* #163030
* #162968
* #162508
* #161695

It appears to be unused based on `cd torch; rg has_symints`.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 17:46:38+00:00,2025-09-24T02:49:43Z,,False,3,0,2,1,38,2,3,,52,307,False,False,False,False,False,False,2,2,379,40,2,38,1,2,4.0,4.0,2025-09-23T18:29:46Z,pytorch
163665,open,Fake process group Direct construction error,ahkush,"Fixes #162129. Added validation in _rank_not_in_group() to check if ```FakeProcessGroup``` is properly initialized before use, raising a clear error message if ```torch.distributed.init_process_group(backend='fake')``` hasn't been called first. 
This prevents silent failures and ensures proper dispatch system integration for all distributed operations. 

Added test case test_fake_process_group_direct_usage_error() that validates the error is raised for ```all_reduce``` and ```all_to_all_single``` operations. 

Please let me know if additional distributed operators should be tested or if any other updates are needed.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 17:31:46+00:00,2025-09-24T17:08:27Z,,False,2,0,3,40,0,2,2,,44,727,False,True,False,False,False,False,2,1,85,114,77,37,2,3,1.0,1.0,2025-09-24T03:31:15Z,pytorch
163662,closed,[export] Remove .contiguous() when saving weights to raw bytes,pytorchbot,"Summary: `.contiguous()` will discard the original storage size of the tensor, and could lead to issues during loading.

Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_1D_tensor_slicing
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_2D_tensor_slicing

Differential Revision: D83016250


",2025-09-23 16:41:11+00:00,2025-09-23T17:15:06Z,2025-09-23T17:15:06Z,True,1,0,1,37,1,2,1,2025-09-23 17:15:06+00:00,62,324,False,False,False,False,False,False,2,0,0,38,37,1,1,1,1.0,0.0,2025-09-23T17:14:36Z,pytorch
163661,closed,[CD] CUDA 13.0 fix preload logic to include nvidia/cu13/lib/,atalman,"Preload logic no longer works with CUDA 13.0
See the installation path:
```
ls /home/ubuntu/.venv/lib/python3.10/site-packages/nvidia/cu13/lib/
libcheckpoint.so   libcudadevrt.a      libcufft.so.12   libcufile_rdma.so.1  libcusolver.so.12    libnvJitLink.so.13  libnvperf_target.so            libnvrtc.alt.so.13    libpcsamplingutil.so
libcublas.so.13    libcudart.so.13     libcufftw.so.12  libcupti.so.13       libcusolverMg.so.12  libnvblas.so.13     libnvrtc-builtins.alt.so.13.0  libnvrtc.so.13
libcublasLt.so.13  libcudart_static.a  libcufile.so.0   libcurand.so.10      libcusparse.so.12    libnvperf_host.so   libnvrtc-builtins.so.13.0      libnvtx3interop.so.1

ls /home/ubuntu/.venv/lib/python3.10/site-packages/nvidia/
cu13  cudnn  cusparselt  nccl  nvshmem
```

Test using script from : https://github.com/pytorch/pytorch/issues/162367
```
Kernel test passed!
```",2025-09-23 16:39:39+00:00,2025-09-24T14:53:01Z,,False,9,0,6,15,4,1,9,2025-09-24 11:27:10+00:00,60,875,False,True,False,False,False,False,1,8,2149,77,44,33,1,6,6.0,9.0,2025-09-23T21:42:04Z,pytorch
163660,open,[inductor] fix issue for example value with unbacked strides,sevenEng,"## Issue

During autotune, we're not applying size hints atomically for the example inputs used for benchmarking.

If there is unbacked symint showing up in inputs' strides, this might lead to CUDA IMA,

and this could be reproduced by the added unittest, with stride being `[128 * u0, 128, 1]` and unbacked fallback being 8192, after calling `benchmark_example_value`, we get back a tensor with stride as `[8192, 128, 1]` as opposed to `[128 * 8192, 128, 1]`


## Fix

Using the atomic API when trying to apply size hints to input tensor' strides.




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 16:32:28+00:00,2025-09-24T20:54:55Z,,False,3,2,1,36,7,2,5,,60,754,False,True,False,False,False,False,2,1,932,43,36,7,1,1,1.0,1.0,2025-09-24T19:56:03Z,pytorch
163656,open,Speedup __instancecheck__ by moving VariableTrackerMeta to C++,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163656

Also, caches the result of `__instancecheck__`

Reduces tracing time of test_ziplongest by 0.7 seconds.

```
                              Comparison Results
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓
┃ Benchmark                    ┃ Baseline Time ┃ New Time ┃ Speedup/Slowdown ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩
│ TestBasicOps.test_ziplongest │ 45.5060s      │ 44.8077s │            0.98x │
└──────────────────────────────┴───────────────┴──────────┴──────────────────┘
```",2025-09-23 16:08:30+00:00,2025-09-24T00:36:58Z,,False,1,0,3,122,3,3,1,,62,650,False,False,False,False,False,False,3,0,14,227,173,54,1,3,1.0,1.0,2025-09-23T23:36:42Z,pytorch
163655,closed,Update pytorch_sphinx_theme2 to latest hash,pytorchbot,"The updated theme:
- Fixes articleBody in the json+ld that caused previous Google Search issues
- Other minor fixes
- 404.html fixes",2025-09-23 15:46:21+00:00,2025-09-23T17:13:51Z,2025-09-23T17:13:51Z,True,1,0,1,1,1,1,1,2025-09-23 17:13:51+00:00,43,132,False,True,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-23T17:13:43Z,pytorch
163654,open,[AMP] Add deprecated decorator for torch.xxx.amp.autocast class,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163654

As the title stated.

**Changes:**
- torch.cuda.amp.autocast
- torch.cpu.amp.autocast
- add explicit __new__ for those class above for inspect.signature to retrieve correct signature",2025-09-23 15:39:42+00:00,2025-09-25T10:46:37Z,,False,2,0,4,44,10,2,2,,63,288,False,False,False,False,False,False,2,1,796,10016,3436,6580,1,4,2.0,1.0,2025-09-23T17:11:02Z,pytorch
163653,closed,[BE] Delete all pre py-3.10 checks,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163653



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-23 15:13:49+00:00,2025-09-23T23:24:01Z,,False,6,10,6,64,488,23,16,2025-09-23 23:22:57+00:00,34,423,False,False,False,False,False,False,23,5,1517,1678,1001,677,1,6,4.0,5.0,2025-09-23T15:33:39Z,pytorch
163652,open,Allow unbacked to unbacked replacements if rhs unbacked symbols are all inputs,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163822
* __->__ #163652

This partially solve the issue https://github.com/pytorch/pytorch/issues/163641. We do not need to ban unbacked to unbacked replacement if all rhs symbols are inputs since we know those symbols are seen by the whole program.

This issue was found as i was tracing some vllm models with unbacked, namely  Qwen/Qwen2-1.5B-Instruct it makes reasoning logic easier to do those replacements. 

as for data dependent similar pattern, I am thinking to create a set of replacements that we apply only during static eval
instead of none. to make reasoning better. 

I am also renaming create_symbol  to create_non_data_dependent_symbol, this function now is only used to create 
no data dependent symbols and should not be used for the purpose of creating data dependent symbols. 
so be explicit is better. 
I kept  create_symbol for backward compatibility and added deprecation annotation so we can remove it. 
cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-23 15:07:54+00:00,2025-09-25T08:48:53Z,,False,3,9,15,181,25,15,12,,78,1229,False,False,False,False,False,False,15,2,142,382,269,113,1,15,3.0,3.0,2025-09-23T16:00:34Z,pytorch
163651,closed,[ROCm][CI] skip test_sparse_triangular_solve,amdfaa,"need more time to debug, but also need clean CI signal test was unskipped by #163495, but had been skipp on rocm prior

Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-23 15:02:57+00:00,2025-09-23T15:56:58Z,,False,6,0,1,2,1,1,6,2025-09-23 15:55:56+00:00,44,258,False,True,False,False,False,False,1,5,1874,3,2,1,1,1,2.0,5.0,2025-09-23T15:44:03Z,pytorch
163649,closed,[EZ] Perma-ignore UP038,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163653
* __->__ #163649
* #163648

As it has been removed, see https://docs.astral.sh/ruff/rules/non-pep604-isinstance/",2025-09-23 14:52:24+00:00,2025-09-23T17:59:26Z,,False,3,0,3,1,1,1,3,2025-09-23 17:58:22+00:00,23,198,False,False,False,True,False,False,1,2,783,1120,934,186,1,3,3.0,2.0,2025-09-23T17:12:49Z,pytorch
163648,closed,[EZ] Fix UP041 violations,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163653
* #163649
* __->__ #163648

I.e. use `TimeoutError` instead of `socket.timeout`",2025-09-23 14:52:18+00:00,2025-09-23T17:58:22Z,,False,2,0,3,1,2,2,2,2025-09-23 17:58:21+00:00,25,165,False,True,False,False,False,False,2,1,48,1121,934,187,1,3,3.0,1.0,2025-09-23T14:58:05Z,pytorch
163647,open,Update testsuite for s390x,AlekseiNikiforovIBM,"Skip test_compiled_autograd_attribution on s390x

It fails both on s390x and x86_64 at least under some circumstances. Disable it for now until on s390x until it works reliably.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 14:40:35+00:00,2025-09-24T17:20:43Z,,False,1,0,2,7,1,2,1,,26,349,False,False,False,False,False,False,2,0,0,8,7,1,1,2,,,,pytorch
163646,closed,[Code Clean] Remove deadcodes about Python3.9 [7/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* __->__ #163646
* #163645
* #163644
* #163643
* #163629
* #163627
* #163626

As the title stated.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 14:38:22+00:00,2025-09-24T07:32:09Z,,False,3,0,3,4,12,4,3,2025-09-24 07:31:00+00:00,51,382,False,False,False,False,False,False,4,2,493,3668,2755,913,1,3,3.0,2.0,2025-09-23T15:32:25Z,pytorch
163645,closed,[Code Clean] Remove deadcodes about Python3.9 [6/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* __->__ #163645
* #163644
* #163643
* #163629
* #163627
* #163626

As the title stated.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-23 14:38:14+00:00,2025-09-24T07:31:01Z,,False,2,0,3,8,56,5,2,2025-09-24 07:30:58+00:00,51,525,False,False,False,False,False,False,5,1,55,3815,2785,1030,1,3,2.0,2.0,2025-09-23T18:10:24Z,pytorch
163644,closed,[Code Clean] Remove deadcodes about Python3.9 [5/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* #163645
* __->__ #163644
* #163643
* #163629
* #163627
* #163626

As the title stated.",2025-09-23 14:38:06+00:00,2025-09-24T07:30:58Z,,False,2,0,2,1,1,1,2,2025-09-24 07:30:57+00:00,51,196,False,False,False,False,False,False,1,1,48,4026,2778,1248,1,2,2.0,1.0,2025-09-23T15:32:46Z,pytorch
163643,closed,[Code Clean] Remove deadcodes about Python3.9 [4/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* #163645
* #163644
* __->__ #163643
* #163629
* #163627
* #163626

As the title stated.

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv",2025-09-23 14:37:59+00:00,2025-09-24T07:31:00Z,,False,3,0,2,20,33,3,3,2025-09-24 07:30:56+00:00,51,239,False,False,False,False,False,False,3,2,120,4068,2790,1278,1,2,3.0,3.0,2025-09-23T18:03:04Z,pytorch
163642,closed,Use cuda nvrtc so file based on cuda version used by torch,atalman,"Fixes https://github.com/pytorch/pytorch/issues/162367

cc @msaroufim @tinglvv @nWEIdia ",2025-09-23 14:09:45+00:00,2025-09-24T14:53:15Z,,False,5,4,3,14,2,1,9,2025-09-24 14:23:44+00:00,58,88,False,True,False,False,False,False,1,4,1492,20,16,4,1,3,5.0,4.0,2025-09-23T14:11:25Z,pytorch
163639,closed,Revert to old behaviour of not padding strides if shape or stride is dynamic,nandesuka,"Differential Revision: D83053287




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 13:09:38+00:00,2025-09-24T18:32:09Z,,False,6,4,1,9,0,1,10,2025-09-24 18:31:05+00:00,76,238,False,False,False,False,False,False,1,2,659,9,9,0,1,1,4.0,4.0,2025-09-23T15:36:48Z,pytorch
163638,closed,Fixes import error 'functionalize' not found in functorch,RiyaP-QA,"Fixes #163637 


cc @zou3519 @Chillee @samdow @kshitij12345",2025-09-23 13:08:44+00:00,2025-09-24T06:33:50Z,,False,4,0,2,1,1,1,4,2025-09-24 06:33:50+00:00,57,59,False,True,False,False,False,False,1,2,79,98,90,8,2,2,1.0,2.0,2025-09-23T13:14:52Z,pytorch
163636,open,[AOTI] [XPU] add null-pointer-deference check for `malloc` in `sycl_runtime_wrappers.h`,shaoyuyoung,"Fixes #163624
I use `TORCH_CHECK` because I think this `npd` issue can be caused by users when they try to allocate too large memory",2025-09-23 12:22:30+00:00,2025-09-25T05:01:06Z,,False,1,2,2,2,0,1,3,,87,132,False,True,False,False,False,False,1,0,39,4,3,1,1,2,2.0,1.0,2025-09-24T16:56:45Z,pytorch
163635,open,Update includes for TypeTraits/TypeList/Metaprogramming.h,lw,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163505
* __->__ #163635
* #163634



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-23 12:08:33+00:00,2025-09-25T14:55:14Z,,False,1,0,2,37,37,28,1,,57,197,False,False,False,False,False,False,28,0,0,11267,9352,1915,1,2,1.0,0.0,2025-09-25T14:55:14Z,pytorch
163634,open,Move TypeTraits/TypeList/Metaprogramming.h to header-only,lw,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163505
* #163635
* __->__ #163634

",2025-09-23 12:08:30+00:00,2025-09-23T16:19:34Z,,False,2,1,2,893,890,6,3,,57,114,False,False,False,False,False,False,6,1,256,12976,10208,2768,1,2,1.0,2.0,2025-09-23T16:13:54Z,pytorch
163633,closed,CUDA 13.0 Warning update for supported architectures,pytorchbot,"Please see build script: https://github.com/pytorch/pytorch/blob/8da008678fcb95dbf55a33451136a242871ae4e2/.ci/manywheel/build_cuda.sh#L69-L71


This should display correct warning:
``
Please install PyTorch with a following CUDA
configurations: 12.6 12.8 13.0 following instructions at
https://pytorch.org/get-started/locally/
``",2025-09-23 11:43:16+00:00,2025-09-23T17:13:06Z,2025-09-23T17:13:06Z,True,1,0,1,1,1,1,1,2025-09-23 17:13:06+00:00,52,329,False,False,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-23T17:12:58Z,pytorch
163632,closed,[3/N] Use std::filesystem in inductor,cyyever,Continued work to use std::fs in inductor.,2025-09-23 10:14:51+00:00,2025-09-24T00:24:41Z,,False,3,2,1,9,57,1,5,2025-09-24 00:23:38+00:00,37,42,False,False,False,False,False,False,1,2,676,66,9,57,1,1,3.0,2.0,2025-09-23T17:32:51Z,pytorch
163631,open,Fix torch.fx.Tracer record_stack_traces regression in PyTorch 2.4,Amrithesh-Kakkoth,"## 🐛 Fix torch.fx.Tracer record_stack_traces regression in PyTorch 2.4

### Problem
The `torch.fx.Tracer` class had a regression in PyTorch 2.4 where `record_stack_traces=True` was no longer capturing user code properly. This was caused by PR #121449 which changed the stack trace capturing mechanism, but the filtering logic wasn't effectively removing internal PyTorch frames.

**Before (PyTorch 2.3):** Stack traces contained user code  
**After (PyTorch 2.4):** Stack traces contained mostly internal PyTorch frames

### Solution
- Added `_find_user_frame_2_4()` method to `TracerBase` class that properly filters out internal PyTorch frames
- Modified stack trace recording logic in `create_node()` method to use the new filtering approach
- Added comprehensive test case `test_stack_traces_nested_modules()` to prevent regression

### Changes Made
1. **torch/fx/proxy.py**: Added new filtering method and updated stack trace recording logic
2. **test/test_fx.py**: Added test case that reproduces the original bug scenario

### Testing
- ✅ Syntax validation passed
- ✅ Code structure validation passed  
- ✅ Fix logic validation passed
- ✅ Test case validation passed
- ✅ Linting validation passed

### Test Commands
```bash
# Test the specific fix
python test/test_fx.py TestFX.test_stack_traces_nested_modules

# Test the original bug reproduction
python reproduce_bug.py

# Test all stack trace functionality  
python test/test_fx.py TestFX.test_stack_traces
```

### Expected Results
- All tests should pass
- Stack traces should contain user code (not just internal PyTorch frames)
- The assertions in the original bug report should pass

### Impact
This fix restores the functionality that was working in PyTorch 2.3, allowing developers to see actual user code in stack traces when using `record_stack_traces=True`, which is crucial for debugging FX graphs.

Fixes #130861 


cc @ezyang @EikanWang @jgong5 @wenzhe-nrv",2025-09-23 09:42:44+00:00,2025-09-24T17:09:48Z,,False,2,3,2,162,11,6,5,,65,1930,False,True,False,False,False,False,6,0,0,187,169,18,1,2,2.0,0.0,2025-09-23T15:32:09Z,pytorch
163629,closed,[Code Clean] Remove deadcodes about Python3.9 [3/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* #163645
* #163644
* #163643
* __->__ #163629
* #163627
* #163626

As the title stated.",2025-09-23 09:30:01+00:00,2025-09-24T07:30:56Z,,False,2,0,3,0,247,1,2,2025-09-24 07:30:55+00:00,51,196,False,False,False,False,False,False,1,1,55,4265,2771,1494,1,3,2.0,2.0,2025-09-23T18:01:48Z,pytorch
163628,open,refactor: replace runtime_error with TORCH_CHECK for better error handling,licy666,"Fixes some parts of issue #148114

@pytorchbot label ""topic: not user facing""

@FFFrog PTAL",2025-09-23 09:17:55+00:00,2025-09-24T08:01:02Z,,False,5,0,1,15,19,5,5,,74,91,False,True,False,False,False,True,5,4,1301,34,15,19,1,1,3.0,5.0,2025-09-23T09:27:05Z,pytorch
163627,closed,[Code Clean] Remove deadcodes about Python3.9 [2/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* #163645
* #163644
* #163643
* #163629
* __->__ #163627
* #163626

As the title stated.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-23 08:59:46+00:00,2025-09-24T07:30:59Z,,False,2,0,3,3,4,3,2,2025-09-24 07:30:54+00:00,51,412,False,False,False,False,False,False,3,1,48,4075,2783,1292,1,3,2.0,1.0,2025-09-23T15:33:14Z,pytorch
163626,closed,[Code Clean] Remove deadcodes about Python3.9 [1/N],FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163728
* #163646
* #163645
* #163644
* #163643
* #163629
* #163627
* __->__ #163626

As the title stated.",2025-09-23 08:43:58+00:00,2025-09-24T07:30:54Z,,False,3,0,2,5,9,4,3,2025-09-24 07:30:53+00:00,51,196,False,False,False,False,False,False,4,2,243,4078,2783,1295,1,2,3.0,3.0,2025-09-23T14:54:53Z,pytorch
163625,open,"Add `register`, `unregister`, `get` method for `_allowed_determinism_checks_to_fns`",zeshengzong,"Fixes #162980
",2025-09-23 08:33:41+00:00,2025-09-23T09:01:00Z,,False,2,0,1,61,0,2,2,,83,14,False,True,False,False,False,False,2,0,0,61,61,0,1,1,,,,pytorch
163623,open,Remove dataclass_slots,cyyever,"`dataclass` now has `slots` kwarg.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 08:20:01+00:00,2025-09-24T17:09:00Z,,False,2,2,1,17,154,5,4,,22,206,False,False,False,False,False,False,5,1,44,171,17,154,1,1,2.0,1.0,2025-09-23T08:21:23Z,pytorch
163622,open,[Release 2.9] Update torch-xpu-ops commit pin,CuiYifeng,"Update the torch-xpu-ops commit to [intel/torch-xpu-ops@789f59](https://github.com/intel/torch-xpu-ops/commit/789f59d8261b521282a26025c4a7a201621b4683):

- Revert tracking of Work status for FlightRecorder in ProcessGroupXCCL to fix a memory leak

Refer https://github.com/intel/torch-xpu-ops/issues/2028#issuecomment-3310297243",2025-09-23 07:47:46+00:00,2025-09-25T01:07:43Z,,False,1,0,1,1,1,1,1,,45,328,False,True,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-23T17:23:09Z,pytorch
163621,open,[Quant] extend the op list for quant lift up,Valentine233,"Add `aten.reshape.default` into the op list of quant lift up, in order to fuse more potential quantized kernels.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 07:39:29+00:00,2025-09-24T02:21:16Z,,False,1,5,2,20,15,2,6,,44,315,False,False,False,False,False,False,2,0,49,35,20,15,1,2,3.0,1.0,2025-09-23T08:06:33Z,pytorch
163620,closed,refactor: replace runtime_error with TORCH_CHECK for better error handling,licy666,"Fixes some parts of #148114
",2025-09-23 07:25:17+00:00,2025-09-23T09:07:34Z,,False,3,0,1,17,19,5,3,2025-09-23 09:07:34+00:00,74,28,False,True,False,False,False,True,5,2,54,36,17,19,1,1,1.0,2.0,2025-09-23T07:26:06Z,pytorch
163619,closed,Add `inference_mode` hint message to use `eval` with inference.,zeshengzong,"Fixes #162923

## Test Result

### Before

<img width=""985"" height=""889"" alt=""image"" src=""https://github.com/user-attachments/assets/41de5cfa-7b25-4ba4-ade8-a6df745dcb30"" />

### After

<img width=""913"" height=""977"" alt=""image"" src=""https://github.com/user-attachments/assets/b6c06860-8db3-4b5d-9d46-31ece01fb04d"" />

",2025-09-23 07:17:22+00:00,2025-09-24T20:08:21Z,,False,4,0,1,6,0,1,4,2025-09-24 20:07:18+00:00,63,318,False,True,False,False,False,False,1,3,570,6,6,0,1,1,3.0,4.0,2025-09-23T07:19:07Z,pytorch
163618,open,[Inductor][Intel GPU] Use same `build_flags` in cpp kernel loader with,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163618

triton runtime.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 07:10:40+00:00,2025-09-24T05:47:41Z,,False,1,0,2,16,10,3,1,,70,312,False,False,False,False,False,False,3,0,0,28,17,11,1,2,,,,pytorch
163617,open,[ContextParallel] add process-time based Round-Robin load-balance to CP,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163806
* __->__ #163617
* #163053
* #161062



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 06:57:24+00:00,2025-09-24T22:33:29Z,,False,2,0,6,209,10,3,2,,71,227,False,False,False,False,False,False,3,0,0,812,354,458,1,6,,,,pytorch
163616,open,Update documentation for torch.index_select,catswe,"Description said ""entries in index which is a LongTensor"" but index_select can accept an IntTensor as the parameter",2025-09-23 06:51:35+00:00,2025-09-24T21:31:53Z,,False,2,2,2,1,1,1,4,,43,115,False,False,False,True,False,False,1,0,77,4,2,2,1,2,2.0,1.0,2025-09-24T16:50:28Z,pytorch
163615,open,Supports woq_int8 inductor pattern on Intel GPU,xiaowangintel,"Summary:

Supports woq_int8 inductor pattern on Intel GPU. When using torch.compile, woq_int8 will be lowering to _weight_int8pack_mm instead of being falled back mul().sum(). The Intel GPU backend of _weight_int8pack_mm was supported in https://github.com/pytorch/pytorch/pull/160938.



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 06:30:43+00:00,2025-09-24T16:52:00Z,,False,1,1,1,23,19,2,2,,47,534,False,False,False,False,False,False,2,0,0,42,23,19,1,1,1.0,0.0,2025-09-23T06:55:01Z,pytorch
163614,closed,Implement CUDA stream protocol,msaroufim,"Fixes #ISSUE_NUMBER
",2025-09-23 06:14:22+00:00,2025-09-23T21:03:13Z,,False,9,0,5,28,0,2,9,2025-09-23 21:02:11+00:00,30,20,False,True,False,False,False,False,2,8,2271,58,43,15,1,5,3.0,8.0,2025-09-23T14:33:41Z,pytorch
163613,open,Remove python code older than 3.10,cyyever,"Because now that the minimum Python version is 3.10

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-23 05:51:32+00:00,2025-09-24T11:19:28Z,,False,1,0,2,9,13,5,1,,34,380,False,False,False,False,False,False,5,0,0,22,9,13,1,2,1.0,0.0,2025-09-23T07:52:09Z,pytorch
163612,open,Remove CI dependencies for Python <= 3.9,cyyever,"Now that we are in Python 3.10.

cc @seemethere @malfet @pytorch/pytorch-dev-infra",2025-09-23 05:39:45+00:00,2025-09-24T16:57:04Z,,False,2,0,1,1,3,1,2,,40,82,False,False,False,False,False,False,1,1,118,4,1,3,1,1,1.0,1.0,2025-09-23T14:45:26Z,pytorch
163610,open,[Code Clean] Replace `std::runtime_error` with `TORCH_CHECK`,KarhouTam,"Including:
- `torch/csrc/instruction_counter`
- `torch/csrc/lazy`
- `torch/csrc/monitor`
- `torch/csrc/profiler`
- `torch/csrc/dynamo`

Fixes part of #148114 

Personal mistake about (PR #163317), this PR does the same thing **and PR #163317 has already been approved by @albanD.**

This is a personal mistake on my part, and I'm so sorry about that. Hope you won't mind @albanD. 🥹

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 05:06:00+00:00,2025-09-25T15:01:09Z,,False,22,8,1,73,75,12,30,,60,553,False,True,False,False,False,False,12,19,4610,148,73,75,1,1,6.0,21.0,2025-09-23T05:08:46Z,pytorch
163609,open,[rfc] Supporting exporting a model with DTensor params/inputs,SherlockNoMad,"As title, one issue was that our fake mode detection didn't understand dtensor.

RFC because:
- I'm a dtensor noob so I don't know if this is the right way to use dtensor
- I don't like making torch/_guards.py aware of DTensor, looking for suggestions on alternative ways to structure the code.

Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-23 04:30:45+00:00,2025-09-24T02:20:36Z,,False,2,4,2,217,2,5,6,,61,419,False,True,False,False,False,False,5,1,109,221,218,3,2,2,2.0,1.0,2025-09-23T04:33:00Z,pytorch
163607,closed,[ez] use list initializer syntax in fill_diagonal_,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163607
* #163485

",2025-09-23 04:04:50+00:00,2025-09-23T21:28:19Z,,False,8,2,3,3,7,1,10,2025-09-23 21:27:15+00:00,50,104,False,False,False,False,False,False,1,7,1446,14,5,9,1,3,3.0,7.0,2025-09-23T17:38:32Z,pytorch
163605,open,Move non-tensor nodes on boundary after split,openzig,"Summary: See details in the post: https://fb.workplace.com/groups/aoti.productionization/permalink/743060011864813/

Test Plan: unit test

Differential Revision: D80903201




cc @ezyang @EikanWang @jgong5 @wenzhe-nrv",2025-09-23 03:24:50+00:00,2025-09-23T05:20:57Z,,False,4,0,1,491,3,3,4,,45,217,False,False,False,False,False,False,3,0,0,494,491,3,1,1,,,,pytorch
163602,open,[hop] trace local_map with local shapes for AP,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163602
* #163322

So the eager semantics of local_map are to no-op plain tensor inputs, and to redistribute DTensor inputs.

In AP, we will do the initial trace with global shapes and shard them later. This means that prior to this PR, we are incorrectly tracing the local_map body with global shapes. This can cause problems if we specialize on shapes e.g. view ops, because we would then error later when we trace again with local shapes post sharding.

The outcome of the discussion in https://docs.google.com/document/d/1qnuXLZk_GYt_PksHTwkn7L2ELRDnYlIRPkHAlXTyuhw/edit?tab=t.0 was that we would trace the local_map body with local shapes (redistribute them as if they were DTensors).

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-23 02:55:00+00:00,2025-09-25T03:41:32Z,,False,1,0,5,129,29,3,1,,46,946,False,False,False,True,False,False,3,0,0,226,166,60,1,5,,,,pytorch
163601,open,Change python grid calc for MTIA back to python mode,nandesuka,"Differential Revision: D83000165




cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 02:54:02+00:00,2025-09-24T18:50:08Z,,False,9,0,1,0,9,2,9,,52,294,False,False,False,False,False,False,2,1,42,9,0,9,1,1,1.0,1.0,2025-09-23T16:39:09Z,pytorch
163600,closed,[AOTI] Pass comments from metadata to the autotune block,desertfire,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163600

Summary: When generating Triton kernels in the compile-time autotune blocks, it will be useful to generate source information as code comments. Previously we ignore these comments for autotune code blocks because the generated main output code will contain the same information, but it won't work if the generated autotune code crashes.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @yushangdi @benjaminglass1",2025-09-23 02:20:12+00:00,2025-09-24T02:03:06Z,,False,3,0,1,5,2,1,3,2025-09-24 02:02:02+00:00,56,660,False,False,False,False,False,False,1,2,493,7,5,2,1,1,3.0,2.0,2025-09-23T17:36:18Z,pytorch
163599,open,[BE] Using std::move to reduce copy constructor calls by one.,thenumberouscode,"inspired by https://github.com/pytorch/pytorch/pull/163416
cc @jerryzh168 @Skylion007 
",2025-09-23 02:04:40+00:00,2025-09-25T02:22:48Z,,False,3,8,1,10,9,2,11,,61,87,False,False,False,False,False,False,2,1,94,19,10,9,1,1,3.0,2.0,2025-09-23T09:04:38Z,pytorch
163598,open,[SDPA] [MPS] Fixes regression in 2.8.0 for scaled_dot_product_attention using mps,Vismai-Khanderao,"Fixes #163597

- Updates fast SDPA implementations to take in query tensor stride info similar to key and value instead of assuming stride.
- Updated tests with additional transpose/permutation layouts. New tests catch the regression.

### Benchmarking with script found in [implementation PR](https://github.com/pytorch/pytorch/pull/152781#:~:text=19.8%25%20speed%20improvement-,Script%20to%20get%20perf%3A,-import%20torch%0Aimport)

Times are averaged over 100000 iterations. This change should not have any significant performance difference. Tested on an M3 Pro

### Vector Fast Path (q_len=1, k_len=256)

- Before: 0.160 ms
- After: 0.157 ms

### Vector 2-pass (q_len=1, k_len=4096)

- Before: 0.342 ms
- After: 0.339 ms

### Vector Fast Path (q_len=8, k_len=256)

- Before: 0.228 ms
- After: 0.231 ms

### Vector 2-pass (q_len=8, k_len=4096)

- Before: 0.432 ms
- After:  0.436 ms

cc @ezyang @gchanan @zou3519 @kadeng @msaroufim",2025-09-23 01:56:52+00:00,2025-09-24T17:20:15Z,,False,3,0,3,79,46,3,3,,81,935,False,True,False,False,True,False,3,1,32,299,166,133,1,3,1.0,1.0,2025-09-23T02:25:25Z,pytorch
163596,closed,[Inductor-FX] Support symbol and dynamic scalar graph inputs and outputs,blaine-rister,"# Problems
This PR fixes a few edge cases that the FX converter missed related to dynamic shapes. 

1. Inductor graphs can sometimes take `sympy.Symbol` inputs. We have logic to convert these to FX placeholder nodes. However, this logic did not update the `self.expr_to_proxy` table mapping symbols to proxy nodes. (There was existing logic to do this for `ir.TensorBox` inputs, but not `sympy.Symbol`.) This caused sympy tracing to fail when these symbol inputs were used in other expressions.

2. We lacked codegen for `ShapeAsConstantBuffer`. This IR node is seen when the graph input or output is a scalar computed from dynamic shapes.

# Fixes
a. Update `self.expr_to_proxy` when generating placeholders for `sympy.Symbol` inputs. Change `SymbolBuffer.get_example` to convert the symbol to a `torch.SymInt`, so we can populate `meta[""val""]` correctly and use the value in other computations.
b. Support `ShapeAsConstantBuffer` by tracing the sympy expression.
c. Move output generation inside the metadata hook, allowing us to populate `meta[""val""]` for the nodes computing `ShapeAsConstantBuffer`.

# Test plan
Added several new CI tests:
 1. `torch.cond` with dynamic shapes. This exposes both issues, as the predicate is a `ShapeAsConstantBuffer` and one of the subgraphs uses a symbol input, due to the closure. Also tests when the parent and subgraphs have different input shapes.
 2. Output dynamic shape scalar. This tests `ShapeAsConstantBuffer` as an output.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 01:03:07+00:00,2025-09-24T06:09:19Z,,False,6,3,19,81,14,3,9,2025-09-24 06:08:16+00:00,72,1675,False,True,False,False,False,False,3,5,1410,30218,22491,7727,1,19,4.0,5.0,2025-09-23T06:02:36Z,pytorch
163593,closed,[Kineto] Add list of string parsing for profiler,muchulee8,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160380
* __->__ #163593

Summary:
We add the parsing for list of string. This is needed for AOTInductor
profiling for input information of Triton kernels.

Test Plan:
Included in commit.
test_profiler_op_event_kwargs_list_of_strings

Reviewers:

Subscribers:

Tasks:

Tags:",2025-09-23 00:38:52+00:00,2025-09-23T22:46:55Z,,False,3,2,3,131,8,3,5,2025-09-23 22:45:51+00:00,48,352,False,False,False,False,False,False,3,2,525,462,419,43,1,3,3.0,3.0,2025-09-23T22:12:13Z,pytorch
163592,closed,[ROCm][CI] cudagraph trees ut fixes,xinyazhang,"Fixes #162125.
Fixes #160719.
Fixes #157901.
Fixes #157871.
Fixes #157761.
Fixes #157723.
Fixes #157643.
Fixes #157616.
Fixes #157556.
Fixes #157533.
Fixes #157449.
Fixes #157428.
Fixes #157413.
Fixes #157367.
Fixes #157350.
Fixes #157339.
Fixes #157312.
Fixes #157280.
Fixes #157258.
Fixes #157173.
Fixes #157143.
Fixes #157112.
Fixes #157086.
Fixes #157058.
Fixes #157035.
Fixes #156984.
Fixes #156957.
Fixes #156954.
Fixes #156922.
Fixes #156886.
Fixes #156838.
Fixes #156808.
Fixes #156801.
Fixes #156778.
Fixes #156755.
Fixes #156735.
Fixes #156693.
Fixes #152561.
Fixes #130749.
Fixes #100074.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 00:29:53+00:00,2025-09-23T15:33:44Z,,False,3,0,4,25,28,1,3,2025-09-23 14:45:20+00:00,35,916,False,True,False,False,False,False,1,2,835,61,29,32,1,4,2.0,2.0,2025-09-23T00:35:08Z,pytorch
163591,closed,[PyTorch] Add SVE128 ISA (#158932),Nicoshev,"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/158932

Importing https://github.com/pytorch/pytorch/pull/138388, as it improves SVE support for perfkernels

Test Plan: We will test it on AdFinder/AdRetriever/AdRanker offline tier

Reviewed By: r1mikey

Differential Revision: D70788867

Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-23 00:25:42+00:00,2025-09-23T00:25:54Z,2025-09-23T00:25:54Z,True,1,0,1,465,186,30,1,2025-09-23 00:25:54+00:00,34,591,False,True,False,False,True,False,30,0,0,651,465,186,1,1,,,,pytorch
163590,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-23 00:25:09+00:00,2025-09-23T14:20:59Z,,False,6,0,1,1,1,1,6,2025-09-23 04:44:18+00:00,46,156,False,False,False,False,False,False,1,5,1659,2,1,1,1,1,3.0,5.0,2025-09-23T00:25:09Z,pytorch
163589,open,[Inductor] deterministic mode,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163589


Add a deterministic mode to skip the on device benchmarking that we know should affect numeric. This include
- pad-mm
- dynamic rblock scaling
- template autotuning
- coordinate descent tuning for reduction
- reduction config autotuning in CachingAutotuner.  For reduction both RBLOCK, num_warps should affect numeric. XBLOCK does not. We can still autotune XBLOCK for reductions.

The mode definitely has perf hit.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-23 00:17:29+00:00,2025-09-25T02:52:04Z,,False,4,18,2,173,6,6,22,,29,722,False,False,False,False,False,False,6,3,2690,195,181,14,1,2,7.0,4.0,2025-09-23T17:55:32Z,pytorch
163587,closed,[export] Remove .contiguous() when saving weights to raw bytes,yiming0416,"Summary: `.contiguous()` will discard the original storage size of the tensor, and could lead to issues during loading.

Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_1D_tensor_slicing
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_2D_tensor_slicing

Differential Revision: D83016250


",2025-09-23 00:12:13+00:00,2025-09-23T16:41:13Z,,False,9,0,1,37,1,2,9,2025-09-23 15:44:58+00:00,62,324,False,False,False,False,False,False,2,6,1861,38,37,1,1,1,5.0,6.0,2025-09-23T00:15:17Z,pytorch
163586,open,[pt2][cache] local and global fresh cache helpers,nmacchioni,"Summary:

tldr;
```
from torch._inductor.cache import InMemoryCache, with_fresh_cache

cache = InMemoryCache()
cache.insert(key, value)

with cache.with_fresh_cache():
    assert cache.get(key) is None

with with_fresh_cache():
    assert cache.get(key) is None

with cache.with_fresh_cache():
    cache.insert(key_2, value_2)
    with with_fresh_cache():
        assert cache.get(key_2) is None
    assert cache.get(key_2) == value_2

assert cache.get(key) == value
```

Now that I type out an example, I realize that calling it ""with_fresh_cache"" is repetitive, so taking ideas on more suitable names

+ more preference towards Sequence over list
+ helper functions in unit tests to improve readability
+ some shuffling on names from foo to _foo for class methods which should not be called directly by the user

Test Plan:
```
buck test fbcode//mode/opt caffe2/test/inductor:cache
```

Differential Revision: D83013658


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 23:44:05+00:00,2025-09-23T15:29:42Z,,False,2,6,2,1734,642,4,8,,49,1125,False,False,False,False,True,False,4,0,0,2524,1808,716,2,2,1.0,0.0,2025-09-23T14:46:15Z,pytorch
163585,closed,CUDA 13.0 Warning update for supported architectures,atalman,"Please see build script: https://github.com/pytorch/pytorch/blob/8da008678fcb95dbf55a33451136a242871ae4e2/.ci/manywheel/build_cuda.sh#L69-L71


This should display correct warning:
``
Please install PyTorch with a following CUDA
configurations: 12.6 12.8 13.0 following instructions at
https://pytorch.org/get-started/locally/
``",2025-09-22 23:17:43+00:00,2025-09-23T11:46:01Z,,False,7,0,1,1,1,1,7,2025-09-23 11:27:16+00:00,52,329,False,False,False,False,False,False,1,6,1505,2,1,1,1,1,5.0,6.0,2025-09-22T23:23:43Z,pytorch
163584,open,Update conv1d meta kernel to match eager,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163584
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419

Fixes #163569

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 23:07:33+00:00,2025-09-23T07:21:07Z,,False,3,1,5,124,12,2,4,,40,412,False,True,False,False,False,False,2,2,946,261,184,77,1,5,2.0,2.0,2025-09-23T03:31:37Z,pytorch
163583,closed,[2.9 cherry pick][triton] update 3.5 pin to bbb06c0334a6772b92d24bde54956e675c8c6604 (#163382),davidberard98,"Includes:
* https://github.com/triton-lang/triton/pull/8211 to work around a PTXAS bug that was causing 03-matrix-multiplication tutorial matmuls to underperform due to excessive WGMMA waits
* https://github.com/triton-lang/triton/pull/8157 to fix a convert_layout bug

Verified that this passes Triton CI in https://github.com/pytorch/pytorch/pull/159158 and improves gemm perf (see https://github.com/pytorch/pytorch/issues/159704)
",2025-09-22 22:59:29+00:00,2025-09-23T01:50:55Z,2025-09-23T01:20:21Z,True,1,0,1,1,1,1,1,2025-09-23 01:20:21+00:00,94,434,False,True,False,False,True,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-23T01:18:48Z,pytorch
163582,open,NOCOMMIT: throwaway PR to repro lint issue,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163582



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 22:55:01+00:00,2025-09-23T01:50:33Z,,False,2,0,2,2,1,1,2,,42,197,False,False,False,False,False,False,1,0,0,3,2,1,1,2,,,,pytorch
163581,open,[cuDNN][Convolution] Disable cuDNN for 3D convolutions with kernel size != 1 for cuDNN 9.8+,eqy,"To workaround #163539

Still confirming whether 9.10 is affected. The original test states that the convolution is ""large,"" but note that the input size does not apepar to require 64-bit indexing.

cc @csarofeen @ptrblck @xwang233 @msaroufim @jerryzh168 @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",2025-09-22 22:52:47+00:00,2025-09-24T17:17:07Z,,False,2,5,5,29,0,2,7,,91,320,False,False,False,False,False,False,2,1,133,43,36,7,3,5,3.0,1.0,2025-09-23T20:48:46Z,pytorch
163580,open,Migrating some more callsites,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163580
* #163260
* #163259
* #163258
* #163137
* #163136
* #163107

Differential Revision: [D83009423](https://our.internmc.facebook.com/intern/diff/D83009423)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-22 22:50:47+00:00,2025-09-23T21:46:55Z,,False,4,4,2,23,11,6,8,,29,461,False,False,False,False,False,False,6,2,320,153,97,56,1,2,3.0,2.0,2025-09-22T22:52:45Z,pytorch
163579,closed,test,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163579
* #160767



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 22:39:35+00:00,2025-09-22T22:48:26Z,,False,1,0,1,7,1,2,1,2025-09-22 22:48:26+00:00,4,307,False,False,False,False,False,False,2,0,0,8,7,1,1,1,,,,pytorch
163578,closed,Fix warn message,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163578



cc @Chillee @yanboliang @BoyuanFeng",2025-09-22 22:24:59+00:00,2025-09-23T22:47:57Z,,False,9,1,1,1,1,1,10,2025-09-23 22:46:54+00:00,16,131,False,True,False,False,False,False,1,7,1726,2,1,1,1,1,6.0,8.0,2025-09-22T22:29:30Z,pytorch
163575,open,Adds Issue#153109 as a test for CUDAPluggableAllocator,syed-ahmed,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163750
* __->__ #163575

",2025-09-22 22:20:06+00:00,2025-09-24T08:07:03Z,,False,3,5,2,80,1,2,8,,54,104,False,False,False,False,False,False,2,2,359,81,80,1,1,2,4.0,2.0,2025-09-22T22:54:27Z,pytorch
163573,open,Add autograd support for jagged tensor operations,stmcgovern,"Fixes #140520 
- Add derivative formulas for _jagged_to_padded_dense_forward and _padded_dense_to_jagged_forward
- Implement CPU and CUDA backward operations
- Add meta function registrations
- Add comprehensive autograd tests including gradcheck
- Enables gradient computation for jagged<->padded dense conversions

I have run the tests on a CPU build but have not yet tested the CUDA dispatch. I'm using a modified ghstack (since I'm working from a fork) which is why there are 2 commits here.",2025-09-22 22:17:49+00:00,2025-09-24T21:20:32Z,,False,2,0,4,274,1,7,2,,49,495,False,True,False,False,False,False,7,1,265,291,282,9,1,3,1.0,1.0,2025-09-23T22:18:56Z,pytorch
163571,closed,fix pickling for BitwiseFn,dolpm,"Summary:
ran into AttributeError: Can't get local object 'make_opaque_bitwise_fn.<locals>.BitwiseFn'

looks like it was fixed for UnaryFn but not BitwiseFn in https://github.com/pytorch/pytorch/pull/138395

Fixes #147841


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 22:02:09+00:00,2025-09-25T04:53:18Z,,False,19,0,1,17,5,3,19,2025-09-25 04:52:15+00:00,26,480,False,True,False,False,False,False,3,17,6457,22,17,5,1,1,4.0,18.0,2025-09-23T14:57:28Z,pytorch
163570,closed,[Inductor] Remove `no_type_check` annotation on properties,blaine-rister,"Some properties with `cache_on_self` were prevously annotated with `no_type_check`, to get around mypy limitations. This PR replaces both annotations with `cache_property_on_self`, to enable type checking.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 21:59:00+00:00,2025-09-23T18:21:11Z,,False,11,0,1,5,8,1,11,2025-09-23 18:20:07+00:00,58,409,False,False,False,False,False,False,1,9,2583,13,5,8,1,1,5.0,9.0,2025-09-22T22:03:06Z,pytorch
163566,open,Use _RecordFunctionFast for torch.autograd.profiler.record_function,azahed98,"This changes `record_function` to use _RecordFunctionFast, which is more efficient. This is important for Compiler runtime overhead in smaller graphs. Currently this profiler takes half of the overhead.

To measure impact on runtime overhead, here are results from [benchmarks/dynamo/microbenchmarks/overheads.py](https://github.com/pytorch/pytorch/blob/main/benchmarks/dynamo/microbenchmarks/overheads.py).

Before changes:
```
requires_grad=False
compiled 56.9us (warmup=19.3s)

requires_grad=True
compiled 101.6us (warmup=0.2s)

inference_mode()
compiled 53.4us (warmup=0.1s)
```

After changes:
```
requires_grad=False
compiled 27.2us (warmup=9.9s)

requires_grad=True
compiled 61.6us (warmup=0.1s)

inference_mode()
compiled 25.3us (warmup=0.1s)
```",2025-09-22 21:46:52+00:00,2025-09-24T09:33:44Z,,False,4,3,5,9,11,2,7,,67,754,False,False,False,False,False,False,2,3,7023,28,13,15,1,5,2.0,3.0,2025-09-22T22:06:05Z,pytorch
163565,closed,Add mistral/gpt-oss to benchmarks,angelayi,"Potential issues
* gpt-oss-20b is probably too big (I can't run on my devserver)
* Mistral requires HF authentication
* Mistral also takes a while to run the performance checks (need to wait for CI)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 21:46:16+00:00,2025-09-24T06:13:44Z,,False,5,0,1,94,0,15,5,2025-09-24 06:12:40+00:00,33,371,False,False,False,False,False,False,15,4,1197,94,94,0,1,1,2.0,4.0,2025-09-23T00:11:45Z,pytorch
163564,closed,[EZ] Remove XLA from unstable.yml,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163564

It runs for 30 min on linux.12xlarge and then fails and it has been like
that since Aug 7th

Besides, there are no more python-3.9 builds left.",2025-09-22 21:44:23+00:00,2025-09-22T22:12:57Z,,False,3,0,1,0,24,1,3,2025-09-22 22:11:53+00:00,33,237,False,False,False,False,False,False,1,2,781,24,0,24,1,1,5.0,2.0,2025-09-22T21:45:30Z,pytorch
163562,closed,[ROCm] Improve perf for elementwise broadcast with mixed dtype,jerrymannil,"* Unroll loops manually to hide memory access latency

Co-author: @amd-hhashemi

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-22 21:29:08+00:00,2025-09-23T18:01:41Z,,False,5,0,1,29,0,1,5,2025-09-23 17:42:52+00:00,62,197,False,False,False,False,True,False,1,4,2880,29,29,0,1,1,3.0,4.0,2025-09-22T21:39:57Z,pytorch
163561,closed,[AOTI] Fix model_package_loader get_cpp_compile_command,xuhancn,"It should fix AOTI UTs of `test_aot_inductor_package.py`, these cases are failed at `compile_so`.

reproducer:
```cmd
pytest test\inductor\test_aot_inductor_package.py -v -k test_multiple_methods
```
<img width=""1262"" height=""95"" alt=""image"" src=""https://github.com/user-attachments/assets/49458536-1cfe-498e-a12a-2bfd8da67a9e"" />

Major fix at `get_cpp_compile_command`. The code is aligned to cpp_builder frontend code:  https://github.com/pytorch/pytorch/blob/3ef1bef36c73b4def0e1b71847e27fde1556c0fb/torch/_inductor/cpp_builder.py#L1780-L1790
https://github.com/pytorch/pytorch/blob/3ef1bef36c73b4def0e1b71847e27fde1556c0fb/torch/_inductor/cpp_builder.py#L1959-L1976

Fixed on Windows:
<img width=""1261"" height=""89"" alt=""Image"" src=""https://github.com/user-attachments/assets/9bf43b11-aac1-4161-a625-e602e313a299"" />

Also validated on Linux:
<img width=""1039"" height=""81"" alt=""Image"" src=""https://github.com/user-attachments/assets/46063e16-6cf1-4a28-8466-0496871b8619"" />

",2025-09-22 21:23:14+00:00,2025-09-23T17:43:56Z,,False,6,2,6,75,26,1,8,2025-09-23 17:38:22+00:00,55,979,False,True,False,False,False,False,1,5,996,163,106,57,2,6,3.0,5.0,2025-09-23T06:22:07Z,pytorch
163560,closed,[torchfuzz] introduce multi process fuzzer,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163560
* #163558
* #163557
* #163556
* #163555
* #163554
* #163553
* #163547

",2025-09-22 21:08:32+00:00,2025-09-23T22:01:58Z,,False,4,11,7,474,49,2,15,2025-09-23 22:00:54+00:00,42,164,False,False,False,False,False,False,2,3,624,661,544,117,1,7,3.0,3.0,2025-09-22T22:04:46Z,pytorch
163559,open,[MTIA] Enable deserialization for FP8 checkpoint loading,PatriceVignola,"Summary: It looks like loading FP8 checkpoints goes through that path which wasn't enabled for MTIA beforehand, whereas loading BF16 checkpoints didn't.

Differential Revision: D82997140


",2025-09-22 21:01:48+00:00,2025-09-25T13:39:52Z,,False,4,0,1,6,0,2,4,,56,189,False,False,False,False,False,False,2,1,84,6,6,0,1,1,2.0,2.0,2025-09-22T21:03:24Z,pytorch
163558,closed,[torchfuzz] introduce tensor and scalar pointwise ops,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* __->__ #163558
* #163557
* #163556
* #163555
* #163554
* #163553
* #163547

",2025-09-22 20:57:35+00:00,2025-09-23T06:21:19Z,,False,9,0,3,57,145,6,9,2025-09-23 06:20:16+00:00,53,164,False,False,False,False,False,False,6,8,3027,204,59,145,1,3,3.0,8.0,2025-09-22T22:03:57Z,pytorch
163557,closed,[torchfuzz] remove unneeded try catch,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* __->__ #163557
* #163556
* #163555
* #163554
* #163553
* #163547

",2025-09-22 20:57:30+00:00,2025-09-23T06:06:16Z,,False,5,0,3,5,11,1,5,2025-09-23 06:05:12+00:00,37,164,False,False,False,False,False,False,1,4,756,18,7,11,1,3,3.0,4.0,2025-09-22T22:03:13Z,pytorch
163556,closed,[torchfuzz] shuffle compatible ops,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* #163557
* __->__ #163556
* #163555
* #163554
* #163553
* #163547

",2025-09-22 20:57:26+00:00,2025-09-23T05:54:51Z,,False,5,0,3,2,0,1,5,2025-09-23 05:53:48+00:00,34,164,False,False,False,False,False,False,1,4,756,4,4,0,1,3,3.0,4.0,2025-09-22T22:02:44Z,pytorch
163555,closed,[torchfuzz] decompose -> fuzz_inputs_specs,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* #163557
* #163556
* __->__ #163555
* #163554
* #163553
* #163547

",2025-09-22 20:57:20+00:00,2025-09-23T05:46:06Z,,False,5,0,3,24,22,10,5,2025-09-23 05:45:02+00:00,42,164,False,False,False,False,False,False,10,4,856,48,26,22,1,3,3.0,4.0,2025-09-22T22:02:15Z,pytorch
163554,closed,[torchfuzz] cache operators,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* #163557
* #163556
* #163555
* __->__ #163554
* #163553
* #163547

",2025-09-22 20:57:15+00:00,2025-09-23T05:29:14Z,,False,5,0,3,14,2,1,5,2025-09-23 05:28:10+00:00,27,164,False,False,False,False,False,False,1,4,756,17,15,2,1,3,3.0,4.0,2025-09-22T22:00:58Z,pytorch
163553,closed,[torchfuzz] remove supports_variable_inputs for now,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* #163557
* #163556
* #163555
* #163554
* __->__ #163553
* #163547

",2025-09-22 20:57:10+00:00,2025-09-23T05:01:22Z,,False,5,0,2,0,32,8,5,2025-09-23 04:44:57+00:00,51,164,False,False,False,False,False,False,8,4,1016,33,1,32,1,2,3.0,4.0,2025-09-22T22:00:37Z,pytorch
163552,closed,Skip on sm100 later since Tests are non determinisitic,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #159494
* __->__ #163552
* #163537


This is tracked https://github.com/pytorch/pytorch/issues/163462

skipping since we are seeing sporadic errors locally and on CI,",2025-09-22 20:38:48+00:00,2025-09-23T15:45:09Z,,False,3,1,3,1,2,1,4,2025-09-23 15:45:09+00:00,54,244,False,False,False,False,False,False,1,2,96,489,197,292,1,3,4.0,2.0,2025-09-22T22:20:47Z,pytorch
163551,closed,[BE] Delete HermeticPyObjectTLS,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #163551

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-22 20:36:34+00:00,2025-09-22T22:29:08Z,,False,3,0,1,13,172,8,3,2025-09-22 22:28:56+00:00,31,150,False,False,False,False,False,False,8,2,98,185,13,172,1,1,2.0,2.0,2025-09-22T20:48:35Z,pytorch
163550,closed,docs and optional kwargs for full graph capture,avikchaudhuri,"Test Plan: existing tests

Differential Revision: D82995546


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 20:36:12+00:00,2025-09-24T01:21:34Z,,False,11,0,1,42,33,3,11,2025-09-24 01:20:31+00:00,47,232,False,False,False,True,False,False,3,1,476,75,42,33,1,1,2.0,1.0,2025-09-22T22:30:39Z,pytorch
163549,open,[Reland][163423] Promote `@requires_nvshmem` instead of `enable_triton`,kwen2501,"#163423 was approved but reverted due to a revert of base.
Relanding without base.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 20:31:44+00:00,2025-09-25T15:20:56Z,,False,9,0,3,276,176,2,9,,71,186,False,False,False,False,False,False,2,8,2038,488,294,194,2,3,2.0,8.0,2025-09-22T20:32:25Z,pytorch
163547,closed,[torchfuzz] Encapsulate fuzzing and codegen logic into ops,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163560
* #163558
* #163557
* #163556
* #163555
* #163554
* #163553
* __->__ #163547

",2025-09-22 19:19:09+00:00,2025-09-23T04:27:07Z,,False,5,12,3,809,357,16,17,2025-09-23 04:26:02+00:00,58,164,False,False,False,False,False,False,16,4,1076,1364,908,456,1,3,3.0,4.0,2025-09-22T19:47:39Z,pytorch
163544,closed,[nvm],bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163544

",2025-09-22 19:05:39+00:00,2025-09-22T19:17:22Z,,False,1,0,2,1137,355,34,1,2025-09-22 19:17:22+00:00,5,94,False,False,False,False,False,False,34,0,0,1752,1267,485,1,2,,,,pytorch
163542,closed,Fix lint,angelayi,"Fixes #ISSUE_NUMBER
",2025-09-22 19:04:01+00:00,2025-09-22T19:11:06Z,,False,3,0,1,0,1,1,3,2025-09-22 19:10:04+00:00,8,20,False,True,False,False,False,False,1,2,783,1,0,1,1,1,2.0,2.0,2025-09-22T19:07:47Z,pytorch
163541,closed,Add XFails for tests erroing on B200,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #159494
* __->__ #163541
* #163537
* #163460

",2025-09-22 18:52:18+00:00,2025-09-22T20:34:03Z,,False,2,0,1,21,0,2,2,2025-09-22 20:34:03+00:00,36,124,False,False,False,False,False,False,2,1,122,21,21,0,1,1,2.0,1.0,2025-09-22T19:18:02Z,pytorch
163540,open,[SymmMem] Add get_nbi the nonblocking version,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163540

```Py
@triton.jit
def foo(dest, src):
    nvshmem.get_nbi(dest, src, 100, 0)
    # Some independent computation which overlaps with the get operation
    ...
    # Wait for completion of the get operation
    nvshmem.quiet()
```

Allows us to overlap comm and compute in the same kernel, instead of two kernels + signals.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 18:45:37+00:00,2025-09-22T22:02:34Z,,False,4,0,1,64,2,2,4,,45,530,False,False,False,False,False,False,2,3,1066,66,64,2,1,1,4.0,3.0,2025-09-22T18:52:54Z,pytorch
163538,open,[TESTING] Perf operator microbenchmarks on 2_9,jainapurva,"Fixes #ISSUE_NUMBER
",2025-09-22 18:19:32+00:00,2025-09-25T08:48:51Z,,False,1,0,6,2,1,1,1,,46,20,False,True,False,False,False,False,1,0,0,473,224,249,2,5,,,,pytorch
163537,closed,Large tests failing on bfloat16,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #159494
* #163552
* __->__ #163537

# Summary

I ran these tests locally, each 10k Tests takes over 5 mins for an extremely beefy cpu to run. I think that this is overkill feel free to disagree. Also the 1 test I ran that failed earlier up in the stack failed with 1 ulp difference so I think that this is kind of an edgecase on how we do testing (will right up issue for my thoughts later)

``` Shell
==================================================================================================== FAILURES =====================================================================================================
_________________________________________________________ TestMatmulCudaCUDA.test_cublas_addmm_reduced_precision_size_10000_backend_cublas_cuda_bfloat16 __________________________________________________________
Traceback (most recent call last):
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 58, in testPartExecutor
    yield
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 634, in run
    self._callTestMethod(testMethod)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3223, in wrapper
    method(*args, **kwargs)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3223, in wrapper
    method(*args, **kwargs)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 426, in instantiated_test
    result = test(self, **param_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1408, in only_fn
    return fn(slf, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 2024, in wrap_fn
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/meta/pytorch/test/test_matmul_cuda.py"", line 190, in test_cublas_addmm_reduced_precision
    self.cublas_addmm(size, dtype, True)
  File ""/home/dev/meta/pytorch/test/test_matmul_cuda.py"", line 162, in cublas_addmm
    assert_close_with_ulp(res_cpu, res_cuda, atol=tolerance.atol, rtol=tolerance.rtol)
  File ""/home/dev/meta/transformer_nuggets/transformer_nuggets/numerics/__init__.py"", line 222, in assert_close_with_ulp
    raise AssertionError(""\n"".join(error_parts))
AssertionError: Tensor-likes are not close!

Mismatched elements: 425 / 100030002 (0.0%)
Greatest absolute difference: 16 at index (2176, 9325) (up to 10 allowed)
Greatest relative difference: 3984 at index (376, 3754) (up to 0.2 allowed)

============================================================
ULP Analysis of Failures:
============================================================

Total failures: 425
ULP distances: min=-32761, max=32763, mean=-11513.7

Top 10 failures by absolute difference:
  #  | Index                      | Abs Diff    | Rel Diff    | ULP  | Expected     | Actual
----------------------------------------------------------------------------------------------------
   1 | (6923, 1580)               | 1.600000e+01 | 5.390625e-01 |  146 |    29.750000 |    13.750000
   2 | (4677, 420)                | 1.600000e+01 | 6.601562e-01 |   95 |    24.250000 |    40.250000
   3 | (2176, 9325)               | 1.600000e+01 | 6.875000e-01 |  210 |    23.250000 |     7.250000
   4 | (5119, 7865)               | 1.600000e+01 | 1.164062e+00 |  146 |   -13.750000 |   -29.750000
   5 | (3218, 8334)               | 1.600000e+01 | 2.593750e+00 |  236 |     6.156250 |    22.125000
   6 | (5245, 241)                | 1.600000e+01 | 5.468750e-01 |   75 |    29.250000 |    45.250000
   7 | (7666, 6549)               | 1.600000e+01 | 1.640000e+03 | 1376 |    -0.009766 |   -16.000000
   8 | (1663, 1115)               | 1.593750e+01 | 8.375000e+00 | -32427 |     1.898438 |   -14.062500
   9 | (3967, 7708)               | 1.593750e+01 | 1.368750e+01 | -32510 |     1.164062 |   -14.750000
  10 | (2874, 2038)               | 1.593750e+01 | 1.710938e+00 |  181 |     9.312500 |    25.250000

Note: Maximum absolute and relative errors occur at different locations
  Max abs diff location (2176, 9325): 210 ULP
  Max rel diff location (376, 3754): 31868 ULP

To execute this test, run the following from the base repo dir:
    python test/test_matmul_cuda.py TestMatmulCudaCUDA.test_cublas_addmm_reduced_precision_size_10000_backend_cublas_cuda_bfloat16

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
________________________________________________________ TestMatmulCudaCUDA.test_cublas_addmm_reduced_precision_size_10000_backend_cublaslt_cuda_bfloat16 _________________________________________________________
Traceback (most recent call last):
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 58, in testPartExecutor
    yield
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 634, in run
    self._callTestMethod(testMethod)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/unittest/case.py"", line 589, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3223, in wrapper
    method(*args, **kwargs)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 3223, in wrapper
    method(*args, **kwargs)
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 426, in instantiated_test
    result = test(self, **param_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_device_type.py"", line 1408, in only_fn
    return fn(slf, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/.conda/envs/nightly/lib/python3.12/site-packages/torch/testing/_internal/common_utils.py"", line 2024, in wrap_fn
    return fn(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/dev/meta/pytorch/test/test_matmul_cuda.py"", line 190, in test_cublas_addmm_reduced_precision
    self.cublas_addmm(size, dtype, True)
  File ""/home/dev/meta/pytorch/test/test_matmul_cuda.py"", line 162, in cublas_addmm
    assert_close_with_ulp(res_cpu, res_cuda, atol=tolerance.atol, rtol=tolerance.rtol)
  File ""/home/dev/meta/transformer_nuggets/transformer_nuggets/numerics/__init__.py"", line 222, in assert_close_with_ulp
    raise AssertionError(""\n"".join(error_parts))
AssertionError: Tensor-likes are not close!

Mismatched elements: 425 / 100030002 (0.0%)
Greatest absolute difference: 16 at index (2176, 9325) (up to 10 allowed)
Greatest relative difference: 3984 at index (376, 3754) (up to 0.2 allowed)

============================================================
ULP Analysis of Failures:
============================================================

Total failures: 425
ULP distances: min=-32761, max=32763, mean=-11513.7

Top 10 failures by absolute difference:
  #  | Index                      | Abs Diff    | Rel Diff    | ULP  | Expected     | Actual
----------------------------------------------------------------------------------------------------
   1 | (6923, 1580)               | 1.600000e+01 | 5.390625e-01 |  146 |    29.750000 |    13.750000
   2 | (4677, 420)                | 1.600000e+01 | 6.601562e-01 |   95 |    24.250000 |    40.250000
   3 | (2176, 9325)               | 1.600000e+01 | 6.875000e-01 |  210 |    23.250000 |     7.250000
   4 | (5119, 7865)               | 1.600000e+01 | 1.164062e+00 |  146 |   -13.750000 |   -29.750000
   5 | (3218, 8334)               | 1.600000e+01 | 2.593750e+00 |  236 |     6.156250 |    22.125000
   6 | (5245, 241)                | 1.600000e+01 | 5.468750e-01 |   75 |    29.250000 |    45.250000
   7 | (7666, 6549)               | 1.600000e+01 | 1.640000e+03 | 1376 |    -0.009766 |   -16.000000
   8 | (1663, 1115)               | 1.593750e+01 | 8.375000e+00 | -32427 |     1.898438 |   -14.062500
   9 | (3967, 7708)               | 1.593750e+01 | 1.368750e+01 | -32510 |     1.164062 |   -14.750000
  10 | (2874, 2038)               | 1.593750e+01 | 1.710938e+00 |  181 |     9.312500 |    25.250000

Note: Maximum absolute and relative errors occur at different locations
  Max abs diff location (2176, 9325): 210 ULP
  Max rel diff location (376, 3754): 31868 ULP

To execute this test, run the following from the base repo dir:
    python test/test_matmul_cuda.py TestMatmulCudaCUDA.test_cublas_addmm_reduced_precision_size_10000_backend_cublaslt_cuda_bfloat16

This message can be suppressed by setting PYTORCH_PRINT_REPRO_ON_FAILURE=0
```
Okay the bfloat16 are forsure  real cc @eqy ",2025-09-22 18:18:33+00:00,2025-09-23T15:45:08Z,,False,5,0,4,12,0,1,5,2025-09-23 15:45:08+00:00,31,9192,False,False,False,False,False,False,1,4,774,494,199,295,1,4,5.0,4.0,2025-09-22T18:28:33Z,pytorch
163535,open,Fixes the sparse tensor and the named tensor issues,arkadip-maitra,"Fixes #148324 
",2025-09-22 17:37:20+00:00,2025-09-25T10:00:41Z,,False,1,5,4,34,1,4,6,,51,15,False,True,False,False,False,False,4,0,0,20338,10824,9514,2,4,2.0,0.0,2025-09-22T21:04:36Z,pytorch
163534,closed,Rename to _debug_mode.py to make it private,SherlockNoMad,"rename debug_mode.py to _debug_mode.py to make it private, per @alban's request. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 17:32:10+00:00,2025-09-23T04:28:16Z,,False,6,0,1,3,6,6,6,2025-09-23 04:27:13+00:00,43,184,False,True,False,False,False,False,6,5,1691,9,3,6,1,1,3.0,6.0,2025-09-22T17:37:07Z,pytorch
163533,open,Megacache integration,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163533
* #163521

This diff adds megacache integration for DynamoCache.

Because DynamoCache requires lazy serialization, i.e. it can only be serialized once all relevant backends have been compiled and we're ready for a save, we actually do the DynamoCache saving only on a call to `torch.compiler.save_cache_artifacts`.

Differential Revision: [D82735763](https://our.internmc.facebook.com/intern/diff/D82735763/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-22 17:24:54+00:00,2025-09-23T23:20:43Z,,False,4,1,3,165,9,4,5,,21,717,False,False,False,False,False,False,4,0,66,199,181,18,1,3,1.0,1.0,2025-09-23T20:21:58Z,pytorch
163532,closed,[DCP] DTensor slice dequantization with proper block alignment,saumishr,"Summary:
When loading quantized tensors with DTensor slicing, the dequantization process was producing numerically incorrect results due to improper block-to-slice coordinate mapping. The previous implementation calculated block boundaries relative to the sliced tensor dimensions instead of the original full tensor dimensions, causing scale factors to be applied to wrong tensor regions.

This fix addresses the issue by:

1. **Proper coordinate mapping**: Added `_get_slice_to_block_mapping()` to correctly map tensor slices to quantization blocks using global coordinates from the full tensor shape.

3. **Block-aligned dequantization**: Updated `_dequantize_tensor()` to use proper block intersection logic, ensuring scale factors are applied to the correct portions of sliced tensors.

The fix ensures that when DTensor requests a slice of a quantized tensor, the dequantization correctly identifies which quantization blocks intersect with the requested slice and applies the appropriate scale factors to the right tensor regions.

Test Plan:
Tested with DTensor configurations where quantized tensors are sliced across different dimensions. Verified that:
1. Dequantized tensor values are numerically correct
2. Block boundaries are properly calculated relative to full tensor shape
3. Scale factors are applied to correct tensor regions
4. Tensor shapes map is built efficiently using only metadata

Correctness validation using https://github.com/wwwjn/torchtitan/blob/dsv3-sd-test/tests/fsdp_dequantized_load.py
```
{
  ""model.layers.0.mlp.gate_proj.weight"": {
    ""mse"": 4.30626645453458e-11,
    ""mae"": 9.98388827611052e-07,
    ""max_abs_diff"": 0.0009703934192657471,
    ""cosine_similarity"": 1.010810375213623,
    ""relative_error"": 0.001330620958469808,
    ""kl_divergence_1_to_2"": ""6.563401e-08"",
    ""kl_divergence_2_to_1"": ""-6.522914e-08"",
    ""js_divergence"": 1.3711876079014476e-10,
    ""shape"": [
      18432,
      7168
    ],
    ""t1_stats"": {
      ""min"": -0.4453125,
      ""max"": 0.30859375,
      ""mean"": -1.2592146958922967e-05
    },
    ""t2_stats"": {
      ""min"": -0.44529813528060913,
      ""max"": 0.3085886240005493,
      ""mean"": -1.2624391274584923e-05
    }
  },
  ""model.layers.0.mlp.up_proj.weight"": {
    ""mse"": 2.5534721906361746e-11,
    ""mae"": 3.118609583907528e-06,
    ""max_abs_diff"": 0.00047551095485687256,
    ""cosine_similarity"": 1.038962483406067,
    ""relative_error"": 0.0013681650161743164,
    ""kl_divergence_1_to_2"": ""-5.8253768e-08"",
    ""kl_divergence_2_to_1"": ""5.8747577e-08"",
    ""js_divergence"": NaN,
    ""shape"": [
      18432,
      7168
    ],
    ""t1_stats"": {
      ""min"": -0.228515625,
      ""max"": 0.2333984375,
      ""mean"": 8.862222955485777e-08
    },
    ""t2_stats"": {
      ""min"": -0.2285017967224121,
      ""max"": 0.23338991403579712,
      ""mean"": 8.824501662729745e-08
    }
  },
  ""model.layers.0.mlp.down_proj.weight"": {
    ""mse"": 2.2803769289536646e-11,
    ""mae"": 2.8916260816913564e-06,
    ""max_abs_diff"": 0.0008973777294158936,
    ""cosine_similarity"": 1.0376262664794922,
    ""relative_error"": 0.001346255769021809,
    ""kl_divergence_1_to_2"": ""1.2744896e-07"",
    ""kl_divergence_2_to_1"": ""-1.2736885e-07"",
    ""js_divergence"": 5.992362162032805e-11,
    ""shape"": [
      7168,
      18432
    ],
    ""t1_stats"": {
      ""min"": -0.54296875,
      ""max"": 0.546875,
      ""mean"": -2.9487239316949854e-07
    },
    ""t2_stats"": {
      ""min"": -0.5429964661598206,
      ""max"": 0.5469087362289429,
      ""mean"": -2.9507478416235244e-07
    }
  }
}
```

https://www.internalfb.com/intern/testinfra/testrun/3940649985202645

Differential Revision: D82975005




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 17:17:44+00:00,2025-09-23T16:49:24Z,,False,3,0,1,229,41,2,3,2025-09-23 16:48:21+00:00,62,3737,False,True,False,False,False,False,2,2,670,270,229,41,1,1,3.0,3.0,2025-09-22T21:42:33Z,pytorch
163531,open,[CI] Update legacy nvidia driver to: 570.133.07,atalman,"Periodic failures when using legacy nvidia driver: https://github.com/pytorch/pytorch/actions/runs/17818330158/job/50660333689

Looks like we are getting something like after downgrading CUDA driver:
```
No devices were found
+ nvidia-smi --query-gpu=gpu_name --format=csv,noheader --id=0
No devices were found
+ NVIDIA_SMI_STATUS=6
+ '[' 6 -eq 0 ']'
+ '[' 6 -eq 14 ']'
+ echo 'ERROR: nvidia-smi exited with unresolved status 6'
+ exit 6
ERROR: nvidia-smi exited with unresolved status 6
```",2025-09-22 17:01:54+00:00,2025-09-23T03:24:23Z,,False,2,2,3,1,1,1,4,,47,491,False,False,False,False,False,False,1,1,142,6,3,3,2,3,2.0,2.0,2025-09-22T17:32:08Z,pytorch
163529,closed,[Inductor] avoid CUDA__equal when constant tensors are from different device,cp2923,"Summary:
otherwise, may hit
```
Exception: Expected all tensors to be on the same device, but got other is on cuda:0, different from other tensors on cpu (when checking argument in method wrapper_CUDA__equal)
```

Test Plan: UTs

Reviewed By: yushangdi

Differential Revision: D82974062




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 16:48:15+00:00,2025-09-22T22:05:19Z,,False,5,0,1,6,4,2,5,2025-09-22 22:04:15+00:00,76,492,False,False,False,False,False,False,2,1,476,10,6,4,1,1,3.0,1.0,2025-09-22T17:44:12Z,pytorch
163527,open,Add magic TORCH_MAKE_PYBIND_ENUM_FASTER macro,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163527

See comment on the macro definition. In short, pybind11 3.x
added `py::native_enum`, and also had to add overhead for that new way
to bind enums on the critical path for calling functions that take
regular old `py::enum_`s as arguments (for example, `__eq__`).

Differential Revision: [D82873169](https://our.internmc.facebook.com/intern/diff/D82873169/)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 16:41:02+00:00,2025-09-24T05:34:05Z,,False,8,0,4,79,0,15,8,,45,733,False,False,False,False,False,False,15,3,695,113,96,17,1,4,3.0,4.0,2025-09-22T18:29:15Z,pytorch
163526,closed,"DO NOT MERGE, dummy change",seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163526
* #163525

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-09-22 16:06:06+00:00,2025-09-23T21:54:54Z,,False,1,0,3,2,0,1,1,2025-09-23 21:54:54+00:00,26,164,False,False,False,False,False,False,1,0,0,26,17,9,1,3,,,,pytorch
163525,closed,ci: Add a way to lint all files in a PR from label,seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163526
* __->__ #163525

Works by either applying `lint-all-files` and should automatically apply when `Reverted` appears on PRs as well

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-09-22 16:05:11+00:00,2025-09-22T21:22:36Z,,False,4,3,3,23,9,2,7,2025-09-22 18:06:42+00:00,50,277,False,False,False,False,False,False,2,3,872,50,32,18,1,3,4.0,3.0,2025-09-22T16:41:18Z,pytorch
163524,open,[WOQ][Inductor] Add test for user facing path of _weight_int8pack_mm,bbeckca,"Summary:
What: Add test for user facing path of _weight_int8pack_mm

Why: Confirm inuctor rewrites pattern to _weight_int8pack_mm for CUDA.

Test Plan:
```
buck2 run 'fbcode//mode/opt' fbcode//caffe2/test/inductor:test_inductor_cuda -- caffe2.test.inductor.test_torchinductor.GPUTests.test_int8_weight_only_quant_user_entry
```

Differential Revision: D82927633


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 15:50:40+00:00,2025-09-22T20:34:23Z,,False,3,0,2,30,1,1,3,,68,565,False,False,False,False,False,False,1,0,0,31,30,1,1,2,,,,pytorch
163523,closed,Fix all lint errors,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163523
* #163512
* #163511

Summary:

Lint fixed",2025-09-22 15:31:24+00:00,2025-09-25T13:56:46Z,,False,2,0,10,802,413,6,2,2025-09-25 13:56:46+00:00,19,134,False,True,False,False,False,False,6,0,0,1233,811,422,1,10,,,,pytorch
163522,closed,[CI] Update NVIDIA driver to `580.82.07`,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163111


To make CI machines capable of running CUDA-13 tests. Unfortunately, this upgrade regresses NUMBA integration, so live patch it with https://github.com/NVIDIA/numba-cuda/commit/6e08c9d08e9de59c7af28b720289debbbd384764

This fix was suggested in https://github.com/pytorch/pytorch/issues/162878#issuecomment-3288635745
",2025-09-22 15:29:38+00:00,2025-09-22T15:46:22Z,2025-09-22T15:45:48Z,True,1,0,1,36,1,3,1,2025-09-22 15:45:48+00:00,40,413,False,True,False,False,False,False,3,0,4,37,36,1,1,1,1.0,1.0,2025-09-22T15:42:14Z,pytorch
163521,open,Support partial _DynamoCacheEntries when not all backends available,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163533
* __->__ #163521

Differential Revision: [D82735769](https://our.internmc.facebook.com/intern/diff/D82735769/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 15:25:42+00:00,2025-09-23T21:20:54Z,,False,5,0,4,89,33,4,5,,67,368,False,False,False,False,False,False,4,0,0,138,97,41,1,3,,,,pytorch
163520,closed,[inductor] Fix bugs in emulate_precision_casts,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163377
* #163482
* __->__ #163520
* #163481

Fixes #163449

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 15:02:18+00:00,2025-09-24T02:52:43Z,,False,2,2,4,139,11,3,4,2025-09-24 02:52:42+00:00,46,360,False,True,False,False,False,False,3,1,53,400,206,194,1,4,2.0,2.0,2025-09-23T16:54:16Z,pytorch
163518,closed,[BE] Use `output_t` directly,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163518

Rather than deref the safe tensor wrapped in `TensorArg`",2025-09-22 14:41:01+00:00,2025-09-22T21:34:48Z,,False,3,0,2,3,3,1,3,2025-09-22 21:33:44+00:00,28,150,False,False,False,False,False,False,1,2,789,8725,7577,1148,1,2,3.0,2.0,2025-09-22T17:17:22Z,pytorch
163517,open,[DO NOT MERGE] Dynamo config set up,sekyondaMeta,"Setting up Dynamo config doc


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 14:40:50+00:00,2025-09-23T21:20:36Z,,False,1,3,12,112,1,3,4,,35,201,False,False,False,True,False,False,3,0,0,183,148,35,3,12,1.0,0.0,2025-09-22T15:38:19Z,pytorch
163516,closed,Improve fake tensor leakage detection in export by not relying on gc too much,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163516

Previously we relied on gc to get the snapshot of fake tensors before and after export to get list of fake tensors that are created during export. This caused some flakiness in our test suite (https://github.com/pytorch/pytorch/issues/162232). it seems super hard to make gc deterministic, so we just instrument fake tensor creation which seems lot better. In addition, it is also quite faster than previous approach becuase we are no longer manually triggering garbage collector. 


cc @ezyang @EikanWang @jgong5 @wenzhe-nrv

Differential Revision: [D82966648](https://our.internmc.facebook.com/intern/diff/D82966648)",2025-09-22 14:38:39+00:00,2025-09-22T22:06:24Z,,False,11,0,2,31,76,5,11,2025-09-22 22:04:27+00:00,77,712,False,False,False,False,True,False,5,9,1948,107,31,76,1,2,4.0,11.0,2025-09-22T14:39:33Z,pytorch
163515,closed,[BE] Delete `skipIfMPSOnMacOS13`,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163515

As PyTorch needs MacOS-14 or newer to use MPS",2025-09-22 14:38:23+00:00,2025-09-22T21:11:30Z,,False,3,0,2,4,69,2,3,2025-09-22 21:10:25+00:00,32,139,False,False,False,False,False,False,2,2,783,73,4,69,1,2,3.0,2.0,2025-09-22T18:30:10Z,pytorch
163513,open,Reduce divergence between c10 and c10_ovrsource by flipping glog and link_whole defaults,ezyang,"Summary:
Carved out of D82283623

https://fb.workplace.com/groups/463532478654961/permalink/1116112103396992/ is also some useful context

#buildall

Test Plan: sandcastle

Reviewed By: rbergerjr

Differential Revision: D82390344


",2025-09-22 14:20:12+00:00,2025-09-23T18:22:54Z,,False,2,0,1,15,3,1,2,,88,232,False,False,False,False,False,False,1,0,0,18,15,3,1,1,1.0,0.0,2025-09-23T16:29:22Z,pytorch
163512,closed,Split scaled MM tests out,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163523
* __->__ #163512
* #163511

Summary:

Split scaled mm tests into a separate file, get them passing with _v2
API

Changes:

* New test file for scaled mm
* Small bugfixes in scaled_mm_v2 APIs.

Test Plan:

`pytest test/test_scaled_mm_cuda.py`

Reviewers:

Subscribers:

Tasks:

Tags:",2025-09-22 13:45:41+00:00,2025-09-25T13:56:46Z,,False,2,0,1,1696,1486,4,2,2025-09-25 13:56:46+00:00,25,368,False,True,False,False,False,False,4,0,0,3182,1696,1486,1,1,,,,pytorch
163511,closed,Add _scaled_mm_v2 API to core,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163523
* #163512
* __->__ #163511

Summary

Add new _v2 version of _scaled_mm, adding multi-level scaling support,
and refactoring the underlying dispatch code to make it more easily
extensible for future scaling methodologies.

Changes

* `_scaled_mm_v2` and `_scaled_mm_v2_out` APIs added to torch
* Scaling types now explicitly defined by the user to remove any
  ambiguity
* Dispatch of low-precision kernels refactored to make it cleaner and
  easier to extend for future precision types (and kernels)",2025-09-22 13:45:37+00:00,2025-09-25T13:56:45Z,,False,3,0,1,938,1,3,3,2025-09-25 13:56:45+00:00,29,585,False,False,False,False,False,True,3,0,0,939,938,1,1,1,,,,pytorch
163509,open,Logaddexp complex inconsistent bw cpu and cuda,cleonard530,"Fixes #158429

Updated LogAddExpKernel.cu to allow for complex numbers. Also, updated unittest to run test_logaddexp on CUDA with complex data types and added a unit test in test_linalg.py to compare results between CUDA and cpu.

@drisspg",2025-09-22 13:35:56+00:00,2025-09-25T14:35:22Z,,False,3,7,8,146,9,4,10,,46,239,False,True,False,False,False,False,4,2,614,62399,46638,15761,1,8,2.0,2.0,2025-09-22T17:20:15Z,pytorch
163508,open,Fix scaled_dot_product_attention reference implementation to match MATH backend,tom-pollak,"- Fixes #119188 (exact match on MATH backend)
- Fixes #123911 (document float32 upcasting behaviour)

### Summary

The documented [reference implementation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) of SDPA doesn't numerically match the MATH. This causes confusion when testing numerical accuracy of kernels / code vs MATH.

### Changes

Updated the reference implementation to match MATH's actual behavior. The key corrections are:

- The MATH backend pre-scales both query and key tensors before matmul for numerical stability, rather than scaling after the matmul operation.
- The MATH backend internally upcasts to float32 for fp16/bf16 inputs, then converts back to original dtype at the end.

Added regression test `test_reference_implementation_bitwise_match_math_backend`. Test verifies exact bitwise match (rtol=0, atol=0) between the reference implementation and MATH.",2025-09-22 13:05:37+00:00,2025-09-25T08:48:43Z,,False,6,2,8,86,2,2,8,,79,938,False,True,False,True,False,False,2,5,1677,138,111,27,1,8,3.0,6.0,2025-09-22T13:11:50Z,pytorch
163507,closed,[BUG] MaxUnpool2d/3d should check output dim before accessing its elements,can-gaa-hou,Fixes #163409,2025-09-22 12:36:30+00:00,2025-09-22T21:37:55Z,,False,3,2,1,38,12,4,5,2025-09-22 21:36:52+00:00,74,13,False,True,False,False,False,False,4,2,493,50,38,12,1,1,4.0,2.0,2025-09-22T15:55:46Z,pytorch
163506,open,PyTorch `histc` fix for values with large magnitudes,ironsided777,"Summary:
The current implementation of the `histc` function on CPU doesn't take into account the nature of the floating point precision represenation when two numbers have very different magnitudes.

In the code of `histc` there is a following logic, which tries to fix an issue when automatically calculated `min` and `max` are identical:
```
if (leftmost_edge == rightmost_edge) {
        leftmost_edge -= 1;
        rightmost_edge += 1;
    }

...

TORCH_CHECK(leftmost_edge < rightmost_edge, ""torch.histc: max must be larger than min"");
```

But, not for all floating point values expanding the range exactly by 1 will give the representable result that is different from the original value.


The test code:

```
info = th.finfo(th.float32)
f_min = info.min

test_tensor = th.ones((224, 224), dtype=th.float64) * f_min
res = th.histc(test_tensor, bins=10)
```

Actual result:
```
RuntimeError: torch.histc: max must be larger than min
```

Expected result:
Everything should work fine.

NOTICE: If we set `f_min` just to small enough number, code works, which demonstrates the correct purpose of the possible range correction.

In short, `f_min + 1 == f_min` executes to true, since we reach the precision of the floating point prepresentation.
Please notice, this is not limitation of the float32 data type, since all computations happen in float64 (C++ data type `double`). The magnitudes are just different enough, that we reach the precision representation with simple approach of `+/-1`.

Interesting is that `histogram` function doesn't throw an exception, because edges range selection is implemented differently.

The fix we propose is to use `std::nextafter` which returns next representable floating point value starting from the current one in the direction of the lowest or max numbers. In theory, mathecmatically correct is to use this function without constrains, but to maintain backward compatibility in case if there is a code which relies on the current logic of `+/-1` offset we call `std::min` and `std::max` to pick the right representable value (i.e. for small floating point values the next representable value has step smaller than 1 for large values it's larger than 1).
We could stick to `histogram` implementation, but again, to avoid possible backward compatibility breaks, we decided to use the fix presented in this change.


*The real use case scenario:*
In our project we use the well-known transformer version from HuggingFace which fills up the buffer with float32 min (please note this is not a minimal value closer to 0, it's minimal absolute value which is often like `-max`).
The code where it sits is here:
https://github.com/huggingface/transformers/blob/v4.51.1/src/transformers/models/mimi/modeling_mimi.py#L1159

Switching to other version of the transformer will lead to other issues in our project and the bug which we fix here may appear in other projects and scenarios.

The real world problem appears when for such tensor the CPU version of the `histc` is called. In our usecase, it happens because this tensor is an input to the softmax activaiton function and as part of the quantisation the input parameter should go trough the observer as well. In our case the default Histogram observer is selected, which calls the `histc`.

Test Plan:
The simple test code snippet doesn't produce failure:
```
f_min = th.finfo(th.float32).min
test_tensor = th.ones((224, 224), dtype=th.float32) * f_min
th.histc(test_tensor, bins=10)
```

**Testing update:**
The `test_histc` has been updated accordingly.
Now when we have +INF as all values of the tensor, the previous representation of the floating number should be <max_float>, hence the assert message is changed from `[inf, inf]` to `[<max_float>|inf, inf]`.
The test also extended to check the assert message when tensor is filled with values -INF and with combination of (-INF, +INF).
The new regexp assert includes possible output as `inf` and any floating point number in scientific representation for one of the bin edges. We left `inf` as possible value due to possible difference in implementation between CPU and CUDA.

Differential Revision: D82955597


",2025-09-22 12:27:20+00:00,2025-09-25T01:50:59Z,,False,7,7,1,62,4,2,14,,52,4161,False,True,False,False,False,False,2,1,163,66,62,4,1,1,3.0,3.0,2025-09-22T12:30:24Z,pytorch
163505,open,Provide boxing helper to stable API,lw,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163505
* #163635
* #163634

Fixes https://github.com/pytorch/pytorch/issues/163346

cc @malfet @zou3519 @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @albanD",2025-09-22 11:58:28+00:00,2025-09-23T16:18:54Z,,False,6,14,7,160,196,3,20,,35,276,False,True,False,False,False,False,3,5,1676,13826,10615,3211,1,7,3.0,6.0,2025-09-22T12:03:24Z,pytorch
163503,open,[dynamo] prevent recompilation limit exceeded on external_utils.wrap_inline,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160611
* #163335
* #162737
* #160601
* __->__ #163503

This will prevent recompilations (i.e. logs to TORCH_LOGS=""recompiles"") due to wrap_inline's guard on `fn`. If we call wrap_inline on enough different functions, we would hit the recompile limit.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 10:44:28+00:00,2025-09-23T20:49:20Z,,False,1,5,2,57,0,3,6,,75,501,False,False,False,False,False,False,3,0,165,141,99,42,1,2,2.0,1.0,2025-09-23T20:06:55Z,pytorch
163499,closed,[AOTI] fix TestAOTInductorPackage temp file locked handler.,xuhancn,"Fix `test\inductor\test_aot_inductor_package.py` common class `TestAOTInductorPackage`'s `check_model` function, temp file locked file handler on Windows. It would caused c++ backend open file failed:
```cmd
FAILED [4.5918s] test/inductor/test_aot_inductor_package.py::TestAOTInductorPackage_cpu::test_add - RuntimeError: File C:/Users/Xuhan/AppData/Local/Temp/tmp21sjnnhl.pt2 cannot be opened.
FAILED [4.1703s] test/inductor/test_aot_inductor_package.py::TestAOTInductorPackage_cpu::test_bool_input - RuntimeError: File C:/Users/Xuhan/AppData/Local/Temp/tmp5kd3apub.pt2 cannot be opened.
FAILED [4.2266s] test/inductor/test_aot_inductor_package.py::TestAOTInductorPackage_cpu::test_linear - RuntimeError: File C:/Users/Xuhan/AppData/Local/Temp/tmpkyy3pxow.pt2 cannot be opened.
FAILED [4.2134s] test/inductor/test_aot_inductor_package.py::TestAOTInductorPackage_cpu::test_metadata - RuntimeError: File C:/Users/Xuhan/AppData/Local/Temp/tmphyer7wi9.pt2 cannot be opened.
......
```

Fix it via `WritableTempFile`, it can release file handler for backend use.

After fixed:
 
<img width=""1904"" height=""176"" alt=""image"" src=""https://github.com/user-attachments/assets/e71b3182-0204-497b-9aca-cbbb33bc4687"" />


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 08:28:45+00:00,2025-09-22T16:55:39Z,,False,3,0,2,7,7,1,3,2025-09-22 16:54:22+00:00,59,1516,False,True,False,False,False,False,1,2,493,14,7,7,1,2,4.0,2.0,2025-09-22T14:36:15Z,pytorch
163498,closed,Use accelerator API in common_dtensor,dilililiwhy,"Fixes #ISSUE_NUMBER

Try to unify the device checking in common_dtensor (testing module) by accelerator API

cc @albanD @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 08:16:45+00:00,2025-09-23T16:31:29Z,,False,13,0,2,19,17,1,13,2025-09-23 16:30:24+00:00,37,218,False,True,False,False,False,False,1,12,2862,42,22,20,1,2,5.0,14.0,2025-09-22T08:19:56Z,pytorch
163497,open,Remove test conditions for old ROCm,cyyever,"Remove test conditions for `HIP_VERSION < 3.5`.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-22 08:16:13+00:00,2025-09-22T20:18:52Z,,False,1,0,1,0,19,1,1,,35,264,False,False,False,False,False,False,1,0,0,19,0,19,1,1,,,,pytorch
163495,open,Remove test conditions for CUDA<12,cyyever,"Because it required that CUDA >=12.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 07:45:14+00:00,2025-09-24T02:49:38Z,,False,6,0,1,6,66,4,6,,34,138,False,False,False,False,False,False,4,5,2216,72,6,66,1,1,4.0,6.0,2025-09-22T20:24:19Z,pytorch
163494,open,[xla hash update] update the pinned xla hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned xla hash.",2025-09-22 07:41:44+00:00,2025-09-22T13:15:26Z,,False,4,0,1,1,1,1,4,,44,155,False,False,False,False,False,False,1,3,887,2,1,1,1,1,2.0,3.0,2025-09-22T07:41:45Z,pytorch
163493,open,Update slow tests,pytorchupdatebot,"This PR is auto-generated weekly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/weekly.yml).
Update the list of slow tests.",2025-09-22 07:40:47+00:00,2025-09-22T12:22:30Z,,False,4,0,1,241,247,1,4,,17,156,False,False,False,False,False,False,1,3,887,488,241,247,1,1,2.0,3.0,2025-09-22T07:40:54Z,pytorch
163492,open,Add eager mode in inductor,jainapurva,"Fixes #ISSUE_NUMBER
",2025-09-22 07:27:35+00:00,2025-09-22T23:46:57Z,,False,2,0,1,88,7,2,2,,26,20,False,True,False,False,False,False,2,0,0,95,88,7,1,1,,,,pytorch
163490,open,[Inductor] support masked vectorization for the tail_loop for bool datatype,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163490
* #163324
* #163316



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-22 07:03:57+00:00,2025-09-24T11:19:46Z,,False,1,0,2,2,0,1,1,,75,314,False,False,False,False,False,False,1,0,0,4,3,1,1,2,,,,pytorch
163489,closed,Replace Literal[None] with None in typing,cyyever,"This PR replaces Literal[None] with None in typing.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 06:50:00+00:00,2025-09-22T23:53:18Z,,False,9,0,1,10,19,6,9,2025-09-22 22:10:11+00:00,41,254,False,False,False,False,False,False,6,8,1398,29,10,19,1,1,5.0,9.0,2025-09-22T06:55:35Z,pytorch
163488,closed,[pt2][cache] rework cache for true generic usage + better tests,nmacchioni,"Differential Revision: D82933509

over the weekend I realized that some of the cache implementation was a bit silly, and too constrained to be actually generic. for example, InMemoryCache[str, bytes] was odd since we'd probably want to be able to store more than just str keys with bytes values. so tldr; everything is now generic, with the one constraint being that Key and Value must both be pickle-able types. this makes things a lot simpler for us, since all caches can now be str -> bytes caches under the hood if we'd like, and Key/Value just get pickled on the way in and out.

with this change, there were also some improvements made to the testing; mainly better coverage, but now we also test each cache across every combination of Key/Value types to ensure that they will work with the types we might specify later

I also hardened some things here and there, for example we now use literal_eval (forgot who mentioned this on the first PR, but thank you for the suggestion!), and all errors coming from the caching will be wrapped in CacheError from now on (although we still raise from the original error context where possible)

putting this PR up now for feedback, in the process of generalizing the code I did remove the documentation since it was becoming outdated but I will add that back in after the PR is green

I have the next PR ready as well (implements a fresh cache context manager), will export once this lands

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 06:11:44+00:00,2025-09-23T12:22:55Z,,False,14,20,1,1231,642,4,34,2025-09-23 07:31:52+00:00,63,1639,False,False,False,True,True,False,4,4,1799,1873,1231,642,1,1,4.0,5.0,2025-09-22T16:52:55Z,pytorch
163487,open,fixes crash for empty dims in quantized tensors,arkadip-maitra,"Fixes #162335 


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-22 05:53:02+00:00,2025-09-22T20:31:11Z,,False,1,0,1,54,6,3,1,,47,98,False,True,False,False,False,False,3,0,0,60,54,6,1,1,,,,pytorch
163486,open,Add type annotations to MPS profiler utilities,bobrenjc93,"## Summary
- drop the local mypy allow-untyped-defs escape hatch in the MPS profiler helpers
- annotate the context managers and bool helpers so they type-check cleanly

## Testing
- python -m mypy torch/mps/profiler.py --config-file mypy-strict.ini

------
https://chatgpt.com/codex/tasks/task_e_68d0ce4df2e483268d06673b65ef7745",2025-09-22 05:44:59+00:00,2025-09-23T21:46:06Z,,False,1,3,4,17,9,1,4,,46,329,False,False,False,False,False,False,1,0,0,40,24,16,1,4,1.0,0.0,2025-09-22T17:25:29Z,pytorch
163485,closed,symintify fill_diagonol_,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163607
* __->__ #163485

Fixes #162271

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 05:38:49+00:00,2025-09-23T21:27:15Z,,False,6,3,2,21,14,2,9,2025-09-23 21:27:15+00:00,24,289,False,True,False,False,False,False,2,5,1004,24212,18044,6168,2,2,3.0,5.0,2025-09-22T18:36:21Z,pytorch
163484,open,Add weights_only=True to torch.load,cyyever,"Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 05:36:35+00:00,2025-09-24T02:11:07Z,,False,1,2,1,18,13,9,3,,35,123,False,True,False,False,False,False,9,0,0,31,18,13,1,1,2.0,0.0,2025-09-22T14:43:21Z,pytorch
163482,closed,[inductor] Fix divmod error in decomp,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163377
* __->__ #163482
* #163520
* #163481

Fixes #163457

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 04:48:17+00:00,2025-09-24T03:20:02Z,,False,3,1,8,35,7,3,4,2025-09-24 02:52:44+00:00,37,352,False,True,False,False,False,False,3,2,493,757,415,342,1,8,4.0,2.0,2025-09-23T16:54:56Z,pytorch
163481,closed,[inductor] Fix issue with scalar arg handling,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163377
* #163482
* #163520
* __->__ #163481

Fixes #163420

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 04:38:48+00:00,2025-09-24T03:25:42Z,,False,4,2,4,104,3,6,6,2025-09-24 02:52:40+00:00,45,352,False,True,False,False,False,False,6,3,867,323,172,151,1,4,4.0,4.0,2025-09-23T16:35:26Z,pytorch
163480,open,fix: handle zero local shards correctly in distributed checkpoint,rphmeier,"This fixes a subtle issue that can occur when there are zero local shards for a sharded tensor in a saved distributed checkpoint.

The following sequence of events could occur:
  1. `_flatten_sharded_tensors` would filter out the item from the state dict
  2. Then `DefaultLoadPlanner.create_local_plan` would wrongly treat the key as missing
  3. Which would then lead it to fall back to the old key format and fail

I fixed this by having `_flatten_sharded_tensors` return a set of paths that were being omitted so they wouldn't count as missing keys in the checkpoint.

(note: i observed this issue when saving/loading parameters of shape `1, N, D` and `1, D`, but it seems like a general issue)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-22 04:08:16+00:00,2025-09-24T03:58:38Z,,False,4,0,1,15,6,3,4,,65,801,False,True,False,False,False,False,3,2,310,21,15,6,1,1,2.0,2.0,2025-09-24T01:59:24Z,pytorch
163479,open,TEMP,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163479



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-22 03:56:06+00:00,2025-09-22T08:01:25Z,,False,2,0,1,135,13,8,2,,4,266,False,False,False,False,False,False,8,0,0,148,135,13,1,1,,,,pytorch
163478,closed,remove allow-untyped-defs from ./torch/fx/experimental/unification/multipledispatch/core.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* #163471
* #163475
* __->__ #163478



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv",2025-09-22 03:47:19+00:00,2025-09-25T06:48:47Z,,False,2,2,14,24,9,4,4,2025-09-25 06:48:47+00:00,91,258,False,False,False,False,False,False,4,1,48,730,218,512,1,14,3.0,1.0,2025-09-24T01:54:03Z,pytorch
163477,open,remove allow-untyped-defs from ./torch/distributed/optim/zero_redundancy_optimizer.pyi,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* #163473
* #163475
* #163474
* #163471
* #163478
* __->__ #163477

",2025-09-22 03:47:14+00:00,2025-09-24T01:58:37Z,,False,2,2,6,12,10,1,4,,86,154,False,False,False,False,False,False,1,0,1,424,161,263,1,6,1.0,1.0,2025-09-24T01:53:46Z,pytorch
163476,closed,remove allow-untyped-defs from ./torch/utils/benchmark/op_fuzzers/sparse_unary.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163476
* #163471
* #163475
* #163478

",2025-09-22 03:47:08+00:00,2025-09-25T06:50:01Z,,False,4,1,15,11,2,1,5,2025-09-25 06:48:50+00:00,81,124,False,False,False,False,False,False,1,3,622,839,290,549,1,15,4.0,3.0,2025-09-24T01:54:24Z,pytorch
163475,closed,remove allow-untyped-defs from ./torch/nn/utils/_expanded_weights/embedding_expanded_weights.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* #163471
* __->__ #163475
* #163478

",2025-09-22 03:47:03+00:00,2025-09-25T06:48:48Z,,False,2,0,12,9,6,1,2,2025-09-25 06:48:48+00:00,95,124,False,False,False,False,False,False,1,1,48,771,248,523,1,12,3.0,1.0,2025-09-24T01:54:12Z,pytorch
163474,open,remove allow-untyped-defs from ./torch/distributions/half_normal.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* #163473
* #163475
* __->__ #163474
* #163471
* #163478
* #163477

",2025-09-22 03:46:59+00:00,2025-09-24T01:56:27Z,,False,2,1,6,14,11,1,3,,67,154,False,False,False,False,False,False,1,0,1,446,188,258,1,6,1.0,1.0,2025-09-24T01:54:08Z,pytorch
163473,open,remove allow-untyped-defs from ./torch/utils/benchmark/op_fuzzers/unary.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* __->__ #163473
* #163475
* #163474
* #163471
* #163478
* #163477

",2025-09-22 03:46:54+00:00,2025-09-23T23:46:57Z,,False,2,1,6,15,2,1,3,,74,154,False,False,False,False,False,False,1,0,0,463,210,253,1,6,1.0,0.0,2025-09-22T19:14:50Z,pytorch
163472,closed,remove allow-untyped-defs from ./torch/onnx/_internal/torchscript_exporter/_globals.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* #163473
* #163475
* #163474
* #163471
* #163478
* #163477
* __->__ #163472

",2025-09-22 03:46:49+00:00,2025-09-23T03:51:39Z,,False,6,0,3,5,6,1,6,2025-09-23 03:50:32+00:00,86,164,False,False,False,False,False,False,1,5,1568,230,110,120,1,3,3.0,5.0,2025-09-22T19:13:18Z,pytorch
163471,closed,remove allow-untyped-defs from ./torch/ao/quantization/quantizer/utils.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163476
* __->__ #163471
* #163475
* #163478

",2025-09-22 03:46:44+00:00,2025-09-25T06:48:49Z,,False,2,0,14,15,7,1,2,2025-09-25 06:48:49+00:00,73,124,False,False,False,False,False,False,1,1,48,747,251,496,1,14,2.0,1.0,2025-09-22T19:16:13Z,pytorch
163470,closed,remove allow-untyped-defs from ./torch/ao/quantization/pt2e/duplicate_dq_pass.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163475
* #163474
* #163471
* #163478
* #163477
* #163476
* #163473
* #163472
* __->__ #163470
* #163469

",2025-09-22 03:46:40+00:00,2025-09-22T20:30:20Z,,False,3,0,2,1,2,1,3,2025-09-22 20:29:13+00:00,80,184,False,False,False,False,False,False,1,2,493,203,99,104,1,2,3.0,2.0,2025-09-22T17:47:33Z,pytorch
163469,closed,remove allow-untyped-defs from ./torch/utils/data/datapipes/iter/fileopener.py,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163475
* #163474
* #163471
* #163478
* #163477
* #163476
* #163473
* #163472
* #163470
* __->__ #163469

",2025-09-22 03:46:34+00:00,2025-09-22T20:29:13Z,,False,2,0,2,4,5,1,2,2025-09-22 20:29:12+00:00,78,184,False,False,False,False,False,False,1,1,48,209,102,107,1,2,3.0,1.0,2025-09-22T17:47:17Z,pytorch
163468,closed,[1/N] Remove 'type: ignore' suppressions ,cyyever,"Remove some unnecessary 'type: ignore' suppressions from python code.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-22 03:42:13+00:00,2025-09-23T13:47:56Z,,False,7,0,1,19,18,5,7,2025-09-23 03:53:16+00:00,41,152,False,False,False,False,False,False,5,6,1683,37,19,18,1,1,4.0,6.0,2025-09-22T17:27:25Z,pytorch
163467,closed,GEMM-template Horizontal ,sunjiweiswift,"Summary
Current, CPP GEMM Template using vertical transverse strategy to do the cache blocking and loop transverse, which assumes Matrix B in L1 cache and Matrix A in L2 cache. Nevertheless, we found when Matrix A is much larger than Matrix B, horizontal transverse can give better performance. In this PR:

We implement the horizontal transverse strategy which is default off and can be turn on by inductor config: config.cpp.cpp_gemm_transverse_strategy = ""HORIZONTAL""
We also implement the heuristic to choose between vertical and horizontal transverse when user set this config as: config.cpp.cpp_gemm_transverse_strategy = ""VERTICAL,HORIZONTAL""
Test Plan

python -u -m pytest -s -v test/inductor/test_cpu_select_algorithm.py -k test_horizontal_transverse

Performance

When M > 256, a significant improvement is achieved

// Performance close to VERTICAL
Lines where the time in the first file is slower:
GEMM(M=1,N=2,K=256) runtime: 0.0092 ms (0.00 TOPS, 0.16 GB/s) vs. 0.0092 ms (0.00 %)
GEMM(M=1,N=2,K=1024) runtime: 0.0098 ms (0.00 TOPS, 0.60 GB/s) vs. 0.0094 ms (4.26 %)
GEMM(M=1,N=50,K=39200) runtime: 0.0831 ms (0.05 TOPS, 45.91 GB/s) vs. 0.0821 ms (1.22 %)
GEMM(M=1,N=768,K=768) runtime: 0.0135 ms (0.09 TOPS, 83.77 GB/s) vs. 0.0135 ms (0.00 %)
GEMM(M=1,N=1024,K=50) runtime: 0.0117 ms (0.01 TOPS, 8.50 GB/s) vs. 0.0114 ms (2.63 %)
GEMM(M=1,N=1024,K=1024) runtime: 0.0144 ms (0.15 TOPS, 139.23 GB/s) vs. 0.0145 ms (-0.69 %)
GEMM(M=4,N=768,K=768) runtime: 0.0133 ms (0.36 TOPS, 85.76 GB/s) vs. 0.0138 ms (-3.62 %)
GEMM(M=4,N=768,K=3072) runtime: 0.0268 ms (0.70 TOPS, 168.91 GB/s) vs. 0.0269 ms (-0.37 %)
GEMM(M=4,N=1000,K=768) runtime: 0.0136 ms (0.45 TOPS, 108.54 GB/s) vs. 0.0135 ms (0.74 %)
GEMM(M=4,N=1000,K=4096) runtime: 0.0527 ms (0.62 TOPS, 149.10 GB/s) vs. 0.0526 ms (0.19 %)
GEMM(M=4,N=3072,K=768) runtime: 0.0228 ms (0.83 TOPS, 198.33 GB/s) vs. 0.0233 ms (-2.15 %)
GEMM(M=4,N=4096,K=4096) runtime: 0.1918 ms (0.70 TOPS, 167.21 GB/s) vs. 0.1926 ms (-0.42 %)
GEMM(M=4,N=4096,K=25088) runtime: 1.1247 ms (0.73 TOPS, 174.47 GB/s) vs. 1.1382 ms (-1.19 %)
GEMM(M=5,N=5,K=64) runtime: 0.0094 ms (0.00 TOPS, 0.13 GB/s) vs. 0.0088 ms (6.82 %)
GEMM(M=8,N=1000,K=512) runtime: 0.0147 ms (0.56 TOPS, 68.16 GB/s) vs. 0.015 ms (-2.00 %)
GEMM(M=8,N=1000,K=2048) runtime: 0.0226 ms (1.45 TOPS, 174.71 GB/s) vs. 0.023 ms (-1.74 %)
GEMM(M=16,N=2,K=768) runtime: 0.0105 ms (0.00 TOPS, 2.51 GB/s) vs. 0.0107 ms (-1.87 %)
GEMM(M=16,N=768,K=768) runtime: 0.0158 ms (1.20 TOPS, 74.20 GB/s) vs. 0.0159 ms (-0.63 %)
GEMM(M=16,N=768,K=3072) runtime: 0.0328 ms (2.30 TOPS, 140.84 GB/s) vs. 0.0326 ms (0.61 %)
GEMM(M=16,N=1000,K=768) runtime: 0.0170 ms (1.44 TOPS, 89.12 GB/s) vs. 0.0172 ms (-1.16 %)
GEMM(M=16,N=1000,K=1280) runtime: 0.0211 ms (1.95 TOPS, 119.25 GB/s) vs. 0.0206 ms (2.43 %)
GEMM(M=16,N=1000,K=4320) runtime: 0.0553 ms (2.50 TOPS, 152.02 GB/s) vs. 0.0561 ms (-1.43 %)
GEMM(M=16,N=3072,K=768) runtime: 0.0250 ms (3.02 TOPS, 184.88 GB/s) vs. 0.0249 ms (0.40 %)
GEMM(M=32,N=512,K=512) runtime: 0.0145 ms (1.16 TOPS, 38.82 GB/s) vs. 0.0145 ms (0.00 %)
GEMM(M=32,N=512,K=768) runtime: 0.0160 ms (1.57 TOPS, 51.63 GB/s) vs. 0.0162 ms (-1.23 %)
GEMM(M=32,N=1000,K=384) runtime: 0.0159 ms (1.55 TOPS, 51.49 GB/s) vs. 0.0152 ms (4.61 %)
GEMM(M=32,N=1000,K=512) runtime: 0.0169 ms (1.94 TOPS, 63.33 GB/s) vs. 0.0167 ms (1.20 %)
GEMM(M=32,N=1000,K=768) runtime: 0.0196 ms (2.51 TOPS, 80.42 GB/s) vs. 0.0188 ms (4.26 %)
GEMM(M=32,N=1000,K=1024) runtime: 0.0221 ms (2.96 TOPS, 93.86 GB/s) vs. 0.0222 ms (-0.45 %)
GEMM(M=32,N=1000,K=1280) runtime: 0.0261 ms (3.14 TOPS, 98.77 GB/s) vs. 0.0253 ms (3.16 %)
GEMM(M=32,N=1000,K=1408) runtime: 0.0279 ms (3.23 TOPS, 101.53 GB/s) vs. 0.0273 ms (2.20 %)
GEMM(M=32,N=1000,K=2048) runtime: 0.0397 ms (3.30 TOPS, 102.98 GB/s) vs. 0.0393 ms (1.02 %)
GEMM(M=32,N=1000,K=2240) runtime: 0.0473 ms (3.03 TOPS, 94.60 GB/s) vs. 0.0477 ms (-0.84 %)
GEMM(M=32,N=1280,K=960) runtime: 0.0196 ms (4.01 TOPS, 126.58 GB/s) vs. 0.0194 ms (1.03 %)
GEMM(M=32,N=32000,K=512) runtime: 1.6477 ms (0.64 TOPS, 20.17 GB/s) vs. 1.6474 ms (0.02 %)
GEMM(M=64,N=10,K=512) runtime: 0.0118 ms (0.06 TOPS, 6.21 GB/s) vs. 0.0115 ms (2.61 %)
GEMM(M=64,N=384,K=384) runtime: 0.0155 ms (1.22 TOPS, 24.18 GB/s) vs. 0.0155 ms (0.00 %)
GEMM(M=64,N=384,K=1152) runtime: 0.0190 ms (2.98 TOPS, 54.31 GB/s) vs. 0.0191 ms (-0.52 %)
GEMM(M=64,N=512,K=256) runtime: 0.0146 ms (1.15 TOPS, 23.51 GB/s) vs. 0.0143 ms (2.10 %)
GEMM(M=64,N=1000,K=384) runtime: 0.0179 ms (2.75 TOPS, 50.38 GB/s) vs. 0.0168 ms (6.55 %)
GEMM(M=64,N=1000,K=512) runtime: 0.0190 ms (3.46 TOPS, 61.27 GB/s) vs. 0.0186 ms (2.15 %)
GEMM(M=64,N=1000,K=640) runtime: 0.0224 ms (3.66 TOPS, 63.41 GB/s) vs. 0.0208 ms (7.69 %)
GEMM(M=64,N=1000,K=768) runtime: 0.0238 ms (4.13 TOPS, 70.57 GB/s) vs. 0.022 ms (8.18 %)
GEMM(M=64,N=1000,K=1024) runtime: 0.0244 ms (5.37 TOPS, 90.11 GB/s) vs. 0.0245 ms (-0.41 %)
GEMM(M=64,N=1000,K=1280) runtime: 0.0281 ms (5.83 TOPS, 96.80 GB/s) vs. 0.0279 ms (0.72 %)
GEMM(M=64,N=1000,K=2048) runtime: 0.0459 ms (5.72 TOPS, 93.31 GB/s) vs. 0.0484 ms (-5.17 %)
GEMM(M=64,N=1152,K=384) runtime: 0.0169 ms (3.35 TOPS, 60.94 GB/s) vs. 0.0163 ms (3.68 %)
GEMM(M=96,N=65,K=512) runtime: 0.0146 ms (0.44 TOPS, 11.61 GB/s) vs. 0.0171 ms (-14.62 %)
GEMM(M=128,N=2,K=4096) runtime: 0.0327 ms (0.06 TOPS, 31.12 GB/s) vs. 0.0326 ms (0.31 %)
GEMM(M=128,N=10,K=64) runtime: 0.0092 ms (0.02 TOPS, 2.09 GB/s) vs. 0.0095 ms (-3.16 %)
GEMM(M=128,N=10,K=184) runtime: 0.0103 ms (0.05 TOPS, 4.92 GB/s) vs. 0.0102 ms (0.98 %)
GEMM(M=128,N=1000,K=256) runtime: 0.0178 ms (3.69 TOPS, 44.75 GB/s) vs. 0.019 ms (-6.32 %)
GEMM(M=128,N=1000,K=384) runtime: 0.0204 ms (4.82 TOPS, 52.43 GB/s) vs. 0.0215 ms (-5.12 %)
GEMM(M=128,N=1000,K=512) runtime: 0.0262 ms (5.00 TOPS, 51.33 GB/s) vs. 0.0249 ms (5.22 %)
GEMM(M=128,N=1000,K=768) runtime: 0.0330 ms (5.96 TOPS, 57.50 GB/s) vs. 0.033 ms (0.00 %)
GEMM(M=128,N=1000,K=1024) runtime: 0.0407 ms (6.44 TOPS, 60.15 GB/s) vs. 0.0419 ms (-2.86 %)
GEMM(M=128,N=1000,K=1280) runtime: 0.0494 ms (6.64 TOPS, 60.71 GB/s) vs. 0.05 ms (-1.20 %)
GEMM(M=128,N=1000,K=1408) runtime: 0.0520 ms (6.93 TOPS, 62.92 GB/s) vs. 0.0534 ms (-2.62 %)
GEMM(M=128,N=1000,K=1536) runtime: 0.0566 ms (6.94 TOPS, 62.67 GB/s) vs. 0.0572 ms (-1.05 %)
GEMM(M=128,N=1000,K=2048) runtime: 0.0812 ms (6.46 TOPS, 57.29 GB/s) vs. 0.0775 ms (4.77 %)
GEMM(M=128,N=1000,K=2304) runtime: 0.1034 ms (5.70 TOPS, 50.29 GB/s) vs. 0.0945 ms (9.42 %)
GEMM(M=128,N=1000,K=2560) runtime: 0.1304 ms (5.03 TOPS, 44.12 GB/s) vs. 0.1176 ms (10.88 %)
GEMM(M=128,N=1000,K=3072) runtime: 0.1939 ms (4.06 TOPS, 35.35 GB/s) vs. 0.1713 ms (13.19 %)
GEMM(M=128,N=1000,K=4096) runtime: 0.3207 ms (3.27 TOPS, 28.24 GB/s) vs. 0.2748 ms (16.70 %)、


GEMM(M=128,N=4096,K=4096) runtime: 1.3166 ms (3.26 TOPS, 25.82 GB/s) vs. 3.5636 ms (-63.05 %)
GEMM(M=128,N=4096,K=9216) runtime: 4.7093 ms (2.05 TOPS, 15.98 GB/s) vs. 4.6948 ms (0.31 %)
GEMM(M=128,N=4096,K=16384) runtime: 5.2330 ms (3.28 TOPS, 25.42 GB/s) vs. 5.1444 ms (1.72 %)
GEMM(M=128,N=16384,K=4096) runtime: 2.2769 ms (7.55 TOPS, 58.41 GB/s) vs. 6.9179 ms (-67.09 %)
GEMM(M=128,N=50400,K=4096) runtime: 6.1589 ms (8.58 TOPS, 66.09 GB/s) vs. 6.0398 ms (1.97 %)

EMM(M=220,N=512,K=512) runtime: 0.0264 ms (4.36 TOPS, 35.17 GB/s) vs. 0.0245 ms (7.76 %)
EMM(M=220,N=512,K=2048) runtime: 0.0815 ms (5.66 TOPS, 37.70 GB/s) vs. 0.0779 ms (4.62 %)

GEMM(M=220,N=1014,K=512) runtime: 0.0302 ms (7.56 TOPS, 53.97 GB/s) vs. 0.0349 ms (-13.47 %)
GEMM(M=220,N=2048,K=512) runtime: 0.8694 ms (0.53 TOPS, 3.54 GB/s) vs. 2.3897 ms (-63.62 %)
// Performance Improvements
GEMM(M=256,N=2,K=1024) runtime: 0.0145 ms (0.07 TOPS, 34.79 GB/s) vs. 0.014 ms (3.57 %)
GEMM(M=256,N=128,K=128) runtime: 0.0131 ms (0.64 TOPS, 11.94 GB/s) vs. 0.0125 ms (4.80 %)
GEMM(M=256,N=128,K=256) runtime: 0.0155 ms (1.08 TOPS, 16.13 GB/s) vs. 0.0145 ms (6.90 %)
GEMM(M=256,N=256,K=128) runtime: 0.0152 ms (1.10 TOPS, 16.45 GB/s) vs. 0.0155 ms (-1.94 %)
GEMM(M=256,N=256,K=256) runtime: 0.0157 ms (2.14 TOPS, 23.89 GB/s) vs. 0.0169 ms (-7.10 %)
GEMM(M=256,N=512,K=512) runtime: 0.0262 ms (5.12 TOPS, 38.12 GB/s) vs. 0.0246 ms (6.50 %)
GEMM(M=256,N=512,K=1024) runtime: 0.0396 ms (6.78 TOPS, 44.20 GB/s) vs. 0.0391 ms (1.28 %)
GEMM(M=256,N=512,K=197951) runtime: 16.2101 ms (3.20 TOPS, 17.90 GB/s) vs. 16.2295 ms (-0.12 %)
GEMM(M=256,N=768,K=768) runtime: 0.0426 ms (7.09 TOPS, 44.04 GB/s) vs. 0.0419 ms (1.67 %)
GEMM(M=256,N=768,K=3072) runtime: 0.4397 ms (2.75 TOPS, 14.50 GB/s) vs. 0.4131 ms (6.44 %)
GEMM(M=256,N=1000,K=128) runtime: 0.2579 ms (0.25 TOPS, 3.08 GB/s) vs. 0.4302 ms (-40.05 %)
GEMM(M=256,N=1000,K=256) runtime: 0.0307 ms (4.27 TOPS, 35.88 GB/s) vs. 0.0295 ms (4.07 %)
GEMM(M=256,N=1000,K=1024) runtime: 0.0671 ms (7.81 TOPS, 43.81 GB/s) vs. 0.0675 ms (-0.59 %)
GEMM(M=256,N=1000,K=1280) runtime: 0.3644 ms (1.80 TOPS, 9.75 GB/s) vs. 0.5249 ms (-30.58 %)
GEMM(M=256,N=1000,K=1984) runtime: 0.4563 ms (2.23 TOPS, 11.49 GB/s) vs. 1.0955 ms (-58.35 %)
GEMM(M=256,N=1000,K=2048) runtime: 0.3607 ms (2.91 TOPS, 14.96 GB/s) vs. 1.0971 ms (-67.12 %)
GEMM(M=256,N=1024,K=3) runtime: 0.3473 ms (0.00 TOPS, 1.46 GB/s) vs. 1.0332 ms (-66.39 %)
GEMM(M=256,N=1024,K=512) runtime: 0.2878 ms (0.93 TOPS, 6.08 GB/s) vs. 0.4721 ms (-39.04 %)
GEMM(M=256,N=1024,K=1024) runtime: 0.0668 ms (8.03 TOPS, 44.88 GB/s) vs. 0.0646 ms (3.41 %)
GEMM(M=256,N=2304,K=768) runtime: 0.9959 ms (0.91 TOPS, 4.90 GB/s) vs. 2.2502 ms (-55.74 %)
GEMM(M=256,N=3072,K=768) runtime: 1.1671 ms (1.04 TOPS, 5.46 GB/s) vs. 3.0596 ms (-61.85 %)
GEMM(M=256,N=50257,K=768) runtime: 5.7232 ms (3.45 TOPS, 17.22 GB/s) vs. 7.2058 ms (-20.58 %)
GEMM(M=256,N=197951,K=512) runtime: 17.7657 ms (2.92 TOPS, 16.34 GB/s) vs. 18.9209 ms (-6.11 %)

GEMM(M=473,N=2,K=768) runtime: 0.0139 ms (0.10 TOPS, 50.31 GB/s) vs. 0.0147 ms (-5.44 %)
GEMM(M=475,N=768,K=768) runtime: 0.4938 ms (1.13 TOPS, 5.10 GB/s) vs. 0.8638 ms (-42.83 %)
GEMM(M=475,N=768,K=3072) runtime: 0.9406 ms (2.38 TOPS, 8.48 GB/s) vs. 1.5037 ms (-37.45 %)
GEMM(M=475,N=3072,K=768) runtime: 1.8788 ms (1.19 TOPS, 4.25 GB/s) vs. 6.8102 ms (-72.41 %)

GEMM(M=512,N=2,K=1536) runtime: 0.0201 ms (0.16 TOPS, 75.18 GB/s) vs. 0.0201 ms (0.00 %)
GEMM(M=512,N=128,K=768) runtime: 0.0210 ms (4.78 TOPS, 50.50 GB/s) vs. 0.0209 ms (0.48 %)
GEMM(M=512,N=512,K=512) runtime: 0.0385 ms (6.96 TOPS, 38.91 GB/s) vs. 0.041 ms (-6.10 %)
GEMM(M=512,N=512,K=2048) runtime: 0.2664 ms (4.03 TOPS, 16.89 GB/s) vs. 0.2298 ms (15.93 %)

GEMM(M=512,N=768,K=128) runtime: 0.2887 ms (0.35 TOPS, 3.68 GB/s) vs. 0.8198 ms (-64.78 %)
GEMM(M=512,N=768,K=768) runtime: 0.3599 ms (1.68 TOPS, 7.29 GB/s) vs. 0.8904 ms (-59.58 %)
GEMM(M=512,N=768,K=3072) runtime: 0.8159 ms (2.96 TOPS, 10.11 GB/s) vs. 1.7363 ms (-53.01 %)
GEMM(M=512,N=1000,K=1280) runtime: 0.9001 ms (1.46 TOPS, 5.19 GB/s) vs. 2.2128 ms (-59.32 %)
GEMM(M=512,N=1000,K=1984) runtime: 0.8326 ms (2.44 TOPS, 8.05 GB/s) vs. 2.4197 ms (-65.59 %)
GEMM(M=512,N=1024,K=1024) runtime: 0.5257 ms (2.04 TOPS, 7.61 GB/s) vs. 1.7488 ms (-69.94 %)
GEMM(M=512,N=1024,K=3072) runtime: 0.9542 ms (3.38 TOPS, 10.48 GB/s) vs. 1.6973 ms (-43.78 %)
GEMM(M=512,N=1024,K=4096) runtime: 1.2711 ms (3.38 TOPS, 10.23 GB/s) vs. 2.8759 ms (-55.80 %)
GEMM(M=512,N=1536,K=1536) runtime: 1.5235 ms (1.59 TOPS, 4.92 GB/s) vs. 3.4486 ms (-55.82 %)
GEMM(M=512,N=1536,K=6144) runtime: 1.9637 ms (4.92 TOPS, 12.99 GB/s) vs. 2.4115 ms (-18.57 %)
GEMM(M=512,N=2048,K=512) runtime: 1.1441 ms (0.94 TOPS, 3.93 GB/s) vs. 4.0707 ms (-71.89 %)
GEMM(M=512,N=2048,K=2048) runtime: 1.7600 ms (2.44 TOPS, 6.82 GB/s) vs. 5.0442 ms (-65.11 %)
GEMM(M=512,N=2048,K=8192) runtime: 2.2562 ms (7.61 TOPS, 18.62 GB/s) vs. 2.2703 ms (-0.62 %)
GEMM(M=512,N=2560,K=2560) runtime: 1.8007 ms (3.73 TOPS, 9.72 GB/s) vs. 4.1385 ms (-56.49 %)
GEMM(M=512,N=2560,K=10240) runtime: 4.3323 ms (6.20 TOPS, 14.43 GB/s) vs. 4.7134 ms (-8.09 %)
GEMM(M=512,N=3072,K=768) runtime: 1.8491 ms (1.31 TOPS, 4.46 GB/s) vs. 7.6777 ms (-75.92 %)
GEMM(M=512,N=3072,K=1024) runtime: 1.9930 ms (1.62 TOPS, 5.02 GB/s) vs. 7.8267 ms (-74.54 %)
GEMM(M=512,N=4096,K=1024) runtime: 1.8094 ms (2.37 TOPS, 7.18 GB/s) vs. 6.811 ms (-73.43 %)
GEMM(M=512,N=6144,K=1536) runtime: 2.7222 ms (3.55 TOPS, 9.37 GB/s) vs. 7.7562 ms (-64.90 %)
GEMM(M=512,N=8008,K=2560) runtime: 3.0235 ms (6.94 TOPS, 16.35 GB/s) vs. 5.1597 ms (-41.40 %)
GEMM(M=512,N=8192,K=2048) runtime: 3.5080 ms (4.90 TOPS, 11.97 GB/s) vs. 8.9103 ms (-60.63 %)
GEMM(M=512,N=10240,K=2560) runtime: 4.0632 ms (6.61 TOPS, 15.38 GB/s) vs. 4.8217 ms (-15.73 %)
GEMM(M=512,N=30000,K=128) runtime: 5.5240 ms (0.71 TOPS, 6.65 GB/s) vs. 20.1042 ms (-72.52 %)
GEMM(M=512,N=30522,K=768) runtime: 6.7020 ms (3.58 TOPS, 11.23 GB/s) vs. 18.1121 ms (-63.00 %)
GEMM(M=512,N=30522,K=1024) runtime: 6.9931 ms (4.58 TOPS, 12.93 GB/s) vs. 18.3904 ms (-61.97 %)
GEMM(M=512,N=32128,K=512) runtime: 6.4790 ms (2.60 TOPS, 9.76 GB/s) vs. 17.9019 ms (-63.81 %)
GEMM(M=512,N=32128,K=1024) runtime: 7.4393 ms (4.53 TOPS, 12.79 GB/s) vs. 18.5621 ms (-59.92 %)
GEMM(M=512,N=50265,K=768) runtime: 10.5373 ms (3.75 TOPS, 11.72 GB/s) vs. 12.5688 ms (-16.16 %)
GEMM(M=512,N=51200,K=2048) runtime: 13.6468 ms (7.87 TOPS, 18.47 GB/s) vs. 12.3374 ms (10.61 %)
GEMM(M=819,N=768,K=768) runtime: 0.7904 ms (1.22 TOPS, 4.46 GB/s) vs. 2.4961 ms (-68.33 %)
GEMM(M=819,N=50358,K=768) runtime: 16.4101 ms (3.86 TOPS, 9.36 GB/s) vs. 15.2728 ms (7.45 %)
GEMM(M=832,N=768,K=768) runtime: 0.6074 ms (1.62 TOPS, 5.87 GB/s) vs. 2.0416 ms (-70.25 %)
GEMM(M=832,N=768,K=3072) runtime: 1.0136 ms (3.87 TOPS, 10.45 GB/s) vs. 1.8642 ms (-45.63 %)
GEMM(M=832,N=3072,K=768) runtime: 2.1773 ms (1.80 TOPS, 4.87 GB/s) vs. 5.4305 ms (-59.91 %)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-22 02:31:05+00:00,2025-09-22T02:43:56Z,,False,1,0,16,481,121,6,1,2025-09-22 02:43:56+00:00,25,13786,False,False,False,False,True,False,6,0,0,3406,1883,1523,2,16,,,,pytorch
163466,open,[BE] Remove PythonOpRegistrationTrampoline,PaliC,Ends up it was a lot easier to remove than expected :) ,2025-09-22 02:15:10+00:00,2025-09-25T15:34:15Z,,False,5,0,4,3,156,8,5,,42,55,False,False,False,False,False,False,8,4,877,27604,16832,10772,2,4,3.0,5.0,2025-09-22T02:15:11Z,pytorch
163465,closed,[2/N] Use filesystem in inductor,cyyever,Use std::filesystem in most inductor code. This is follow-up of https://github.com/pytorch/pytorch/pull/152288 .,2025-09-22 01:53:44+00:00,2025-09-23T12:42:49Z,,False,6,0,1,6,30,2,6,2025-09-23 03:56:19+00:00,32,112,False,False,False,False,False,False,2,5,1693,36,6,30,1,1,3.0,5.0,2025-09-22T17:28:12Z,pytorch
163464,open,[BE] Remove HermeticPyObjectTLS and Simplify PythonOpRegistrationTrampoline,PaliC,Removes HermeticPyObjectTLS as we no longer need since torch deploy is no longer supported. PythonOpRegistrationTrampoline is also drastically simplified as and being prepped for removal in a future PR.,2025-09-22 00:54:11+00:00,2025-09-25T15:34:23Z,,False,8,1,6,31,202,10,9,,75,202,False,False,False,False,False,False,10,7,1579,27853,16949,10904,2,6,4.0,8.0,2025-09-22T00:54:12Z,pytorch
163463,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-22 00:28:36+00:00,2025-09-22T21:26:02Z,,False,6,0,2,2,2,2,6,2025-09-22 21:25:00+00:00,46,156,False,False,False,False,False,False,2,5,1716,4,2,2,2,2,3.0,5.0,2025-09-22T00:28:37Z,pytorch
163461,closed,[WOQ][Inductor] Enable CUDA coverage for _weight_int8pack_mm,bbeckca,"Summary:
What: Unskip the CUDA path for test_int8_weight_only_quant in test_torchinductor.py as the kernel was added by #159325.

Why: Confirm CUDA backend for _weight_int8pack_mm is registered.

Test Plan:
```
buck2 test 'fbcode//mode/opt' fbcode//caffe2/test/inductor:test_inductor_cuda
```
https://www.internalfb.com/intern/testinfra/testrun/2533275104869494

Differential Revision: D82926440




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 00:25:17+00:00,2025-09-24T19:21:45Z,,False,12,0,1,0,1,1,12,2025-09-24 19:20:42+00:00,60,601,False,False,False,False,False,False,1,3,816,1,0,1,1,1,4.0,3.0,2025-09-22T03:34:59Z,pytorch
163460,closed,Triton template IMA reads on B200,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #159494
* #163537
* __->__ #163460



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-22 00:24:28+00:00,2025-09-22T20:35:45Z,,False,3,0,1,7,6,2,3,2025-09-22 20:34:41+00:00,33,317,False,False,False,False,False,False,2,2,674,13,7,6,1,1,4.0,3.0,2025-09-22T16:43:43Z,pytorch
163459,closed,switch from stack based to graph based aproach,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163459

",2025-09-21 21:50:37+00:00,2025-09-22T16:42:43Z,,False,3,0,5,488,637,7,3,2025-09-22 16:41:38+00:00,46,94,False,False,False,False,False,False,7,2,702,893,372,521,1,5,3.0,3.0,2025-09-22T03:03:19Z,pytorch
163458,closed,[SymmMem] Fix put_signal + wait_until hang,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163194
* #163152

The test used a wrong ptr to refer to remote address:
```
            dst_ptr = out_hdl.buffer_ptrs[peer]
            src_ptr = inp_hdl.buffer_ptrs[rank]
            sig_ptr = out_hdl.signal_pad_ptrs[peer]
```
All three indices should be `rank` instead of `peer` because NVSHMEM APIs accept local address as input and perform translation internally. Without correct signal address, the peer would be waiting, thus hang.

Also adjusted the signature of `nvshmem.putmem_signal_block` to accept tensor instead of pointer.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-21 21:00:37+00:00,2025-09-23T17:10:02Z,2025-09-23T17:10:02Z,True,1,0,1,44,59,2,1,2025-09-23 17:10:02+00:00,42,737,False,True,False,False,False,False,2,0,0,103,44,59,1,1,1.0,0.0,2025-09-23T17:09:51Z,pytorch
163456,closed,[BE]: Add a few more missing move from return indices,Skylion007,"@ezyang A follow up where I found a few more missing returns of this style in the codebase. Follow up to #163416

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-21 19:13:22+00:00,2025-09-22T20:25:28Z,,False,3,0,1,5,5,3,3,2025-09-22 20:24:26+00:00,53,260,False,False,False,False,False,False,3,2,496,10,5,5,1,1,4.0,3.0,2025-09-22T01:37:38Z,pytorch
163455,closed,Avoid `at::alias` in the `repeat` op implementation,haifeng-jin,"Avoid `at::alias` in the `repeat` op implementation

## Summary

This PR removed the usage of `at::alias` in the implementation and just `permute`+`reshape` the tensor to fit the specs of the result.
This is a less hacky and a more readable way of implementing the op.
All the new ops we are using are view-only ops, which does not introduce overhead of changing the storage.

## Who want this

We are using `PrivateUse1` and accelerator, but this request to avoid `at::alias` in any op should be general enough for any backend who is using XLA, or who do not have explicit control over the memory allocation on the devices.

## Why we/they need this

As we support TPU, we are overriding some ATen ops by binding them to PrivateUse1.
However, it is not recommended to override the `repeat` op directly as we saw the following in `RegistrationDeclaration.h`.

```
at::Tensor repeat(const at::Tensor & self, c10::SymIntArrayRef repeats); // {""schema"": ""aten::repeat(Tensor self, SymInt[] repeats) -> Tensor"", ""dispatch"": ""True"", ""default"": ""True""}
```

We had to reuse the existing implementation of `repeat` to decomposite to other ops.
However, we are unable to support the current implementation, which uses `at::alias`.
It have two tensors share the same storage and modify one of them and return the other assuming it is changed, too.

As, we do not have explicit control over the memory allocation of the tensors using XLA/PJRT.


## Alternatives

We are open to alternative solutions that work for us if this PR is not in favor of the PyTorch community.
For example, we may just bind our version of `repeat` op implementation to both `PrivateUse` and `AutogradPrivateUse1`.
However, to my understanding, this would not work well with torch dynamo and `torch.compile`.

Would you mind guiding us on how to solve this?

Thanks!
",2025-09-21 17:44:04+00:00,2025-09-25T04:35:58Z,,False,3,4,3,22,6,1,7,2025-09-24 22:28:27+00:00,51,1832,False,False,False,False,False,False,1,2,493,34,25,9,1,3,3.0,2.0,2025-09-21T18:01:19Z,pytorch
163454,open,[PowerPC] Disable MKLDNN TF32 on PowerPC to fix build failure,Tiwari-Avanish,"The commits f4d8bc46c7706f872abcb4ec41f0b32207d5d826 added TF32 support for x86 CPUs,
which causes build failures on PowerPC systems with mkldnn.  

This patch disables TF32 paths on PowerPC while keeping x86 TF32 support intact,
allowing PyTorch to build successfully on PowerPC.

I have run the mkldnn test case on PowerPC, and it passed successfully.

`pytest test/test_mkldnn.py
87 passed, 2 skipped in 1709.02s (0:28:29`

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-21 17:43:43+00:00,2025-09-24T06:03:25Z,,False,3,0,1,19,4,3,3,,61,508,False,True,False,False,False,False,3,2,328,23,19,4,1,1,1.0,2.0,2025-09-21T17:52:08Z,pytorch
163453,open,[ROCm][DO NOT MERGE] Dummy PR to run CI,rraminen,"Fixes #160598
Fixes #160551
Fixes #160507
Fixes #121806


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-21 16:17:54+00:00,2025-09-24T19:46:41Z,,False,6,0,1,1,1,1,6,,39,174,False,True,False,False,False,False,1,4,2822,2,1,1,1,1,2.0,4.0,2025-09-23T22:29:46Z,pytorch
163452,closed,[MPS] Fix compile linalg inv,Isalia20,"Fixes #161969


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-21 16:11:07+00:00,2025-09-22T10:37:59Z,,False,3,13,4,16,10,2,16,2025-09-22 10:36:56+00:00,28,217,False,True,False,False,False,False,2,2,493,34,20,14,1,4,3.0,2.0,2025-09-21T17:45:42Z,pytorch
163448,closed,Run CI,rraminen,"Fixes #160598
Fixes #160551
Fixes #160507
Fixes #121806",2025-09-21 15:12:29+00:00,2025-09-21T16:09:37Z,,False,2,0,1,1,0,1,2,2025-09-21 16:09:37+00:00,6,55,False,True,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
163447,closed,[torch] DRY a couple of lines in unpickler,yfeldblum,"Test Plan: CI.

Reviewed By: dolpm

Differential Revision: D82660989




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-21 13:03:31+00:00,2025-09-21T20:30:38Z,,False,4,0,1,1,3,1,4,2025-09-21 20:29:36+00:00,42,120,False,False,False,False,False,False,1,2,493,4,1,3,1,1,2.0,2.0,2025-09-21T17:48:36Z,pytorch
163446,open,[AMP][Refactor] Simplify dtype support logic in autocast context manager,KarhouTam,"
## Description:

This PR refactors the autocast context manager in `autocast_mode.py` to simplify and centralize the logic for checking supported dtypes for each device. The previous implementation repeated similar checks for multiple device types. Now, a single mapping `device_supported_dtypes` is used to associate device types with their supported dtypes, and the validation logic is unified. 

In my view, this makes the code easier to maintain and extend for new devices.

Please share any suggestions and comments with me.

BTW, in the original `xla` branch, the `supported_dtype` are `[torch.float16, torch.bfloat16]`, https://github.com/pytorch/pytorch/blob/5d8a226e23339e7243a2a84afd174f685f145b68/torch/amp/autocast_mode.py#L358-L363 but the warning message has only `torch.bfloat16`.

cc @mcarilli @ptrblck @leslie-fang-intel @jgong5",2025-09-21 12:56:45+00:00,2025-09-25T15:01:12Z,,False,4,13,5,44,96,1,17,,72,846,False,False,False,False,False,True,1,3,643,262,105,157,1,5,4.0,4.0,2025-09-21T12:58:45Z,pytorch
163445,closed,Simplify BFLOAT16_AVAILABLE,cyyever,"Simplify `BFLOAT16_AVAILABLE` by using `torch.cuda.is_bf16_supported()`  and `torch.xpu.is_bf16_supported()`. Outdated comments are also removed.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-21 11:50:39+00:00,2025-09-22T07:33:37Z,,False,8,4,1,1,6,2,12,2025-09-22 07:31:49+00:00,27,248,False,False,False,False,False,False,2,7,1824,7,1,6,1,1,4.0,7.0,2025-09-21T17:55:06Z,pytorch
163444,closed,Enable half precision types on test_conv_cudnn_nhwc_support,cyyever,This PR adds flaot16 and bfloat16 cases to `test_conv_cudnn_nhwc_support` and removes outdated comments.,2025-09-21 10:43:33+00:00,2025-09-22T04:18:43Z,,False,3,0,1,1,4,1,3,2025-09-22 04:11:23+00:00,59,104,False,False,False,False,False,False,1,2,549,5,1,4,1,1,3.0,3.0,2025-09-21T17:37:07Z,pytorch
163443,closed,Remove C++ and test branches for CUDA<12,cyyever,Remove conditional branches for CUDA<12.,2025-09-21 10:20:10+00:00,2025-09-22T18:22:25Z,,False,3,0,1,3,39,6,3,2025-09-22 18:20:11+00:00,40,40,False,False,False,False,False,False,6,2,493,42,3,39,1,1,3.0,2.0,2025-09-22T15:30:55Z,pytorch
163442,closed,Remove outdated commented CMake code,cyyever,Policies `CMP0023` and `CMP0022` have been removed in CMake 4.,2025-09-21 09:33:52+00:00,2025-09-22T23:53:10Z,,False,3,0,1,1,2,1,3,2025-09-22 23:07:39+00:00,36,62,False,False,False,False,False,False,1,2,493,3,1,2,1,1,2.0,2.0,2025-09-22T20:26:30Z,pytorch
163441,closed,[submodule] Bump libfmt to 12.0.0,cyyever,"libfmt 12.0 brings new optimisations and fixes some compilation issues for clang 21 (https://github.com/fmtlib/fmt/pull/4477).
For a detailed release log, see https://github.com/fmtlib/fmt/releases/tag/12.0.0 ",2025-09-21 09:06:10+00:00,2025-09-22T01:24:11Z,,False,3,0,1,1,1,1,3,2025-09-21 22:37:27+00:00,33,209,False,True,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-21T17:37:33Z,pytorch
163440,closed,Remove workarounds for Python 3.6,cyyever,"This PR removes tuple unpacking workarounds for Py 3.6 form two distributed files. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-21 08:53:54+00:00,2025-09-22T04:18:49Z,,False,3,0,1,2,8,2,3,2025-09-22 04:08:08+00:00,33,186,False,False,False,False,False,False,2,2,493,10,2,8,1,1,3.0,2.0,2025-09-21T17:46:34Z,pytorch
163439,closed,Use functools.cache on has_efa,cyyever,"Cache the result of `has_efa` by `functools.cache`.
",2025-09-21 08:45:07+00:00,2025-09-23T13:25:30Z,,False,7,0,1,5,10,1,7,2025-09-23 05:03:05+00:00,30,52,False,False,False,False,False,False,1,6,1748,15,5,10,1,1,3.0,6.0,2025-09-21T08:53:26Z,pytorch
163438,closed,[BC breaking] Remove deprecated imports for torch.utils.data.datapipes.iter.grouping,cyyever,"This PR removes import tricks of `SHARDING_PRIORITIES` and  `ShardingFilterIterDataPipe` from `torch.utils.data.datapipes.iter.grouping`. They are declared to be removed in PyTorch 2.1 but not.
Before change:
```
import torch.utils.data.datapipes.iter.grouping.SHARDING_PRIORITIES
import torch.utils.data.datapipes.iter.grouping.ShardingFilterIterDataPipe
```
works
After change:
there is an import error exception.",2025-09-21 08:01:07+00:00,2025-09-23T05:04:33Z,,False,10,0,1,0,69,2,10,2025-09-23 05:02:10+00:00,84,415,False,False,False,False,False,False,2,9,2579,69,0,69,1,1,3.0,10.0,2025-09-22T20:48:09Z,pytorch
163437,open,[Code Clean] Replace std::runtime_error with TORCH_CHECK,zhudada0120,"Replace the runtime_error of the vallina C++ exceptions with TORCH_CEHCK
Including:
- torch/csrc/export
- torch/csrc/cuda

Fixes #148114 ",2025-09-21 07:35:13+00:00,2025-09-24T19:46:15Z,,False,6,5,3,61,92,6,11,,56,137,False,True,False,False,False,False,6,4,467,159,64,95,1,3,4.0,5.0,2025-09-21T07:42:50Z,pytorch
163436,open,refactor operators into their own files,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* #163735
* #163734
* #163733
* #163732
* __->__ #163436
* #163406

",2025-09-21 06:56:42+00:00,2025-09-24T05:33:24Z,,False,2,0,3,5940,350,73,2,,39,154,False,False,False,False,False,True,73,0,0,9232,5955,3277,1,3,,,,pytorch
163434,closed,[inductor] Freeze layouts in FlexAttention,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163584
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* __->__ #163434
* #163419

Fixes #163300

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-21 05:04:48+00:00,2025-09-23T16:21:29Z,,False,6,2,6,114,3,4,8,2025-09-23 15:37:51+00:00,42,412,False,True,False,False,False,False,4,5,1010,17853,8928,8925,1,6,3.0,5.0,2025-09-21T15:17:22Z,pytorch
163433,closed,"[2/n] Support module.to(""cuda:0"") in FakeTensorMode on cuda-less machine",SherlockNoMad,"Summary:
To support exporting a cuda model on a CPU-only machine under fake tensor mode. 
User commonly need to move sample inputs to the cuda device with .to(""cuda:0"") or .to(""cuda"") call. 
This diff supports this. 

I expect the following pattern to work

```
with FakeTensorMode(allow_non_fake_inputs=True):
    cuda_module = module.to(""cuda:0"")
    cuda_sample_inputs = tuple([x.to(""cuda:0"") for x in sample_inputs])

    with torch.no_grad():
        ep = torch.export.export(cuda_module, cuda_sample_inputs)

```

Before 
Moving module.to(""cuda:0"") under fake tensor mode would have parameter on `meta` device.

After
parameters would be on ""cuda:0"" .

Test Plan: buck2 run  fbcode//caffe2/test:fake_tensor -- --r test_move_module

Reviewed By: mikaylagawarecki

Differential Revision: D80102876


",2025-09-21 02:39:36+00:00,2025-09-22T20:17:39Z,,False,6,6,1,44,13,3,12,2025-09-22 20:16:37+00:00,72,804,False,False,False,False,False,False,3,1,529,57,44,13,1,1,3.0,2.0,2025-09-22T14:54:09Z,pytorch
163432,open,[precompile] Add option to disable guard check on aot-compiled function.,zhxchen17,"Summary:
Under circumstances it seems reasonable to return a callable directly without guard check when user use aot_compile on a function with single compilation result.

When having multiple entries (aot_compile_module), we should start enabling guard check to differetiate different compiled functions apart.

Test Plan: CI

Differential Revision: D82904540




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-21 02:36:55+00:00,2025-09-23T19:32:09Z,,False,7,0,1,21,1,2,7,,72,535,False,False,False,False,False,False,2,5,949,22,21,1,1,1,4.0,6.0,2025-09-22T20:12:43Z,pytorch
163431,open,[BE] Remove multinterpreter logic from pythonFallbackKernel and python_dispatch,PaliC,"Removes some logic that assumes we have multiple interpreters running around and instead uses the global interpreter.

Also remove a giant comment about pyinterpreter tags I forgot to remove earlier. ",2025-09-21 02:04:28+00:00,2025-09-22T13:29:28Z,,False,3,2,1,30,141,3,5,,79,200,False,False,False,False,False,False,3,2,147,171,30,141,1,1,2.0,2.0,2025-09-21T02:04:28Z,pytorch
163430,closed,[CI][CUDA][B200] Skip tests to unblock #159494,nWEIdia,"Help unblock #159494  which fixes the issue that there are no per-commit B200 tests even though runners are available in https://hud.pytorch.org/runners/pytorch 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @ptrblck @eqy @tinglvv @atalman @malfet @drisspg @ngimel ",2025-09-21 01:57:43+00:00,2025-09-22T18:54:03Z,,False,4,0,4,9,2,2,4,2025-09-22 18:54:02+00:00,46,422,False,True,False,False,False,False,2,3,536,23,15,8,1,4,2.0,4.0,2025-09-21T02:02:01Z,pytorch
163426,closed,[Flex attention] Fix flex attention head broadcast,Isalia20,"Fixes part of #163314

In particular bug: **Bug 1: H=None Broadcasting Produces Incorrect Results**

This fixes a shape bug when slicing BlockMask on the Q-tile axis with an int (**mask[:, :, i]**). That form of indexing collapses the Q dimension, so kv_num_blocks/kv_indices lose their expected [B, H, Q_tiles, …] shape. Due to them losing shape, even though the mask_mod remains ""interpretable"", the kernel’s stride math then reads wrong offsets. Due to this we get silent numerical mismatches compared to regular SDPA, especially when single position decoding/H broadcasting.

The B=None, H=None works case is accidental: with singleton batch/head the kernel maps to index 0 via `sparse_idx_z = off_zq % 1` and `sparse_idx_hq = off_hq % 1` and with a single Q tile `q_start // SPARSE_Q_MULTIPLE = 0`. The missing Q-tiles stride is multiplied by 0, so the bad offset from the collapsed Q axis doesn’t move the pointer and it happens to read the first tile correctly. Once H > 1 or there are multiple Q tiles, those terms become nonzero and the kernel indexes with wrong strides which causes silent error

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @drisspg @yanboliang @BoyuanFeng",2025-09-20 22:43:11+00:00,2025-09-23T13:03:01Z,,False,8,2,2,89,7,2,10,2025-09-23 13:01:56+00:00,50,1350,False,True,False,False,False,False,2,6,1808,98,90,8,1,2,4.0,9.0,2025-09-20T23:10:36Z,pytorch
163424,closed,[MTIA] Enable MTIA serialization,PatriceVignola,"Differential Revision: D82900370


",2025-09-20 22:03:29+00:00,2025-09-22T20:57:15Z,,False,3,0,1,6,0,2,3,2025-09-22 20:57:10+00:00,32,35,False,False,False,False,False,False,2,0,0,6,6,0,1,1,,,,pytorch
163423,open,[SymmMem] Promote `@requires_nvshmem` instead of `enable_triton`,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163423

### Issue
The previous `enable_triton` UI requires the user-defined Triton kernel have a ""nvshmem"" in its name.
If users did not do so, the kernel would miss the NVSHMEM init, and silently hit CUDA IMA. 

The `@require_nvshmem` decorator eliminates the above name requirement (and the `enable_triton` call).

### Usage:
```
@requires_nvshmem
@triton.jit
def foo(...):
    ...

foo[(1, 1)](...)
```
It also remove the need of passing `extern_lib` to `foo` (handled by the decorator now).

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @mcarilli @ptrblck @leslie-fang-intel @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-20 21:54:10+00:00,2025-09-22T21:31:33Z,,False,9,5,3,275,176,2,14,,64,1083,False,False,False,False,False,False,2,8,2203,487,293,194,2,3,6.0,9.0,2025-09-21T02:43:31Z,pytorch
163422,closed,[inductor] Fix error from custom CUDA allocators,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163584
* #163377
* #163482
* #163520
* #163481
* __->__ #163422
* #163412
* #163393
* #163434
* #163419

Fixes #163257

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 21:28:26+00:00,2025-09-23T15:39:01Z,,False,6,2,6,5,1,1,8,2025-09-23 15:38:00+00:00,48,412,False,True,False,False,False,False,1,5,1675,17705,8906,8799,1,6,4.0,6.0,2025-09-20T21:44:55Z,pytorch
163421,open,[wip] consolidate on fullgraph name,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163478
* #163477
* #163476
* #163475
* #163474
* #163473
* #163472
* #163471
* #163470
* #163469
* __->__ #163421



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-20 20:56:07+00:00,2025-09-22T03:47:24Z,,False,1,0,3,102,98,22,1,,35,410,False,False,False,False,False,False,22,0,0,204,104,100,2,3,,,,pytorch
163419,closed,[inductor] libdevice.sqrt => tl.sqrt_rn,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163584
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* __->__ #163419

Fixes #163082

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 19:34:16+00:00,2025-09-23T16:21:19Z,,False,4,0,5,16,2,3,4,2025-09-23 15:37:49+00:00,39,412,False,True,False,False,False,False,3,3,144,17746,8827,8919,1,5,3.0,3.0,2025-09-20T20:12:00Z,pytorch
163418,open,[BE]: Add typing utils to copy signatures from methods or signatures,Skylion007,"Testing out this little shim to improve typing through out the codebase.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @lolpack @maggiemoss @ndmitchell @kinto0 @samwgoldman",2025-09-20 19:26:56+00:00,2025-09-23T09:37:08Z,,False,3,6,5,88,39,4,9,,68,229,False,False,False,False,True,False,4,2,3726,173,111,62,1,5,4.0,4.0,2025-09-21T02:48:52Z,pytorch
163417,closed,Add torchfuzz initial impl.,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163417


all details are in readme.md
Note: one thing i want to do soonest is to switch to graph representation instead of stack representation
for the fuzzed ops should make things easier as things get more complicated. ",2025-09-20 19:04:25+00:00,2025-09-21T19:19:00Z,,False,3,2,9,2522,0,8,5,2025-09-21 19:17:56+00:00,27,307,False,False,False,False,False,False,8,2,722,2924,2723,201,1,9,3.0,3.0,2025-09-20T19:45:53Z,pytorch
163416,closed,[BE][Ez]: Prevent copies of std::vector in CUDA ForeachOps,Skylion007,"No need for unnecessary copy of std::vectors. This Tensor list is copied throughout the foreach paths and this code is on a hot path for torch optimizers. Auto move elision will not happen on the return statement since it's a subelement of a vector that needs to be copied out before the std::vector is dtor'd. This should reduce quite a few list copies along this path.
",2025-09-20 18:59:13+00:00,2025-09-21T05:25:17Z,,False,3,0,2,14,14,7,3,2025-09-21 05:24:15+00:00,58,371,False,False,False,False,False,False,7,2,493,28,14,14,1,2,2.0,2.0,2025-09-21T02:39:05Z,pytorch
163415,closed,[inductor] Don't require_dense for grid_sampler_2d_backward,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419
* __->__ #163415
* #163414
* #163387
* #163398
* #163386

Fixes #163372

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 16:56:54+00:00,2025-09-22T21:54:15Z,,False,4,0,4,51,1,2,4,2025-09-22 21:53:09+00:00,59,440,False,True,False,False,False,False,2,3,541,17756,8850,8906,1,4,3.0,3.0,2025-09-20T20:12:40Z,pytorch
163414,closed,[inductor] Skip test_baddmm on XPU,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419
* #163415
* __->__ #163414
* #163387
* #163398
* #163386

Fixes #161484

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 16:26:40+00:00,2025-09-22T21:53:10Z,,False,3,2,5,3,0,1,5,2025-09-22 21:53:08+00:00,34,440,False,True,False,False,False,False,1,2,96,17709,8803,8906,1,5,3.0,2.0,2025-09-20T21:47:33Z,pytorch
163412,closed,[dynamo] Fix TorchFunctionMode handling with get_rng_state,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163584
* #163377
* #163482
* #163520
* #163481
* #163422
* __->__ #163412
* #163393
* #163434
* #163419

Fixes #162624
Fixes #162586

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-20 16:20:39+00:00,2025-09-23T15:37:58Z,,False,3,0,6,22,3,2,3,2025-09-23 15:37:58+00:00,58,395,False,True,False,False,False,False,2,2,96,17771,8979,8792,1,6,2.0,2.0,2025-09-22T20:15:55Z,pytorch
163411,closed,Update fbgemm submodule,cthi,"Test Plan: 

As titled, includes some new changes fbgemm to see if CUDA13 breakage is fixed.

Reviewers:

Subscribers:

Tasks:

Tags:

Fixes #ISSUE_NUMBER
",2025-09-20 15:56:18+00:00,2025-09-22T15:47:18Z,,False,4,0,1,1,1,1,4,2025-09-22 15:46:15+00:00,23,155,False,True,False,False,False,False,1,3,602,2,1,1,1,1,3.0,4.0,2025-09-20T17:19:19Z,pytorch
163410,open,[precompile] Add an graph capture API as a stage of aot compile.,zhxchen17,"Summary: For expressing the intention of capturing a single graph with particular example inputs, we want to add a new stage to the API which returns a single captured graph + metadata. This will be useful when power user don't want the full dynamo + aot autograd + , but rather want to use these components in a modularized way.

Test Plan:
test_graph_capture_basic_fn
test_graph_capture_basic_module_forward

Differential Revision: D82892673




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-20 14:37:34+00:00,2025-09-20T17:19:06Z,,False,2,0,1,88,0,3,2,,64,618,False,False,False,False,False,False,3,0,0,88,88,0,1,1,,,,pytorch
163407,open,Slightly improve speed of AdamW by rearranging order of bias_correction2_sqrt,ad8e,"Source of idea: https://x.com/Tim_Dettmers/status/1969131103798567295

> Looking closer, PyTorch also uses FP32, but here's the real reason why bnb Adam is better: we optimized for float numerics, order does matter! Computing sqrt(v) + eps*c2 then dividing avoids amplifying errors vs PyTorch's sqrt(v)/c2 + eps. Same math, better stability!

I don't believe this is true. The stability benefit should be meaningless; there is some discussion at https://discord.com/channels/729741769192767510/1079865324087803985/1418777002592174190 (accessing this requires joining the [EleutherAI discord](https://discord.gg/zBGx3azzUn))

However, Dettmer's version is faster than PyTorch's, because it does 2 tensor-wise multiplications instead of 1 elementwise division on sqrt(v). So I copied it to the torch implementation. My own optimizer does not have an eps/beta2 interaction, so I have no prior testing of this change. If momentum buffers are changed to fp16/bf16, instead of float32 as they are now, this new formulation will have higher precision than the old formulation.

Changed:
_single_tensor_adam, non-capturable
_multi_tensor_adam, non-capturable
_multi_tensor_adam, capturable

Not changed:
_single_tensor_adam, capturable. The existing code is extremely inefficient for reasons I don't understand. Thus, I left it alone.
fused adam

Not tested:
Whether ""capturable multi tensor adam"" still satisfies being ""capturable"". I don't know the contract it must satisfy, so I can't test it.

Tested:
It's faster for normal usecases, but testing is not comprehensive. If there are only 2 tensors, the change to capturable `_multi_tensor_adam` is slower.
Loss curves match exactly with the changed versions. In this testcase, `eps` is set to a deliberately high number to make differences obvious:
```
import torch
import torch.nn as nn
from torch.optim.adamw import AdamW

torch.manual_seed(42)
torch.cuda.manual_seed_all(42)

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 10)
        self.linear2 = nn.Linear(10, 1)

    def forward(self, x):
        return self.linear2(self.linear(x))

model = SimpleModel()

learning_rate = 0.1
weight_decay = 0.01

# to change: capturable, foreach
optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, eps=1.0, capturable=True)

criterion = nn.MSELoss()

input_data = torch.randn(32, 10)
target_data = torch.randn(32, 1)

# optional: cuda
model = model.cuda()
input_data = input_data.cuda()
target_data = target_data.cuda()

num_epochs = 10
for epoch in range(num_epochs):
    outputs = model(input_data)
    loss = criterion(outputs, target_data)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

Unknown:
Why the old code used `(lr / bc) * -1`. I changed it instead to `-(lr / bc)` (and then `-(lr * bc2 / bc1)`), but maybe there is some hidden behavior?

cc @jerryzh168",2025-09-20 06:25:25+00:00,2025-09-23T05:43:43Z,,False,6,7,2,23,9,1,13,,77,3018,False,False,False,False,True,False,1,5,2424,34,24,10,1,2,5.0,6.0,2025-09-20T17:47:24Z,pytorch
163406,open,torchfuzz,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163736
* __->__ #163406

",2025-09-20 05:12:09+00:00,2025-09-25T06:39:43Z,,False,2,0,7,8728,0,91,2,,9,104,False,False,False,False,False,False,91,0,0,112493,77161,35332,1,7,,,,pytorch
163405,open,code movement,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163436
* #163406
* __->__ #163405
* #163404
* #163403
* #163402
* #163401

",2025-09-20 05:12:05+00:00,2025-09-21T06:56:46Z,,False,1,0,1,417,134,4,1,,13,154,False,False,False,False,False,False,4,0,0,551,417,134,1,1,,,,pytorch
163404,open,skip erroring at known issues and add debug mode,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163436
* #163406
* #163405
* __->__ #163404
* #163403
* #163402
* #163401

",2025-09-20 05:12:02+00:00,2025-09-21T06:56:45Z,,False,2,0,1,243,46,1,2,,48,154,False,True,False,False,False,False,1,0,0,289,243,46,1,1,,,,pytorch
163403,open,add optins to run given seed single and print test output,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163436
* #163406
* #163405
* #163404
* __->__ #163403
* #163402
* #163401

",2025-09-20 05:11:58+00:00,2025-09-21T06:56:44Z,,False,2,0,1,501,330,1,2,,57,154,False,False,False,False,False,False,1,0,0,831,501,330,1,1,,,,pytorch
163402,open,fuzzer WIP,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163436
* #163406
* #163405
* #163404
* #163403
* __->__ #163402
* #163401

",2025-09-20 05:11:54+00:00,2025-09-21T06:56:44Z,,False,2,0,1,2181,0,4,2,,10,154,False,False,False,False,False,False,4,0,0,2181,2181,0,1,1,,,,pytorch
163401,open,add support for hint_override in mark_unbacked,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163436
* #163406
* #163405
* #163404
* #163403
* #163402
* __->__ #163401

",2025-09-20 05:11:50+00:00,2025-09-21T06:56:43Z,,False,1,0,1,92,12,4,1,,46,154,False,False,False,False,False,False,4,0,0,104,92,12,1,1,,,,pytorch
163399,open,Add cse for make_block_ptr in Triton codegen,nandesuka,"Summary: per title

Test Plan: added test cases

Differential Revision: D82648215




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 04:18:55+00:00,2025-09-25T15:34:14Z,,False,11,0,1,68,5,3,11,,44,287,False,False,False,False,False,False,3,2,99,73,68,5,1,1,2.0,3.0,2025-09-24T03:35:18Z,pytorch
163398,closed,[inductor] Fix bug where viewed outputs get padded,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419
* #163415
* #163414
* #163387
* __->__ #163398
* #163386

Fixes #163328

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 03:57:09+00:00,2025-09-22T21:53:06Z,,False,3,0,5,72,1,2,3,2025-09-22 21:53:05+00:00,50,440,False,True,False,False,False,False,2,2,96,17737,8829,8908,1,5,2.0,2.0,2025-09-22T13:14:13Z,pytorch
163396,open,[BE][CI] Unify requirments,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163653
* #163649
* #163648
* __->__ #163396

I.e. both Linux, Windows and MacOS CI workflows will uses the same `requirements-ci.txt`
Keep `pip-requirements-macOS.txt` as symlink to this one (will be removed later on)",2025-09-20 01:09:08+00:00,2025-09-23T22:46:49Z,,False,1,0,5,14,83,7,1,,26,296,False,False,False,False,False,False,7,0,0,16305,13283,3022,1,5,1.0,0.0,2025-09-20T23:52:59Z,pytorch
163395,closed,[graph partition] Add way to register custom rule (#163310),zou3519,"This PR adds an experimental way to register a custom rule for if inductor should partition the graph around an operator.

Test Plan:
- new test

Pull Request resolved: https://github.com/pytorch/pytorch/pull/163310
Approved by: https://github.com/ProExpertProg, https://github.com/BoyuanFeng, https://github.com/eellison
ghstack dependencies: #162117, #162307, #162651

Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 01:03:03+00:00,2025-09-23T01:18:07Z,2025-09-23T01:18:07Z,True,1,0,1,105,0,2,1,2025-09-23 01:18:07+00:00,59,594,False,True,False,False,False,False,2,0,0,105,105,0,1,1,1.0,0.0,2025-09-23T01:18:02Z,pytorch
163394,closed,Combine strong and weak refcounts in intrusive_ptr in a single refcount,mcfi,"Summary:
Currently, we assume that refcount_ and weakcount_ are always stored in an 8-byte aligned address right next to each other. Based on this assumption, we load 8 bytes in intrusive_ptr::reset_ to check the values of both counts. However, that assumption is not part of C++ language standard so it's essentially undefined behavior.

This change eliminates that assumption by combining refcount_ and weakcount_ in a single 64-bit count and we use the lower 32 bits for refcount_ and upper 32 bits for the weakcount_.

In addition to eliminating the undefined behavior, the change also eliminates the read of weakcount_ after decrementing refcount_ in intrusive_ptr::reset_. This claws back lost performance introduced in https://github.com/pytorch/pytorch/pull/162784 for non-final refcount_ decrementing.

Reviewed By: yfeldblum

Differential Revision: D82869192


",2025-09-20 00:41:03+00:00,2025-09-22T17:54:34Z,,False,10,10,1,144,119,2,20,2025-09-22 17:53:31+00:00,71,871,False,False,False,False,False,False,2,2,1085,263,144,119,1,1,4.0,4.0,2025-09-20T16:54:07Z,pytorch
163393,closed,[inductor] Support out_dtype arg to matmul,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163584
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* __->__ #163393
* #163434
* #163419

Fixes #163275

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-20 00:33:50+00:00,2025-09-23T15:37:53Z,,False,3,3,8,100,23,4,6,2025-09-23 15:37:53+00:00,42,420,False,True,False,False,False,False,4,2,130,17926,9167,8759,1,8,4.0,3.0,2025-09-22T13:06:14Z,pytorch
163392,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-20 00:23:57+00:00,2025-09-21T04:46:06Z,,False,9,0,1,1,1,1,9,2025-09-21 04:34:17+00:00,46,156,False,False,False,False,False,False,1,8,1911,2,1,1,1,1,3.0,8.0,2025-09-20T00:23:58Z,pytorch
163391,open,[DO NOT LAND] hack fsdp2 to support AC(fully_shard(model)),weifengpy,"support AC(fully_shard for torchtitan. for fsdp2 + EP, titan has fully_shard(AC(layer)) and fully_shard(layer.moe.experts): https://github.com/pytorch/torchtitan/issues/1624

repro
NGPU=4 CONFIG_FILE=""./torchtitan/models/deepseek_v3/train_configs/deepseek_v3_16b.toml"" ./run_train.sh --parallelism.expert_parallel_degree=2
```
[rank0]:[titan] 2025-09-24 14:29:13,258 - root - INFO - step:  1  loss: 12.0215  grad_norm:  1.6971  memory: 45.95GiB(48.37%)  tps: 628  tflops: 6.65  mfu: 0.67%
[rank0]:[titan] 2025-09-24 14:29:13,258 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-09-24 14:29:19,726 - root - INFO - step: 10  loss:  8.7136  grad_norm:  8.2123  memory: 60.42GiB(63.60%)  tps: 11,399  tflops: 120.58  mfu: 12.19%
```

fully_shard(AC(layer)) and fully_shard(layer.moe.expert) is like this
* fully_shard: pre_backward and post_backward
* AC: recompute_fn pre_forward and post_forward

this PR is a hack to prove feasibility
* AC triggers _pre_forward, don't let it change post_forward_order
* backward_prefetch is mssed up, disable it and rely on implicit prefetching

NGPU=4 DP=4 EP=1: `NGPU=4 CONFIG_FILE=""./torchtitan/models/deepseek_v3/train_configs/deepseek_v3_16b.toml"" ./run_train.sh`
```
[rank0]:[titan] 2025-09-23 15:26:33,405 - root - INFO - step:  1  loss: 12.0140  grad_norm:  1.7175  memory: 39.80GiB(41.89%)  tps: 1,446  tflops: 15.30  mfu: 1.55%
[rank0]:[titan] 2025-09-23 15:26:33,405 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-09-23 15:26:40,323 - root - INFO - step: 10  loss:  8.6579  grad_norm: 10.2424  memory: 53.92GiB(56.76%)  tps: 10,658  tflops: 112.74  mfu: 11.40%
```

NGPU=4 DP=4 EP=2: `NGPU=4 CONFIG_FILE=""./torchtitan/models/deepseek_v3/train_configs/deepseek_v3_16b.toml"" ./run_train.sh --parallelism.expert_parallel_degree=2`

```
[rank0]:[titan] 2025-09-23 15:43:41,702 - root - INFO - step:  1  loss: 12.0520  grad_norm:  1.7147  memory: 38.52GiB(40.55%)  tps: 1,074  tflops: 11.36  mfu: 1.15%
[rank0]:[titan] 2025-09-23 15:43:41,702 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-09-23 15:43:47,939 - root - INFO - step: 10  loss:  8.7658  grad_norm: 11.8868  memory: 52.59GiB(55.37%)  tps: 11,821  tflops: 125.05  mfu: 12.64%
```


NGPU=4 DP=4 EP=2: `NGPU=4 CONFIG_FILE=""./torchtitan/models/deepseek_v3/train_configs/deepseek_v3_16b.toml"" ./run_train.sh --parallelism.expert_parallel_degree=2` without explicit prefetching

```
[rank0]:[titan] 2025-09-24 14:18:36,888 - root - INFO - step:  1  loss: 12.0258  grad_norm:  1.7011  memory: 39.05GiB(41.11%)  tps: 451  tflops: 4.77  mfu: 0.48%
[rank0]:[titan] 2025-09-24 14:18:36,888 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-09-24 14:18:43,211 - root - INFO - step: 10  loss:  8.8057  grad_norm: 17.1135  memory: 52.81GiB(55.59%)  tps: 11,662  tflops: 123.36  mfu: 12.47%
```


NGPU=4 DP=4 EP=1, no ac: `NGPU=4 CONFIG_FILE=""./torchtitan/models/deepseek_v3/train_configs/deepseek_v3_16b.toml"" ./run_train.sh --parallelism.expert_parallel_degree=1 --activation_checkpoint.mode=none`
```
[rank0]:[titan] 2025-09-23 15:50:54,421 - root - INFO - step:  1  loss: 12.0143  grad_norm:  1.6909  memory: 61.36GiB(64.60%)  tps: 1,802  tflops: 19.06  mfu: 1.93%
[rank0]:[titan] 2025-09-23 15:50:54,422 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-09-23 15:51:00,467 - root - INFO - step: 10  loss:  8.7015  grad_norm:  7.2523  memory: 79.04GiB(83.21%)  tps: 12,195  tflops: 129.00  mfu: 13.04%
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163391
* #163130

Summary:

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-20 00:18:46+00:00,2025-09-25T01:51:23Z,,False,2,0,2,19,4,1,2,,58,3952,False,False,False,False,False,False,1,1,42,31,23,8,1,2,1.0,1.0,2025-09-20T00:23:29Z,pytorch
163390,closed,[dynamo] Fix handling of kwargs in exception constructor,rtimpe,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163390



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-20 00:15:16+00:00,2025-09-24T22:45:20Z,,False,8,0,2,30,3,4,8,2025-09-24 22:44:17+00:00,56,266,False,True,False,False,False,False,4,7,2269,26912,16356,10556,1,2,3.0,7.0,2025-09-22T18:29:41Z,pytorch
163389,open,[dynamo][guards] Use only arguments for the framelocals to dict conversion,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163389
* #163385



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-20 00:05:48+00:00,2025-09-20T03:48:02Z,,False,2,0,1,50,8,1,2,,74,276,False,False,False,False,False,False,1,0,0,58,50,8,1,1,,,,pytorch
163388,open,[Inductor][Intel GPU] Save `threads_per_warp` from tirton compiled kernel for launching kernel correctly in cpp wrapper.,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163315

On the Inductor XPU backend, `threads_per_warp` is not always 32. For Intel GEMM Triton kernels, it can be 16. This information must be preserved for XPU so that the Cpp wrapper can launch the kernel with the correct configuration.
 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 00:02:49+00:00,2025-09-23T23:31:12Z,,False,2,0,1,13,4,3,2,,120,530,False,False,False,False,False,False,3,1,167,17,13,4,1,1,3.0,1.0,2025-09-22T21:26:26Z,pytorch
163387,closed,[inductor] Fallback on strided complex add,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419
* #163415
* #163414
* __->__ #163387
* #163398
* #163386

Fixes #163243
Fixes #162561

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-20 00:00:54+00:00,2025-09-22T21:53:07Z,,False,3,2,7,70,0,3,5,2025-09-22 21:53:07+00:00,42,454,False,True,False,False,False,False,3,2,96,17960,9088,8872,1,7,3.0,2.0,2025-09-22T13:09:42Z,pytorch
163386,closed,Better decomp for torch.eye,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163482
* #163520
* #163481
* #163422
* #163412
* #163393
* #163434
* #163419
* #163415
* #163414
* #163387
* #163398
* __->__ #163386

",2025-09-20 00:00:50+00:00,2025-09-22T21:53:05Z,,False,7,5,4,14,4,1,12,2025-09-22 21:53:04+00:00,27,224,False,False,False,False,False,False,1,6,812,17419,8749,8670,1,4,5.0,7.0,2025-09-20T00:07:55Z,pytorch
163385,open,[dynamo][guards] Revert introduction of different types of lambda_guards,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163389
* __->__ #163385

With
https://fb.workplace.com/groups/260102303573409/permalink/787294574187510/
issue, it might be a better idea to just speedup _realize_dict and keep
the changes very local. So reverting this PR as well, to return to clean
slate.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 23:50:44+00:00,2025-09-20T04:46:24Z,,False,1,0,1,5,131,3,1,,72,507,False,False,False,False,False,False,3,0,0,136,5,131,1,1,,,,pytorch
163384,open,"Amin, amax and aminmax registration for MTIA device",trirpi,"Summary: This diff introduces amax/amin/aminmax AOT registration.

Differential Revision: D82682931


",2025-09-19 23:45:35+00:00,2025-09-20T01:49:54Z,,False,4,0,1,3,3,1,4,,51,102,False,False,False,False,False,False,1,1,46,6,3,3,1,1,1.0,1.0,2025-09-19T23:47:29Z,pytorch
163383,closed,Clean up obsoleted vLLM tests,huydhn,"They have been removed in https://github.com/vllm-project/vllm/pull/25117 and https://github.com/vllm-project/vllm/pull/22772, thus failing in trunk at the moment after the latest pin commit update

cc @zou3519 ",2025-09-19 23:11:58+00:00,2025-09-20T02:49:42Z,,False,3,0,1,1,5,1,3,2025-09-20 02:48:40+00:00,29,211,False,False,False,False,False,False,1,2,790,6,1,5,1,1,5.0,2.0,2025-09-19T23:19:01Z,pytorch
163382,closed,[triton] update 3.5 pin to bbb06c0334a6772b92d24bde54956e675c8c6604,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163382

Includes:
* https://github.com/triton-lang/triton/pull/8211 to work around a PTXAS bug that was causing 03-matrix-multiplication tutorial matmuls to underperform due to excessive WGMMA waits
* https://github.com/triton-lang/triton/pull/8157 to fix a convert_layout bug

Verified that this passes Triton CI in https://github.com/pytorch/pytorch/pull/159158 and improves gemm perf (see https://github.com/pytorch/pytorch/issues/159704)",2025-09-19 23:06:35+00:00,2025-09-22T22:42:03Z,,False,9,0,1,1,1,1,9,2025-09-22 20:21:02+00:00,67,527,False,True,False,False,True,False,1,6,2055,2,1,1,1,1,5.0,7.0,2025-09-19T23:08:15Z,pytorch
163381,closed,Lazy import to avoid circular import issue for DebugMode,SherlockNoMad,as title.,2025-09-19 22:52:28+00:00,2025-09-20T14:47:34Z,,False,3,0,1,5,4,1,3,2025-09-20 01:54:59+00:00,56,9,False,True,False,False,False,False,1,2,493,9,5,4,1,1,3.0,2.0,2025-09-19T22:55:06Z,pytorch
163380,closed,[Graph Partition] improve custom op output alias,pytorchbot,"For a custom op with multiple outputs, we will see the following generated code:
```
buf1 = op1(arg0)
buf3 = buf0[0]
buf4 = buf0[1]
del buf1 # <--- if buf1 is not accessed in the future
```

If `buf1` is not accessed in the future, it's good to deallocate early. So we don't delay `del` until both buf3 and buf4 are not used anymore. Note that buf3 and buf4 hold reference to the data such that `del buf1` does not prevent their usage.

However, when there are mutating args, we don't see `del buf1` immediately.

```python
@torch.library.custom_op(
    ""mylib::op1"",
    mutates_args=[""x""],
    schema=""(Tensor(a!)?  x) -> (Tensor, Tensor)"",
    device_types=""cuda"",
)
def op1(x) -> tuple[torch.Tensor, torch.Tensor]:
    x = x + 1
    return (x + 1, x + 2)
```

<img width=""661"" height=""821"" alt=""image"" src=""https://github.com/user-attachments/assets/3d1d1f5a-9749-4652-bb02-da593c78702d"" />



Why? Because `buf3` is a MultiOutput with `buf1` as input and believes `buf1` (an output of FallbackKernel op1) has inputs that alias output.
https://github.com/pytorch/pytorch/blob/72fedf05752069c9e8b97c64397aedf6ee2bf5ec/torch/_inductor/ir.py#L7976-L7982


According to `[NOTE: FallbackKernel supported operators]`, as a mutating op that are auto-functionalizable, buf1's output should NOT alias any of the inputs. This PR improves get_inputs_that_alias_output of Fallback Kernel.

Use case: [moe custom op in vllm](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/layer.py#L2057-L2064)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 22:46:32+00:00,2025-09-19T23:36:03Z,2025-09-19T23:36:03Z,True,1,0,1,78,4,3,1,2025-09-19 23:36:03+00:00,48,1725,False,False,False,False,True,False,3,0,0,82,78,4,1,1,1.0,0.0,2025-09-19T23:17:17Z,pytorch
163379,open,[CUDA] revert PR 130472,pytorchbot,"This change may also resolve https://github.com/pytorch/pytorch/issues/161789, though verification is still needed.

PR #130472 would introduced the problem of  freeing the same address without clean metadata. according to the below discussion, reverted it.",2025-09-19 22:10:07+00:00,2025-09-20T07:01:31Z,,False,1,0,1,17,105,4,1,,23,257,False,False,False,False,False,False,4,0,0,122,17,105,1,1,1.0,0.0,2025-09-20T07:01:31Z,pytorch
163378,open,Refactor Provenance Tracking,yushangdi,"Summary:
- Move the `provenance_level` flag check to inside the `set_kernel_post_grad_provenance_tracing` call to simply the code

- Move the `set_kernel_post_grad_provenance_tracing` call and `write_provenance_debug_handle` call to `codegen_comment`.

- If some `call_kernel` call sites don't have a proceeding `codegen_comment` call, add one. Now all `call_kernel` call sites are accompanied with a  `codegen_comment` call.

- Add a `codegen_comment` method to BaseScheduling and remove the noop `codegen_comment` method in Scheduling


- Remove `debug_handle` from `call_kernel`.

Test Plan:
CI

```
buck run @//mode/opt-split-dwarf fbcode//caffe2/test/inductor:provenance_tracing
```

Differential Revision: D82839271




cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 21:51:45+00:00,2025-09-24T19:46:44Z,,False,3,0,1,99,102,12,3,,28,1041,False,True,False,False,False,True,12,0,0,201,99,102,1,1,1.0,0.0,2025-09-19T22:08:56Z,pytorch
163377,open,Fix prims.broadcast_in_dim functionalization,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163377

Fixes #163037

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 21:44:48+00:00,2025-09-25T09:20:36Z,,False,2,1,19,222,0,3,3,,44,322,False,True,False,False,False,False,3,1,4368,19272,10074,9198,1,19,3.0,2.0,2025-09-24T03:18:52Z,pytorch
163376,open,[SymmMem] Barrier on team instead of world,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162681
* #162680
* __->__ #163298

As titled. Avoiding a potential hang when running dispatch and combine in subgroups.

The rest is just re-arrange of the tests to create a sub-group test class. (no substantial change)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-19 21:38:22+00:00,2025-09-19T23:20:20Z,,False,1,0,1,107,72,2,1,,42,413,False,False,False,False,False,False,2,0,0,179,107,72,1,1,,,,pytorch
163375,open,[SymmMem] Fix memory allocation hold-up,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162681
* __->__ #162680
* #163298

Problem:
Without MemPool it looks like nvshmem backend never deallocates memory.

Cause:
Handles in `symm_mems_` (a map) keeps reference to memory allocations.

Solution:
- Remove reference to allocation from handles -- the reference is never used anyway.
- Use `unique_ptr` instead of `shared_ptr` to wrap allocation to ensure single ownership.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-19 21:37:45+00:00,2025-09-20T00:34:23Z,,False,1,0,1,18,13,2,1,,39,574,False,True,False,False,False,False,2,0,0,31,18,13,1,1,,,,pytorch
163373,closed,[ROCm] Fix environment variable AOTRITON_INSTALLED_PREFIX,xinyazhang,"Early assignment of `__AOTRITON_LIB` breaks the usage of environment variable `$AOTRITON_INSTALLED_PREFIX`

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-19 21:17:11+00:00,2025-09-22T15:02:26Z,,False,8,1,1,7,7,1,9,2025-09-22 15:01:22+00:00,57,224,False,True,False,False,False,False,1,7,1233,14,7,7,1,1,6.0,7.0,2025-09-19T21:18:00Z,pytorch
163371,closed,use reduction hint for aggressive rblock,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163371

I had been using tiling scores to essentially check if this is an inner reduction. since that is not fully rolled out for dynamic shapes, use reduction hint when they are not available.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 20:46:58+00:00,2025-09-23T22:05:28Z,,False,3,1,3,21,10,1,4,2025-09-23 22:04:25+00:00,40,483,False,False,False,False,False,False,1,2,498,38910,28781,10129,1,3,3.0,3.0,2025-09-22T21:17:20Z,pytorch
163370,closed,Add analytics ID to cpp docs,svekars,cc @sekyondaMeta @AlannaBurke,2025-09-19 20:45:46+00:00,2025-09-23T21:34:13Z,,False,5,0,1,1,0,1,5,2025-09-19 21:45:23+00:00,28,29,False,False,False,True,False,False,1,4,1242,1,1,0,1,1,4.0,5.0,2025-09-19T20:58:04Z,pytorch
163369,open,distributed autotuning,aorenste,"**Posted for internal discussion - not ready for general review yet.**

Co-authored-by: Paul Zhang <paulzhan@umich.edu>

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163369
* #163368



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 20:34:34+00:00,2025-09-22T17:07:05Z,,False,5,9,2,437,4,6,14,,22,527,False,False,False,False,False,False,6,3,276,35641,22757,12884,1,2,4.0,3.0,2025-09-19T22:00:35Z,pytorch
163368,open,refactor: move replace_operation_buffer to global scope,aorenste,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163369
* __->__ #163368

",2025-09-19 20:34:27+00:00,2025-09-22T06:50:37Z,,False,2,0,2,27,27,1,2,,55,104,False,False,False,False,False,True,1,0,0,35000,22220,12780,1,2,,,,pytorch
163367,open,[CuTe] Add layout overlap checking and layout remapping tensor util function,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163358
* #161224
* #163213
* __->__ #163367
* #163288
* #163212

While refactoring the bookkeeping for DeviceMesh while leveraging CuTe layout, we found that we need to have two more util functions. One is to check whether one layout has overlap inside it or not. For example, (2,2):(2:1) has no overlap while (2,2):(2:2) has overlap.

Another function (the name is open for discussion) is that layout acts as a backend of mesh tensor bookkeeping (indexing indices), it needs to be used as indices for remap back to the mesh tensor for new DeviceMesh generation and backend init. For example, in the case of 2K to 4K, the underlying layout is (2K, 1) but the actual value of the mesh tensor is [2K, 2K+1, ....,]. While flattening, slicing, we need to remap the layout back to the new mesh tensor so it maps the actual device allocation. For example, in the 2K to 4K case, if the shape is (1K, 1K) with dim_names (""dp"", ""tp""). Then when slicing ""tp"", the mesh tensor should be (2K, 2K+1, ..., 3K-1) or (3K, 3K+1, ... 4K-1). not the global ranks generated from the layout. (1K, 1).


cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-19 20:32:44+00:00,2025-09-24T22:33:08Z,,False,3,17,4,209,0,2,20,,76,1254,False,False,False,False,False,True,2,2,759,10031,6079,3952,1,4,5.0,3.0,2025-09-20T16:40:57Z,pytorch
163366,closed,use reduction hint for aggressive rblock,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163366
* #163365



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 20:31:18+00:00,2025-09-19T20:50:26Z,,False,1,0,1,13,9,1,1,2025-09-19 20:50:25+00:00,40,307,False,False,False,False,False,False,1,0,0,22,13,9,1,1,,,,pytorch
163365,closed,Less aggressive persistent reduction when it could induce large masking with dynamic shapes,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163365

As per comment in source code:
```
            # If we are are coalescing on xblock (not ReductionHint.INNER) and this is not a tiny kernel
            # (not ReductionHint.OUTER_TINY), do not use persistent reduction if it induces tile
            # quantization. Peristent reduction forces rblock == rnumel, if the bounds between lower
            # and upper are large, for the lower values we will be masking off large % of read/writes,
            # when we could expand the coalescing xblock instead.
```

For the test case in question, this pr improves perf from 0.8573521325143717 -> 0.043151492193814305 because we were egregiously masking out rblock values (58/64 values). 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D82853279](https://our.internmc.facebook.com/intern/diff/D82853279)",2025-09-19 20:30:05+00:00,2025-09-23T22:00:04Z,,False,7,2,6,65,0,2,9,2025-09-23 21:59:00+00:00,91,1073,False,False,False,False,True,False,2,6,1122,105,85,20,1,6,6.0,7.0,2025-09-19T20:45:23Z,pytorch
163364,closed,[Opitmus] fix fp8 activation quatization for duplicates forward output,mengluy0125,"Summary: We observe a case then the fwd graph has duplicated return nodes, which will lead to errors due to fx renaming the node, thus we add poi info into the node name.

Test Plan:
### unit test

```
CUDA_VISIBLE_DEVICES=3 buck2 test mode/opt -m ovr_config//triton:beta -c fbcode.nvcc_arch=b200a -c fbcode.platform010_cuda_version=12.8 //caffe2/test/functorch:test_aotdispatch -- test_quantize_activation_duplicate_nodes
```

Buck UI: https://www.internalfb.com/buck2/de5eccc6-4064-4214-843d-70b8e3829afe
Test UI: https://www.internalfb.com/intern/testinfra/testrun/4503599937670844
Network: Up: 217KiB  Down: 72KiB  (reSessionID-73e5c269-4f4d-4a54-896a-79c077eea326)
Executing actions. Remaining     0/2                                                        0.1s exec time total
Command: test.     Finished 1 local
Time elapsed: 45.9s
Tests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0


### E2E

before
f798417700

after

Differential Revision: D82844100




cc @mlazos",2025-09-19 20:08:28+00:00,2025-09-20T06:34:27Z,,False,7,0,1,233,10,2,7,2025-09-20 06:33:24+00:00,70,988,False,True,False,False,False,False,2,1,476,243,233,10,1,1,2.0,1.0,2025-09-19T23:53:57Z,pytorch
163363,closed,Skip test_ind_worker_queue on Windows and macOS (flaky),pytorchbot,"Fixes https://github.com/pytorch/pytorch/issues/68643

It was closed by the bot yesterday and the issue was still there https://github.com/pytorch/pytorch/actions/runs/17595694816/job/49989589647.  It's better to just skip it directly in the code as this test has been disabled on Windows and MacOS since 2021 O_o",2025-09-19 20:06:02+00:00,2025-09-19T20:07:00Z,2025-09-19T20:07:00Z,True,1,0,1,5,0,1,1,2025-09-19 20:07:00+00:00,55,313,False,True,False,False,False,False,1,0,0,5,5,0,1,1,1.0,0.0,2025-09-19T20:06:53Z,pytorch
163362,closed, Fix for ValueError: ProcessGroupXCCL::gather: invalid tensor type at index 0 ,jemitche1,"Issue and Resolution: A device mismatch -  the process group requires tensors participating in gather to be on XPU, but gathers into a CPU tensor

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 19:52:10+00:00,2025-09-21T06:42:16Z,,False,3,1,145,1398,1086,96,4,2025-09-21 06:42:16+00:00,78,416,False,True,False,False,False,False,96,1,204,146100,79495,66605,5,29,1.0,1.0,2025-09-21T02:37:11Z,pytorch
163361,closed,Add decomp rule to assert_tensor_metadata for BatchedTensors ,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163008


Whenever there is device move, export introduces assert_tensor_metadata aten operator to make sure to guard for device specialization. This aten op didn't work with Vmap because we didn't register explicit decomp rule saying we just skip BatchedTensor and call it on underlying tensor 

Differential Revision: [D82483979](https://our.internmc.facebook.com/intern/diff/D82483979)",2025-09-19 19:50:26+00:00,2025-09-19T20:01:21Z,2025-09-19T20:00:57Z,True,1,0,1,18,0,2,1,2025-09-19 20:00:57+00:00,61,473,False,False,False,False,False,False,2,0,0,18,18,0,1,1,1.0,0.0,2025-09-19T19:57:58Z,pytorch
163360,closed,[Caffe2] Improve SVE batch box cox by 2%,Nicoshev,"Summary:
Improve bound checking on exp computation, decreasing the longest dependency chain by 1.

Box-cox benchmarks show about 2% of improved throughput.
Precision remains unaltered.

before:

NonZeroLambdaBatch                                        155.30us     6.44K

after:

NonZeroLambdaBatch                                        151.78us     6.59K

Test Plan:
Correctness:

buck2 test @//mode/opt //koski/functions_contrib/df4ai/tests:batch_box_cox_test


Performance:

buck2 run @//mode/opt //koski/functions_contrib/df4ai/benchmark:boxcox_benchmark

Differential Revision:
D82847111

Privacy Context Container: L1208939


",2025-09-19 19:34:34+00:00,2025-09-20T06:43:31Z,,False,5,0,1,12,6,1,5,2025-09-20 06:42:29+00:00,40,634,False,False,False,False,True,False,1,2,518,18,12,6,1,1,3.0,2.0,2025-09-19T21:33:48Z,pytorch
163358,open,[For Discussion][DeviceMesh] Implement a concatenate api for submesh,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163358
* #161224
* #163213
* #163367
* #163288
* #163212

Today FSDP needs to slicing out spmd mesh from root mesh here: https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/_fully_shard/_fsdp_param.py#L301. But essentially, users want is a concatenate of some submesh into a big mesh and used as a spmd mesh. This PR is tentatively trying to implement this API for users.

One thing to note is that, all sub-mesh needs to slicing/flatten or unflatten from same root mesh otherwise the indices make no sense when it comes to mesh indexing and device allocation.


cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-19 18:34:58+00:00,2025-09-24T22:46:02Z,,False,1,8,4,69,1,2,9,,68,758,False,False,False,False,False,False,2,0,0,10135,6074,4061,1,4,2.0,0.0,2025-09-21T01:13:26Z,pytorch
163357,open,Adding check for square matrix for input tensor in matrix_exp backwar…,mansiag05,"…d op.

Fixes #146796 
",2025-09-19 18:24:54+00:00,2025-09-19T21:46:05Z,,False,3,0,1,16,0,2,3,,70,23,False,True,False,False,False,False,2,2,505,16,16,0,1,1,2.0,3.0,2025-09-19T18:52:59Z,pytorch
163356,open,[TESTING] Performance operator microbenchmarks baseline pytorch 2.8,jainapurva,"Fixes #ISSUE_NUMBER
",2025-09-19 17:49:32+00:00,2025-09-25T08:48:50Z,,False,1,0,6,49,4,3,1,,67,20,False,True,False,False,False,False,3,0,0,569,318,251,2,6,,,,pytorch
163355,open,Fix: Prevent Direct Inheritance of torch._C.ScriptObject,krastogi-in,"Direct Inheritance of ScriptObject causes Segmentation fault or Assertion Error. Reason being, @torch.jit.script decorator compiles the Python class and creates a proper classType with static schema. This constructs ivalue::Object with valid information and initializes all constructors properly.
With direct inheritance, super.init() calls C++ default constructor. Default Constructor leaves ivalue uninitialized or corrupted. When someone created class variable that triggers setattr, which calls self.type(). Here type() method calls ivalue->type->expect()

Adding py::isfinal() gives meanigful error:
```TypeError: type 'torch._C.ScriptObject' is not an acceptable base type```

I think design was not to support direct inheritance . However, If we need to support it then this will be a feature enhancement. Would you like me to implement the feature to enable indirect inheritance?



Fixes #161854


cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-19 17:38:22+00:00,2025-09-22T21:01:14Z,,False,2,0,1,1,1,1,2,,56,954,False,True,True,False,False,False,1,1,69,2,1,1,1,1,1.0,1.0,2025-09-19T19:44:39Z,pytorch
163354,closed,[CUDA][cuBLAS][FP8] Forward-fix #162022,eqy,"@ngimel is right, `ciflow/h100` doesn't actually appear to test the PR :(

cc @ptrblck @msaroufim @jerryzh168 @csarofeen @xwang233",2025-09-19 17:26:35+00:00,2025-09-21T00:56:17Z,,False,3,0,1,1,1,1,3,2025-09-21 00:55:15+00:00,39,130,False,True,False,False,False,False,1,2,498,2,1,1,1,1,4.0,2.0,2025-09-19T23:14:36Z,pytorch
163353,closed,[ez][CI] Run vllm workflow on vllm pin updates,clee2000,"As in title

The auto pin update was merged without running vllm workflow",2025-09-19 17:09:12+00:00,2025-09-19T17:33:55Z,,False,3,0,1,3,0,1,3,2025-09-19 17:32:52+00:00,46,73,False,False,False,False,False,False,1,2,781,3,3,0,1,1,3.0,2.0,2025-09-19T17:29:42Z,pytorch
163352,closed,Don't use declarations in global namespace in stable headers,mikaylagawarecki,"Fixes https://github.com/pytorch/pytorch/issues/163338

Configured https://clang.llvm.org/extra/clang-tidy/checks/google/global-names-in-headers.html for torch/csrc/stable

Note that doesn't error for the DeleterFnPtr case, but will generate the following for the `using torch::stable::Tensor;`

```
>>> Lint for torch/csrc/stable/ops.h:

  Error (CLANGTIDY) [google-global-names-in-headers,-warnings-as-errors]
    using declarations in the global namespace in headers are prohibited

         10  |#include <torch/csrc/inductor/aoti_torch/generated/c_shim_aten.h>
         11  |#include <torch/headeronly/core/ScalarType.h>
         12  |
    >>>  13  |using torch::stable::Tensor;
         14  |
         15  |namespace torch::stable {
         16  |
   ```


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163352

",2025-09-19 16:35:03+00:00,2025-09-19T21:16:58Z,,False,3,0,4,46,27,3,3,2025-09-19 21:15:55+00:00,60,857,False,True,False,False,False,False,3,2,499,89,54,35,1,4,3.0,3.0,2025-09-19T18:32:45Z,pytorch
163351,closed,[dynamo] Fix issue with namedtuple slicing,jansel,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163377
* #163393
* #163387
* #163386
* __->__ #163351

Fixes #163253

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela @mlazos",2025-09-19 16:23:22+00:00,2025-09-20T00:43:09Z,,False,3,0,1,23,0,2,3,2025-09-20 00:42:05+00:00,42,371,False,True,False,False,False,False,2,2,493,23,23,0,1,1,4.0,2.0,2025-09-19T17:51:52Z,pytorch
163350,closed,Realize LazyVariableTracker before raising exception,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163350

Improves error message reported on #163321

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 15:59:42+00:00,2025-09-19T19:26:24Z,,False,3,0,2,3,1,1,3,2025-09-19 19:25:20+00:00,52,308,False,False,False,False,True,False,1,2,493,6,4,2,1,2,4.0,2.0,2025-09-19T16:06:02Z,pytorch
163349,closed,[CD] Simplify NVIDIA driver installation step,malfet,"Undo changes introduced in https://github.com/pytorch/pytorch/pull/160956 as driver has been updated to 580 for both fleets

Fixes https://github.com/pytorch/pytorch/issues/163342",2025-09-19 15:54:47+00:00,2025-09-24T19:43:35Z,,False,6,0,1,0,2,1,6,2025-09-19 18:50:51+00:00,45,179,False,True,False,False,False,False,1,5,1916,2,0,2,1,1,5.0,5.0,2025-09-19T15:55:04Z,pytorch
163348,open,[CD][Security] Do not switch between runners for binary builds,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163348



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-19 15:33:48+00:00,2025-09-20T04:21:34Z,,False,1,0,2,99,657,19,1,,62,212,False,False,False,False,False,False,19,0,0,812,127,685,1,2,,,,pytorch
163345,open,[ROCm][DO NOT MERGE] Dummy PR to test disabled open issues,pragupta,"Fixes #159489
Fixes #159488 
Fixes #152700
Fixes #125555
Fixes #134139
Fixes #125991
Fixes #125918


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-19 14:58:41+00:00,2025-09-22T06:22:00Z,,False,5,0,1,1,1,1,5,,58,217,False,True,False,False,False,False,1,3,453,2,1,1,1,1,2.0,3.0,2025-09-20T22:19:39Z,pytorch
163344,closed,Simplify _compute_local_shape_and_global_offset and make it SPMD.,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163344

There is only one substantive change: the branch on
`global_offset[shard_dim] <= local_offset[shard_dim]`
is removed because it is unnecessary: you can always treat the
first shard uniformly with the rest of the shards, because your
global offset is guaranteed to be zero in this case anyway.

I also switch the shard_size case to sym_ite, to make it possible
for LocalTensor to deal with the MPMD-ness here, but it's equivalent
to the old if-then-else.

I tried to rewrite the comments to be more clear what is going on
algorithmically here.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-19 14:52:56+00:00,2025-09-24T02:25:17Z,,False,3,0,2,81,58,1,3,2025-09-24 02:24:11+00:00,65,780,False,False,False,False,False,False,1,2,502,141,82,59,1,2,5.0,4.0,2025-09-22T15:34:36Z,pytorch
163341,closed,[BE] Introduce `CONDA_ROOT_DIR`,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162310
* #162862
* __->__ #163341
* #163339

Which equal to `%CONDA_PARENT_DIR%/Miniconda3`, and replace this pattern with `%CONDA_ROOT_DIR%` throughout the codebase",2025-09-19 14:16:23+00:00,2025-09-24T22:30:44Z,,False,5,2,1,6,6,3,7,2025-09-19 19:45:35+00:00,31,244,False,False,False,False,False,False,3,4,999,12,6,6,1,1,5.0,4.0,2025-09-19T18:44:40Z,pytorch
163340,closed,Delete functorch C extension entirely.,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163340

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @mcarilli @ptrblck @leslie-fang-intel @jgong5 @albanD @voznesenskym @penguinwu @EikanWang @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 13:53:46+00:00,2025-09-24T06:10:05Z,,False,28,0,4,0,4884,12,28,2025-09-24 06:09:01+00:00,38,490,False,False,False,False,False,False,12,25,6007,15617,7458,8159,1,4,6.0,27.0,2025-09-21T02:57:36Z,pytorch
163339,closed,Move ROCM trunk wheel builds to 3.10,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162310
* #162862
* #163341
* __->__ #163339

This code is a delicious spaghetti: Sometimes python version is defined in jinja template (see https://github.com/pytorch/pytorch/pull/162297 ) sometimes in shell script (see https://github.com/pytorch/pytorch/pull/162877 ), but this time around it's in a python file (and there is another one called `generate_binary_build_matrix.py` that defines `FULL_PYTHON_VERSIONS`)

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-19 13:33:16+00:00,2025-09-24T22:25:47Z,,False,5,0,2,8,8,2,5,2025-09-19 18:52:03+00:00,36,613,False,False,False,False,False,False,2,4,1301,16,8,8,1,2,5.0,4.0,2025-09-19T18:43:10Z,pytorch
163335,open,"[dynamo, nested graph breaks] enable basic nested graph break wrapped tests",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160611
* __->__ #163335
* #162737
* #160601
* #163503



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 11:07:02+00:00,2025-09-23T00:02:10Z,,False,1,0,3,81,76,6,1,,75,306,False,False,False,False,False,False,6,0,0,298,180,118,1,3,,,,pytorch
163334,open,[TESTING] Prepare perf number 2.9,huydhn,"No need to review

Notes:
* Reinstall torch==2.9.0 RC from https://download.pytorch.org/whl/test/cu128
* Rebuild torchrec and fbgemm because these packages don't have RC wheels for 2.9 yet

Runs:
1. https://github.com/pytorch/pytorch/actions/runs/17875805215 (RC1)
2. https://github.com/pytorch/pytorch/actions/runs/17944558378 (RC3)
3. https://github.com/pytorch/pytorch/actions/runs/17968862007 (RC4)
4. https://github.com/pytorch/pytorch/actions/runs/17996391195 (RC4 + new LLM models from https://github.com/pytorch/pytorch/pull/163565)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 10:34:37+00:00,2025-09-25T13:00:04Z,,False,1,0,11,34,10,5,1,,33,712,False,False,False,False,False,False,5,0,0,31362,19580,11782,1,11,,,,pytorch
163333,open,[TESTING] Prepare perf baseline number 2.8,huydhn,"No need to review

Notes:
* Reinstall torch==2.8.0
* Revert https://github.com/pytorch/pytorch/pull/159922 as nativert benchmark wasn't part of 2.8, so it couldn't be run anyway

Runs:
1. https://github.com/pytorch/pytorch/actions/runs/17875630289
2. https://github.com/pytorch/pytorch/actions/runs/17944602603
3. https://github.com/pytorch/pytorch/actions/runs/17996350479

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 10:28:20+00:00,2025-09-25T11:19:46Z,,False,1,0,8,30,55,5,1,,42,545,False,False,False,False,False,False,5,0,0,31393,19571,11822,1,8,,,,pytorch
163332,open,[XPU] Enhance XPUGeneratorImpl functionality to support XPUGraph,majing921201,"As this [XPUGraph RFC](https://github.com/pytorch/pytorch/issues/162143) descripted. This PR enhances `XPUGeneratorImpl` to support XPUGraph.
In this PR, we add `XPUGerneratorState` and `PhiloxXpuState`. Which makes XPUGraph update philox state during graph capture and replay correctly

XPUGraph PR submission plan:

- [ ] 1, Enhance XPUGenerator functionality. Add XPUGeneratorState and philoxState
- [ ] 2, implemenet XPUGraph capture_begin/capture_end/instantiate functionality
- [ ] 3, implemenet XPUGraph replay/debug_dump/reset functionality
- [ ] 4, python APIs: is_current_stream_capturing/graph_pool_handle/graph
- [ ] 5, python APIs: make_graphed_callables
",2025-09-19 10:15:18+00:00,2025-09-25T03:33:43Z,,False,5,7,5,263,21,6,12,,64,668,False,True,False,False,False,False,6,4,368,492,367,125,1,5,3.0,5.0,2025-09-19T18:20:43Z,pytorch
163326,open,[Only for perf test] Copy of PR #159459,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163326


On behalf of  @jianyizh  This is the clone of ##159459 , only used for cuda performance test.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 06:52:25+00:00,2025-09-24T00:01:48Z,,False,2,0,1,13,2,1,2,,39,391,False,False,False,False,False,False,1,0,0,15,13,2,1,1,,,,pytorch
163324,open,[Inductor] support masked vectorization for the tail_loop for fp8 datatype,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163490
* __->__ #163324
* #163316



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-19 06:46:09+00:00,2025-09-22T09:46:43Z,,False,1,0,3,19,13,2,1,,74,314,False,False,False,False,False,False,2,0,0,104,87,17,1,3,,,,pytorch
163322,closed,[hop] support local_map + SAC,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163602
* __->__ #163322

Some ops like local_map hop's deferred mode are not desugared by make_fx, this means that when we apply SAC tags, we will need to define dispatch rules for the SAC torch dispatch modes as pointed out here: https://github.com/pytorch/pytorch/issues/162246#issuecomment-3259176721. This PR adds those rules.

Additionally it fixes a pre-existing issue where we weren't coercing tangent layout (that AOTAutograd typically does) when partitioning the HOP joint.",2025-09-19 06:36:36+00:00,2025-09-24T04:58:48Z,,False,5,13,6,286,40,2,18,2025-09-24 04:57:43+00:00,29,561,False,True,False,False,False,False,2,4,601,504,375,129,1,6,4.0,5.0,2025-09-19T16:46:45Z,pytorch
163320,open,[DTensor] Handle Broadcast properly for aten.expand,SherlockNoMad,"Fixes #160968

Think about following case
```
        mesh = init_device_mesh(DEVICE_TYPE, (4,))
        tensor = torch.randn(1, 4)
        dt = distribute_tensor(tensor, mesh, [Shard(0)])
        out = dt.expand(4, 4)
```
Tensor of shape[1, 4] is unevenly sharded on 0-th dim into 4 shard. Only the first rank holds the local_tensor [1, 4], other ranks holds local_tensor of shape[0, 4]. 

When expanding to shape of [4, 4], (or more generally [N, 4]), there could be 2 potential resulting placements
1. [S(0)].  Each rank would result in local_tensor of [1, 4]. This would require one-to-all broadcast.
2. [Replicated]. Each rank would result in local_tensor of [4, 4]. But in practice, it would first replicate the input of shape [1, 4], and then expand locally. 

In both cases, this would require a broadcasting collective, and the redistribution cost is the same. 
However, we are using ""AllGather followed by Slice"" to simulate ""one-to-all broadcasting"" collective. 

So I am choosing Option 2, to replicate the input tensor first, and then expand locally. 

Generally, for view operation on DTensor, we would need some kind of ""lazy communication"": record the op, and only materialize communication when need. 



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-19 04:55:44+00:00,2025-09-21T01:24:09Z,,False,2,0,2,25,0,2,2,,51,1323,False,True,False,False,False,False,2,1,269,25,25,0,1,2,1.0,1.0,2025-09-21T01:24:09Z,pytorch
163319,closed,[inductor] fix as_strided lowering with .view(dtype) inputs,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163319

FIXES https://github.com/pytorch/pytorch/issues/163286

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 03:51:14+00:00,2025-09-23T12:52:07Z,,False,7,1,4,38,3,3,8,2025-09-23 12:51:01+00:00,59,351,False,True,False,False,False,False,3,6,1418,51,43,8,1,4,3.0,6.0,2025-09-22T13:35:49Z,pytorch
163318,closed,[Flex] Changing how bwd configs are setup and updating default b200 config,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163318
```Shell
Up to 4x perf boost

🔝 Top 5 Performance Differences (by absolute %):
shape: (5, 7)
┌───────────┬────────────────┬────────────────────────────────┬───────────────────┬─────────────────────────────┬─────────────────────────────────┬────────────┐
│ attn_type ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)          ┆ TFlops BWD (base) ┆ TFlops BWD (better_configs) ┆ better_configs_speedup_over_ba… ┆ pct_delta  │
│ ---       ┆ ---            ┆ ---                            ┆ ---               ┆ ---                         ┆ ---                             ┆ ---        │
│ str       ┆ str            ┆ str                            ┆ f64               ┆ f64                         ┆ f64                             ┆ f64        │
╞═══════════╪════════════════╪════════════════════════════════╪═══════════════════╪═════════════════════════════╪═════════════════════════════════╪════════════╡
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 32768, 16, 32768, 128) ┆ 124.775035        ┆ 532.580435                  ┆ 4.268325                        ┆ 326.832527 │
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 16384, 16, 16384, 128) ┆ 124.494557        ┆ 519.798488                  ┆ 4.175271                        ┆ 317.527078 │
│ causal    ┆ torch.bfloat16 ┆ (4, 16, 32768, 16, 32768, 128) ┆ 123.984189        ┆ 512.877391                  ┆ 4.136635                        ┆ 313.663544 │
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 8192, 16, 8192, 128)   ┆ 122.827725        ┆ 496.195958                  ┆ 4.039772                        ┆ 303.977164 │
│ causal    ┆ torch.bfloat16 ┆ (4, 16, 16384, 16, 16384, 128) ┆ 123.826738        ┆ 484.244647                  ┆ 3.910663                        ┆ 291.066303 │
└───────────┴────────────────┴────────────────────────────────┴───────────────────┴─────────────────────────────┴─────────────────────────────────┴────────────┘

🔺 Top 5 Cases Where better_configs (change) is Faster than base (baseline):
shape: (5, 7)
┌───────────┬────────────────┬────────────────────────────────┬───────────────────┬─────────────────────────────┬─────────────────────────────────┬────────────┐
│ attn_type ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)          ┆ TFlops BWD (base) ┆ TFlops BWD (better_configs) ┆ better_configs_speedup_over_ba… ┆ pct_delta  │
│ ---       ┆ ---            ┆ ---                            ┆ ---               ┆ ---                         ┆ ---                             ┆ ---        │
│ str       ┆ str            ┆ str                            ┆ f64               ┆ f64                         ┆ f64                             ┆ f64        │
╞═══════════╪════════════════╪════════════════════════════════╪═══════════════════╪═════════════════════════════╪═════════════════════════════════╪════════════╡
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 32768, 16, 32768, 128) ┆ 124.775035        ┆ 532.580435                  ┆ 4.268325                        ┆ 326.832527 │
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 16384, 16, 16384, 128) ┆ 124.494557        ┆ 519.798488                  ┆ 4.175271                        ┆ 317.527078 │
│ causal    ┆ torch.bfloat16 ┆ (4, 16, 32768, 16, 32768, 128) ┆ 123.984189        ┆ 512.877391                  ┆ 4.136635                        ┆ 313.663544 │
│ noop      ┆ torch.bfloat16 ┆ (4, 16, 8192, 16, 8192, 128)   ┆ 122.827725        ┆ 496.195958                  ┆ 4.039772                        ┆ 303.977164 │
│ causal    ┆ torch.bfloat16 ┆ (4, 16, 16384, 16, 16384, 128) ┆ 123.826738        ┆ 484.244647                  ┆ 3.910663                        ┆ 291.066303 │
└───────────┴────────────────┴────────────────────────────────┴───────────────────┴─────────────────────────────┴─────────────────────────────────┴────────────┘

🔻 Top 5 Cases Where better_configs (change) is Slower than base (baseline):
shape: (5, 7)
┌───────────────┬────────────────┬───────────────────────────────┬───────────────────┬─────────────────────────────┬─────────────────────────────────┬───────────┐
│ attn_type     ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)         ┆ TFlops BWD (base) ┆ TFlops BWD (better_configs) ┆ better_configs_speedup_over_ba… ┆ pct_delta │
│ ---           ┆ ---            ┆ ---                           ┆ ---               ┆ ---                         ┆ ---                             ┆ ---       │
│ str           ┆ str            ┆ str                           ┆ f64               ┆ f64                         ┆ f64                             ┆ f64       │
╞═══════════════╪════════════════╪═══════════════════════════════╪═══════════════════╪═════════════════════════════╪═════════════════════════════════╪═══════════╡
│ document_mask ┆ torch.bfloat16 ┆ (4, 16, 8192, 16, 8192, 128)  ┆ 267.502004        ┆ 250.728732                  ┆ 0.937297                        ┆ -6.270335 │
│ document_mask ┆ torch.bfloat16 ┆ (4, 16, 8192, 4, 8192, 128)   ┆ 248.510516        ┆ 235.210874                  ┆ 0.946483                        ┆ -5.351742 │
│ document_mask ┆ torch.bfloat16 ┆ (4, 16, 16384, 4, 16384, 128) ┆ 282.856295        ┆ 271.806926                  ┆ 0.960936                        ┆ -3.906354 │
│ document_mask ┆ torch.bfloat16 ┆ (4, 16, 8192, 16, 8192, 64)   ┆ 282.212695        ┆ 280.519092                  ┆ 0.993999                        ┆ -0.600116 │
│ document_mask ┆ torch.bfloat16 ┆ (4, 16, 32768, 4, 32768, 128) ┆ 295.864073        ┆ 294.477894                  ┆ 0.995315                        ┆ -0.468519 │
└───────────────┴────────────────┴───────────────────────────────┴───────────────────┴─────────────────────────────┴─────────────────────────────────┴───────────┘

📊 Performance Summary:
============================================================
Baseline: base
Change:   better_configs
Geometric Mean Speedup (change over baseline): 1.9954x
Geometric Mean % Change: +99.54%
Median Speedup (change over baseline): 2.1590x
Speedup Std Dev: 0.9800
Valid Comparisons: 60/60

```

cc @jerryzh168 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-09-19 03:38:26+00:00,2025-09-19T17:20:18Z,,False,3,1,7,169,76,2,4,2025-09-19 16:57:24+00:00,74,6264,False,False,False,True,False,False,2,2,493,363,228,135,1,7,3.0,2.0,2025-09-19T05:41:22Z,pytorch
163317,closed,[Code Clean] Replace `std::runtime_error` with `TORCH_CHECK`,KarhouTam,"Including:
- `torch/csrc/instruction_counter`
- `torch/csrc/lazy`
- `torch/csrc/monitor`
- `torch/csrc/profiler`
- `torch/csrc/dynamo`

Fixes part of [#148114](https://github.com/pytorch/pytorch/issues/148114)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-19 03:27:52+00:00,2025-09-23T03:36:42Z,,False,5,4,2,74,79,12,9,2025-09-23 03:35:02+00:00,60,381,False,True,False,False,False,False,12,4,906,159,77,82,2,2,3.0,6.0,2025-09-19T03:31:16Z,pytorch
163316,open,[Inductor] support masked vectorization for the tail_loop for float64 datatype,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163490
* #163324
* __->__ #163316



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-19 03:27:01+00:00,2025-09-22T09:46:46Z,,False,1,0,3,65,0,2,1,,78,314,False,False,False,False,False,False,2,0,0,73,69,4,1,3,,,,pytorch
163315,closed,[Inductor][Intel GPU] Save `threads_per_warp` from tirton compiled kernel for launching kernel correctly in cpp wrapper.,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163315

On the Inductor XPU backend, `threads_per_warp` is not always 32. For Intel GEMM Triton kernels, it can be 16. This information must be preserved for XPU so that the Cpp wrapper can launch the kernel with the correct configuration.
 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 03:20:26+00:00,2025-09-20T00:02:51Z,,False,8,4,2,13,4,3,12,2025-09-19 21:06:59+00:00,120,530,False,False,False,False,False,False,3,6,1251,25,17,8,1,2,6.0,6.0,2025-09-19T03:24:58Z,pytorch
163313,closed,Remove autograd code for Python < 3.9,cyyever,"As PyTorch is moving to Python 3.10, it is safe to remove code for Python < 3.9.",2025-09-19 02:47:04+00:00,2025-09-22T01:34:47Z,,False,4,0,1,0,10,1,4,2025-09-21 15:35:09+00:00,37,80,False,False,False,False,False,False,1,3,537,10,0,10,1,1,3.0,3.0,2025-09-19T02:49:28Z,pytorch
163310,closed,[graph partition] Add way to register custom rule,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163310
* #162651
* #162307
* #162117

This PR adds an experimental way to register a custom rule for if
inductor should partition the graph around an operator.

Test Plan:
- new test

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-19 01:56:34+00:00,2025-09-19T23:30:55Z,,False,3,10,3,105,0,2,13,2025-09-19 23:28:06+00:00,49,471,False,False,False,False,False,False,2,2,640,119,112,7,1,3,5.0,3.0,2025-09-19T13:17:59Z,pytorch
163308,open,Better error handling in torch/nativert/*,licy666,"Replace the **runtime_error** of the vallina C++ exceptions with **TORCH_CEHCK** in **torch/nativert/***

The vallina C++ exception should not exist in the core part of pytorch for its corss-languanges trait. Comparing with the vallina C++ exceptions, TORCH_CHECK have the richer error context and It has the unified error handling mechanism. This commit replace the runtime_error with TORCH_CHECK of the files in
torch/nativert/*   .

Fixes part of #148114
",2025-09-19 00:58:26+00:00,2025-09-25T11:17:15Z,,False,11,8,1,55,59,8,19,,41,458,False,True,False,False,False,False,8,9,2303,114,55,59,1,1,6.0,10.0,2025-09-19T01:03:22Z,pytorch
163306,open,Performance improvements to Context::float32Precision,lakshayg,"Fixes #161822
",2025-09-19 00:39:59+00:00,2025-09-22T21:31:52Z,,False,2,0,1,126,105,2,2,,53,14,False,True,False,False,True,False,2,0,0,231,126,105,1,1,,,,pytorch
163305,closed,[inductor][triton heuristics] move allow tf32 out of config params,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163206
* #163209
* __->__ #163305

# why

- this is not directly controlled by the config arg but rather by the
  input and by the inductor wide setting
- it's always the same for every choice
- we want the config kwargs to be *programable* and this is not
  programable in that sense but rather needs to use inductor config

# what

- move generating the ALLOW_TF32 kwarg in Triton templates into
  get_extra_kwargs

# testing

with some annotations, this is now the kwargs and extra_kwargs on addmm

```
{'EVEN_K': True, 'USE_FAST_ACCUM': False, 'ACC_TYPE': 'tl.float32', 'num_stages': 1, 'num_warps': 2, 'BLOCK_M': 32, 'BLOCK_N': 32, 'BLOCK_K': 16, 'hint_override': None, 'GROUP_M': 8} # choice/config kwargs
{'ALLOW_TF32': True, 'epilogue_fn': <function addmm_epilogue.<locals>.epilogue at 0x7f64d54ff600>, 'epilogue_fn_hash': ""['addmm_epilogue', torch.float32, 1, 1]"", 'prefix_args': 1} # extra kwargs
```

they're both passed onto the template

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D82871312](https://our.internmc.facebook.com/intern/diff/D82871312)",2025-09-19 00:32:27+00:00,2025-09-20T05:30:43Z,,False,3,2,2,21,11,2,5,2025-09-20 05:30:42+00:00,66,1310,False,True,False,False,False,False,2,2,234,4331,2262,2069,1,2,3.0,3.0,2025-09-19T01:38:59Z,pytorch
163304,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-19 00:25:25+00:00,2025-09-19T04:26:48Z,,False,3,0,1,1,1,1,3,2025-09-19 04:25:46+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-19T00:25:26Z,pytorch
163301,open,[rfc] Supporting exporting a model with DTensor params/inputs,suo,"As title, one issue was that our fake mode detection didn't understand dtensor.

RFC because:
- I'm a dtensor noob so I don't know if this is the right way to use dtensor
- I don't like making torch/_guards.py aware of DTensor, looking for suggestions on alternative ways to structure the code.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 22:21:37+00:00,2025-09-23T04:46:35Z,,False,5,0,1,92,0,2,5,,61,398,False,False,False,False,False,False,2,3,334,92,92,0,1,1,2.0,4.0,2025-09-18T22:21:50Z,pytorch
163299,open,[CUDA] Cleanup persistent cuBLASLt workspaces after compile-regions test,eqy,"Fixes some tests that seemed to start flaking out as reported in #163202, due to cuBLASLt workspaces becoming persistent following that change.

It's relatively obvious why the workspaces/allocations corresponding to them should be cleaned up for `test_memory_snapshot_script` but less obvious for `test_memory_plots_free_segment_stack`?  Why does not cleaning up workspace prevent `empty_cache` from showing up?



cc @ptrblck @msaroufim @jerryzh168 @mruberry @csarofeen @xwang233",2025-09-18 21:49:32+00:00,2025-09-22T20:54:59Z,,False,1,0,2,3,0,1,1,,72,481,False,True,False,False,False,False,1,0,0,5,4,1,1,2,,,,pytorch
163298,closed,[SymmMem] Barrier on team instead of world,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162681
* #162680
* __->__ #163298

As titled. Avoiding a potential hang when running dispatch and combine in subgroups.

The rest is just re-arrange of the tests to create a sub-group test class. (no substantial change)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 21:41:56+00:00,2025-09-19T22:06:38Z,,False,6,0,2,107,72,2,6,2025-09-19 20:19:49+00:00,42,413,False,False,False,False,False,False,2,5,1047,199,117,82,1,2,4.0,5.0,2025-09-19T20:02:30Z,pytorch
163297,open,[CUDA][Expandable Segments] Follow-up cleanups for even more expandable segments tests,eqy,"Gets original setting even earlier in case of crashes, fixes previous get call where set should be 

cc @ptrblck @msaroufim @jerryzh168 @mruberry",2025-09-18 21:33:34+00:00,2025-09-19T16:48:05Z,,False,1,0,3,4,4,1,1,,86,145,False,True,False,False,False,False,1,0,0,14,7,7,2,3,1.0,0.0,2025-09-19T16:00:40Z,pytorch
163296,open,[testing] Add test owner labels for some cuda? tests,clee2000,"I am trying to give some test files better owner labels than `module: unknown`.  I am not sure them, but they seem pretty reasonable

cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-18 21:29:30+00:00,2025-09-21T03:41:20Z,,False,6,0,1,2,2,2,6,,52,173,False,False,False,False,False,False,2,5,1211,4,2,2,1,1,3.0,5.0,2025-09-18T21:35:38Z,pytorch
163295,closed,[export] Fix wrap_with_set_grad_enabled retracing,angelayi,"Fixes https://github.com/pytorch/pytorch/issues/163294

The code `with torch.set_grad_enabled(enable_grad)` calls `torch._C._set_grad_enabled` three times -- (1) when [initializing set_grad_enabled](https://github.com/pytorch/pytorch/blob/bb7c9a2d4127ff178fe8787caf070d7072d015b6/torch/autograd/grad_mode.py#L187C9-L187C35), (2) when [entering the context](https://github.com/pytorch/pytorch/blob/bb7c9a2d4127ff178fe8787caf070d7072d015b6/torch/autograd/grad_mode.py#L194), and (3) when [exiting the context](https://github.com/pytorch/pytorch/blob/bb7c9a2d4127ff178fe8787caf070d7072d015b6/torch/autograd/grad_mode.py#L197). 

This results in the the retraced export module to have a duplicate `torch._C._set_grad_enabled` like:
```
def forward(self, arg0_1):
    add = torch.ops.aten.add.Tensor(arg0_1, 1);  arg0_1 = None
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    add_1 = torch.ops.aten.add.Tensor(add, 2);  add = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
    add_2 = torch.ops.aten.add.Tensor(add_1, 3);  add_1 = None
    return (add_2,)
```

When export runs the `replace_set_grad_with_hop_pass`, it will look through the graph for `torch._C._set_grad_enabled` and create subgraphs. The duplicate `torch._C._set_grad_enabled` results in an empty submod in the graph, which resulted in an error in [this post](https://fb.workplace.com/groups/1028545332188949/posts/1844720036398281/?comment_id=1862175381319413). ",2025-09-18 21:18:51+00:00,2025-09-21T22:55:48Z,,False,5,0,1,10,14,2,5,2025-09-21 22:54:45+00:00,49,1597,False,True,False,False,False,False,2,4,986,24,10,14,1,1,3.0,4.0,2025-09-18T21:23:29Z,pytorch
163293,closed,Change DebugMode record_torchfunction default to False,SherlockNoMad,"Result is too noisy with `record_torchfunction = True`. Change it to False, to make it clean. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 21:03:49+00:00,2025-09-19T00:31:58Z,,False,3,0,1,6,6,2,3,2025-09-19 00:30:55+00:00,54,197,False,True,False,False,False,False,2,2,498,12,6,6,1,1,3.0,3.0,2025-09-18T21:31:25Z,pytorch
163292,open,"[dynamo, 3.14] fix WITH_EXCEPT_START",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* __->__ #163292
* #163191
* #163110
* #163109
* #163009
* #161839
* #161555
* #161838

",2025-09-18 20:59:12+00:00,2025-09-25T00:04:10Z,,False,2,0,2,11,3,1,2,,36,184,False,True,False,False,False,False,1,0,0,16,12,4,1,2,,,,pytorch
163291,open,[DTensor] Fix adding Partial(sum) with Scalar,SherlockNoMad,"Fixes #163193

When adding Partial(Sum) with a ScalarTensor, a few thing are happening
1. ScalarTensor is auto converted as Replicated DTensor
2. Propagator decide to redistribute R -> Partial(sum)
3. This result as the div by num_shards
```
    aten::add.Tensor(dt: f32[][P], t: i64[])
      redistribute_input(1, [R] -> [P])
        aten::div.Tensor(t: i64[], 4)
      aten::add.Tensor(t: f32[], t: f32[])
```

When adding Partial(Sum) with a Scalar
1. Scalar remains as scalar 
2. Propagator only sees a single Partial DTensor, and sets output placement also as Partial
3. As a result, scalar is added locally in all ranks, resulting in a wrong global result.
```
    aten::add.Tensor(dt: f32[][P], 2)
      aten::add.Tensor(t: f32[], 2)
```

In contrast, aten.sub(Partial, ScalarTensor) would redistribute Partial tensor as Replicate first. Although, it could have follow the same strategy used in aten.add. 
```
    aten::sub.Tensor(dt: f32[][P], t: i64[])
      redistribute_input(0, [P] -> [R])
        _c10d_functional::all_reduce(t: f32[], sum, 0)
        _c10d_functional::wait_tensor(t: f32[])
      aten::sub.Tensor(t: f32[], t: i64[])
```
cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 20:39:31+00:00,2025-09-20T00:18:37Z,,False,2,16,3,55,1,2,18,,45,1253,False,True,False,False,False,False,2,1,18,68,61,7,1,3,6.0,1.0,2025-09-18T20:47:27Z,pytorch
163289,closed,Deprecate Lite Interpreter,JacobSzwejbka,"Summary:
Point people lowering to lite interpreter to the existence of ExecuTorch. 

Added the typing deprecation, a warnings deprecation

Test Plan: Try using it, see deprecation warning

Reviewed By: lucylq

Differential Revision: D82759566




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 20:22:10+00:00,2025-09-18T23:57:25Z,,False,6,0,1,19,0,1,6,2025-09-18 23:56:23+00:00,26,294,False,False,False,False,False,False,1,2,501,19,19,0,1,1,3.0,2.0,2025-09-18T20:33:57Z,pytorch
163288,open,[DeviceMesh] Add extra check in flatten result cache lookup,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163358
* #161224
* #163213
* #163367
* __->__ #163288
* #163212

while refactoring DeviceMesh bookkeeping, we found that there is one corner case which we just don't check whether the dims to be flattened into is same as the dims which an existing flattened name maps to. So we need to add extra cases in the unit test and extra check logic in the code.



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 20:17:01+00:00,2025-09-24T21:40:30Z,,False,2,5,2,24,3,2,7,,59,529,False,False,False,False,False,True,2,1,79,9687,5813,3874,1,2,5.0,2.0,2025-09-18T20:37:48Z,pytorch
163285,open,Raise TypeError in torch.export.load when input isn't a file or buffer,dsashidh,"Fixes #163040 

Replaces a vague AttributeError with a clearer TypeError when torhc.export.load( ) is misused. For example, calling it with an ExportedProgram now raises immediately insteald of hitting .tell() downstream. The check I added matches the existed warning and turns it into a proper error as suggested in the TODO comment that existed before my change.
",2025-09-18 20:01:59+00:00,2025-09-23T20:42:03Z,,False,7,0,1,3,5,1,7,,70,365,False,True,False,False,False,False,1,6,2084,8,3,5,1,1,3.0,7.0,2025-09-18T20:02:11Z,pytorch
163284,open,[effects] Add register_effectful_op,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163714
* __->__ #163284
* #163278
* #163277
* #163279

",2025-09-18 19:54:58+00:00,2025-09-25T00:36:10Z,,False,1,2,4,183,93,7,3,,35,134,False,False,False,False,False,False,7,0,0,69131,51728,17403,1,3,1.0,0.0,2025-09-19T13:35:04Z,pytorch
163282,closed,[Fix] Restrict stride normalization to 1D tensors on export,Kathryn-cat,"This change restricts the DLPack stride normalization to apply only to 1D tensors of shape (1,).

### Rationale
The previous implementation normalized the strides for any multi-dimensional tensor containing a dimension of size 1. While well-intentioned, this ""over-normalization"" discards critical memory layout information, causing issues for downstream consumers who rely on strides to infer alignment and contiguity.

For example:

* A row-major tensor with `shape=(1, 128)` and `stride=(128, 1)` would be incorrectly normalized to `stride=(1, 1)`.

* A column-major tensor with `shape=(1024, 1)` and `stride=(1, 1024)` would also be normalized to `stride=(1, 1)`.

This loss of stride information makes it impossible for consumers to detect the original memory layout (e.g., row-major vs. column-major) and breaks assumptions about memory alignment needed for optimized indexing or specialized hardware APIs like GPU TMA.

The original intent of the normalization was to handle the simple case of a 1D tensor with shape=(1,) and a non-standard stride. This fix reverts to that specific, non-problematic behavior, ensuring that multi-dimensional tensors retain their precise stride information during DLPack export.

### Related Issues
#163274
",2025-09-18 19:47:30+00:00,2025-09-22T19:11:12Z,,False,6,0,2,2,19,1,6,2025-09-22 19:10:08+00:00,59,1247,False,True,False,False,False,False,1,4,579,23,3,20,1,2,3.0,4.0,2025-09-18T21:38:03Z,pytorch
163281,closed,[MPS] Compute `offset2bag/bag_size/max_indices` in `_embedding_bag`,kurtamohler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163281

Part of #162270

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 19:36:58+00:00,2025-09-23T22:46:09Z,,False,3,11,5,161,50,6,14,2025-09-23 22:30:51+00:00,67,312,False,False,False,False,False,False,6,2,493,2542,1796,746,1,5,3.0,2.0,2025-09-18T19:54:34Z,pytorch
163280,closed,Fix performance regression when indexing by Numpy arrays,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163280

Benchmark script:

```
import time
import numpy as np
import torch

def main() -> None:
    for i in range(10):
        block_indices = np.arange(16384, dtype=np.int32)
        block_indices = block_indices.reshape(-1).clip(max=255)
        batch_indices = np.zeros(16384, dtype=np.int64)
        virtual_batches = 32
        block_table = torch.randn(32, 256)
        start = time.perf_counter()
        block_table[batch_indices, block_indices].view(virtual_batches, -1)
        end = time.perf_counter()
        time_elapsed_ms = (end - start) * 1000
        print(f""Function execution time: {time_elapsed_ms:.1f}ms"")

if __name__ == ""__main__"":
    main()
```

Before:

```
(a) [ezyang@devvm006.dkl0 ~/local/b/pytorch] python ben.py
Function execution time: 28.5ms
Function execution time: 12.9ms
Function execution time: 12.6ms
Function execution time: 13.5ms
Function execution time: 12.0ms
Function execution time: 13.4ms
Function execution time: 12.9ms
Function execution time: 12.9ms
Function execution time: 13.1ms
Function execution time: 13.0ms
```

After:

```
Function execution time: 17.8ms
Function execution time: 2.5ms
Function execution time: 1.3ms
Function execution time: 2.5ms
Function execution time: 2.3ms
Function execution time: 1.3ms
Function execution time: 2.4ms
Function execution time: 2.5ms
Function execution time: 2.5ms
Function execution time: 2.4ms
```

Signed-off-by: Edward Z. Yang <ezyang@meta.com>",2025-09-18 19:12:56+00:00,2025-09-19T05:04:03Z,,False,3,0,1,3,1,1,3,2025-09-19 05:03:00+00:00,56,1531,False,True,False,False,False,False,1,2,493,4,3,1,1,1,4.0,2.0,2025-09-18T22:23:11Z,pytorch
163279,open,[opaque_obj] Add __eq__ and __deepcopy__,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163714
* #163284
* #163278
* #163277
* __->__ #163279



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 18:58:00+00:00,2025-09-25T00:35:56Z,,False,1,2,5,83,3,2,3,,40,183,False,False,False,False,False,False,2,0,0,68843,51505,17338,1,5,2.0,0.0,2025-09-19T13:33:17Z,pytorch
163278,open,[opaque_obj] Add make_fx tracing support,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163714
* #163284
* __->__ #163278
* #163277
* #163279

",2025-09-18 18:57:57+00:00,2025-09-25T00:36:11Z,,False,1,2,7,168,48,3,3,,40,134,False,False,False,False,False,False,3,0,0,73631,54028,19603,1,6,2.0,0.0,2025-09-18T21:39:20Z,pytorch
163277,open,[opaque obj] Error for torch.library.custom_op infer_schema,angelayi,"Unsure how we can get infer_schema to infer the scriptObject type from just the type annotation, so for now will just error clearly and ask users to specify a schema.

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163714
* #163284
* #163278
* __->__ #163277
* #163279

",2025-09-18 18:57:53+00:00,2025-09-25T00:36:12Z,,False,1,1,7,33,5,5,2,,59,302,False,False,False,False,False,False,5,0,0,73349,53836,19513,1,7,1.0,0.0,2025-09-19T13:28:47Z,pytorch
163276,closed,[opaque_obj] Add set_payload + docs,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163278
* #163277
* __->__ #163276
* #162660



cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 18:57:49+00:00,2025-09-22T20:03:37Z,,False,8,6,5,115,3,4,14,2025-09-22 20:02:32+00:00,35,173,False,False,False,True,False,False,4,7,2031,51722,37337,14385,1,5,4.0,8.0,2025-09-19T13:23:46Z,pytorch
163273,open,Extend triton_mm auto-tune options for HIM shapes,RandySheriff,"Summary:
Add an option to auto-tune for shape:
```
M=1024 N=171712 K=1024
```

Test Plan:
```
TRITON_PRINT_AUTOTUNING=1 buck2 run mode/opt-amd-gpu -c fbcode.enable_gpu_sections=true //pytorch/tritonbench:run -- --op fp8_gemm_rowwise --no_use_tma --no_use_persistent --m 1024 --n 171712 --k 1024 --bias
```
Before:
 {F1982074581} 
After, saw 10%~ boost:
{F1982074585}

Differential Revision: D82687336




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 18:15:10+00:00,2025-09-22T21:35:34Z,,False,27,0,1,1,0,1,27,,49,606,False,False,False,False,False,False,1,0,5,1,1,0,1,1,1.0,1.0,2025-09-22T16:52:39Z,pytorch
163272,open,support a few features for vllm inductor precompile,dolpm,"add support to directly serialize to byes and deserialize from bytes so we can store in vllm's serializable callable.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 18:13:49+00:00,2025-09-25T15:07:22Z,,False,1,6,2,105,12,8,7,,51,376,False,False,True,False,False,False,8,0,0,117,105,12,1,2,3.0,0.0,2025-09-19T15:49:43Z,pytorch
163270,closed,"[export] add node.meta[""val""] when HOO's output is empty",yushangdi,"Summary: as title. Otherwise the verifier will complain that node.meta has no val

Test Plan: 

Differential Revision: D82691627


",2025-09-18 17:41:01+00:00,2025-09-18T21:26:08Z,,False,3,2,1,26,0,2,5,2025-09-18 21:26:08+00:00,56,131,False,False,False,False,False,False,2,0,0,26,26,0,1,1,2.0,0.0,2025-09-18T21:21:17Z,pytorch
163269,closed,Update pytorch_sphinx_theme2 to latest hash,svekars,"The updated theme:
- Fixes articleBody in the json+ld that caused previous Google Search issues
- Other minor fixes
- 404.html fixes",2025-09-18 17:22:51+00:00,2025-09-23T15:46:24Z,,False,5,0,1,1,1,1,5,2025-09-22 23:20:25+00:00,43,132,False,True,False,False,False,False,1,4,1032,2,1,1,1,1,4.0,5.0,2025-09-22T18:00:36Z,pytorch
163268,closed,[PyTorch] Add SVE128 ISA (#158932),Nicoshev,"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/158932

Importing https://github.com/pytorch/pytorch/pull/138388, as it improves SVE support for perfkernels

Test Plan: We will test it on AdFinder/AdRetriever/AdRanker offline tier

Reviewed By: r1mikey

Differential Revision: D70788867

Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 17:02:56+00:00,2025-09-18T17:03:04Z,2025-09-18T17:03:04Z,True,1,0,1,466,186,32,1,2025-09-18 17:03:04+00:00,34,591,False,True,False,False,True,False,32,0,0,652,466,186,1,1,,,,pytorch
163267,closed,Improve error messages and validation in CachingAutotuner (#147170),Harshad2321,"## Summary
Improves error messages and adds comprehensive validation to CachingAutotuner to help users debug issues more effectively.

Fixes #147170

## Changes Made
- **Enhanced argument validation** with clear error messages showing expected vs actual argument counts
- **Improved stream parameter validation** with actionable error messages  
- **Added comprehensive configuration validation** with detailed error reporting
- **Enhanced check_config()** function with parameter validation and helpful error messages
- **Improved check_max_block()** with better error reporting for block size limits
- **Enhanced triton_config()** with input validation and type checking
- **Added comprehensive logging** throughout for better debugging capabilities

## Test Plan
- Verified all new error paths produce clear, actionable messages
- Tested argument count validation with various input scenarios
- Validated stream parameter handling
- Confirmed configuration validation works correctly
- All existing functionality preserved

## Impact
- Users will get much clearer error messages when CachingAutotuner encounters issues
- Easier debugging of triton kernel compilation problems
- Better developer experience when working with autotuning
- Maintains backward compatibility while improving error handling

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 16:45:00+00:00,2025-09-18T16:55:49Z,,False,4,0,2,156,19,1,4,2025-09-18 16:55:49+00:00,67,1506,False,True,False,False,True,False,1,1,43,241,200,41,2,2,1.0,1.0,2025-09-18T16:51:25Z,pytorch
163265,closed,"[Release 2.9] [cuDNN][SDPA][submodule] Roll-back cuDNN frontend upgrade, update Met…",eqy,"…a registration (#163104)

For https://github.com/pytorch/torchtitan/issues/1713

Also note that we will need to rollback the cuDNN frontend upgrade in 2.9 as it currently introduces a segmentation fault by assuming tensors have their strides and sizes populated at graph creation time https://github.com/NVIDIA/cudnn-frontend/blame/1a7b4b78db44712fb9707d21cd2e3179f1fd88b8/include/cudnn_frontend/node/sdpa_support_surface.h#L447%C2%A0

Pull Request resolved: https://github.com/pytorch/pytorch/pull/163104
Approved by: https://github.com/drisspg

Cherrypicking fix from `main` to `release/2.9`
",2025-09-18 16:17:43+00:00,2025-09-19T17:53:05Z,2025-09-19T17:53:04Z,True,1,0,1,31,4,4,1,2025-09-19 17:53:04+00:00,84,595,False,True,False,False,False,False,4,0,0,35,31,4,1,1,1.0,0.0,2025-09-19T17:52:55Z,pytorch
163264,closed,[Code Clean] Replace std::runtime_error with TORCH_CHECK,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163264

Related ISSUE: https://github.com/pytorch/pytorch/issues/148114",2025-09-18 16:07:21+00:00,2025-09-25T11:29:58Z,,False,9,0,6,135,132,19,9,2025-09-25 11:28:54+00:00,56,169,False,False,False,False,False,False,19,8,1980,32024,19426,12598,1,6,4.0,9.0,2025-09-24T20:31:02Z,pytorch
163263,closed,[Autograd] The behavior of Tensor.requires_grad=True is different from that of Tensor.requires_grad_(True).,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163264
* __->__ #163263

As the title stated.

```Python
>>> a = torch.randn(3, 3, device=""cpu"", requires_grad=True)
>>> b = torch.pow(a, 2)
>>>
>>> b.requires_grad=False
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().
>>> b.requires_grad=True
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: you can only change requires_grad flags of leaf variables.

>>> b.requires_grad_(False)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
RuntimeError: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().
>>> b.requires_grad_(True)
tensor([[4.1244, 3.3084, 0.1038],
        [0.3303, 0.0662, 0.1792],
        [0.0080, 0.0127, 0.4904]], grad_fn=<PowBackward0>)
```",2025-09-18 16:07:14+00:00,2025-09-25T08:41:55Z,,False,9,0,5,9,16,5,9,2025-09-25 08:41:55+00:00,107,1155,False,False,False,False,False,False,5,8,1994,25382,18472,6910,1,5,4.0,8.0,2025-09-18T16:40:02Z,pytorch
163262,closed,[SymmMem] Fix NVSHMEM plugin + Triton 3.5,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163194
* __->__ #163152

1. The dispatch signatures defined in `core.extern_elementwise` call must match the C signature of the NVSHMEM functions, in particular the dtypes. Otherwise, there would be weird errors, such as IMA or hang. When matched, most of time the NVSHMEM device function will be inlined into the generated PTX. When not matched, it is represented as a function call in the PTX (not sure if it is the function call that goes wrong).

2. When calling the `core.extern` wrappers from the `triton.jit` kernels, the input must be cast to match the signatures defined in 1, e.g. via `nbytes.to(tl.int64)`. Otherwise, Triton will report a key error when searching for such kernel.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 15:49:58+00:00,2025-09-19T17:51:02Z,2025-09-19T17:51:02Z,True,1,0,1,105,49,2,1,2025-09-19 17:51:02+00:00,41,885,False,True,False,False,False,False,2,0,0,154,105,49,1,1,1.0,0.0,2025-09-19T17:50:54Z,pytorch
163261,open,[DRAFT] shake up CI again,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163261
* #163260
* #163259
* #163258
* #163137
* #163136
* #163107

Differential Revision: [D82732851](https://our.internmc.facebook.com/intern/diff/D82732851)",2025-09-18 14:53:08+00:00,2025-09-18T18:50:31Z,,False,2,0,1,1,1,1,2,,25,245,False,False,False,False,False,False,1,1,160,2,1,1,1,1,1.0,1.0,2025-09-18T14:54:33Z,pytorch
163260,open,AOTI util deprecated flow using the new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* __->__ #163260
* #163259
* #163258
* #163137
* #163136
* #163107

AOTI utils expect free function sometimes so adjust export API to handle that, haven't seen any methods getting exported. Some AOTI flows also require we populate `dynamo_flat_name_to_original_fqn` so i just copy how it is done in eval_frame.py


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela

Differential Revision: [D82732612](https://our.internmc.facebook.com/intern/diff/D82732612)",2025-09-18 14:21:44+00:00,2025-09-23T21:01:55Z,,False,6,5,4,75,36,2,11,,46,708,False,False,False,False,False,False,2,4,640,23217,16632,6585,1,4,2.0,4.0,2025-09-18T14:49:47Z,pytorch
163259,open,Move control flow export tests to new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* #163260
* __->__ #163259
* #163258
* #163137
* #163136
* #163107

Differential Revision: [D82732614](https://our.internmc.facebook.com/intern/diff/D82732614)",2025-09-18 14:21:38+00:00,2025-09-23T20:33:43Z,,False,5,0,4,23,20,1,5,,44,245,False,False,False,False,False,False,1,4,693,23149,16580,6569,1,4,2.0,5.0,2025-09-18T14:49:40Z,pytorch
163258,open,Use new_tracer_experimental for torchao strict export,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* #163260
* #163259
* __->__ #163258
* #163137
* #163136
* #163107


Export team is fixing up the old strict export implementation, as a result it fails a check where we proxy the whole module under given directories. _WrapperModule is a way for torchao to workaround the issue where export requiring nn.module to trace so it should never get proxied in the graph.

Differential Revision: [D82732613](https://our.internmc.facebook.com/intern/diff/D82732613)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-18 14:21:32+00:00,2025-09-23T20:46:18Z,,False,5,0,4,12,6,2,5,,53,715,False,True,False,False,False,False,2,4,640,23124,16569,6555,1,4,1.0,4.0,2025-09-18T14:49:34Z,pytorch
163256,closed,[ROCm] update ci_expected_accuracy for dynamo benchmarks,amdfaa,"Some tests that were already failing changed status to skipped.  Some model entries were missing.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-18 14:01:56+00:00,2025-09-18T19:06:26Z,,False,3,0,1,24,4,5,3,2025-09-18 19:05:24+00:00,56,383,False,False,False,False,False,False,5,2,850,28,24,4,1,1,3.0,2.0,2025-09-18T16:05:51Z,pytorch
163255,closed,Better error handling in torch/nativert/*,licy666,"Replace the **runtime_error** of the vallina C++ exceptions with **TORCH_CEHCK** in torch/nativert/*

The vallina C++ exception should not exist in the core part of pytorch for its corss-languanges trait. Comparing with the vallina C++ exceptions, TORCH_CHECK have the richer error context and It has the unified error handling mechanism. This commit replace the runtime_error with TORCH_CHECK of the files in torch/nativert/*   .

Fixes part of #148114

@pytorchbot label ""topic: not user facing""",2025-09-18 11:20:13+00:00,2025-09-18T11:33:27Z,,False,3,0,1,65,53,8,3,2025-09-18 11:33:26+00:00,41,497,False,True,False,False,False,False,8,0,0,118,65,53,1,1,,,,pytorch
163254,open,IGNORE: Test multicloud-arc-runner,afrittoli,"This is a test for the arc runner, do not review

cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal",2025-09-18 10:42:59+00:00,2025-09-18T14:11:10Z,,False,1,0,1,5,362,1,1,,34,215,False,False,False,False,False,False,1,0,0,367,5,362,1,1,,,,pytorch
163252,open,Add SDPA patterns for T5 variants when batch size is 1,CaoE,"As mentioned in
https://github.com/pytorch/pytorch/blob/main/torch/_inductor/fx_passes/fuse_attention.py#L838, this PR generates patterns  for the cases batch size == 1.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 08:11:26+00:00,2025-09-25T08:22:40Z,,False,1,4,1,654,88,5,5,,54,372,False,False,False,False,False,False,5,0,0,742,654,88,1,1,2.0,0.0,2025-09-25T07:50:45Z,pytorch
163251,open,[Intel GPU] Enable triton online softmax kernels on XPU.,weishi-deng,"This pr is to enable triton online softmax kernels for xpu devices, so we add a device check in prepare_softmax_extra_check.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 07:45:03+00:00,2025-09-24T23:01:14Z,,False,6,4,5,17,7,4,10,,56,327,False,False,False,False,False,False,4,5,1923,46,28,18,1,5,3.0,5.0,2025-09-18T07:55:13Z,pytorch
163250,closed,[c10d][CI] Mute pkg_resources deprecation warning,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163250

",2025-09-18 07:33:22+00:00,2025-09-18T07:36:36Z,,False,3,0,1,10,0,1,3,2025-09-18 07:36:36+00:00,49,106,False,False,False,False,False,False,1,1,35,10,10,0,1,1,1.0,1.0,2025-09-18T07:36:36Z,pytorch
163249,open,Add sum support for qlinear_binary pattern,CaoE,cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben,2025-09-18 06:52:12+00:00,2025-09-24T06:22:40Z,,False,1,0,1,30,13,3,1,,42,201,False,False,False,False,False,False,3,0,0,43,30,13,1,1,,,,pytorch
163246,closed,Add support for name kwarg in mark_dynamic,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163421
* __->__ #163246

Ergonomic improvement to allow sharing symbols without having to do the complex torch._check paradigm as described by @anijain2305 in his recent UED:

```
Different symbols for KV cached tensors - One property in my case was that the KV cache for different attention blocks had the same seq length, but there is no API to enforce that. The only way is to add torch._check but torch.compile must trace those functions to instruct the dynamic shape infra. This required me to change the model code.
Changing model code is not the best experience. Lets see how transformers maintainers react to my PR. Maybe an API with fullgraph=True is a better bet here.
```

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-18 06:11:32+00:00,2025-09-21T03:19:26Z,,False,3,0,5,57,1,4,3,2025-09-21 03:19:26+00:00,42,941,False,False,False,False,True,False,4,2,1380,161,117,44,1,5,2.0,2.0,2025-09-21T02:41:52Z,pytorch
163245,open,[export] Make RNNs exportable on GPUs,yiming0416,"Summary: Fixes https://github.com/pytorch/pytorch/issues/155309

`torch.export` fails to export an RNN model on GPU with `cudnn` enabled. This is because RNN module's `flatten_parameters()` method calls `p.data_ptr()` for aliasing detection, causing ""Cannot access data pointer of Tensor"" errors on GPU.

This PR fixes this issue by making following changes:

1. Replace `p.data_ptr()` with `StorageWeakRef(p.untyped_storage())` for PT2 compatibility.
2. Fallback to `p.data_ptr()` when `p.untyped_storage()` fails (e.g., for some tensor subclasses `untyped_storage()` doesn't work)
3. Move `StorageWeakRef` class from `multiprocessing/reductions.py` to `utils/weak.py`

CAVEAT: The current way of saving and loading weights in `torch.export` cannot preserve the weight storage sharing info of RNNs on GPUs. Originally, the weights of a RNN model on a GPU are on the same memory chunk but with different offsets. After `torch.export.save` and `torch.export.load()`, however,  this won't be preserved.


Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_export_lstm_gpu
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_export_gru_gpu
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_export_rnn_flatten_parameters

Rollback Plan:

Differential Revision: D82687470

cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-18 05:32:59+00:00,2025-09-22T20:46:18Z,,False,9,8,1,177,67,26,17,,37,1528,False,True,False,False,False,False,26,1,104,244,177,67,1,1,5.0,1.0,2025-09-19T17:21:21Z,pytorch
163244,closed,Update torch-xpu-ops commit pin,Chao1Han,"Update the torch-xpu-ops commit to https://github.com/intel/torch-xpu-ops/commit/24fab67b6ecf7620d0cf776047a3056c5b518bab, includes:

- Clean up getDeviceIndexOfCurrentQueue
- Fix hardswish gradients corner case
- Fix xccl contiguous check
- Move checks from nonzero kernel to operator
- support high priority stream for xccl
",2025-09-18 05:01:24+00:00,2025-09-19T02:05:48Z,,False,4,0,1,1,1,1,4,2025-09-19 02:04:44+00:00,31,326,False,True,False,False,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-18T05:08:03Z,pytorch
163242,closed,Make `Tensor.__dlpack__(stream=None)` capture-safe during CUDA Graph capture,eee4017,"Many extensions (including pybind helpers) call `Tensor.__dlpack__()` without a stream argument. Before #150217, `stream=None` behaved like “no cross-stream sync” and was safe inside CUDA Graph capture. After #150217, `stream=None` maps to the legacy default stream, adding a cross-stream wait that invalidates capture when running on a non-default stream.

See this example

```
import torch
s = torch.cuda.Stream()
x = torch.randn(8, device=""cuda"")
g = torch.cuda.CUDAGraph()

with torch.cuda.stream(s):
    with torch.cuda.graph(g):
        _ = x + 1
        cap = x.__dlpack__()
        _ = torch.utils.dlpack.from_dlpack(cap)
```

This PR partially reverts #150217 that stream=None defaults to no sync.





cc @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng",2025-09-18 04:25:41+00:00,2025-09-24T16:05:28Z,,False,17,0,5,7,4,1,17,2025-09-24 16:04:21+00:00,76,766,False,False,False,False,False,False,1,16,7774,33,18,15,1,5,7.0,16.0,2025-09-18T14:29:41Z,pytorch
163241,closed,[DTensor] Fix DTensor.mean with uneven sharding,SherlockNoMad,"Fixes #162692

When input is uneven sharded, redistribute input as Replicated. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 04:19:31+00:00,2025-09-18T19:54:56Z,,False,3,0,2,59,1,3,3,2025-09-18 19:53:54+00:00,47,182,False,True,False,False,False,False,3,2,523,64,61,3,1,2,3.0,3.0,2025-09-18T16:46:27Z,pytorch
163240,closed,[AI Codemod][DevmatePerfOptimizationVectorReallocation] fbcode/caffe2/torch/csrc/jit/serialization/unpickler.cpp,yfeldblum,"Reviewed By: marksantaniello, yfeldblum

Differential Revision: D82140619




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 04:13:15+00:00,2025-09-21T13:04:50Z,,False,11,3,1,2,3,1,14,2025-09-20 23:26:27+00:00,112,125,False,False,False,False,False,False,1,3,520,5,2,3,1,1,3.0,3.0,2025-09-18T04:36:40Z,pytorch
163239,open,Build vLLM nightly wheels for CUDA 13.0,huydhn,Testing now that https://github.com/vllm-project/vllm/pull/24599 has been merged,2025-09-18 03:43:12+00:00,2025-09-24T15:30:13Z,,False,10,0,3,11,3,2,10,,39,80,False,False,False,False,False,False,2,9,4890,2234,1072,1162,1,3,4.0,9.0,2025-09-18T07:42:42Z,pytorch
163238,closed,Massive hack to make autograd shut up about threaded PG mutations,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163238

See the Note for explanation.

Signed-off-by: Edward Z. Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-18 03:32:43+00:00,2025-09-18T18:14:04Z,,False,7,2,6,46,11,2,9,2025-09-18 18:13:01+00:00,65,267,False,False,False,False,False,False,2,6,2138,104362,65945,38417,1,6,3.0,7.0,2025-09-18T03:35:13Z,pytorch
163237,open,[pcache] MultiCache implementation,nmacchioni,"Summary:
Initial implementation of MultiCache + MemoizedOnDiskCache as an example

Still a WIP, but wanted to get the design out for rough feedback

Not sure if MultiCache can be thread-safe, given that insert_async is a Generator (so how would we prevent concurrent writes?)

Still need to add additional unit tests;
Should have the same functionality as AsyncCache instances, with different return types for insert and insert_async

Will also want to check:
Get with a single populated cache hits
Get with all populated caches hits
Insert with all empty caches updates all caches
Insert with some empty caches updates some caches

Should we allow different values in different caches to exist for the same key? That would be weird behavior

Rollback Plan:

Differential Revision: D82698606




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 03:27:37+00:00,2025-09-19T14:17:18Z,,False,4,0,1,171,10,2,4,,34,997,False,False,False,False,False,False,2,1,552,181,171,10,1,1,1.0,1.0,2025-09-19T14:17:18Z,pytorch
163236,closed,[export] Tag lifted constants with source object id.,zhxchen17,"Summary:
Adds a metadata to node which should achieve the same end goal as https://github.com/pytorch/pytorch/pull/162926

Basically we want to have a generic way to associate graph inputs with its original source. To achieve this, we can store an additional field ""source_id"" on the placeholder node so that export downstream can track the mapping between tensor objects and its placeholder node.

Test Plan:
buck2 run --flagfile fbcode//mode/opt fbcode//caffe2/test:test_export -- -r test_source_id

Rollback Plan:

Differential Revision: D82698063


",2025-09-18 03:10:08+00:00,2025-09-19T21:29:27Z,,False,6,1,1,46,0,2,7,2025-09-19 21:29:27+00:00,52,553,False,False,False,False,False,False,2,1,50,46,46,0,1,1,2.0,1.0,2025-09-18T03:45:15Z,pytorch
163235,closed,[OpenReg][Docs] Correct docs about `openreg` usage example.,KarhouTam,"## Why this PR?
I've tried to follow the guidance of the `OpenReg` [usage example](https://github.com/pytorch/pytorch/tree/main/test/cpp_extensions/open_registration_extension/torch_openreg/third_party/openreg) and found that the command for compiling `example.cpp` (`g++ -o out example/example.cpp -L ./build -lopenreg`) is not compatible with my `gcc` (v11.4). 

Since I installed my `gcc` through `apt install build-essential`, and I think that's a common way to install `gcc` for a few developers? I believe it's necessary to slightly modify the command to add `-I ./` to explicitly indicate the header file search path.

## What I've changed?
- I added `-I ./` to correctly search for `./include/openreg.h`. 
- I also added a `pwd` comment for better readability and removed unused imports in `example/example.cpp`.",2025-09-18 02:50:49+00:00,2025-09-24T14:33:52Z,,False,12,1,3,4,4,3,13,2025-09-23 06:16:48+00:00,59,820,False,False,False,True,False,False,3,10,3989,10,5,5,1,3,4.0,12.0,2025-09-18T02:53:05Z,pytorch
163234,closed,[Inductor-FX] Support torch.cond,blaine-rister,"# Feature

Support `torch.cond` in the FX converter. The generated FX IR is conceptually indentical to what would come from `torch.export`:
- Submodules as stored as attributes, and accessed via `getattr`.
- The conditional is represented as `torch.ops.higher_order.cond`, which takes in the subgraphs, a predicate and submodule inputs.

# Implementation overview

The FX backend generates code for subgraphs using the following steps:
1. When `codegen_conditional` is called in `WrapperFxCodegen`, we emit a `ConditionalLine`. 
   a. We also codegen the true/false subgraphs at this time, storing their subgms for later.
2. At the beginning of FX conversion, generate `get_attr` nodes accessing each subgraph. It's important to do this at the start, before registering the node metadata hook. This also matches the convention followed by torch.export.
3. When we see the `ConditionalLine` in the FX converter, we generate a corresponding `torch.ops.higher_order.cond`.

# Implementation details
This ended up being a substantial change, as wrapper codegen has some special logic for subgraphs. 

Certain methods of `PythonWrapperCodegen` are overridden by `SubgraphPythonWrapperCodegen`. To apply these overrides, we use multiple inheritance with the registered subclass of `WrapperFxCodegen`.

Unlike most other wrapper codegen methods, which map 1:1 to Wrapper IR lines, subgraph codegen generates a number of wrapper lines including `EnterSubgraphLine` and `ExitSubgraphLine`, along with Python or C++ code calling the subgraph as a function. These lines are used for some backends' memory planning.

In contrast, FX IR typically represents a subgraph call as a single HOP node, or a `call_module` op. To account for this difference, this PR introduces a new wrapper IR line called `ConditionalLine`, which is only used by the FX backend. We override the `codegen_conditional` method to emit this line. This sidesteps having to port the existing subgraph codegen and associated memory planning to Wrapper IR. (In principle, it seems possible to adapt the existing backends to `ConditionalLine`, but it could be a larger refactor, since we'd also have to update the memory planning.)

Some of the lower-level subgraph codegen methods are still shared between the FX and Python backends, such as `generate_subgraph_common`. Those were easier to port to Wrapper IR.

This also required generalizing the way the FX converter handles graph inputs and outputs. Previously, it assumed the IO signature was the same as `V.graph.module`, but this is only true for the parent graph, and not subgraphs. Instead, we need to call `get_graph_inputs` and `get_graph_outputs` to populate the inputs and outputs for subgraphs.

# Test plan
This PR adds a couple of tests using torch.cond. Here's an example graph generated by one of them:
```
graph():
    %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
    %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]
    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]
    %cond : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%arg0_1, %true_graph_0, %false_graph_0, (%arg1_1,)), kwargs = {})
    %buf1 : [num_users=2] = call_function[target=operator.getitem](args = (%cond, 0), kwargs = {})
    %triton_kernel_wrapper_mutation : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 6, constant_args_idx: 6, grid: [(1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {in_out_ptr0: %buf1, xnumel: 6, XBLOCK: 8}})
    return buf1
```

It also removes an existing negative test which checked that a certain error was raised when subgraphs were encountered.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 02:08:45+00:00,2025-09-20T03:53:36Z,,False,6,8,14,297,74,5,14,2025-09-20 03:52:34+00:00,32,3971,False,False,True,False,False,True,5,5,1356,13689,9940,3749,1,14,4.0,6.0,2025-09-18T22:21:28Z,pytorch
163233,open,[CPU] Support transpose and packing fusion for bit8,Valentine233,"To be used by CPU INT8 SDPA in TorchAO https://github.com/pytorch/ao/pull/3025. This change has a kernel improvement of about 9%.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-18 01:53:07+00:00,2025-09-23T04:42:07Z,,False,2,0,3,132,0,2,2,,51,212,False,False,False,False,True,False,2,1,55,152,142,10,1,3,3.0,1.0,2025-09-19T06:03:33Z,pytorch
163232,closed,Skip reuse PyTorch wheel when building vLLM,huydhn,"This issues starts surfacing in [trunk](https://hud.pytorch.org/hud/pytorch/pytorch/b26d4c9a7a41727e7f842224a6566396fb664476/1?per_page=50&name_filter=vllm).  When building vLLM, uv doesn't like that we rename CI wheel without changing its metadata to match it.

cc @zou3519 
",2025-09-18 01:10:08+00:00,2025-09-18T01:43:39Z,,False,4,0,1,2,0,1,4,2025-09-18 01:42:36+00:00,43,276,False,False,False,False,False,False,1,3,917,2,2,0,1,1,4.0,3.0,2025-09-18T01:16:25Z,pytorch
163231,closed,[CP] Fix cuDNN CP LSE dimension bug,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* __->__ #163231
* #163131
* #163115
* #162541
* #162540
* #162539

We should only unsqueeze if necessary.

Fix https://github.com/pytorch/pytorch/issues/162743

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-18 01:06:55+00:00,2025-09-20T06:14:52Z,,False,5,2,2,15,3,1,7,2025-09-20 06:13:49+00:00,35,352,False,True,False,False,False,False,1,4,563,26,19,7,1,2,5.0,4.0,2025-09-18T03:47:13Z,pytorch
163227,closed,[Graph Partition] improve custom op output alias,BoyuanFeng,"For a custom op with multiple outputs, we will see the following generated code:
```
buf1 = op1(arg0)
buf3 = buf0[0]
buf4 = buf0[1]
del buf1 # <--- if buf1 is not accessed in the future
```

If `buf1` is not accessed in the future, it's good to deallocate early. So we don't delay `del` until both buf3 and buf4 are not used anymore. Note that buf3 and buf4 hold reference to the data such that `del buf1` does not prevent their usage.

However, when there are mutating args, we don't see `del buf1` immediately.

```python
@torch.library.custom_op(
    ""mylib::op1"",
    mutates_args=[""x""],
    schema=""(Tensor(a!)?  x) -> (Tensor, Tensor)"",
    device_types=""cuda"",
)
def op1(x) -> tuple[torch.Tensor, torch.Tensor]:
    x = x + 1
    return (x + 1, x + 2)
```

<img width=""661"" height=""821"" alt=""image"" src=""https://github.com/user-attachments/assets/3d1d1f5a-9749-4652-bb02-da593c78702d"" />



Why? Because `buf3` is a MultiOutput with `buf1` as input and believes `buf1` (an output of FallbackKernel op1) has inputs that alias output.
https://github.com/pytorch/pytorch/blob/72fedf05752069c9e8b97c64397aedf6ee2bf5ec/torch/_inductor/ir.py#L7976-L7982


According to `[NOTE: FallbackKernel supported operators]`, as a mutating op that are auto-functionalizable, buf1's output should NOT alias any of the inputs. This PR improves get_inputs_that_alias_output of Fallback Kernel.

Use case: [moe custom op in vllm](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/layer.py#L2057-L2064)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @zou3519",2025-09-18 00:56:00+00:00,2025-09-19T22:46:34Z,,False,7,5,4,78,4,3,12,2025-09-19 17:01:40+00:00,48,1734,False,False,False,False,True,False,3,6,2189,1723,1277,446,1,4,6.0,7.0,2025-09-18T04:30:08Z,pytorch
163226,closed,"Revert ""[ROCm] Use MI325 (gfx942) runners for binary smoke testing (#162044)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit cd529b686d54bbaa443f5b310140de48422d96c7.

Reverted https://github.com/pytorch/pytorch/pull/162044 on behalf of https://github.com/jeffdaily due to mi200 backlog is purged, and mi300 runners are failing in GHA download ([comment](https://github.com/pytorch/pytorch/pull/162044#issuecomment-3254427869))

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-18 00:41:19+00:00,2025-09-18T03:21:04Z,,False,1,0,1,24,20,5,1,2025-09-18 03:21:04+00:00,77,534,False,False,False,False,False,False,5,0,0,44,24,20,1,1,,,,pytorch
163225,closed,[CUDAGraph] add config to error on skipping cudagraph (#161862),ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Many users want a config to force all cuda ops captured by cudagraph. When not possible, pt2 should error.

This PR adds `torch._inductor.triton.cudagraph_or_error` for that (default as False). Also added an environment variable `TORCHINDUCTOR_CUDAGRAPH_OR_ERROR` to control.
Approved by: https://github.com/ezyang, https://github.com/mlazos

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 00:41:14+00:00,2025-09-18T03:21:01Z,,False,2,0,1,24,0,3,2,2025-09-18 03:21:01+00:00,63,638,False,False,False,False,False,False,3,0,0,24,24,0,1,1,,,,pytorch
163224,closed,"Revert ""Always build USE_DISTRIBUTED. (#160449)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit 90b08643c3a6eb1f3265b7d1388bd76660759f46.

Reverted https://github.com/pytorch/pytorch/pull/160449 on behalf of https://github.com/jeanschmidt due to Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 00:41:10+00:00,2025-09-18T03:20:56Z,,False,1,0,1,214,123,28,1,2025-09-18 03:20:56+00:00,48,563,False,False,False,False,False,False,28,0,0,337,214,123,1,1,,,,pytorch
163223,closed,Fix SEMI_STRUCTURED_SUPPORTED_BACKENDS selection on CUDA and ROCm,dnikolaev-amd,"It should work with the current CUDA/ROCm device_capability enumeration anyway. But it will help to avoid unexpected triggering in the future


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-18 00:37:38+00:00,2025-09-18T06:30:33Z,,False,4,0,1,2,2,1,4,2025-09-18 06:29:31+00:00,65,260,False,True,False,False,False,False,1,2,493,4,2,2,1,1,2.0,2.0,2025-09-18T02:53:05Z,pytorch
163222,closed,"Revert ""Always build USE_DISTRIBUTED. (#160449)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit 90b08643c3a6eb1f3265b7d1388bd76660759f46.

Reverted https://github.com/pytorch/pytorch/pull/160449 on behalf of https://github.com/jeanschmidt due to Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 00:37:07+00:00,2025-09-18T03:20:17Z,,False,1,0,1,214,123,28,1,2025-09-18 03:20:17+00:00,48,563,False,False,False,False,False,False,28,0,0,337,214,123,1,1,,,,pytorch
163221,closed,"Revert ""[ROCm] Use MI325 (gfx942) runners for binary smoke testing (#162044)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit cd529b686d54bbaa443f5b310140de48422d96c7.

Reverted https://github.com/pytorch/pytorch/pull/162044 on behalf of https://github.com/jeffdaily due to mi200 backlog is purged, and mi300 runners are failing in GHA download ([comment](https://github.com/pytorch/pytorch/pull/162044#issuecomment-3254427869))

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-18 00:29:54+00:00,2025-09-18T03:20:52Z,,False,1,0,1,24,20,5,1,2025-09-18 03:20:52+00:00,77,534,False,False,False,False,False,False,5,0,0,44,24,20,1,1,,,,pytorch
163220,closed,[CUDAGraph] add config to error on skipping cudagraph (#161862),ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Many users want a config to force all cuda ops captured by cudagraph. When not possible, pt2 should error.

This PR adds `torch._inductor.triton.cudagraph_or_error` for that (default as False). Also added an environment variable `TORCHINDUCTOR_CUDAGRAPH_OR_ERROR` to control.
Approved by: https://github.com/ezyang, https://github.com/mlazos

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 00:29:50+00:00,2025-09-18T03:20:48Z,,False,2,0,1,24,0,3,2,2025-09-18 03:20:48+00:00,63,638,False,False,False,False,False,False,3,0,0,24,24,0,1,1,,,,pytorch
163219,closed,"Revert ""Always build USE_DISTRIBUTED. (#160449)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit 90b08643c3a6eb1f3265b7d1388bd76660759f46.

Reverted https://github.com/pytorch/pytorch/pull/160449 on behalf of https://github.com/jeanschmidt due to Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 00:29:45+00:00,2025-09-18T03:20:44Z,,False,1,0,1,214,123,28,1,2025-09-18 03:20:44+00:00,48,563,False,False,False,False,False,False,28,0,0,337,214,123,1,1,,,,pytorch
163218,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-18 00:25:27+00:00,2025-09-18T04:32:53Z,,False,3,0,1,1,1,1,3,2025-09-18 04:31:50+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-18T00:25:28Z,pytorch
163217,closed,"Revert ""Always build USE_DISTRIBUTED. (#160449)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit 90b08643c3a6eb1f3265b7d1388bd76660759f46.

Reverted https://github.com/pytorch/pytorch/pull/160449 on behalf of https://github.com/jeanschmidt due to Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 00:24:09+00:00,2025-09-18T03:20:40Z,,False,1,0,1,214,123,28,1,2025-09-18 03:20:40+00:00,48,563,False,False,False,False,False,False,28,0,0,337,214,123,1,1,,,,pytorch
163216,closed,"Revert ""Always build USE_DISTRIBUTED. (#160449)""",ColinPeppler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

This reverts commit 90b08643c3a6eb1f3265b7d1388bd76660759f46.

Reverted https://github.com/pytorch/pytorch/pull/160449 on behalf of https://github.com/jeanschmidt due to Already discussed with @ezyang about the internal quirks and errors ([comment](https://github.com/pytorch/pytorch/pull/160449#issuecomment-3254219358))

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-18 00:20:09+00:00,2025-09-18T03:20:24Z,,False,1,0,1,214,123,28,1,2025-09-18 03:20:24+00:00,48,563,False,False,False,False,False,False,28,0,0,337,214,123,1,1,,,,pytorch
163215,open,[inductor] do comm compute overlap at aten fx level,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163754
* __->__ #163215

This is first part of the stack that does comm/compute reordering, and then uses the exposure analysis to do bucketing.

Subsequent prs will handle:
- use of exposure analysis to do bucketing
- make sure inductor respects comm/compute overlapping done at fx level
- non-profiling mm estimation/rank broadcasting of profile results 

Other mis:
- Validate accuracy of nccl estimations  ( use ruisi's profiling instead ?)


For a llama 2d parallelism test, on forward, we overlap all but 2 of potentially hidden collectives. For backward, we overlap 217/269 of potentially hidden collectives. If you increase `compute_overlap_multipler` (for fudge factor of inaccurate comms estimation), that goes down to all but 16 of potentially hidden collectives.

fwd example: https://gist.github.com/eellison/76209c49d8829c5f1e323d34a3f040c3

bwd example: https://gist.github.com/eellison/6cfc2285df53a94cfa4012f5fdae5c51



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-18 00:15:22+00:00,2025-09-24T13:26:30Z,,False,5,15,11,1090,9,6,20,,51,1218,False,False,False,False,False,False,6,4,247,127415,82796,44619,1,11,3.0,4.0,2025-09-22T13:32:58Z,pytorch
163214,closed,[CI] reuse old whl: fix metadata file not getting version replaced,clee2000,"In the .dist-info/METADATA file, the version was not being written with the new sha.

On python <3.11 (I think), the glob `**` will only match directories, so change this to `*`, which I checked that it will match both files and directories on py3.9 and py3.13

There's probably also a bunch of mismatches in RECORD but thats a problem for later",2025-09-17 23:53:19+00:00,2025-09-18T16:09:34Z,,False,4,0,1,1,1,1,4,2025-09-18 16:08:33+00:00,66,345,False,True,False,False,False,False,1,3,1148,2,1,1,1,1,3.0,4.0,2025-09-18T03:26:23Z,pytorch
163213,open,[DeviceMesh] Simplifying internal bookkeeping with CuTe layout,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163358
* #161224
* __->__ #163213
* #163367
* #163288
* #163212

We want to refactor the internal bookkeeping of DeviceMesh so that:
Simply the bookkeeping logics and make it generic enough so that it is easy to support new transformations like flatten noncontiguous dim, reshape and unflatten. (We leveraged the CuTe layout). This new layout also let us handle non-contiguous slicing, flatten, transpose possible.


Concretely, in this PR, we do the following:
1. Use the _Layout to handle all index operations rather use a map to record mesh dims.
2. Replaced`flatten_name_to_root_dims` with `flatten_name_to_root_layout`. Basically one (size, stride) pair maps to one PG. One mesh_dim_name can only map to only layout. (More than one mesh_dim_name can map to the same layout).
3. Replaced `_get_slice_mesh_dims` with `_get_slice_mesh_layout`.
4. Use a new function `check_overlap` to check layout overlap.
5. Use a new function `to_remapping_tensor` to use layout ranks as indices when the mesh tensor is not representable as CuTe.
6. Separated the Devicemesh creation via mesh into a util function `_create_mesh_from_ranks`.
7. Added a type alias for `tuple[Optional[str], Optional[C10dBackend.Options]]` to be `BackendConfig`


The PR looks big indeed but we don't change any existing behavior of DeviceMesh, so it is a pure refactor.

With this refactoring we also enabled the slicing and flatten of non-contiguous dims of a device mesh which is hard to implement without cute layout.

This is a continue of https://github.com/pytorch/pytorch/pull/161106 (original one got messed with EasyCLA)

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 23:40:14+00:00,2025-09-25T00:35:49Z,,False,1,38,9,196,166,2,39,,62,1773,False,False,False,False,False,True,2,0,177,10516,6347,4169,1,9,5.0,1.0,2025-09-18T05:02:11Z,pytorch
163212,open,[DeviceMesh] Introduce CuTe layout into devicemesh code base for internal bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163358
* #161224
* #163213
* #163367
* #163288
* __->__ #163212

DeviceMesh essentially is a way to specify how devices interact with each other or device layout. They are all integers but because they can have various shapes and meshes, it make internal bookkeeping internally way more challenging. Currently our internal bookkeeing inside DeviceMesh is not scalable, so in order to support new functions like `_unflatten`, we need to introduce very complicated logics inside DeviceMesh as pointed out per comment (https://github.com/pytorch/pytorch/pull/159482/files#r2256025452). So thanks to @lw 's suggestion and PoC PR (https://github.com/pytorch/pytorch/pull/160429), we realize that by leveraging CuTe layout algebra([ref](https://docs.nvidia.com/cutlass/media/docs/cpp/cute/02_layout_algebra.html)) from Cutlass will greatly simply our internal mechanical bookkeeping for and make the abstraction ops way easier on top of it. So to make things go incrementally, we propose couple steps here https://github.com/pytorch/pytorch/issues/160337#issuecomment-3195106243.

On top of what we have been doing about PyCute we want to continue add methods into the wrapper class so that we can get rank indexes needed for ProcessGroup Creation with a layout object. We also added detailed explanations and comments (thanks to llm) and unit test to show case the code indeed is working as expected. 

More PRs are on the way.

This is a continue of https://github.com/pytorch/pytorch/pull/161016 (originally messed with EasyCLA)

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 23:40:11+00:00,2025-09-24T21:40:30Z,,False,1,3,2,343,1,2,4,,85,1699,False,False,False,True,False,False,2,0,32,9994,6126,3868,1,2,4.0,2.0,2025-09-18T04:05:15Z,pytorch
163211,closed,[DeviceMesh] Introduce CuTe layout into devicemesh code base for internal bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163211
* #162690
* #162414
* #162534
* #162413

DeviceMesh essentially is a way to specify how devices interact with each other or device layout. They are all integers but because they can have various shapes and meshes, it make internal bookkeeping internally way more challenging. Currently our internal bookkeeing inside DeviceMesh is not scalable, so in order to support new functions like `_unflatten`, we need to introduce very complicated logics inside DeviceMesh as pointed out per comment (https://github.com/pytorch/pytorch/pull/159482/files#r2256025452). So thanks to @lw 's suggestion and PoC PR (https://github.com/pytorch/pytorch/pull/160429), we realize that by leveraging CuTe layout algebra([ref](https://docs.nvidia.com/cutlass/media/docs/cpp/cute/02_layout_algebra.html)) from Cutlass will greatly simply our internal mechanical bookkeeping for and make the abstraction ops way easier on top of it. So to make things go incrementally, we propose couple steps here https://github.com/pytorch/pytorch/issues/160337#issuecomment-3195106243.

On top of what we have been doing about PyCute we want to continue add methods into the wrapper class so that we can get rank indexes needed for ProcessGroup Creation with a layout object. We also added detailed explanations and comments (thanks to llm) and unit test to show case the code indeed is working as expected. 

More PRs are on the way.

This is a continue of https://github.com/pytorch/pytorch/pull/161016 (originally messed with EasyCLA)

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 23:35:54+00:00,2025-09-17T23:41:41Z,,False,1,0,1,341,1,2,1,2025-09-17 23:41:41+00:00,85,1689,False,False,False,True,False,False,2,0,0,342,341,1,1,1,,,,pytorch
163210,closed,[BE] Remove bottleneck,PaliC,"cc @ezyang @gchanan

Some cleanup related to this RFC: https://github.com/pytorch/pytorch/issues/68742",2025-09-17 23:30:55+00:00,2025-09-18T12:09:20Z,,False,5,0,3,0,495,10,5,2025-09-18 12:08:16+00:00,22,102,False,False,False,False,False,False,10,3,774,525,15,510,1,3,3.0,4.0,2025-09-17T23:30:55Z,pytorch
163209,closed,[inductor][choices] move extra kwargs out of get_template_configs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163206
* __->__ #163209
* #163305

# why

- extra kwargs are input/op dependent and not config dependent. We don't
  plan to serialize/deserialize them, and so they need to be fed in
  later beore making the KTC, rather than when getting the config values
  directly

# what

- move extra_kwargs into the KTC and get_ktc interface directly

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v -k ""_addmm""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D82871310](https://our.internmc.facebook.com/intern/diff/D82871310)",2025-09-17 23:11:22+00:00,2025-09-20T05:31:48Z,,False,5,0,3,8,6,3,5,2025-09-20 05:30:43+00:00,65,792,False,False,False,False,False,False,3,4,664,8683,4887,3796,1,3,3.0,4.0,2025-09-20T00:00:41Z,pytorch
163208,open,[Graph Partition] better support for custom op mutation,BoyuanFeng,"Scheduler relies on node.last_usage to free buffers. `last_usage` may contain a buffer that is allocated in previous graph partition AND not directly accessed in the current graph partition.

## Before
One example is 
```python
            @torch.library.custom_op(
                ""mylib::op1"",
                mutates_args=[""x""],
                schema=""(Tensor(a!)?  x) -> (Tensor, Tensor)"",
                device_types=""cuda"",
            )
            def op1(x) -> tuple[torch.Tensor, torch.Tensor]:
                x = x + 1
                return (x + 1, x + 2)
```

In the generated code, there would be
```python
def partition0(x):
    buf0 = op1(x)
    buf1 = buf0[0]
    buf2 = buf0[1]
    return (buf1, buf2)

def partition1(buf1, buf2):
    buf3 = buf1 + buf2
    del buf0 #<--- error
    del buf1
    del buf2
    return buf3
```

Since `buf0` is not directly accessed by partition1, partition1 does not use it as a graph partition inputs.


## After
This PR fixes the issue by including `(buffer_names_to_free - output_names)` to partition inputs. Here, output_names contains all outputs generated in the current graph partition.

The generated code would be
```python
def partition0(x):
    buf0 = op1(x)
    buf1 = buf0[0]
    buf2 = buf0[1]
    return (buf0, buf1, buf2)

def partition1(buf0, buf1, buf2):
    buf3 = buf1 + buf2
    del buf0
    del buf1
    del buf2
    return buf3
```



## TODO

This works from codegen perspective. But cudagraph trees assume all inputs/outputs are tensors. It does not allow buf0 which is `a tuple of Tensor` instead of a `Tensor`.

cc @mlazos @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 23:09:36+00:00,2025-09-18T03:23:18Z,,False,1,0,1,62,0,2,1,,55,1801,False,True,False,False,False,False,2,0,0,62,62,0,1,1,,,,pytorch
163207,closed,[DO NOT LAND] Make LSTM exportable,yiming0416,"Summary:
LSTM was not exportable as it failed at `_detect_attribute_assignment` check.

This is because the `_flat_weights` attribute in LSTM is a list of parameters that could be updated by the `_update_flat_weights` method, and this could cause silent fake tensor leakage.

We add `torch.compiler.is_exporting()` check in the `_update_flat_weights` to make sure that `_flat_weights` won't be updated during export so that no fake tensor leakage will happen.

Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_lstm_export

Rollback Plan:

Differential Revision: D82682791


",2025-09-17 22:47:57+00:00,2025-09-24T21:23:06Z,,False,5,0,1,12,1,2,5,2025-09-24 21:23:06+00:00,34,596,False,False,False,False,False,False,2,3,2250,13,12,1,1,1,2.0,3.0,2025-09-17T23:13:18Z,pytorch
163206,open,[inductor][template heuristics] decompose k as first KernelTemplateParams,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163206
* #163209
* #163305

# why

- start moving schema-less dicts to structured KernelTemplateParams so
  that its clear which params each template has and which are
  required/defaulted

# what

- modify the get_template_configs to expect a dict or a
  KernelTemplateParams object, and only wrap the dict in that case but
  forward the KernelTemplateParams directly

- implement DataclassTemplateParams
- add DecomposeKTemplateParams as a version of that with k_split params

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v -k ""decompose_k""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D82871309](https://our.internmc.facebook.com/intern/diff/D82871309)",2025-09-17 22:46:25+00:00,2025-09-20T04:46:23Z,,False,2,0,4,40,8,3,2,,73,942,False,False,False,False,False,False,3,1,158,8837,5020,3817,1,4,1.0,1.0,2025-09-20T00:26:12Z,pytorch
163205,closed,[ROCm][SymmMem] Fix skip condition for PLATFORM_SUPPORTS_SYMM_MEM,pragupta,"It seems `TEST_CUDA` is set to true even for ROCm (MI200) jobs. Changing if TEST_CUDA to an else condition to avoid running symmetric memory UTs on MI200. For other non-rocm arch, it should return true and can be skipped using other skip decorators. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-17 22:39:20+00:00,2025-09-19T12:13:55Z,,False,4,0,2,10,8,1,4,2025-09-19 12:12:51+00:00,65,457,False,True,False,False,False,False,1,2,823,22,12,10,2,2,3.0,3.0,2025-09-18T03:53:25Z,pytorch
163204,open,[CI] Build slimmer torch for SymmMem CI,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163204

",2025-09-17 22:36:54+00:00,2025-09-18T01:50:22Z,,False,1,0,1,8,0,1,1,,39,106,False,False,False,False,False,False,1,0,153,8,8,0,1,1,1.0,1.0,2025-09-17T22:42:54Z,pytorch
163203,closed,[testing] Add test owner labels for some ao sparse tests,clee2000,"I am trying to give some test files better owner labels than `module: unknown`.  I am not sure them, but they seem pretty reasonable",2025-09-17 22:21:26+00:00,2025-09-18T16:09:20Z,,False,3,0,1,11,11,11,3,2025-09-18 16:08:16+00:00,56,132,False,False,False,False,False,False,11,2,783,22,11,11,1,1,3.0,2.0,2025-09-18T12:41:33Z,pytorch
163201,open,[inductor][choices] remove kwargs overriders,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163201
* #163200
* #163199
* #163198

# why

- usage is fully migrated towards passing the kwargs through
  kernel_inputs
- interface simplification

# what

- remove unused kwarg_overriders in get_template_configs and
  _finalize_template_configs

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-17 21:44:33+00:00,2025-09-18T00:35:50Z,,False,1,0,1,3,23,3,1,,44,602,False,False,False,False,False,False,3,0,0,26,3,23,1,1,,,,pytorch
163200,open,[inductor][heuristics] move use_fast_accum into kernel_inputs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

# why

- remove last user of kwarg overriders

# what

- all scaled_mm kernels (triton, aten) that need this, are now getting
  it from the kwargs inside kernel_inputs

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-17 21:44:29+00:00,2025-09-18T00:35:23Z,,False,1,0,1,25,4,3,1,,61,530,False,False,False,False,False,False,3,0,0,29,25,4,1,1,,,,pytorch
163199,open,[inductor][heuristics] move out_dtype into kernel_inputs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163201
* #163200
* __->__ #163199
* #163198

# why

- step closer towards simplifying the interface to not need kwargs
  overriders

# what

- avoid passing out_dtype through kwargs_overriders
- extract where needed for ATen templates through kernel_inputs

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-17 21:44:23+00:00,2025-09-19T00:50:02Z,,False,2,9,1,27,19,4,11,,56,604,False,False,False,False,False,False,4,1,1095,46,27,19,1,1,3.0,3.0,2025-09-18T17:23:32Z,pytorch
163198,open,[inductor] rename kernel inputs scalars to kwargs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163201
* #163200
* #163199
* __->__ #163198

# why

- evolve the kernel inputs to be nodes + kwargs
- clearer that this is not limited to just scalars

- this is 1/n of removing the extra_kwargs and overriders from
  get_template_configs in favor of simplifying it to just being
  - template
  - kernel inputs
  - heuristic generated kwargs

# what

- rename scalars related stuff to kwargs
- replace invocations in addmm related configs

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v -k ""_addmm""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-17 21:44:18+00:00,2025-09-18T20:19:28Z,,False,2,0,1,31,18,5,2,,49,797,False,False,False,False,False,False,5,1,408,49,31,18,1,1,2.0,2.0,2025-09-18T17:24:38Z,pytorch
163197,open,[WIP][ROCm][inductor] heuristic improvements for pointwise kernels,naromero77amd,"Heuristic improvements for pointwise kernels for MI350.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 21:31:58+00:00,2025-09-23T15:20:55Z,,False,1,0,8,41,6,2,1,,66,358,False,False,False,False,True,False,2,0,0,57,46,11,4,8,,,,pytorch
163195,open,[testing] Add test owner labels for some tests (some core stuff?),clee2000,"I am trying to give some test files better owner labels than `module: unknown`.  I am not sure them, but they seem pretty reasonable

",2025-09-17 21:30:43+00:00,2025-09-22T14:46:09Z,,False,1,3,2,8,8,8,4,,65,134,False,False,False,False,False,False,8,0,191,18,9,9,2,2,5.0,2.0,2025-09-18T18:51:10Z,pytorch
163194,closed,[SymmMem] Fix put_signal + wait_until hang,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #163194
* #163152

The test used a wrong ptr to refer to remote address:
```
            dst_ptr = out_hdl.buffer_ptrs[peer]
            src_ptr = inp_hdl.buffer_ptrs[rank]
            sig_ptr = out_hdl.signal_pad_ptrs[peer]
```
All three indices should be `rank` instead of `peer` because NVSHMEM APIs accept local address as input and perform translation internally. Without correct signal address, the peer would be waiting, thus hang.

Also adjusted the signature of `nvshmem.putmem_signal_block` to accept tensor instead of pointer.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 21:16:10+00:00,2025-09-21T21:00:38Z,,False,6,0,1,44,59,2,6,2025-09-18 18:20:04+00:00,42,737,False,True,False,False,False,False,2,5,1304,103,44,59,1,1,4.0,5.0,2025-09-18T13:56:46Z,pytorch
163192,closed,Add GPU health check functionality from NVIDIA Resiliency Extension,ShreyaGupta08,"## Summary
This PR upstreams GPU health monitoring capabilities from the [NVIDIA Resiliency Extension](https://github.com/NVIDIA/nvidia-resiliency-ext/blob/e0fa23e766a61d9eddb6601d1a2ad720311d14a6/src/nvidia_resiliency_ext/shared_utils/health_check.py#L202) into PyTorch's distributed elastic training system. The implementation provides comprehensive GPU health monitoring, recovery action detection, and integration with PyTorch's existing health check infrastructure.

## Changes
- **Core Implementation**: Added `GPUHealthCheck` class based on NVIDIA Resiliency Extension
- **Server Integration**: Added `GPUHealthCheckServer` for distributed elastic training
- **Testing**: Comprehensive test suite in `test/distributed/elastic/agent/server/test/`
- **Documentation**: RST documentation in `docs/source/elastic/gpu_health_check.rst`
- **Examples**: Usage examples in `torch/distributed/elastic/examples/`

## Features
- **GPU Recovery Action Detection**: Monitors GPU health using CUDA's GPU Recovery API
- **Driver Version Validation**: Ensures compatibility with CUDA driver version r570 or newer
- **Thread-Safe Operations**: Safe for use in multi-threaded environments using `threading.RLock`
- **Asynchronous Monitoring**: Supports both synchronous and asynchronous health checks
- **GB200 Platform Support**: Special handling for NVIDIA GB200 platforms
- **Integration with Health Check Servers**: Seamless integration with PyTorch's distributed elastic health check system

## API Reference
- `GPUHealthCheck`: Main class for GPU health monitoring
- `GPUHealthCheckServer`: Health check server with GPU monitoring capabilities
- `PynvmlMixin`: Mixin class providing PyNVML functionality
- Convenience functions: `create_gpu_health_check()`, `quick_gpu_health_check()`

## GPU Recovery Actions Detected
- `NVML_GPU_RECOVERY_ACTION_NONE`: GPU is healthy, no action needed
- `NVML_GPU_RECOVERY_ACTION_GPU_RESET`: GPU requires a reset
- `NVML_GPU_RECOVERY_ACTION_NODE_REBOOT`: Node requires a reboot
- `NVML_GPU_RECOVERY_ACTION_DRAIN_P2P`: Peer-to-peer traffic needs to be drained
- `NVML_GPU_RECOVERY_ACTION_DRAIN_AND_RESET`: GPU operating at reduced capacity

## Testing
- Unit tests cover all major functionality
- Tests handle cases where PyNVML is not available
- Integration tests with health check servers
- Follows PyTorch testing conventions

## Documentation
- Complete API reference in RST format
- Usage examples and integration guide
- Troubleshooting section for common issues

## Requirements
- CUDA-compatible GPU
- CUDA driver version r570 or newer
- PyNVML library (`pip install nvidia-ml-py`)
- Python 3.7+

## Example Usage
```python
from torch.distributed.elastic.utils.gpu_health_check import create_gpu_health_check

# Create a GPU health check for all GPUs
gpu_check = create_gpu_health_check(
    device_index=None,  # Check all GPUs
    interval=60,        # Check every 60 seconds
    on_failure=lambda: print(""GPU health check failed!"")
)

# Perform a synchronous health check
is_healthy = gpu_check()
print(f""GPU health status: {'Healthy' if is_healthy else 'Unhealthy'}"")
```

## Integration with Distributed Training
The GPU health check system integrates seamlessly with PyTorch's distributed elastic training:
1. **Health Check Servers**: Use `GPUHealthCheckServer` in your elastic agent
2. **Monitoring Loops**: Integrate health checks into training loops
3. **Failure Handling**: Implement custom failure callbacks for recovery actions
4. **Status Reporting**: Use health status APIs for monitoring and alerting

## Files Added/Modified
- `torch/distributed/elastic/utils/gpu_health_check.py` - Core implementation
- `torch/distributed/elastic/agent/server/health_check_server.py` - Server integration
- `test/distributed/elastic/agent/server/test/gpu_health_check_test.py` - Test suite
- `torch/distributed/elastic/examples/example_gpu_health_check.py` - Usage examples
- `docs/source/elastic/gpu_health_check.rst` - Documentation
- `docs/source/distributed.elastic.md` - Updated documentation index

## Related Issues
This addresses the need for robust GPU health monitoring in distributed training workloads, particularly for long-running training jobs where GPU failures can cause significant disruption.

## Checklist
- [x] Code follows PyTorch style guidelines
- [x] Tests pass locally
- [x] Documentation updated
- [x] Examples provided
- [x] Thread-safe implementation
- [x] Error handling for missing dependencies
- [x] Integration with existing health check system

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 20:49:53+00:00,2025-09-24T23:24:44Z,,False,3,0,7,1166,2,7,3,2025-09-21 02:58:34+00:00,67,4620,False,False,True,True,False,False,7,1,101,203864,132495,71369,2,7,1.0,1.0,2025-09-24T23:24:44Z,pytorch
163191,open,"[dynamo, 3.14] fix inactive ctx handling in resume functions",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* __->__ #163191
* #163110
* #163109
* #163009
* #161839
* #161555
* #161838

",2025-09-17 20:42:40+00:00,2025-09-25T00:04:09Z,,False,2,0,2,5,7,1,2,,60,184,False,True,False,False,False,False,1,0,0,4201,3364,837,1,2,,,,pytorch
163190,closed,wip mark_dynamic name,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163190
* #163123
* #163121



cc @ezyang @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-17 20:41:53+00:00,2025-09-18T20:00:44Z,,False,1,0,3,41,10,5,1,2025-09-18 20:00:44+00:00,21,294,False,False,False,False,False,False,5,0,0,58,44,14,1,3,,,,pytorch
163188,open,[aoti] AOTI mingw cross compilation ,yushangdi,"To run this, you need to install `mingw64-gcc-c++` and download windows cuda library toolkit. 

See design doc and demo instructions in https://docs.google.com/document/d/1iDaChqA5nNKkBFTzsdkmoomvQlXHbnlb1Z4yEp7xaJA/edit?tab=t.0


If cross_platform_target is windows, we do the following:

- do not link to `sleef`. This can be improved in the future if we need it. Currently I avoid it because that requires extra setup on the linux side
- Use `mingw64-gcc-c++` to compile
- Use `WINDOWS_CUDA_HOME` instead of `CUDA_HOME` when linking to cuda


```
 python test/inductor/test_aot_inductor_windows.py -k so
 ```
 
 
 Other changes:
 - de-couples compile_standalone config and dynamic link flag
 - create a new aot_inductor_mode config module, which is used to control configs in aot_inductor.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 20:26:52+00:00,2025-09-25T01:51:04Z,,False,3,20,4,203,59,12,23,,36,997,False,False,False,True,True,False,12,2,620,390,267,123,1,4,4.0,3.0,2025-09-17T22:04:49Z,pytorch
163187,closed,[Reland] Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available,SherlockNoMad,"Reland of #160532

Summary:

To support exporting a cuda model on a CPU-only machine under fake tensor mode. User commonly need to move sample inputs to the cuda device with .to(""cuda:0"") or .to(""cuda"") call. This diff supports this.
I expect the following pattern to work
```
with FakeTensorMode(allow_non_fake_inputs=True):
    cuda_module = module.to(""cuda:0"")
    cuda_sample_inputs = tuple([x.to(""cuda:0"") for x in sample_inputs])
    with torch.no_grad():
        ep = torch.export.export(cuda_module, cuda_sample_inputs)
```

Pull Request resolved: https://github.com/pytorch/pytorch/pull/163016
Approved by: https://github.com/huydhn
",2025-09-17 20:19:20+00:00,2025-09-18T04:47:30Z,,False,6,0,1,210,5,6,6,2025-09-18 04:46:29+00:00,94,642,False,False,False,False,False,False,6,5,1710,215,210,5,1,1,3.0,5.0,2025-09-17T21:09:33Z,pytorch
163186,open,Update LPPool docs to clarify ceil_mode padding semantics when ceil_mode=True,Jonahcb,"# Summary

- Add a note to each `nn.LPPool*d` docstring explaining how `ceil_mode=True` interacts with right padding.  
- Mirror the same clarification in the `torch.nn.functional.lp_pool*` docstrings so the rendered functional docs stay in sync.

# Motivation

The current PyTorch spec for **LPPool** does not fully match runtime behavior, which has led to downstream confusion in other specs (e.g., ONNX) and runtimes (e.g., [onnxruntime issue #25848](https://github.com/microsoft/onnxruntime/issues/25848)). A corresponding clarification was also made in the ONNX spec: [onnx/onnx#5741](https://github.com/onnx/onnx/pull/5741).

PyTorch’s **LPPool** implementation calls into **AvgPool**, which enforces the rule that windows starting entirely in the right padded region are ignored when `ceil_mode=True`. As a result, **LPPool** inherits the same behavior.  

This is an edge case where the output size formula shown in the LPPool docs/spec is not sufficient on its own. Without the added caveat, the documentation is technically incorrect. This PR brings the LPPool docs in line with actual behavior.

Note that this is a trivial fix to the spec as all major implementers of the spec adhere to this caveat.

For comparison, both **MaxPool** and **AvgPool** already include this clarification in their spec. Their docstrings explicitly state:

> *When `ceil_mode=True`, sliding windows are allowed to go off-bounds if they start within the left padding or the input. Sliding windows that would start in the right padded region are ignored.*

Adding the same note to LPPool ensures consistency across all pooling operators.




",2025-09-17 20:17:33+00:00,2025-09-22T16:32:46Z,,False,2,0,1,21,0,2,2,,77,1631,False,True,False,True,False,False,2,0,0,21,21,0,1,1,,,,pytorch
163185,open,[CP] Introduce flex_cp_forward custom op for FlexAttention CP,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163185
* #162542
* #163231
* #163131
* #163115
* #162541
* #162540
* #162539

The custom op will fetch the required K and V. Currently, the forward pass is just an all-gather, and the backward pass is a reduce-scatter.  While the logic is the same as all_gather_tensor_autograd, the custom op avoids the Autograd warning that wait_tensor() is registered to autograd.

For the next step, we should explore how to interpolate the required communication based on the information from BlockMask.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 19:55:11+00:00,2025-09-18T04:00:51Z,,False,1,0,3,150,16,3,1,,61,673,False,False,False,False,False,False,3,0,0,5481,4290,1191,1,3,,,,pytorch
163183,open,[torchrec][LocalShardsWrapper] Implement tensor padding for local shards wrapper,jeffkbkim,"Differential Revision: D82663766

This patch implements the constant padding functionality (aten.constant_pad_nd.default) for LocalShardsWrapper. The method applies constant padding to the local shards based on the provided padding specification.

Padding may be used to resize tensors to the same size in order to perform an all gather. LocalShardsWrapper, a wrapper around a DTensor's local_tensor, can now support padding across its local shards.

Depending on the sharding type (RW, CW), the padding on [left, right, top, bottom] directions will be either applied to the first/last shard, or all local shards.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 19:08:44+00:00,2025-09-23T23:20:07Z,,False,32,0,1,577,6,2,32,,80,717,False,False,False,False,False,False,2,4,821,583,577,6,1,1,2.0,5.0,2025-09-17T19:21:42Z,pytorch
163182,open,[inductor] can_free check against graph outputs directly instead of outputNode as users,xuanzhang816,"In the example in unittest, `x**2` are offloaded to CPU and thus should be deleted right afterwards via `del buf0`. However, there is no `del buf0` in the generated code, as shown below:

```
    def call(self, args):
        arg0_1, arg1_1 = args
        args.clear()
        assert_size_stride(arg0_1, (32, 32), (32, 1))
        assert_size_stride(arg1_1, (32, 32), (32, 1))
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [x], Original ATen: [aten.pow]
            stream0 = get_raw_stream(0)
            triton_poi_fused_pow_0.run(arg0_1, buf0, 1024, stream=stream0)
            del arg0_1
            buf1 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [y], Original ATen: [aten.mm]
            extern_kernels.mm(buf0, arg1_1, out=buf1)
            del arg1_1
            # Topologically Sorted Source Nodes: [], Original ATen: []
            buf2 = torch.ops.aten.to.device(buf0, device=device(type='cpu'), dtype=torch.float32)
        buf3 = buf2
        assert_size_stride(buf3, (32, 32), (32, 1), 'torch.ops.aten.to.device')
        assert_alignment(buf3, 16, 'torch.ops.aten.to.device')
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf4 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [z], Original ATen: [aten.mm]
            extern_kernels.mm(buf1, buf1, out=buf4)
            del buf1
        return (buf3, buf4, )
```

With the change, we now have the following generated code, where differences are highlighted.
```
    def call(self, args):
        arg0_1, arg1_1 = args
        args.clear()
        assert_size_stride(arg0_1, (32, 32), (32, 1))
        assert_size_stride(arg1_1, (32, 32), (32, 1))
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [x], Original ATen: [aten.pow]
            stream0 = get_raw_stream(0)
            triton_poi_fused_pow_0.run(arg0_1, buf0, 1024, stream=stream0)
            del arg0_1
            buf1 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [y], Original ATen: [aten.mm]
            extern_kernels.mm(buf0, arg1_1, out=buf1)
            del arg1_1
            # Topologically Sorted Source Nodes: [], Original ATen: []
            buf2 = torch.ops.aten.to.device(buf0, device=device(type='cpu'), dtype=torch.float32)
        buf3 = buf2
        assert_size_stride(buf3, (32, 32), (32, 1), 'torch.ops.aten.to.device')
        assert_alignment(buf3, 16, 'torch.ops.aten.to.device')
        **del buf0**
        **del buf2**
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf4 = empty_strided_cuda((32, 32), (32, 1), torch.float32)
            # Topologically Sorted Source Nodes: [z], Original ATen: [aten.mm]
            extern_kernels.mm(buf1, buf1, out=buf4)
            del buf1
        return (buf3, buf4, )
```



Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163182



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 19:05:02+00:00,2025-09-22T14:26:28Z,,False,3,0,4,49,3,2,3,,87,3531,False,False,False,False,False,False,2,2,327,5629,4085,1544,1,4,1.0,2.0,2025-09-17T19:21:18Z,pytorch
163181,closed,fdjlkj,xuanzhang816,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #163181


",2025-09-17 19:04:57+00:00,2025-09-18T17:48:13Z,,False,2,0,1,72,42,1,2,2025-09-17 19:16:12+00:00,6,96,False,False,False,False,False,False,1,0,0,114,72,42,1,1,,,,pytorch
163180,closed,temp,xuanzhang816,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #163180


",2025-09-17 19:04:53+00:00,2025-09-18T17:48:04Z,,False,2,0,1,71,18,3,2,2025-09-17 19:16:08+00:00,4,96,False,False,False,False,False,False,3,0,0,89,71,18,1,1,,,,pytorch
163179,closed,temp,xuanzhang816,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #163179

",2025-09-17 19:04:48+00:00,2025-09-18T17:47:56Z,,False,2,0,1,161,3,1,2,2025-09-17 19:16:05+00:00,4,95,False,False,False,False,False,False,1,0,0,164,161,3,1,1,,,,pytorch
163176,open,[testing] Add test owner labels for some tests,clee2000,"I am trying to give some test files better owner labels than `module: unknown`.  I am not sure them, but they seem pretty reasonable


cc @lolpack @maggiemoss @ndmitchell @kinto0 @samwgoldman @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @mcarilli @ptrblck @leslie-fang-intel",2025-09-17 18:40:36+00:00,2025-09-19T20:21:09Z,,False,1,0,1,8,8,8,1,,46,392,False,False,False,False,False,False,8,0,0,16,8,8,1,1,,,,pytorch
163175,closed,feat(quantization): Add ONNX export for dynamic linear\n\nThis commit…,Vinayak-Pawar,"Problem: UnsupportedOperatorError when exporting a dynamically quantized model to ONNX.

Challenges: Severe and intractable issues with the local build environment prevented a direct fix to the PyTorch source code.

Solution: A strategic pivot was made to work around the build issues. A standalone Python script, now located at examples/quantization/solve_with_manual_onnx_quant.py, was created. 

This script:
Uses a stable, pre-compiled version of PyTorch.
Takes a standard floating-point model.
Intercepts the aten::linear operator during ONNX export.
Manually injects QuantizeLinear and DequantizeLinear nodes into the ONNX graph, effectively creating the desired dynamic quantization logic at the ONNX level without relying on the broken PyTorch quantization backend.",2025-09-17 18:35:12+00:00,2025-09-18T01:42:02Z,,False,1,0,1,74,0,1,1,2025-09-18 01:42:02+00:00,70,773,False,True,False,False,False,False,1,0,0,74,74,0,1,1,,,,pytorch
163174,open,[testing] Add test owner labels for some distributed tests,clee2000,"I am trying to give some test files better owner labels than `module: unknown`.  I am not sure them, but they seem pretty reasonable


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 18:30:39+00:00,2025-09-21T05:46:02Z,,False,4,0,1,7,7,7,4,,58,236,False,False,False,False,False,False,7,3,1285,14,7,7,1,1,2.0,3.0,2025-09-21T02:56:51Z,pytorch
163173,closed,[pcache] Generalize testing + All Caches thread-safe,nmacchioni,"Summary:
1. Generalized testing by auto-detecting Cache types and splitting testing by abstract base class
- Now checks that all Cache types are thread-safe
- Will fail tests if any new Cache is added and is untested (for example, any cache with non-str key or non-bytes value)
2. All Caches are thread-safe
- InMemoryCache was the only one not thread-safe, so added a lock for access
- Realized that to implement MultiCache we should just have this requirement.
* Also, OnDiskCache is now a functioning AsyncCache with a default base_dir using Python's tempfile.gettempdir, i.e. OnDiskCache is no longer an abstract cache class

Test Plan:
```
[nmacchioni@*** / ()]$ buck test fbcode//mode/opt caffe2/test/inductor:pcache
Tests finished: Pass 28. Fail 0. Fatal 0. Skip 0. Build failure 0
[nmacchioni@*** / ()|remote/fbcode/warm_gpu_od_stable...)]$
```

Rollback Plan:

Differential Revision: D82660240




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 18:29:30+00:00,2025-09-18T00:44:51Z,,False,6,2,1,240,225,2,8,2025-09-18 00:32:49+00:00,52,1108,False,False,False,False,False,False,2,2,785,465,240,225,1,1,3.0,2.0,2025-09-17T21:44:35Z,pytorch
163172,closed,Just Sample upload and update,Vinayak-Pawar,"Fixes #ISSUE_NUMBER
",2025-09-17 17:29:36+00:00,2025-09-17T17:30:38Z,,False,2,0,13,403,1,10,2,2025-09-17 17:30:38+00:00,29,20,False,True,False,False,False,False,10,1,23,591,516,75,2,13,1.0,1.0,2025-09-17T17:30:14Z,pytorch
163171,open,[cuDNN][conv][64-bit] Disable cuDNN for 64-bit depthwise convs again,eqy,"test is breaking, will check if there's an older version that we can enable on to avoid completely dropping support

cc @csarofeen @ptrblck @xwang233 @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-17 17:22:10+00:00,2025-09-25T00:36:01Z,,False,4,3,6,19,12,2,7,,68,228,False,False,False,False,False,False,2,3,498,131,69,62,2,6,3.0,3.0,2025-09-17T17:30:34Z,pytorch
163169,closed,[dynamo][ez] Initialize tracer_output to None by default.,zhxchen17,"Summary:
In edge cases, tracer_output can be left unset if there's double exception raised which causes the following issue:
```
UnboundLocalError: local variable 'tracer_output' referenced before assignment
```

Default initialize this variable so that it's always present.

Test Plan:
CI

Rollback Plan:

Differential Revision: D82652815




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-17 17:10:44+00:00,2025-09-18T01:31:29Z,,False,4,0,1,1,0,1,4,2025-09-18 01:30:26+00:00,57,514,False,False,False,False,False,False,1,2,493,1,1,0,1,1,3.0,2.0,2025-09-17T17:15:07Z,pytorch
163168,closed,dynamo: Handle objects in graph that do not support weakref,c00w,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163168

We are seeing crashes of the form
```
Traceback (most recent call last):
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/symbolic_convert.py"", line 1487, in run
    while self.step():
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/symbolic_convert.py"", line 1348, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/symbolic_convert.py"", line 2437, in LOAD_ATTR
    self._load_attr(inst)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/symbolic_convert.py"", line 2425, in _load_attr
    result = BuiltinVariable(getattr).call_function(
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/builtin.py"", line 1347, in call_function
    return handler(tx, args, kwargs)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/builtin.py"", line 967, in <lambda>
    tx, [v.realize() for v in args], kwargs
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/builtin.py"", line 967, in <listcomp>
    tx, [v.realize() for v in args], kwargs
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/lazy.py"", line 72, in realize
    self._cache.realize()
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/lazy.py"", line 33, in realize
    self.vt = builder.VariableBuilder(tx, self.source)(self.value)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/builder.py"", line 445, in __call__
    vt = self._wrap(value)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/variables/builder.py"", line 1043, in _wrap
    torch._dynamo.utils.store_user_object_weakref(value)
  File ""/packages/aps_ads_vm/launcher_multiapp-inplace#link-tree/torch/_dynamo/utils.py"", line 4694, in store_user_object_weakref
    user_obj_id_to_weakref[obj_id] = weakref.ref(obj)
torch._dynamo.exc.InternalTorchDynamoError: TypeError: cannot create weak reference to 'torch.Event' object
```

This pull request makes us gracefully graph break, vs explicitly crashing.

I've added a test which reproduces the issue. There is a side discussion re:
how did torch.Event support ever work here, since it appears you cannot take a
weakref to a torch.Event

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-17 17:01:02+00:00,2025-09-22T22:12:15Z,,False,4,0,1,41,1,3,4,2025-09-22 22:11:11+00:00,59,2699,False,False,False,False,False,False,3,3,656,42,41,1,1,1,4.0,4.0,2025-09-17T20:19:23Z,pytorch
163167,closed,Add fake_impl for _native_multi_head_attention,ydwu4,"Test Plan:
See added test in test_export.py

Rollback Plan:

Reviewed By: henryoier

Differential Revision: D77747446

Re-landing through https://github.com/pytorch/pytorch/pull/163700


",2025-09-17 16:57:25+00:00,2025-09-23T22:20:36Z,,False,18,0,1,137,0,2,18,2025-09-23 22:20:36+00:00,46,187,False,False,False,False,False,False,2,10,3525,137,137,0,1,1,5.0,10.0,2025-09-17T22:18:09Z,pytorch
163166,closed,compile_kernel: Add DLPack test,msaroufim,"Note to self: i should probably. start using gh stack

This is rebased on top of https://github.com/pytorch/pytorch/pull/163165 so you only need to review this commit https://github.com/pytorch/pytorch/pull/163166/commits/7387c1becff11aaeaa1e53a958a0cb67e0f76431

This test doesn't add any new functionality it just ensures DLPack conversion is working well",2025-09-17 16:41:01+00:00,2025-09-17T22:56:52Z,,False,3,0,3,36,0,1,3,2025-09-17 22:55:51+00:00,31,357,False,False,False,False,False,False,1,2,499,52,44,8,1,3,4.0,4.0,2025-09-17T16:54:07Z,pytorch
163165,closed,compile_kernel remove header_code arg,msaroufim,"We previously asked users to seperate these because we didn't have any way of adding extern C declarations. Now we don't and we don't need this confusing flag anymore

BC breaking but is fine for this API since it doesn't have major users yet. Please just put your all your code in `kernel_source` moving forward 

## BC note
The header_code parameter has been removed from torch.cuda._compile_kernel. Previously, users could pass separate header code that would be prepended to the kernel source. Now, header code must be included directly in the kernel_source parameter.

Note this only affects torch.cuda._compile_kernel, which is a private API.

Example:

Before
```python
kernel = compile_kernel(
    kernel_source=""global void my_kernel() { ... }"",
    kernel_name=""my_kernel"",
    header_code=""#define SCALE 2.0f\n__device_ float scale(float x) { return x * SCALE; }""
  )
```

After
```python
kernel_source = """"""
#define SCALE 2.0f
device float scale(float x) { return x * SCALE; }

global void my_kernel() { ... }
""""""
kernel = _compile_kernel(kernel_source, ""my_kernel"")
```",2025-09-17 16:11:26+00:00,2025-09-17T19:48:38Z,,False,3,0,1,3,18,3,3,2025-09-17 19:47:35+00:00,37,1082,False,False,False,False,False,False,3,2,635,21,3,18,1,1,4.0,4.0,2025-09-17T16:51:23Z,pytorch
163164,closed,[PyTorch] Add SVE128 ISA (#158932),Nicoshev,"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/158932

Importing https://github.com/pytorch/pytorch/pull/138388, as it improves SVE support for perfkernels

Test Plan: We will test it on AdFinder/AdRetriever/AdRanker offline tier

Reviewed By: r1mikey

Differential Revision: D70788867

Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 15:44:59+00:00,2025-09-17T15:46:00Z,2025-09-17T15:45:38Z,True,1,0,2,1,1,1,1,2025-09-17 15:45:39+00:00,34,591,False,True,False,False,True,False,1,0,0,691,519,172,1,1,,,,pytorch
163163,closed,add more restriction to fusion with large accumulate reads,xuanzhang816,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163163



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 15:40:14+00:00,2025-09-19T01:21:39Z,,False,4,0,1,19,10,3,4,2025-09-19 01:20:33+00:00,58,297,False,False,False,False,False,False,3,3,535,29,19,10,1,1,3.0,3.0,2025-09-17T16:03:56Z,pytorch
163162,closed,[PyTorch] Add SVE128 ISA (#158932),Nicoshev,"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/158932

Importing https://github.com/pytorch/pytorch/pull/138388, as it improves SVE support for perfkernels

Reviewed By: r1mikey

Differential Revision: D70788867

Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 15:30:58+00:00,2025-09-17T15:31:07Z,2025-09-17T15:31:07Z,True,1,0,1,519,172,34,1,2025-09-17 15:31:07+00:00,34,517,False,True,False,False,True,False,34,0,0,691,519,172,1,1,,,,pytorch
163161,open,Add vmap test for assert_tensor_metadata,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163161

Differential Revision: [D82645343](https://our.internmc.facebook.com/intern/diff/D82645343)",2025-09-17 15:24:36+00:00,2025-09-20T17:43:00Z,,False,2,1,1,14,0,1,3,,40,185,False,False,False,False,False,False,1,1,160,14,14,0,1,1,2.0,1.0,2025-09-17T15:25:50Z,pytorch
163160,open,[ncclx][c10d] check if process group nccl is using nccl,riklas,"Test Plan:
OSS CI

Rollback Plan:

Differential Revision: D82639915




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 15:15:07+00:00,2025-09-24T01:51:28Z,,False,6,11,1,24,12,2,17,,55,173,False,False,False,False,False,False,2,3,798,36,24,12,1,1,3.0,3.0,2025-09-17T17:53:45Z,pytorch
163158,open,IGNORE: Test multicloud-arc-runner,zxiiro,"Fixes #ISSUE_NUMBER
",2025-09-17 14:15:01+00:00,2025-09-17T16:21:26Z,,False,1,0,1,293,292,1,1,,34,20,False,True,False,False,False,False,1,0,0,585,293,292,1,1,,,,pytorch
163155,closed,very small typo in fsdp2 comment,zhc7,cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci,2025-09-17 11:23:33+00:00,2025-09-17T20:20:46Z,,False,3,0,1,1,1,1,3,2025-09-17 20:19:44+00:00,32,101,False,False,False,False,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-17T13:52:04Z,pytorch
163154,open,Fixes floating point exception in torch.nn.PixelShuffle,arkadip-maitra,"Fixes #162251

**Previous Output:**
`Floating point exception (core dumped)`

**Now Output:**
`RuntimeError: upscale factor is too large, (upscale_factor}^2 overflowed: upscale_factor=545460846592`",2025-09-17 10:09:24+00:00,2025-09-21T09:46:11Z,,False,2,1,3,17,2,3,3,,55,197,False,True,False,False,False,False,3,1,64,33,24,9,1,3,2.0,1.0,2025-09-20T17:43:58Z,pytorch
163152,closed,[SymmMem] Fix NVSHMEM plugin + Triton 3.5,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #163194
* __->__ #163152

1. The dispatch signatures defined in `core.extern_elementwise` call must match the C signature of the NVSHMEM functions, in particular the dtypes. Otherwise, there would be weird errors, such as IMA or hang. When matched, most of time the NVSHMEM device function will be inlined into the generated PTX. When not matched, it is represented as a function call in the PTX (not sure if it is the function call that goes wrong).

2. When calling the `core.extern` wrappers from the `triton.jit` kernels, the input must be cast to match the signatures defined in 1, e.g. via `nbytes.to(tl.int64)`. Otherwise, Triton will report a key error when searching for such kernel.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 08:49:01+00:00,2025-09-18T15:50:00Z,,False,6,5,2,105,49,2,11,2025-09-18 00:50:25+00:00,41,885,False,True,False,False,False,False,2,5,1237,166,111,55,1,2,5.0,6.0,2025-09-17T16:02:44Z,pytorch
163151,open,remove unnecessary sync point in AveragedModel update,gl3lan,"Summary:
The test `bool(self.n_averaged == 0)` is a CPU/GPU synchronization point that is called for each update.
This test is only meant to know whether the AveragedModel copy has been initialized or not.
This diff introduces a CPU-based variable for that purpose.
When loading from checkpoint we also make sure the parameter is refreshed.

After this fix, each `update_parameter` call is reduced to 6ms from 333ms (98% reduction).

Test Plan:
Test plan from GitHub:
contbuild & OSS CI
Test plan from GitHub:
CI

Rollback Plan:

Differential Revision: D82621539


",2025-09-17 08:30:38+00:00,2025-09-17T15:13:39Z,,False,8,0,1,50,4,2,8,,53,565,False,True,False,False,False,False,2,3,1590,54,50,4,1,1,2.0,4.0,2025-09-17T14:53:47Z,pytorch
163150,open,fix test_type_hints,parsshar-RH,"Fixes #163149

### Summary:
Fixes mypy type checking failures in `test_type_hints` by consolidating typing imports and eliminating duplicate/conflicting import patterns that caused mypy to fail resolving type annotations.

### Impact:

- `test_type_hints` works fine now
- module: tests

cc @mruberry",2025-09-17 07:22:44+00:00,2025-09-21T17:55:14Z,,False,3,4,2,9,2,3,7,,19,300,False,True,False,False,False,False,3,1,33,15,11,4,1,2,2.0,1.0,2025-09-17T07:23:43Z,pytorch
163148,closed,[unit test] correct wrong input shape in test_flop_fx,thenumberouscode,"The input tensor shape does not match the weight tensor shape, which was detected by the validation logic implemented in my other PR(https://github.com/pytorch/pytorch/pull/160408).  The input tensor should have a shape of (2, 2, 3), since dimension 1 of the input (representing input channels) must match dimension 0 of the weight tensor (representing input channels). ref https://docs.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose1d.html


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @eellison ",2025-09-17 06:52:05+00:00,2025-09-18T18:39:08Z,,False,3,0,1,1,1,1,3,2025-09-18 18:38:04+00:00,53,665,False,False,False,True,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-18T15:55:37Z,pytorch
163147,closed,[Inductor][Triton][FP8] Add a Blackwell-specific scaled persistent + TMA template for GEMMs,jananisriram,"Summary:
X-link: https://github.com/meta-pytorch/tritonbench/pull/432

Add a Blackwell-specific scaled persistent + TMA Triton template to Inductor. This diff builds on D82515450 by adding a new set of mixins which inherit the scaling epilogue and add scaled persistent + TMA kwargs to the template.

This diff also adds a benchmark for the scaled Blackwell persistent + TMA template to TritonBench `fp8_gemm`.

Note that this diff is a minimal extension to the above diff; rather than adding a new kernel for the scaled version, we opted to simply extend the epilogue to account for scaling. This template is accurate for per-tensor and per-row scaling but may require modifications for other scaling modes, such as deepseek-style scaling, which apply scaling prior to the GEMM computation.

In addition, note that epilogue subtiling is currently unsupported for both the scaled and non-scaled Blackwell templates, and functionality will be added in a subsequent diff.

Test Plan:
Verified that the scaled Blackwell template adds the scaling epilogue to the generated Triton kernel by inspecting the Inductor-generated Triton kernel.

Benchmarking command:
```
TRITON_PRINT_AUTOTUNING=1 TORCHINDUCTOR_CACHE_DIR=~/personal/cache_dir_inductor TRITON_CACHE_DIR=~/personal/cache_dir_triton TRITON_ALWAYS_COMPILE=1 TORCH_LOGS=+inductor TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 ENABLE_PERSISTENT_TMA_MATMUL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 buck2 run mode/{opt,inplace} pytorch/tritonbench:run -c fbcode.nvcc_arch=b200a -c fbcode.enable_gpu_sections=true -c fbcode.platform010_cuda_version=12.8 -- --op fp8_gemm --only torch_fp8_gemm,blackwell_pt2_fp8_gemm --metrics tflops,accuracy --input-loader=/home/jananisriram/personal/fp8_shapes_testing.json --scaling_rowwise --output=""/home/jananisriram/personal/fp8_shapes_testing_results.csv"" --atol=1e-2 --rtol=0.5 2>&1 | tee ~/personal/fp8_shapes_testing.log
```

Rollback Plan:

Differential Revision: D82597111




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 06:19:17+00:00,2025-09-19T17:24:44Z,,False,7,3,1,176,0,3,10,2025-09-19 17:23:41+00:00,91,2158,False,False,False,False,False,False,3,2,506,176,176,0,1,1,3.0,3.0,2025-09-18T23:35:54Z,pytorch
163145,closed,[Triton] [Inductor] Enable Epilogue Subtiling in the blackwell ws template,njriasan,"Summary: Enables support for epilogue subtiling in the blackwell ws template. This requires the ability to call `store_output` twice in the same kernel and reuse the same tensor descriptor across allocations.

Test Plan:
Tested with test_max_autotune.py on a Blackwell server.

Rollback Plan:

Differential Revision: D82610077




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-17 05:38:30+00:00,2025-09-24T05:39:07Z,,False,27,11,5,131,57,7,38,2025-09-24 05:38:05+00:00,74,540,False,False,False,False,False,False,7,9,6213,358,216,142,1,5,4.0,10.0,2025-09-17T05:58:02Z,pytorch
163144,open,[Inductor] optimize the heuristics of sum reduction,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163144


Fix https://github.com/pytorch/pytorch/issues/151400.
**Summary:**
Optimize the heuristics of sum reduction, reduce the chunk size of cascade sum to improve numerical stability.

**Example:**
Take https://github.com/pytorch/pytorch/issues/151400 as an example:
```
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch._inductor import config

config.fallback_random = True
torch.set_grad_enabled(False)
torch.manual_seed(0)


class Model(torch.nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x):
        vec = x.flatten()
        vec_one = torch.ones_like(vec)
        x = torch.outer(vec, vec_one)
        return torch.mean(x, dim=1)


model = Model()

x = torch.randn(3, 8, 64, 64)  # error will be amplified as the input tensor gets larger

inputs = [x]


def run_test(model, inputs, backend):
    if backend != ""eager"":
        model = torch.compile(model, backend=backend)
    torch.manual_seed(0)
    output = model(*inputs)
    return output


output = run_test(model, inputs, 'eager')
c_output = run_test(model, inputs, 'inductor')
fp64 = run_test(model.to(dtype=torch.float64), [inputs[0].to(dtype=torch.float64)], 'eager')

print(torch.allclose(output, c_output, rtol=1e-3, atol=1e-3))
print(torch.max(torch.abs(c_output - output)))
print(torch._dynamo.utils.same(output, c_output, fp64))

```

**logs:**
- Before
```
False
tensor(0.0052)
False
```

- After
```
True
tensor(0.0004)
True
```

- 
**Generated code:**
- Before
```
cpp_fused_mean_mul_ones_like_view_0 = async_compile.cpp_pybinding(['float*', 'const float*'], '''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       const float* in_ptr0)
{
    auto out_ptr0 = in_out_ptr0;
    #pragma omp parallel num_threads(240)
    {
        int tid = omp_get_thread_num();
        {
            #pragma omp for
            for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(98304L); x0+=static_cast<int64_t>(16L))
            {
                {
                    float tmp_acc0 = 0;
                    at::vec::Vectorized<float> tmp_acc0_vec = at::vec::Vectorized<float>(0);
                    for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(98304L); x1+=static_cast<int64_t>(1L))
                    {
                        {
                            if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                            {
                                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                                auto tmp1 = static_cast<float>(1.0);
                                auto tmp2 = at::vec::Vectorized<float>(tmp1);
                                auto tmp3 = tmp0 * tmp2;
                                tmp_acc0_vec = tmp_acc0_vec + tmp3;
                            }
                        }
                    }
                    if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                    {
                        tmp_acc0_vec.store(out_ptr0 + static_cast<int64_t>(x0));
                    }
                }
                {
                    if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                    {
                        auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                        auto tmp1 = static_cast<float>(98304.0);
                        auto tmp2 = at::vec::Vectorized<float>(tmp1);
                        auto tmp3 = tmp0 / tmp2;
                        tmp3.store(in_out_ptr0 + static_cast<int64_t>(x0));
                    }
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, = args
        args.clear()
        assert_size_stride(arg0_1, (3, 8, 64, 64), (32768, 4096, 64, 1))
        buf0 = empty_strided_cpu((98304, ), (1, ), torch.float32)
        buf1 = buf0; del buf0  # reuse
        # [Provenance debug handles] cpp_fused_mean_mul_ones_like_view_0:1
        cpp_fused_mean_mul_ones_like_view_0(buf1, arg0_1)
        del arg0_1
        return (buf1, )
```

- After
```
cpp_fused_mean_mul_ones_like_view_0 = async_compile.cpp_pybinding(['float*', 'const float*'], '''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       const float* in_ptr0)
{
    auto out_ptr0 = in_out_ptr0;
    #pragma omp parallel num_threads(240)
    {
        int tid = omp_get_thread_num();
        {
            #pragma omp for
            for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(98304L); x0+=static_cast<int64_t>(16L))
            {
                {
                    float tmp_acc0 = 0;
                    at::vec::Vectorized<float> tmp_acc0_vec = at::vec::Vectorized<float>(0);
                    at::vec::Vectorized<float> masked_tmp_acc0_vec = at::vec::Vectorized<float>(0);
                    CascadeSumHelper<float, 4096> scalar_cascade_helper0(static_cast<int64_t>(98304L));
                    CascadeSumHelper<at::vec::Vectorized<float>, 4096> cascade_helper0(static_cast<int64_t>(98304L));
                    CascadeSumHelper<at::vec::Vectorized<float>, 4096> masked_cascade_helper0(static_cast<int64_t>(0L));
                    for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(98304L); x1+=static_cast<int64_t>(1L))
                    {
                        {
                            if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                            {
                                auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                                auto tmp1 = static_cast<float>(1.0);
                                auto tmp2 = at::vec::Vectorized<float>(tmp1);
                                auto tmp3 = tmp0 * tmp2;
                                tmp_acc0_vec = cascade_sum_combine(tmp3, &cascade_helper0);
                            }
                        }
                    }
                    tmp_acc0 = cascade_sum_final(&scalar_cascade_helper0);
                    tmp_acc0_vec = cascade_sum_final(&cascade_helper0);
                    masked_tmp_acc0_vec = cascade_sum_final(&masked_cascade_helper0);
                    if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                    {
                        tmp_acc0_vec = tmp_acc0_vec + masked_tmp_acc0_vec;
                        tmp_acc0_vec.store(out_ptr0 + static_cast<int64_t>(x0));
                    }
                }
                {
                    if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(98304L)))
                    {
                        auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr0 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                        auto tmp1 = static_cast<float>(98304.0);
                        auto tmp2 = at::vec::Vectorized<float>(tmp1);
                        auto tmp3 = tmp0 / tmp2;
                        tmp3.store(in_out_ptr0 + static_cast<int64_t>(x0));
                    }
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, = args
        args.clear()
        assert_size_stride(arg0_1, (3, 8, 64, 64), (32768, 4096, 64, 1))
        buf0 = empty_strided_cpu((98304, ), (1, ), torch.float32)
        buf1 = buf0; del buf0  # reuse
        # [Provenance debug handles] cpp_fused_mean_mul_ones_like_view_0:1
        cpp_fused_mean_mul_ones_like_view_0(buf1, arg0_1)
        del arg0_1
        return (buf1, )
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-17 05:37:02+00:00,2025-09-19T01:52:00Z,,False,1,0,3,18,12,3,1,,51,8792,False,True,False,False,True,False,3,0,0,40,23,17,1,3,,,,pytorch
163141,open,Add a new API torch.xpu.is_tf32_supported for Intel GPU,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163141

# Motivation
Aligned with other backends, this PR introduces a new API `torch.xpu.is_tf32_supported`, which should be used before `torch.backends.mkldnn.allow_tf32=True` or provide hardware capability information to the Triton

# Additional Context
Refer to https://github.com/intel/llvm/blob/sycl/sycl/doc/extensions/experimental/sycl_ext_matrix/sycl_ext_oneapi_matrix.asciidoc#runtime-query, SYCL provides a runtime query interface to check if the device supports tf32 matmul acceleration on hardware.",2025-09-17 03:41:25+00:00,2025-09-19T16:52:10Z,,False,1,0,5,33,0,7,1,,55,597,False,False,False,True,False,False,7,0,0,6035,4756,1279,1,5,1.0,0.0,2025-09-19T03:29:20Z,pytorch
163140,open,[ROCm][CI] Upgrade ROCm to 7.0.1,pruthvistony,"Upgrade all the ROCm docker image to ROCm 7.0.1 release version.

cc @jeffdaily @sunway513 @jithunnair-amd @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-17 03:23:56+00:00,2025-09-25T00:36:59Z,,False,8,0,3,10,23,4,8,,32,168,False,False,False,True,False,False,4,6,900,33,10,23,1,3,2.0,6.0,2025-09-18T15:01:57Z,pytorch
163139,open,[PT2][AutoAC] force save custom triton kernels,xuanzhang816,"Below are trace from before and after the changes. The highlighted region are `_attn_fwd`. Before the change, these kernels are recomputed in the backward; and after the change, they are no longer recomputed.

**before**
<img width=""2218"" height=""370"" alt=""image"" src=""https://github.com/user-attachments/assets/67710ba7-13c9-4e9c-806d-301d72b1c784"" />

**after**
<img width=""2210"" height=""388"" alt=""image"" src=""https://github.com/user-attachments/assets/d9294c1a-f0f0-4529-bae7-44d3cc421625"" />


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163139

",2025-09-17 03:08:30+00:00,2025-09-17T05:45:59Z,,False,2,0,1,14,0,2,2,,46,592,False,False,False,False,False,False,2,1,44,14,14,0,1,1,1.0,1.0,2025-09-17T03:11:11Z,pytorch
163138,closed,Don't register wrong overload to prim decomp,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163138

These decompositions take precedence before CIA decomps in fake tensor prop, as a result, we would hit this implementation for all where overloads which is wrong in some cases. For the overloads that can't be implemented by this decomp, we just run the default CIA impl. Previously this doesn't matter because in post-dispatch IR, aten.where would have decomposed but when user tries to preserve aten.where this issue will surface because fake tensor will start seeing aten.where. 

Differential Revision: [D82604702](https://our.internmc.facebook.com/intern/diff/D82604702)",2025-09-17 03:06:29+00:00,2025-09-18T17:02:28Z,,False,7,0,3,41,4,3,7,2025-09-18 17:01:22+00:00,44,668,False,False,False,False,False,False,3,6,1318,45,41,4,1,3,4.0,7.0,2025-09-17T03:07:44Z,pytorch
163137,open,Move inductor.aot_compile to use new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* #163260
* #163259
* #163258
* __->__ #163137
* #163136
* #163107

Differential Revision: [D82603768](https://our.internmc.facebook.com/intern/diff/D82603768)

I feel no one probably uses this API now but still useful path for more test cases. 
cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-17 02:46:44+00:00,2025-09-23T20:46:38Z,,False,8,1,5,19,17,2,9,,43,502,False,False,False,False,False,False,2,7,914,23179,16604,6575,1,5,3.0,7.0,2025-09-17T02:49:11Z,pytorch
163136,open,Fix bug with renaming submodules in dynamo for new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* #163260
* #163259
* #163258
* #163137
* __->__ #163136
* #163107

Differential Revision: [D82603767](https://our.internmc.facebook.com/intern/diff/D82603767)

Previously, i forgot to add handle call_module case which now will have export_root prepended to their names. Basically i want to clean up sth like:
```
graph():
      %l_self_export_root_sub_mod = call_module[target=l_self_export_root_sub_mod](%x, %y)
      %l_self_export_root_sub_mod_1 = call_module[target=l_self_export_root_sub_mod](%x, %y)
  ```
      

Dynamo graph can have call_module nodes that have messed up name due to our wrapper. 
cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-17 02:46:38+00:00,2025-09-23T20:45:51Z,,False,7,4,5,31,5,2,11,,57,863,False,True,False,False,False,False,2,5,915,23157,16598,6559,1,5,2.0,6.0,2025-09-17T02:49:06Z,pytorch
163135,closed,Aoti windows mingw,yushangdi,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-17 02:44:08+00:00,2025-09-18T00:50:07Z,,False,1,0,2,630,196,16,1,2025-09-18 00:50:07+00:00,18,223,False,True,False,False,False,False,16,0,0,840,637,203,1,2,,,,pytorch
163134,closed,Remove data_source argument from Sampler,cyyever,`data_source` is declared being removed in PT 2.2 but not.,2025-09-17 01:47:56+00:00,2025-09-21T05:45:48Z,,False,4,0,1,2,17,2,4,2025-09-21 05:44:44+00:00,40,58,False,False,False,False,False,False,2,3,635,19,2,17,1,1,2.0,3.0,2025-09-21T03:02:22Z,pytorch
163133,closed,[mypy] add some import ignores to onnx,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163133
* #162702
* #161458

these keep appearing when I run `lintrunner`",2025-09-17 01:23:15+00:00,2025-09-17T09:33:49Z,,False,3,0,1,2,2,2,3,2025-09-17 09:32:43+00:00,38,158,False,False,False,False,False,False,2,2,493,4,2,2,1,1,3.0,2.0,2025-09-17T02:00:08Z,pytorch
163131,closed,[CP][BE] Correct an incorrect docstring,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* #163231
* __->__ #163131
* #163115
* #162541
* #162540
* #162539



cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 01:01:46+00:00,2025-09-19T23:56:10Z,,False,6,0,2,1,1,1,6,2025-09-19 23:55:06+00:00,39,260,False,False,False,True,False,False,1,5,1364,5291,4122,1169,1,2,4.0,5.0,2025-09-17T01:04:14Z,pytorch
163130,closed,[FSDP2] idempotent reset_sharded_param: no-op if _local_tensor is already padded,weifengpy,"resolves https://github.com/pytorch/torchtitan/issues/1136

torchtitan use cached state dict for ft. reset_sharded_param should be idempotent if model.parameters() are padded already

```
# pad DTensor._local_tensor
fully_shard(model)
sd = fsdp_model.state_dict()
# reset_sharded_param should be a no-op in lazy_init
loss = fsdp_model(inp).sum()
```

this PR make `reset_sharded_param` idempotent by checking storage data ptr and return early

unit test
```
pytest -s test/distributed/_composable/fsdp/test_fully_shard_state_dict.py -k test_cached_state_dict
```
Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163130



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-17 01:01:31+00:00,2025-09-18T21:12:35Z,,False,15,2,7,60,2,2,17,2025-09-18 09:20:40+00:00,80,760,False,False,False,False,False,False,2,14,3546,108,83,25,1,7,6.0,15.0,2025-09-17T01:07:13Z,pytorch
163128,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-17 00:26:09+00:00,2025-09-17T04:27:13Z,,False,3,0,1,1,1,1,3,2025-09-17 04:26:10+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-17T00:26:10Z,pytorch
163127,closed,[ROCm] Fix mx fp8 and fp4 code after scaling refactor changes.,jagadish-amd,"PR #151360 added mx fp8 and fp4 support on ROCm.
1. However, on recent upstream, scaling function in Blas.cpp along with test_matmul_cuda changes triggered failures.
This patch corrects is_blockwise_1x32_scaling function code.

2. Fixes the m, n, k dimensions for ROCm mx case.

3.  Modify FP4E2M1FN_LARGEST_POW2 (largest power of 2 representable in `torch.float4_e2m1fn_x2`) to 2.
This resulted in higher SQNR value for mx fp4 test.

Testing result on gfx950 w/ ROCm7.0

PYTORCH_TEST_WITH_ROCM=1 python test/test_matmul_cuda.py -k test_blockwise -v Ran 452 tests in 22.698s
OK passed 111
This is same as before. (when PR 151360 was merged)


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-16 23:58:55+00:00,2025-09-19T12:31:00Z,,False,9,1,3,33,15,3,10,2025-09-19 12:29:56+00:00,62,759,False,True,False,False,False,True,3,8,2267,62,40,22,2,3,2.0,8.0,2025-09-17T01:27:01Z,pytorch
163126,closed,[dtensor][compile] Disable proxy mode in sharding prop rules,anijain2305,cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci,2025-09-16 23:34:47+00:00,2025-09-17T16:50:42Z,,False,5,0,1,14,5,2,5,2025-09-17 16:49:39+00:00,60,101,False,False,False,False,False,False,2,2,493,19,14,5,1,1,3.0,2.0,2025-09-17T01:02:47Z,pytorch
163125,closed,Use computed buffer sizes of torch for cusparseLt metadata,aartbik,"Making sure buffer allocation matches what is computed by cusparseLt compression
",2025-09-16 23:28:39+00:00,2025-09-19T22:14:47Z,,False,13,0,5,20,17,4,13,2025-09-19 22:12:43+00:00,58,81,False,False,False,False,False,False,4,12,6762,61,32,29,1,5,3.0,13.0,2025-09-17T15:36:23Z,pytorch
163124,open,"More custom sympy expressions use fixes: FlooDiv, Mod, Min, Max.",laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 23:22:16+00:00,2025-09-17T01:54:19Z,,False,2,0,1,38,20,1,2,,64,297,False,True,False,False,False,False,1,0,0,58,38,20,1,1,,,,pytorch
163123,closed,Turn on capture_dynamic_output_shape_ops when fullgraph=True,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163246
* __->__ #163123
* #163121


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 23:12:16+00:00,2025-09-18T21:25:25Z,,False,5,1,9,17,24,4,6,2025-09-18 21:24:20+00:00,60,285,False,False,False,False,False,False,4,4,800,136782,90961,45821,1,9,3.0,5.0,2025-09-18T04:11:47Z,pytorch
163122,closed,[optim] override SWALR.state_dict and load_state_dict,filipviz,"Fixes #163105

Note that the new `SWALR.load_state_dict` is **not backwards compatible**:
```python
@override
def load_state_dict(self, state_dict: dict[str, Any]) -> None:
  """"""Load the scheduler's state.

  Args:
      state_dict (dict): scheduler state. Should be an object returned
          from a call to :meth:`state_dict`.
  """"""
  self.__dict__.update(state_dict)
  self._set_anneal_func(self._anneal_strategy)
```

If we'd like to maintain compatibility with old state_dicts (loaded with `weights_only=False`), we could use something along these lines:
```python
@override
def load_state_dict(self, state_dict: dict[str, Any]) -> None:
    """"""Load the scheduler's state.

    Args:
        state_dict (dict): scheduler state. Should be an object returned
            from a call to :meth:`state_dict`.
    """"""
    anneal_func = state_dict.pop(""anneal_func"", None)
    strategy = state_dict.get(""_anneal_strategy"")
    self.__dict__.update(state_dict)

    if anneal_func is not None:
        state_dict[""anneal_func""] = anneal_func
        if strategy is None:
            if anneal_func == self._linear_anneal:
                strategy = ""linear""
            elif anneal_func == self._cosine_anneal:
                strategy = ""cos""

    if strategy is None:
        strategy = getattr(self, ""_anneal_strategy"", ""cos"")

    self._set_anneal_func(strategy)
```

But given the fact that loading an `SWALR` state_dict before this PR would have caused an error, this seems okay. A GitHub/Google search for `SWALR.load_state_dict` had no results. Happy to change if not, or add a warning just in case.",2025-09-16 23:07:13+00:00,2025-09-17T18:18:32Z,,False,3,0,1,34,4,2,3,2025-09-17 18:17:30+00:00,53,1606,False,True,False,False,False,False,2,2,589,38,34,4,1,1,2.0,3.0,2025-09-17T15:30:41Z,pytorch
163121,closed,Turn on capture_scalar_outputs when fullgraph=True,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163246
* #163123
* __->__ #163121

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 23:07:03+00:00,2025-09-18T21:24:19Z,,False,3,1,7,17,26,6,4,2025-09-18 21:24:18+00:00,50,284,False,False,False,False,False,False,6,2,165,136735,90944,45791,1,7,3.0,2.0,2025-09-17T20:16:47Z,pytorch
163120,closed,[optim] prevent unintended aliasing in lr_scheduler; update type annotations/docs,filipviz,"1. Prevents unintended aliasing of `self._last_lr`/`get_last_lr(...)` with `group[""lr""]` when `group[""lr""]` is a tensor.
2. Prevents unintended aliasing of `LRScheduler.base_lrs` with the `group[""initial_lr""]`s.
3. Updates `test/optim/test_lrscheduler.py` to test tensor LRs.
4. Changes type annotations for `_last_lr`, `get_last_lr()`, `base_lrs`, `get_lr()`, and `_get_closed_form_lr()` from `list[float]` to `list[float | Tensor]`; adds documentation.

Fixes #163103

LR schedulers can behave in unexpected ways when using a tensor LR due to patterns like this:
```python
self._last_lr: list[float] = [group[""lr""] for group in self.optimizer.param_groups]
```

This PR adds a helper to address this:
```python
def _param_groups_val_list(optimizer: Optimizer, key: str) -> list[Any]:
    """"""Create a list containing group[key] for each optimizer param_group.
    Prevents aliasing when group[key] could be a Tensor.
    Raises a KeyError when group[key] does not exist.
    """"""
    return [
        group[key].clone() if isinstance(group[key], Tensor) else group[key]
        for group in optimizer.param_groups
    ]
```",2025-09-16 22:53:33+00:00,2025-09-25T07:00:05Z,,False,15,19,10,509,79,3,34,2025-09-25 06:59:02+00:00,81,1123,False,True,False,True,False,False,3,14,10119,3867,3012,855,2,10,4.0,16.0,2025-09-17T02:51:57Z,pytorch
163119,open,[CUDA] hack for host-memory stats test,eqy,"Basically this test appears to fail when run with other tests that manipulate pinned memory, even with the `@serialTest` decorator

An ugly workaround here is to ensure it runs first due to alphabetical ordering...

cc @ptrblck @msaroufim @jerryzh168",2025-09-16 22:48:06+00:00,2025-09-19T15:50:56Z,,False,4,0,1,2,1,1,4,,38,250,False,False,False,False,False,False,1,3,523,3,2,1,1,1,2.0,3.0,2025-09-18T14:26:56Z,pytorch
163118,closed,Update _common_operator_config_utils.py,jerryzh168,"Fixes #ISSUE_NUMBER
",2025-09-16 22:45:58+00:00,2025-09-25T03:25:18Z,,False,1,0,1,12,0,1,1,2025-09-25 03:25:18+00:00,39,20,False,True,False,False,False,False,1,0,0,12,12,0,1,1,,,,pytorch
163117,closed,[inductor][choices] pass through annotations from KTC to ChoiceCaller,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163117

# why

- KTC might regenerate a choicecaller e.g. through FlexibleLayout
  optimization. This in turn would delete any annotations

# what

- provide an annotations dict inside KTC
- forward that dict towards the ChoiceCaller's annotations

- ChoiceCaller users e.g. in selectalgorithm now have access to the KTC
  and can register handlers do record/make decisions based on the KTC

# testing

n/a

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D82587631](https://our.internmc.facebook.com/intern/diff/D82587631)",2025-09-16 22:39:45+00:00,2025-09-17T18:07:58Z,,False,4,0,1,3,0,1,4,2025-09-17 18:06:53+00:00,69,774,False,False,False,False,False,False,1,3,655,3,3,0,1,1,3.0,4.0,2025-09-16T22:50:36Z,pytorch
163116,closed,[CD] Do not enable GenAI on Windows,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163116

Follow up after https://github.com/pytorch/pytorch/pull/162209 as looks
like it causes some of the Windows builds to fail with
```
C:/actions-runner/_work/pytorch/pytorch/pytorch/third_party/fbgemm/fbgemm_gpu/experimental/gen_ai/src/quantize/common/include\fbgemm_gpu/quantize/utils.h(19): error C3861: '__builtin_clz': identifier not found
```

May be fixes https://github.com/pytorch/pytorch/issues/162881",2025-09-16 22:34:49+00:00,2025-09-17T14:10:18Z,,False,3,0,1,1,1,1,3,2025-09-17 14:09:14+00:00,35,501,False,True,False,False,False,False,1,2,800,2,1,1,1,1,4.0,3.0,2025-09-16T22:45:49Z,pytorch
163115,closed,[CP][BE] Cosmetic refactors for CP code base,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* #163231
* #163131
* __->__ #163115
* #162541
* #162540
* #162539

Summary:
This PR is extracted from https://github.com/pytorch/pytorch/pull/162542, to make the original PR
easier to review. This PR only contains cosmetic changes.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 22:14:39+00:00,2025-09-19T18:22:16Z,,False,6,0,2,101,104,1,6,2025-09-19 07:21:49+00:00,44,424,False,False,False,False,False,True,1,5,1893,5494,4222,1272,1,2,4.0,5.0,2025-09-16T22:49:16Z,pytorch
163114,open,[DRAFT] Shake up CI,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163114
* #163107
* #162993
* #162992
* #162682
* #162559
* #162558
* #162557

Differential Revision: [D82584056](https://our.internmc.facebook.com/intern/diff/D82584056)",2025-09-16 22:05:34+00:00,2025-09-17T05:46:39Z,,False,2,0,1,1,1,1,2,,19,255,False,False,False,False,False,False,1,1,160,2,1,1,1,1,1.0,1.0,2025-09-16T22:07:19Z,pytorch
163113,closed,"TESTESTTEST<Replace this line with a title. Use 1 line only, 67 chars or less>",c00w,"Test Plan:
test

Rollback Plan:
steps:
  - jk.update:
      jk: pytorch/compiler:enable_compiler_set_eval_frame
      constant_bool: null
      consistent_pass_rate: null
      fractional_host_rollout: null
      sampling_rate: null
  - manual.note:
      content: ''

Differential Revision: D82583797


",2025-09-16 22:04:53+00:00,2025-09-16T22:05:31Z,,False,2,0,1,2,0,1,2,2025-09-16 22:05:31+00:00,78,304,False,False,False,False,False,False,1,0,0,2,2,0,1,1,,,,pytorch
163112,closed,Update Gloo submodule,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163112

Which makes PyTorch buildable with gcc-15, tested by running the build inside `fedora:44` docker
```
docker run --rm -it fedora:44 bash -c ""yum install -y g++ python3-devel git; git clone https://github.com/pytorch/pytorch; cd pytorch; git checkout 8f710acce8332979c9a7bf97e72666dfd35c43e6; python3 -mpip install -r requirements.txt; python3 setup.py bdist_wheel""
```

Fixes https://github.com/pytorch/pytorch/issues/156595",2025-09-16 21:57:45+00:00,2025-09-17T03:05:16Z,,False,3,0,1,1,1,1,3,2025-09-17 03:04:12+00:00,21,517,False,True,False,True,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-17T01:11:14Z,pytorch
163111,closed,[CI] Update NVIDIA driver to `580.82.07`,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163111


To make CI machines capable of running CUDA-13 tests. Unfortunately, this upgrade regresses NUMBA integration, so live patch it with https://github.com/NVIDIA/numba-cuda/commit/6e08c9d08e9de59c7af28b720289debbbd384764

This fix was suggested in https://github.com/pytorch/pytorch/issues/162878#issuecomment-3288635745
",2025-09-16 21:49:22+00:00,2025-09-22T15:29:40Z,,False,11,1,7,36,1,3,12,2025-09-17 17:37:09+00:00,40,413,False,True,False,False,False,False,3,10,2828,91,63,28,1,7,6.0,11.0,2025-09-17T01:51:31Z,pytorch
163110,open,"[dynamo, 3.14] support LOAD_CONST on slice, codegen LOAD_CONST slice instead of BINARY/STORE_SLICE",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* __->__ #163110
* #163109
* #163009
* #161839
* #161555
* #161838



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 21:41:50+00:00,2025-09-25T00:04:08Z,,False,2,0,3,19,6,2,2,,98,356,False,False,False,False,False,False,2,0,0,13931,10399,3532,1,3,,,,pytorch
163109,open,"[dynamo, 3.14] fix context managers",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* #163110
* __->__ #163109
* #163009
* #161839
* #161555
* #161838

",2025-09-16 21:41:45+00:00,2025-09-25T00:04:07Z,,False,2,0,3,161,55,5,2,,35,184,False,True,False,False,False,False,5,0,0,14092,10515,3577,1,3,,,,pytorch
163107,closed,Update tests to check for more robust pattern,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163580
* #163260
* #163259
* #163258
* #163137
* #163136
* __->__ #163107

Landing this instead of https://github.com/pytorch/pytorch/pull/162994. 

Here is how i think the whole dynamo + frame construction logic work: 
1) There is no way to create a frame object in python land as this is created in runtime from cpython. So that's why aot_compile creates FrameInfo this way. (kind of like simulating the runtime) i guess you could write your own very simple eval_frame.c where you can interject the frame construction but we probably don't want that. 
2) When there is no wrapper (the old export or aot_compile), we first assign sources by iterating over f_locals which contain both local args and closure variables (this is implementation details of cpython frame construction). So thats why closure variables end up getting LocalSource names as can be shown in this test case (https://github.com/pytorch/pytorch/blob/f6ea41ead27205a5ece3e9a41b7af30fafe67d7a/test/export/test_export.py#L1369). Note that L[""self""] here means we are referring to local object self. Important thing to keep in mind here is this self is not actually model self, but the outer self.
3) When we switch to wrapper case, we end up trying to inline the original inner module. When doing so, we need to track all local and closures for this inner module as can be seen here (https://github.com/pytorch/pytorch/blob/f6ea41ead27205a5ece3e9a41b7af30fafe67d7a/torch/_dynamo/variables/functions.py#L463) Here we are not looking into inner frame's f_locals but just directly look at closures. I guess this is because we are one more frame up so there is no access to frame f_locals at this point. And it is probably not good idea to change dynamo's logic here. As a result, i get following error message that is different from old export:
""While exporting, we found certain side effects happened in the model.forward. Here are the list of potential sources you can double check: [""L['self']._export_root.forward.__func__.__closure__[1].cell_contents.bank"", ""L['self']._export_root.forward.__func__.__closure__[1].cell_contents.bank_dict"", ""L['self']._export_root.forward.__func__.__closure__[0].cell_contents""]""

My initial attempt of solving this was taking inner closures and put them to f_locals for the frame i am constructing which turned out too compilcated because we needed to muck around bytecode instructions as well. So i am thinking we should just update the test to reflect new names and follow up with better post-processing step to have better names.

Differential Revision: [D82582029](https://our.internmc.facebook.com/intern/diff/D82582029)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 21:33:58+00:00,2025-09-23T21:12:55Z,,False,12,3,6,26,12,2,15,2025-09-23 21:11:51+00:00,45,2880,False,False,False,False,False,False,2,11,2707,28157,20260,7897,1,6,3.0,11.0,2025-09-16T21:44:58Z,pytorch
163106,closed,Fix some edge cases,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162295


``` Summary
🔝 Top 5 Performance Differences (by absolute %):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 56.937931         ┆ 58.960459            ┆ 1.035522                  ┆ 3.552163  │
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 128) ┆ 89.221306         ┆ 86.295642            ┆ 0.967209                  ┆ -3.27911  │
│ causal         ┆ torch.bfloat16 ┆ (2, 16, 4096, 4, 4096, 128) ┆ 111.552594        ┆ 114.380841           ┆ 1.025353                  ┆ 2.535349  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16, 1024, 64) ┆ 74.830149         ┆ 76.685445            ┆ 1.024793                  ┆ 2.479344  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 55.279932         ┆ 56.369312            ┆ 1.019707                  ┆ 1.97066   │
└────────────────┴────────────────┴─────────────────────────────┴───────────────────┴──────────────────────┴───────────────────────────┴───────────┘

🔺 Top 5 Cases Where no_peel (change) is Faster than base (baseline):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 56.937931         ┆ 58.960459            ┆ 1.035522                  ┆ 3.552163  │
│ causal         ┆ torch.bfloat16 ┆ (2, 16, 4096, 4, 4096, 128) ┆ 111.552594        ┆ 114.380841           ┆ 1.025353                  ┆ 2.535349  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16, 1024, 64) ┆ 74.830149         ┆ 76.685445            ┆ 1.024793                  ┆ 2.479344  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 55.279932         ┆ 56.369312            ┆ 1.019707                  ┆ 1.97066   │
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 4096, 4, 4096, 64)  ┆ 111.08814         ┆ 112.447047           ┆ 1.012233                  ┆ 1.22327   │
└────────────────┴────────────────┴─────────────────────────────┴───────────────────┴──────────────────────┴───────────────────────────┴───────────┘

🔻 Top 5 Cases Where no_peel (change) is Slower than base (baseline):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 128) ┆ 89.221306         ┆ 86.295642            ┆ 0.967209                  ┆ -3.27911  │
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 1024, 4, 1024, 64)  ┆ 78.23082          ┆ 76.693169            ┆ 0.980345                  ┆ -1.965531 │
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 2048, 4, 2048, 128) ┆ 96.95663          ┆ 95.573333            ┆ 0.985733                  ┆ -1.426717 │
│ alibi          ┆ torch.bfloat16 ┆ (4, 16, 2048, 4, 2048, 64)  ┆ 93.373473         ┆ 92.294147            ┆ 0.988441                  ┆ -1.155924 │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 2048, 4, 2048, 128) ┆ 96.95147          ┆ 96.105389            ┆ 0.991273                  ┆ -0.872685 │
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos @Chillee @yanboliang @BoyuanFeng",2025-09-16 21:30:04+00:00,2025-09-25T14:29:40Z,2025-09-25T14:29:39Z,True,1,0,1,162,133,3,1,2025-09-25 14:29:39+00:00,19,5353,False,True,False,False,False,False,3,0,0,295,162,133,1,1,2.0,0.0,2025-09-16T23:56:46Z,pytorch
163104,closed,"[cuDNN][SDPA][submodule] Roll-back cuDNN frontend upgrade, update Meta registration",eqy,"For https://github.com/pytorch/torchtitan/issues/1713

Also note that we will need to rollback the cuDNN frontend upgrade in 2.9 as it currently introduces a segmentation fault by assuming tensors have their strides and sizes populated at graph creation time https://github.com/NVIDIA/cudnn-frontend/blame/1a7b4b78db44712fb9707d21cd2e3179f1fd88b8/include/cudnn_frontend/node/sdpa_support_surface.h#L447%C2%A0

cc @csarofeen @ptrblck @xwang233",2025-09-16 21:18:58+00:00,2025-09-17T15:50:00Z,,False,8,0,1,31,4,4,8,2025-09-17 15:48:58+00:00,83,442,False,False,False,False,False,False,4,6,867,35,31,4,1,1,3.0,6.0,2025-09-16T21:20:22Z,pytorch
163102,open,Eliminate use of deprecated 'device' argument in Tensor.pin_memory(),drivanov,"A crash was observed due to the usage of the deprecated `device` parameter. 
```
../opt/pyg/pytorch_geometric/test/test_edge_index.py::test_data_loader[True-0-int64] DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/Memory.cpp:46.)
DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/Memory.cpp:31.)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py"", line 91, in pin_memory
    clone[i] = pin_memory(item, device)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/pin_memory.py"", line 57, in pin_memory
    return data.pin_memory(device)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/torch_geometric/edge_index.py"", line 1211, in __torch_dispatch__
    return HANDLED_FUNCTIONS[func](*args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: _pin_memory() takes 1 positional argument but 2 were given
```

This PR removes its use to ensure compatibility with current PyTorch versions
",2025-09-16 20:51:02+00:00,2025-09-23T17:45:55Z,,False,6,0,4,83,22,3,6,,68,1352,False,False,False,False,False,False,3,4,653,115,88,27,1,4,2.0,4.0,2025-09-19T23:50:00Z,pytorch
163101,closed,[CI] Remove functorch doc build jobs,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163101


As repo has been archived, there couldn't be any doc updates
",2025-09-16 20:32:33+00:00,2025-09-16T22:27:07Z,,False,3,0,1,0,54,2,3,2025-09-16 22:26:02+00:00,36,156,False,False,False,True,False,False,2,2,786,54,0,54,1,1,5.0,2.0,2025-09-16T20:38:45Z,pytorch
163100,open,upgrade agios to py3.12,williamwen42,"Summary:
Python 3.10 is being deprecated by EOY 2025.
By using Python 3.10 past EOL deadline, changes in the codebase that are incompatible with Python 3.10 might break your build, test, or release.
You will not be able to initiate new builds using Python 3.10 in H1 2026!
So we apply py3.12 to all agios project;

When upgrading from Python 3.10 → 3.12, hit two errors:
Error A: 
warning: void function 'THP_PyObject_VirtualFree' should not return void expression
That was a simple mismatch (trying to return rom a void function). Simple Fix;

Error B (the important one)
error: initializer element is not a compile-time constant;
const uint8_t* THP_PyOpcode_Caches = _PyOpcode_Caches;
Analysis: 
In Python 3.10, _PyOpcode_Caches was defined as a global array → original code
const uint8_t* THP_PyOpcode_Caches = _PyOpcode_Caches;  // allowed
In Python 3.11+, _PyOpcode_Caches changed → it’s no longer treated as a compile-time constant (more like a runtime pointer).
So when tried to initialize THP_PyOpcode_Caches with it, the compiler rejected it:

initializer element is not a compile-time constant

Fix:
const uint8_t* THP_PyOpcode_Caches = NULL;

void THP_Init(void) {
    THP_PyOpcode_Caches = _PyOpcode_Caches;  // set at runtime
}

ensured THP_Init() is called during module initialization.
PyMODINIT_FUNC PyInit__C(void) {
    THP_Init();          // <-- pointer set here
    return initModule(); // continue with normal init
}

Test Plan:
manul test with different pipelines (pp3, sensel etc.)

Rollback Plan:

Differential Revision: D82250826




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 20:21:41+00:00,2025-09-19T07:13:27Z,,False,6,0,1,3,3,1,6,,23,1730,False,True,False,False,False,False,1,2,136,6,3,3,1,1,2.0,2.0,2025-09-18T18:46:39Z,pytorch
163099,closed,Add validation for precomputed format in YAML parsing,dsashidh,"Fixes #162398 

## Problem

When parsing YAML files with malformed precomputed sections the parser crashes with: AttributeError: dict object has no attribute split

This happens when the YAML has precomputed: as a dictionary instead the expected list format. Although the error shows what went wrong, it doesn't explain what format is expected.

## Solution

Add input validation in NativeFunction.from_yaml() to check that precomputed is a list before passing it to Precompute.parse( ). This provide a clearer error message showing the expected format: ValueError: precomputed must be a list of strings, got dict: {dim: int dim, int valid}

## Testing

Tested locally with malformed YAML to confirm the new validation catch the error and provides a helpful message.

",2025-09-16 20:20:25+00:00,2025-09-22T14:05:55Z,,False,3,0,1,5,0,1,3,2025-09-21 02:51:24+00:00,53,768,False,True,False,False,False,False,1,2,218,5,5,0,1,1,1.0,2.0,2025-09-16T20:20:36Z,pytorch
163098,closed,[optim] prevent problematic tensor aliasing in lr_scheduler,filipviz,"Prevents edge cases in SequentialLR and ReduceLROnPlateau which could corrupt learning rates or trigger recompilation.

Supersedes #162360
Fixes #162359
Fixes #163093

While putting #162360 together, I noticed the class of issue I was fixing (i.e. unintended aliasing in lr_schedulers when using Tensor lrs) appeared in several other places. @janeyx99 suggested I put together a follow-up PR.

There are several bugs resembling the one fixed in #162360. I added a helper to fix these:
```python
def _update_param_group_val(param_group: dict[str, Any], key: str, val: float | Tensor):
    """"""Set param_group[key] to val without aliasing or assignment when they're both tensors.
    Raises a KeyError if param_group[key] does not exist.
    """"""
    if isinstance(param_group[key], Tensor):
        param_group[key].fill_(_to_scalar(val))
    else:
        param_group[key] = val
```

And applied it to fix bugs in `SequentialLR.__init__` and `LRScheduler._update_lr`. I also added it to `CyclicLR.__init__` which was using an equivalent pattern, and `CosineAnnealingWarmRestarts.step` which *should* have had a similar issue:
```python
for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
    param_group[""lr""] = lr
```

But did not, because `get_lr()` actually returns tensors when using a tensor lr (despite its `list[float]` return type annotation). Relying on this propagation seems fragile, so I conservatively added the method here as well. I'll be fixing the type annotations and several related issues in followup PRs built off of this one.",2025-09-16 20:14:56+00:00,2025-09-17T13:41:31Z,,False,12,0,4,44,7,2,12,2025-09-17 13:40:28+00:00,59,1565,False,True,False,False,False,False,2,11,3922,221,129,92,1,4,3.0,12.0,2025-09-16T20:23:05Z,pytorch
163097,closed,[Cherry Pick][Graph Partition] allow sharing default device context,BoyuanFeng,"Fix New Feature: Cherry pick #162873 into PyTorch 2.9 for inductor graph partition.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-16 20:14:23+00:00,2025-09-19T22:43:18Z,2025-09-19T18:10:30Z,True,3,0,1,150,21,4,3,2025-09-19 18:10:30+00:00,67,294,False,True,True,False,False,False,4,2,468,171,150,21,1,1,3.0,2.0,2025-09-19T18:04:43Z,pytorch
163096,closed,DisableTorchFunction in debug_string,SherlockNoMad,"debug_string() invokes some torch functions under the hood. 
Use DisableTorchFunction() to avoid re-invoking __torch_function__ when calling debug_sting() inside DebugMode()

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 20:11:20+00:00,2025-09-17T00:20:56Z,,False,6,0,1,20,5,2,6,2025-09-17 00:19:53+00:00,36,276,False,True,False,False,False,False,2,5,1712,25,20,5,1,1,3.0,6.0,2025-09-16T20:21:12Z,pytorch
163095,open,Turn on Persistent TMA matmul by default,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163095



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 20:10:33+00:00,2025-09-24T18:37:30Z,,False,1,0,2,1,1,1,1,,40,297,False,False,False,False,False,False,1,0,0,41015,27062,13953,1,2,,,,pytorch
163094,closed,[inductor] Fix convolution autotune check when groups != 1,yushangdi,"When generating the triton template for convolution, we check `V.graph.sizevars.statically_known_equals(in_chan * groups, x.get_size()[1]) `. Note that in this check, we should consider the groups. 

This check verifies, at compile time, that the total number of input channels expected by the convolution weights (in_chan * groups) exactly matches the number of channels in the input tensor (x.get_size()[1]).


This fix is good in general as it allows for conv triton template to be generated when `groups> 1`. It's also required for unified runtime to use AOTI as a backend delegate, because unified runtime is libtorch-free, so we cannot use the ATEN fallback of conv2d. 

```
 python test/inductor/test_select_algorithm.py -k test_convolution2_group
 ```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 20:06:52+00:00,2025-09-17T09:10:38Z,,False,3,0,1,26,1,2,3,2025-09-17 09:09:35+00:00,58,962,False,True,False,False,False,False,2,2,495,27,26,1,1,1,3.0,2.0,2025-09-17T00:28:19Z,pytorch
163092,closed,[submodule] CUTLASS upgrade to 4.2.0 and change cutlass to cutlass_cppgen,henrylhtsang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163092

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 19:43:31+00:00,2025-09-18T18:04:59Z,,False,8,0,5,9,9,5,8,2025-09-18 18:03:54+00:00,73,295,False,False,False,False,False,False,5,5,807,8666,6656,2010,1,5,4.0,7.0,2025-09-16T19:44:43Z,pytorch
163091,open,Update cutlass version for fbcode,henrylhtsang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163091

Differential Revision: [D82567751](https://our.internmc.facebook.com/intern/diff/D82567751/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 19:38:35+00:00,2025-09-23T16:49:06Z,,False,18,0,12,12,12,4,18,,33,389,False,False,False,False,False,False,4,3,798,14407,10334,4073,1,11,2.0,3.0,2025-09-16T22:16:46Z,pytorch
163090,closed,[multi-kernel] shape-similarity kernel selection,pianpwk,"Introduces a variant of size-hint multi-kernel, where for novel runtime shapes, instead of performing full benchmarking to determine the optimal kernel, selects one of many kernels pre-generated from multi-kernel hints, based off similarity b/w hint / runtime input & output shapes (L1 distance in log2 space).

Some caveats/changes:
- Size-hint multi-kernel now only kicks in if the kernel has dynamic shapes
- Pre-generation still only does 1-d search over specified hints, e.g. `matmul([s0, s1], [s1, s2])` with size-hints `[64, 256]` only generates 2 kernels - based on tuning shapes ([64, 64], [64, 64]) and ([256, 256], [256, 256]). Extending this to reasonable n-d search (via user API?) is an extension

Benchmarking results, compared to multi-kernel w/ full benchmarking (hints 64, 4096), and compiling with the ground truth hint:
<img width=""1902"" height=""1222"" alt=""550541081_1088709150049684_6528797079439730237_n"" src=""https://github.com/user-attachments/assets/056cca48-c16a-4451-9b4a-fa13a7a058a9"" />

Full benchmarking doing worse is extremely weird, but we did see similar spikes in #156628

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-16 19:31:18+00:00,2025-09-23T21:01:53Z,,False,3,9,11,230,66,4,12,2025-09-23 21:00:50+00:00,48,1318,False,False,False,False,False,False,4,2,493,45617,33825,11792,1,11,3.0,2.0,2025-09-17T04:29:03Z,pytorch
163089,open,Fix ONNX dynamic_axes segfault by adding validation,ghostspiders,"Fixes #163033


cc @justinchuby @titaiwangms",2025-09-16 19:22:46+00:00,2025-09-23T23:56:03Z,,False,3,3,1,71,0,2,6,,51,44,False,True,False,False,False,False,2,2,748,71,71,0,1,1,3.0,5.0,2025-09-16T21:09:34Z,pytorch
163088,closed,remove tolerance override for dynamo test_mixed_device_dtype in SGD,janeyx99,"In reaction to https://github.com/pytorch/pytorch/issues/116202#issuecomment-3145929113

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163088

",2025-09-16 19:22:23+00:00,2025-09-17T18:18:31Z,,False,3,0,1,0,10,1,3,2025-09-17 18:17:26+00:00,67,183,False,False,False,False,False,False,1,2,496,10,0,10,1,1,3.0,3.0,2025-09-16T19:32:50Z,pytorch
163087,open,Enable several unit tests on ROCm,BLOrange-AMD,"Enables
test_nn::TestNNDeviceTypeCUDA::test_transformerencoderlayer_cuda_float16
test_nn::TestNNDeviceTypeCUDA::test_transformerencoderlayer_cuda_float32
test_nn::TestNNDeviceTypeCUDA::test_transformerencoderlayer_cuda_float64
test_nn::TestNNDeviceTypeCUDA::test_transformerencoderlayer_gelu_cuda_float16
test_linalg::TestLinalgCUDA::test_eigh_svd_illcondition_matrix_input_should_not_crash_cuda_float32
test_linalg::TestLinalgCUDA::test_eigh_svd_illcondition_matrix_input_should_not_crash_cuda_float64

Fixes https://github.com/pytorch/pytorch/issues/134687

FSDP + TP UTs
Fixes https://github.com/pytorch/pytorch/issues/159489
Fixes https://github.com/pytorch/pytorch/issues/125555
Fixes https://github.com/pytorch/pytorch/issues/134139
Fixes https://github.com/pytorch/pytorch/issues/125991
Fixes https://github.com/pytorch/pytorch/issues/125918


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-16 19:10:13+00:00,2025-09-25T01:08:01Z,,False,6,0,3,2,7,2,6,,33,967,False,True,False,False,False,False,2,4,1518,9,2,7,1,3,4.0,4.0,2025-09-16T20:07:50Z,pytorch
163086,closed,Rm pytorch deps platform args,jaejunku,"Summary: Platform args was a buck1 concept that we decided to port over to buck2 in order to make the migration easier. However, platforms args existing in the repo blocks some buck modernization like modefile free efforts, so we're trying to get rid of the usage.

Test Plan:
CI

Rollback Plan:

Differential Revision: D82470032




cc @malfet @seemethere",2025-09-16 19:00:07+00:00,2025-09-19T02:14:10Z,,False,5,0,1,15,1,1,5,2025-09-19 02:13:07+00:00,29,356,False,False,False,False,False,False,1,2,580,16,15,1,1,1,4.0,3.0,2025-09-16T19:01:12Z,pytorch
163085,closed,[ez][CI] Fix docs push in nightly workflow,pytorchbot,"HUD metrics page says docs push hasn't happened in 21 days
<img width=""293"" height=""142"" alt=""image"" src=""https://github.com/user-attachments/assets/f930aab8-0503-4bf2-b962-8c375dec6b78"" />

I guess main branch docs just haven't been updated?  Did anyone notice?  Do we care?

Either way I think this should fix it

Likely started after https://github.com/pytorch/pytorch/pull/161182",2025-09-16 18:59:49+00:00,2025-09-16T19:04:18Z,2025-09-16T19:04:17Z,True,1,0,1,1,1,1,1,2025-09-16 19:04:17+00:00,42,383,False,True,False,True,False,False,1,0,0,2,1,1,1,1,2.0,0.0,2025-09-16T19:03:57Z,pytorch
163083,closed,[sdpa] make sure to recompile if alignment is different than before,ColinPeppler,"## Context
An example from Qwen2-7B
- This come from running torch.compile with a sequence length that is
divisible by 8 (no padding needed). Call this `Run1`.
- If we then run the compiled model with a difference length that isn't
divisible by 8 (requires padding). Call this `Run2`.
- Then we'll see this error.
```
File ""/var/tmp/torchinductor_nobody/2w/c2wby7ilxbna45xrtrrfjqpeutwouruviu2742ockunnd2bleeiz.py"", line 1963, in call
    buf24 = torch.ops.aten._scaled_dot_product_efficient_attention_backward.default(reinterpret_tensor(buf18, (s85, 3584 // s19, s48, 512 // (512 // s19)), (s48*(512 // (512 // s19))*(3584 // s19), 512 // (512 // s19), (512 // (512 // s19))*(3584 // s19), 1), 0), buf20, buf21, buf22, buf23, getitem, getitem_1, getitem_2, getitem_3, 0.0, [True, True, True, False], scale=0.08838834764831845)
File ""torch/_ops.py"", line 841, in __call__
    return self._op(*args, **kwargs)
RuntimeError: attn_bias is not correctly aligned (strideM). attn_bias.stride(2) = 6102, and should be a multiple of 4.
```
- We only see the error because we did not recompile on `Run2`. Instead we ran the inputs on the same graph as `Run1`.


### A bit more on why.
Here we check whether to realize the unpadded buffer (unwrapped slice) which we want for `Run1` but not for `Run2`.
https://github.com/pytorch/pytorch/blob/0897affcd5e0e5281db7202408ca637bbf2f7abe/torch/_inductor/lowering.py#L2687-L2694

## Fix
Size hint doesn't guard, so the fix is to use `guard_or*` to guard. 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163083



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 18:26:21+00:00,2025-09-23T21:06:01Z,,False,7,7,5,125,17,3,14,2025-09-23 01:33:36+00:00,67,1787,False,True,False,False,False,False,3,6,1837,160,134,26,1,5,5.0,7.0,2025-09-16T18:31:30Z,pytorch
163081,open,Fix arg parser one pos arg,cleonard530,"Fixes #130609
tensor.rehape() was running with extra arguments whenever the first argument was a tuple. This is because methods with one sequence arguments can be passed in as a Sequence (tuple, list, etc.) or variable argument (*args). But when multiple arguments where passed in and the first argument was a sequence, it would run and just ignore the other arguments.
For example, if someone wrote

```
x = torch.ones((4, 3))
y = x.reshape((2, 6), torch.float32)
```

the process would run without any errors but the 'torch.float32' wouldn't do anything. This could be problematic if the developer thinks they are changing the the tensor dtype (or anything else they think they are doing) but really nothing is happening other than reshaping (silent bugs).

This PR fixes this issue for any method that has one integer sequence argument, not just reshape() (including tile() and view())  

**Warning:** this could break downstream code if someone has already implemented something like this. However, they could also have a silent bug in their code that goes unnoticed because no errors are thrown so it is still best to fix this issue.

@malfet 
",2025-09-16 18:05:22+00:00,2025-09-18T14:12:06Z,,False,2,0,8,146,3,6,2,,26,1149,False,True,False,False,False,False,6,1,38,187,165,22,1,8,1.0,1.0,2025-09-16T20:25:11Z,pytorch
163080,closed,write conv1d decomposition,yushangdi,"In Unified Runtime, we cannot have any fallback ops (for now). Not all conv1d ops can avoid fallbacks now, so we write a decomposition for it.

it's not registered to the default decomposition table as currently only executorch/unified runtime needs it. But it might benefit inductor as well because conv2d can generate triton kernels while there's no triton codegen for conv1d. I don't know if the conv2d triton kernel will have better perf compared to aten::conv1d, so it's not registered by default yet. 

To register it, one just needs to do `import torch._decomp as decomp;decomp.register_decomposition(torch.ops.aten.conv1d.default, conv1d_to_conv2d)`


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 16:59:18+00:00,2025-09-17T19:23:43Z,,False,3,0,1,91,0,2,3,2025-09-17 19:22:41+00:00,26,861,False,False,False,False,False,False,2,2,493,91,91,0,1,1,3.0,2.0,2025-09-17T16:36:41Z,pytorch
163079,open,[ROCm][tunableop] Improvements to tunableop Numerical Check,sarthaktandon9amd,"Modified the flag PYTORCH_TUNABLEOP_NUMERICAL_CHECK, so that it accepts the numerical tolerances in the format atol_rtol as compared to the previous 0 and 1. Retains previous functionality with default values as well. 

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-16 16:39:25+00:00,2025-09-25T14:29:52Z,,False,2,9,7,215,58,8,11,,59,336,False,False,False,False,True,False,8,0,457,893,525,368,1,7,2.0,2.0,2025-09-17T19:46:45Z,pytorch
163078,closed,[PyTorch] Compile SVE's box-cox only when building targeting SVE,Nicoshev,"Summary:
Internally, we are building PyTorch on the compat layer.
Need to avoid compiling sve's box-cox, as sve is not marked as build target.


Rollback Plan:

Reviewed By: rraometa, YifanYuan3

Differential Revision:
D82544412

Privacy Context Container: L1208939


",2025-09-16 16:07:04+00:00,2025-09-17T03:36:17Z,,False,8,0,1,2,2,1,8,2025-09-17 03:35:15+00:00,64,268,False,False,False,False,False,False,1,5,1430,4,2,2,1,1,5.0,5.0,2025-09-16T16:19:01Z,pytorch
163077,closed,update test_quantization tests to run weekly,liangel-02,"Fixes #162854
",2025-09-16 14:21:00+00:00,2025-09-24T11:32:18Z,,False,5,10,1,86,16,8,15,2025-09-24 11:31:15+00:00,44,14,False,True,False,False,False,False,8,3,712,102,86,16,1,1,5.0,6.0,2025-09-16T19:35:16Z,pytorch
163074,closed,fix f-string in errors.py,henrykrumb,"Add missing ""f"" for formatted f-string in UnsupportedOperandError, change ""op_name"" (undefined) to ""name"" for more descriptive error message in case of an unsupported operand with an unrecognized namespace.

cc @justinchuby @titaiwangms",2025-09-16 13:28:31+00:00,2025-09-16T19:20:35Z,,False,4,0,1,1,1,1,4,2025-09-16 19:19:32+00:00,25,236,False,True,False,False,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-16T14:10:29Z,pytorch
163073,closed,"[inductor] in emulate_precision_casts, disable fma fusion in triton",v0i0,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163073



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 13:22:48+00:00,2025-09-24T00:00:24Z,,False,10,0,5,63,0,4,10,2025-09-23 23:59:20+00:00,67,297,False,False,False,False,False,False,4,9,2950,75,69,6,1,5,5.0,13.0,2025-09-16T13:46:35Z,pytorch
163070,closed,[TEST][CUDA] Use proper dtype in test_cuda_tensor_pow_scalar_tensor_cuda,Aidyn-A,"The test `test_binary_ufuncs.py::TestBinaryUfuncsCUDA::test_cuda_tensor_pow_scalar_tensor_cuda` fails with a mismatched `dtype`:
```Python
AssertionError: The values for attribute 'dtype' do not match: torch.float32 != torch.float64.
```
This PR forces both arguments to use the same `dtype` to fix the test failure.

cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-16 12:29:53+00:00,2025-09-16T18:29:56Z,,False,3,0,1,2,2,1,3,2025-09-16 18:28:53+00:00,72,357,False,True,False,False,False,False,1,2,493,4,2,2,1,1,3.0,2.0,2025-09-16T16:18:07Z,pytorch
163069,open,[async_tp] Base support ag-transpose-mm(mat_B) case,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163069
* #163068
* #162794

Supporting ag+mm fusion for the pattern:

```
mat_B = allgather(shard)
mat_B_t = mat_B.t()
return mm(mat_A, mat_B_t)
```
This pattern decomposed to A_shards operator, using `MM = A@B => MM.t() = (B.t() @ A.t()).t()`:
```
mat_A_t = mat_A.t()
mat_B = allgather(shard)
mm_t = mm(mat_B, mat_A_t)
return mm_t.t()
```

2. I put it under config, as with these decomposition we are changing the order of computation.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 11:42:21+00:00,2025-09-25T15:47:07Z,,False,1,0,6,238,37,3,1,,51,825,False,False,False,False,False,False,3,0,0,59592,41898,17694,1,6,,,,pytorch
163068,open,[async_tp] Support ag+mm with gather_dim lastdim of mat_A,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163069
* __->__ #163068
* #162794


Adding ag+mm support for the case, when gather_dim is last dim of matmul (reduction dim).

When we decompose matmul by reduction dimension we result in partials that needs additional reduction,
we allocate memory for accumulator.

This could be bad for performance and outweight the advantages of pipelining.

So we need to do more investigation when this decomposition can give performance wins.
Because of this I added a config that is turned off by default for experimentation and collecting results on different workloads.

scaled_mm was not tested for this decomposition and needs separate PR.




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 11:42:16+00:00,2025-09-25T15:47:16Z,,False,2,0,5,183,5,4,2,,57,1018,False,False,False,False,False,False,4,1,24,59284,41725,17559,1,5,1.0,1.0,2025-09-21T03:08:43Z,pytorch
163067,open,Add DTensor/CP support for Fused SDPA overrideable,sspintel,"In summary, this PR aims to extend DTensor and Context Parallel support for the aten._scaled_dot_product_fused_attention_overrideable ops so that vendors of devices who override this operator (in this case, HPU) can benefit from torch's DTensor framework and CP support for the attention operator.

- Add DTensor strategies for SDPA overrideable forward and backward
- Add overrideable in Ring Attention list of supported ops

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @drisspg ",2025-09-16 11:07:11+00:00,2025-09-24T03:50:16Z,,False,8,0,5,377,4,4,8,,50,538,False,False,False,False,False,False,4,6,1675,475,424,51,1,5,2.0,6.0,2025-09-16T11:14:49Z,pytorch
163065,closed,DEBUG AsyncTP,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163065
* #162794



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 09:43:23+00:00,2025-09-24T14:46:22Z,,False,1,0,1,284,8,5,1,2025-09-24 14:46:22+00:00,13,406,False,True,False,False,False,False,5,0,0,292,284,8,1,1,,,,pytorch
163063,closed,Restore environment after NcclUserBufferRegistrationTest,Flamefire,"This test sets ""NCCL_ALGO=NVLS"" in NcclUserBufferRegistrationTest which affects tests run in the same process such as `test_on_completion_hook_*` that fail with
> invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.26.2
> ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
> Last error:
> Error : no algorithm/protocol available for function Broadcast with datatype ncclInt8. NCCL_ALGO was set to NVLS.




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 09:24:55+00:00,2025-09-16T18:15:29Z,,False,4,0,1,13,8,1,4,2025-09-16 17:37:12+00:00,56,545,False,True,False,False,False,False,1,3,646,21,13,8,1,1,2.0,3.0,2025-09-16T14:51:55Z,pytorch
163060,closed,[c10d] Differentiate participate requirements between init_device_group and direct construction,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Pull-Request-resolved: https://github.com/pytorch/pytorch/pull/162571

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 08:54:57+00:00,2025-09-17T00:03:27Z,,False,3,0,1,40,16,2,3,2025-09-17 00:03:27+00:00,95,266,False,False,False,False,False,False,2,1,9,56,40,16,1,1,1.0,1.0,2025-09-17T00:03:27Z,pytorch
163059,closed,[c10d] Remove init_process_group from DeviceMesh,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163060
* __->__ #163059
* #163058
* #163057
* #163056
* #163055

Pull-Request-resolved: https://github.com/pytorch/pytorch/pull/162549",2025-09-16 08:54:53+00:00,2025-09-17T00:03:08Z,,False,3,0,1,155,139,2,3,2025-09-17 00:03:08+00:00,48,213,False,False,False,False,False,False,2,1,9,294,155,139,1,1,1.0,1.0,2025-09-17T00:03:08Z,pytorch
163058,open,Refactor test_dtensor_compile,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162571
* #162549
* __->__ #163058
* #162770
* #162545
* #162529

I don't know how the following worked previously:
```
store = FakeStore()
init_process_group(""fake"", rank=0, world_size=2, store=store)
init_device_mesh(""cpu"", ...)
```
But it is not supported now.
Updating tests accordingly.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 08:54:49+00:00,2025-09-18T13:42:01Z,,False,1,6,5,140,69,4,7,,29,652,False,False,False,False,False,True,4,0,0,359,224,135,1,5,2.0,0.0,2025-09-18T04:10:02Z,pytorch
163057,closed,[c10d] destroy_process_group() works without default group created,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163060
* #163059
* #163058
* __->__ #163057
* #163056
* #163055

Pull-Request-resolved: https://github.com/pytorch/pytorch/pull/162770",2025-09-16 08:54:45+00:00,2025-09-17T00:02:34Z,,False,2,0,1,29,7,2,2,2025-09-17 00:02:33+00:00,66,213,False,False,False,False,False,False,2,1,9,36,29,7,1,1,1.0,1.0,2025-09-17T00:02:33Z,pytorch
163056,closed,[c10d] new_group works without world group created,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163060
* #163059
* #163058
* #163057
* __->__ #163056
* #163055

Pull-Request-resolved: https://github.com/pytorch/pytorch/pull/162545",2025-09-16 08:54:41+00:00,2025-09-17T00:02:26Z,,False,2,0,1,59,25,2,2,2025-09-17 00:02:25+00:00,50,213,False,False,False,False,False,False,2,1,9,84,59,25,1,1,1.0,1.0,2025-09-17T00:02:25Z,pytorch
163055,closed,[c10d] Add dist.init(),kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163060
* #163059
* #163058
* #163057
* #163056
* __->__ #163055

Pull-Request-resolved: https://github.com/pytorch/pytorch/pull/162529",2025-09-16 08:54:36+00:00,2025-09-17T00:02:16Z,,False,2,0,1,253,91,11,2,2025-09-17 00:02:16+00:00,22,213,False,False,False,False,False,False,11,1,9,344,253,91,1,1,1.0,1.0,2025-09-17T00:02:16Z,pytorch
163054,open,[CD] Delete duplicate MSVC2022 installation in triton xpu Windows build,chuanqi129,"Fixes #ISSUE_NUMBER
",2025-09-16 08:45:49+00:00,2025-09-16T10:46:13Z,,False,1,0,1,6,44,3,1,,71,20,False,True,False,False,False,False,3,0,0,50,6,44,1,1,,,,pytorch
163053,open,[ContextParallel][example] FLOPS measure,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163806
* #163617
* __->__ #163053
* #161062

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 08:33:10+00:00,2025-09-24T22:33:28Z,,False,1,1,5,1932,116,2,2,,40,225,False,False,False,False,False,False,2,0,0,31547,23153,8394,1,5,1.0,0.0,2025-09-24T03:46:54Z,pytorch
163050,closed,[ROCm] test_aot_inductor: Enable fp8 tests.,jagadish-amd,cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben,2025-09-16 07:36:27+00:00,2025-09-18T14:06:29Z,,False,6,0,1,7,11,1,6,2025-09-18 14:05:25+00:00,43,315,False,False,False,False,False,False,1,5,1753,18,7,11,1,1,2.0,5.0,2025-09-16T15:33:36Z,pytorch
163049,open,[xpu] Support high stream for ProcessGroupXCCL,Chao1Han,"Add high priority stream support for ProcessGroupXCCL. Just like CUDA, XPU streams also support execution with higher priority compared to other streams. Implementation in https://github.com/intel/torch-xpu-ops/pull/1715, add register here.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 07:10:15+00:00,2025-09-22T21:25:35Z,,False,12,1,5,26,2,1,13,,46,344,False,False,False,False,False,False,1,10,1396,42,33,9,1,5,5.0,11.0,2025-09-18T03:07:16Z,pytorch
163047,closed,enable sync batchnorm for HPU device,harikodali,"Add HPU to list of supported devices for SyncBatchNorm
",2025-09-16 06:49:46+00:00,2025-09-16T20:46:43Z,,False,4,0,1,1,0,1,4,2025-09-16 20:45:41+00:00,36,55,False,False,False,False,False,False,1,3,534,1,1,0,1,1,3.0,4.0,2025-09-16T07:28:40Z,pytorch
163044,closed,[AOTI-FX] Solve for undefined symbols in dynamic input shapes,blaine-rister,"# Problem
When dynamic shapes are passed to AOTInductor, they usually have a very basic form like `(s0, 5, 27)`. In these cases it's straightforward to generate code defining the symbol `s0` as a specific dimension of the input tensor. However, AOTI can handle slightly more generic expressions than this, such as `(2 * s0, 5, 27)`. In these cases, we don't immediately know the value of `s0`, but we need to solve for it, since it may be referenced in other parts of the program such as kernel call arguments, launch grids, etc.

# Feature
This PR adds support for more generic dynamic input expressions in the FX backend, following the implementation already present in AOTI's C++ backend:
 1. Check if the expression contains *one* undefined symbol, as multiple variables would make the equation underdetermined. Let's call this `s0`. (We could potentially generalize this, but this PR focuses on cases AOTI can already handle.)
 2. Generate a new symbol for the relevant size or stride of the input tensor. Let's call this `size`. This is computed with FX nodes just as a normal symbol would be.
 3. Use sympy to solve for `s0` in terms of `size`. Let's call the resulting expression `solution`.
 4. Since we know `s0` is an integer, `solution == floor(solution)`. Take the floor and then convert division to `FloorDiv`. This is required to trace through the expression, since the return value of regular division is not guaranteed to be an integer.
 5. Generate FX for the modified `solution`, which defines the value `s0`.
 6. Override the relevant method of `PythonWrapperCodegen` to a no-op, since the FX converter handles the above on its own.

# Test plan
In addition to the existing dynamic shapes tests, this PR adds new test cases where the input shape contains a non-trivial expression. This dynamic input dimension is then multiplied by other dimensions to form the argument to a `reshape`.

Here's an example graph from one of the CI tests. In this case, the input expression was `2*x + 1`, and the solution is `x = (sym_size_int - 1) / 2`:
```
graph():
    %arg0_1 : [num_users=2] = placeholder[target=arg0_1]
    %sym_size_int : [num_users=1] = call_function[target=torch.ops.aten.sym_size.int](args = (%arg0_1, 0), kwargs = {})
    %sym_sum : [num_users=1] = call_function[target=torch.sym_sum](args = ([-1, %sym_size_int],), kwargs = {})
    %floordiv : [num_users=1] = call_function[target=operator.floordiv](args = (%sym_sum, 2), kwargs = {})
    %mul : [num_users=2] = call_function[target=operator.mul](args = (8, %floordiv), kwargs = {})
    %sym_sum_1 : [num_users=2] = call_function[target=torch.sym_sum](args = ([4, %mul],), kwargs = {})
    %buf0 : [num_users=2] = call_function[target=torch.empty_strided](args = ([%sym_sum_1], [1]), kwargs = {dtype: torch.float32, device: cuda:0})
    %sym_sum_2 : [num_users=1] = call_function[target=torch.sym_sum](args = ([35, %mul],), kwargs = {})
    %floordiv_1 : [num_users=1] = call_function[target=operator.floordiv](args = (%sym_sum_2, 32), kwargs = {})
    %triton_kernel_wrapper_mutation : [num_users=0] = call_function[target=torch.ops.higher_order.triton_kernel_wrapper_mutation](args = (), kwargs = {kernel_idx: 0, constant_args_idx: 0, grid: [(%floordiv_1, 1, 1)], tma_descriptor_metadata: {}, kwargs: {in_ptr0: %arg0_1, out_ptr0: %buf0, xnumel: %sym_sum_1, XBLOCK: 32}})
    return buf0
```

The `sym_size_int` node returns the first dimension of the input tensor. Next, `floordiv` computes the input symbol in terms of the input size. Then, the launch grid is computed by `floordiv_1`, the kernel argument by `sym_sum_1`.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-16 06:07:47+00:00,2025-09-17T04:13:08Z,,False,3,4,3,120,38,2,7,2025-09-17 04:12:06+00:00,61,3808,False,False,True,False,False,False,2,2,493,168,125,43,1,3,4.0,2.0,2025-09-16T18:16:00Z,pytorch
163042,closed,[docs] Add cross-reference to torch.nn.ReLU from Softplus (and LeakyReLU “See also”),Username46786,"### What
- Convert plain ""ReLU"" in the Softplus docstring to a Sphinx class reference so it links to `torch.nn.ReLU`.
- Add a short ""See also"" line in LeakyReLU pointing to `torch.nn.ReLU`.

### Why
Improves navigation in the activation docs by making related functions clickable and easier to discover.

### How
- Edited `torch/nn/modules/activation.py`:
  - Softplus docstring: use `:class:`~torch.nn.ReLU``.
  - LeakyReLU docstring: add ""See also :class:`~torch.nn.ReLU`"".

### Notes
Docs-only change. Happy to tweak wording/placement if preferred.
",2025-09-16 05:52:03+00:00,2025-09-17T12:50:00Z,,False,3,1,1,6,3,1,4,2025-09-17 12:50:00+00:00,84,552,False,False,False,True,True,False,1,0,0,9,6,3,1,1,1.0,0.0,2025-09-16T13:33:41Z,pytorch
163041,closed,[ONNX] Fix rotary_embedding_23 implementation,pytorchbot,"The implementation of rotary_embedding_23 when input is 3D was incorrect.

## Tested

Locally with 

```py
import onnx_ir as ir
import onnx
import torch
import os
import numpy as np

base_path = ""/home/justinchu/dev/onnx/onnx/backend/test/data/node""
test_names = [
    ""test_rotary_embedding"",
    ""test_rotary_embedding_3d_input"",
    ""test_rotary_embedding_interleaved"",
    ""test_rotary_embedding_no_position_ids"",
    ""test_rotary_embedding_no_position_ids_interleaved"",
    ""test_rotary_embedding_no_position_ids_rotary_dim"",
    ""test_rotary_embedding_with_interleaved_rotary_dim"",
    ""test_rotary_embedding_with_rotary_dim"",
]
model_paths = [os.path.join(base_path, name) for name in test_names]


for path in model_paths:
    print(f""Checking {path} for issues..."")

    model = onnx.load(os.path.join(path, ""model.onnx""))
    input0 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_0.pb""))
    ).numpy()
    input1 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_1.pb""))
    ).numpy()
    input2 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_2.pb""))
    ).numpy()
    if os.path.exists(os.path.join(path, ""test_data_set_0"", ""input_3.pb"")):
        input3 = ir.from_proto(
            onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_3.pb""))
        ).numpy()
    else:
        input3 = None
    output0 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""output_0.pb""))
    ).numpy()

    m = ir.from_proto(model)

    node = m.graph[-1]
    print(node)
    assert node.op_type == ""RotaryEmbedding""

    interleaved = node.attributes.get_int(""interleaved"", 0)
    num_heads = node.attributes.get_int(""num_heads"", 0)
    rotary_embedding_dim = node.attributes.get_int(""rotary_embedding_dim"", 0)

    torch_out = torch.onnx.ops.rotary_embedding(
        torch.tensor(input0),
        torch.tensor(input1),
        torch.tensor(input2),
        position_ids=torch.tensor(input3) if input3 is not None else None,
        interleaved=bool(interleaved),
        num_heads=num_heads,
        rotary_embedding_dim=rotary_embedding_dim,
    )
    torch_out = torch_out.detach().cpu().numpy()
    np.testing.assert_allclose(torch_out, output0)
```

Fix https://github.com/pytorch/pytorch/issues/162848

cc @titaiwangms @kunal-vaishnavi",2025-09-16 04:42:23+00:00,2025-09-17T00:16:23Z,2025-09-17T00:16:23Z,True,1,0,1,119,19,2,1,2025-09-17 00:16:23+00:00,45,2395,False,True,False,False,False,False,2,0,0,138,119,19,1,1,1.0,0.0,2025-09-17T00:16:11Z,pytorch
163039,closed,[CPU] Adding missing brackets in native MaxUnpool log,can-gaa-hou,"As stated in the title. 


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-16 03:31:55+00:00,2025-09-16T21:29:21Z,,False,8,0,1,2,2,1,8,2025-09-16 21:28:17+00:00,53,108,False,False,False,False,False,False,1,6,1644,4,2,2,1,1,3.0,6.0,2025-09-16T06:24:50Z,pytorch
163038,closed,[C10d] Code clean for torch.distributed.init_process_group,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163038

As the title stated.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-16 02:59:19+00:00,2025-09-16T08:16:32Z,,False,3,0,1,2,2,1,3,2025-09-16 08:15:28+00:00,58,217,False,False,False,False,False,False,1,2,493,4,2,2,1,1,2.0,2.0,2025-09-16T05:09:00Z,pytorch
163036,closed,Fix invalid indices bug for max_unpool2d/3d on MPS,can-gaa-hou,Fixes #163035,2025-09-16 02:28:34+00:00,2025-09-19T05:14:28Z,,False,9,7,3,39,0,2,16,2025-09-19 05:13:24+00:00,50,13,False,True,False,False,False,False,2,6,1372,127,83,44,2,3,5.0,8.0,2025-09-16T07:10:08Z,pytorch
163034,open,Add device argument to torch.random.get_rng_state,evan-conway,"Fixes #162812

Adds support for either passing a device directly into get_rng_state, or passing in a string or int (which is then wrapped into a device inside, as in torch.cuda.get_rng_state).

I wasn't exactly sure where tests for this should go, please let me know. I used this script for testing:
```python
import torch

# note: when running with CUDA GPU, first three tests will give the same result,
# as will the last two

# test with no device specified
print(torch.get_rng_state())

# test with CPU
cpu_device = torch.device(""cpu"")
print(torch.get_rng_state(cpu_device))

# test with direct name
print(torch.get_rng_state(""cpu""))

# test with CUDA
cuda_device = torch.device(""cuda:0"")
print(torch.get_rng_state(cuda_device))

# test with integer
print(torch.get_rng_state(0))
```",2025-09-16 02:15:57+00:00,2025-09-21T12:55:07Z,,False,4,3,4,17,6,2,7,,49,787,False,True,False,False,False,False,2,3,174,35,23,12,1,4,4.0,3.0,2025-09-16T15:15:21Z,pytorch
163031,closed,Use C++-accessible Placement in compute_global_tensor_info,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163031
* #162990
* #163030
* #162968
* #162508
* #161695

This seems to speed things up, though there is more overhead than I'd like to unwrap a Placement via pybind11. Might be time to introduce nanobind; I'm not sure that we can use it in general, but the Placement type system is (currently?) isolated. (Alternatively, I could look into whether we can cut this overhead.)",2025-09-16 01:11:02+00:00,2025-09-22T23:34:37Z,,False,3,0,5,7,22,1,3,2025-09-22 23:34:37+00:00,58,460,False,False,False,False,False,False,1,1,71,19475,14103,5372,1,5,1.0,1.0,2025-09-22T23:34:37Z,pytorch
163030,open,C++-accessible Placements via pybind11,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163667
* #162990
* __->__ #163030
* #162968
* #162508
* #161695

This makes Placement data representation available in C++ via pybind11.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 01:10:58+00:00,2025-09-24T20:33:19Z,,False,3,14,14,377,92,12,17,,38,486,False,False,False,False,False,False,12,2,331,19710,14246,5464,1,14,4.0,2.0,2025-09-16T01:12:58Z,pytorch
163029,open,Continue to build nightly CUDA 12.9 for internal,huydhn,"Revert part of https://github.com/pytorch/pytorch/pull/161916 to continue building CUDA 12.9 nightly


cc @albanD",2025-09-16 00:57:55+00:00,2025-09-22T19:45:52Z,,False,4,0,2,3543,501,9,4,,48,113,False,False,False,False,False,False,9,3,1006,4044,3543,501,1,2,3.0,4.0,2025-09-16T01:21:09Z,pytorch
163028,open,[user-streams] Handle aliasing properly,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* #162903
* #162902
* #162900
* __->__ #163028
* #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 00:48:41+00:00,2025-09-20T05:20:26Z,,False,1,0,5,2,1,1,1,,39,346,False,False,False,False,False,False,1,0,0,35802,26750,9052,1,5,2.0,0.0,2025-09-19T20:21:01Z,pytorch
163027,open,[user-streams] Move stream code to streams module,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* #162903
* #162902
* #162900
* #163028
* #162899
* __->__ #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-16 00:48:36+00:00,2025-09-20T05:17:17Z,,False,1,0,3,161,140,5,1,,49,346,False,False,False,False,False,False,5,0,0,36090,26900,9190,1,3,2.0,0.0,2025-09-17T19:06:55Z,pytorch
163025,closed,[CI] disable rerun of distributed tests,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163025

#162978 identified an issue that distributed test failures were wrongly muted. 
Per discussion with @malfet, one solution is to disable rerun of distributed tests in `run_test.py`.
The PR makes use of the `is_distributed_test` flag to identify those tests.
",2025-09-16 00:06:56+00:00,2025-09-16T03:12:57Z,,False,4,0,1,7,5,1,4,2025-09-16 03:11:52+00:00,39,351,False,False,False,False,False,False,1,3,739,12,7,5,1,1,3.0,4.0,2025-09-16T00:20:06Z,pytorch
163024,closed,"Revert ""Make distributed modules importable even when backend not bui…",Camyll,"…lt (#159889)"" (#162568)

This reverts commit a0d026688cd69583d5a4e0c6f3e5fda141a7f4a9.

Revert ""Always build USE_DISTRIBUTED. (#160449)""

This reverts commit d80297a6846f1f2c36fd4f19e22919f2abe8fcea.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/162568
Approved by: https://github.com/huydhn

Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-15 23:57:47+00:00,2025-09-19T17:34:55Z,2025-09-19T17:34:55Z,True,1,0,1,451,774,50,1,2025-09-19 17:34:55+00:00,70,480,False,True,False,False,False,False,50,0,20,1225,451,774,1,1,1.0,1.0,2025-09-17T17:26:02Z,pytorch
163023,open,[inductor] return constant shape for ops that do not return,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162275
* __->__ #163023

Returning None is bad because None means that we don't know the shape.
Returning () is a good choice as that shape broadcasts with any other
shape.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 23:44:56+00:00,2025-09-16T23:12:35Z,,False,7,0,2,9,6,1,7,,59,454,False,False,False,False,False,False,1,5,995,21,12,9,1,2,3.0,6.0,2025-09-15T23:52:18Z,pytorch
163022,closed,Cherry pick revert of Make distributed modules importable even when backend not built,Camyll,"Cherry picks revert #159889 for release 2.9

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 23:42:25+00:00,2025-09-15T23:45:43Z,,False,1,0,2,1074780,782,4441,1,2025-09-15 23:42:54+00:00,85,146,False,False,False,False,False,False,4441,0,0,1075562,1074780,782,2,2,,,,pytorch
163021,closed,[mps] Take into account offset,angelayi,Fixes issue when running AOTI + MPS on voxtral model,2025-09-15 23:33:55+00:00,2025-09-16T07:15:39Z,,False,3,0,1,1,1,1,3,2025-09-16 07:14:36+00:00,30,52,False,True,False,False,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-15T23:36:57Z,pytorch
163020,closed,[WIP] mm recompilations hook,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163020

Note : idea seems not very convincing so probably killing it

MM Recompilation API

This API allows users to trigger recompilations for matrix multiplications in 
compiled models based on custom conditions. By providing a callable function, 
users can control when recompilations occur. This can be used as a way to ensure
the mm space is properly splitted to regions that are potentially max auto tuned. 

It requires that the users know how they want to split the space. 

cons:
- does not work with unbacked. 
- compile time probably >> dynamic dispatch ( require repeated dynamo + auto grad)
- Compile time can go crazy exponentially. 

pros:
- Very Simple
- User have all power to control recompilations of mm. 

Example Usage:
```
        def mm_recompile_hook(m, n, k, sizevars):
            # Specialization for square matrices
            if sizevars.guard_or_false(sympy.And(sympy.Eq(m, n), sympy.Eq(n, k))):
                pass
            # Specialization for any small given 1 small dim.
            elif sizevars.guard_or_false(sympy.Or(sympy.Lt(m, 16), sympy.Lt(n, 16), sympy.Lt(k,16))):
                pass
            else:
            # all > 16
                pass

        # do it again but with mm_recompile_hooks
        with torch._inductor.config.patch(mm_recompile_hooks={""mm"":mm_recompile_hook}):
            run_all()
            self.assertEqual(cnt.frame_count, 4)
```
This could be
```
 def mm_recompile_hook(m, n, k, sizevars):
        # Specialization for square matrices
        if m==n and n==k:
                # Specialization for any small given 1 small dim.
               pass
        elif sm<16 or n<16 or k<16:
               pass
        else:
          # all > 16
              pass

        # do it again but with mm_recompile_hooks
        with torch._inductor.config.patch(mm_recompile_hooks={""mm"":mm_recompile_hook}):
            run_all()
            self.assertEqual(cnt.frame_count, 4)
```

The callable will force recompilations based on conditions specified. 
For instance, the code above would trigger three recompilations:
    1. When m == n == k (square matrices)
    2. When all dimensions (m, n, k) are greater than 16
    3. When at least one dimension is less than 16

TODO: Add controllers for other matrix operations (e.g., bmm, add_mm)



Note 1:  if we call this before tuned_mm we can avoid passing sizevars we can make it simpler by operating on symNodes.
Note 3:  We need a story for reconciling this with overriden hint.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 23:31:16+00:00,2025-09-16T23:31:14Z,,False,3,0,3,97,0,3,3,2025-09-16 23:30:37+00:00,28,2787,False,False,False,False,False,False,3,1,17,255,176,79,1,3,1.0,1.0,2025-09-16T17:47:24Z,pytorch
163019,closed,[functionalize] Avoid one more call to custom get_device on FunctionalTensorWrapper,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163019
* #162987

Trying to reduce the number of `__torch_dispatch__` calls of FakeTensorMode in the AOT metadata collection pass.",2025-09-15 23:28:10+00:00,2025-09-19T02:53:14Z,,False,3,0,3,1,1,1,3,2025-09-19 02:52:10+00:00,83,216,False,False,False,False,False,False,1,2,523,13286,9409,3877,1,3,5.0,4.0,2025-09-16T15:40:15Z,pytorch
163018,closed,Set the credential to upload vLLM nightly wheels on schedule and workflow_dispatch,huydhn,"The build is ok, but uploading is failing at the moment https://github.com/pytorch/pytorch/actions/runs/17734972779/job/50416387786
",2025-09-15 23:23:46+00:00,2025-09-16T22:27:27Z,,False,8,0,2,2,2,1,8,2025-09-16 22:26:24+00:00,82,132,False,False,False,False,False,False,1,7,1977,4,2,2,1,2,4.0,7.0,2025-09-16T00:14:53Z,pytorch
163017,closed,[Bugfix] Match eager stride semantics for cloned tensors with preserve_format in compile,Lucaskabela,"Fixes #161010 by making `clone_meta` match the semantics of strides for eager mode.  

This is: 
  * Case 1: Tensor is_non_overlapping_and_dense; in this case, stride should match input tensor stride
  * Case 2: Otherwise, stride should be contiguous computed from input tensor using `compute_elementwise_output_strides`


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",2025-09-15 23:21:18+00:00,2025-09-19T19:42:40Z,,False,5,3,6,86,9,3,8,2025-09-19 19:41:36+00:00,88,480,False,True,False,False,False,False,3,4,651,133,105,28,2,6,6.0,4.0,2025-09-15T23:21:55Z,pytorch
163016,closed,"[Reland] Return NoOpDeviceGuardImpl in replace of CudaDeviceGuard when device is not available, or cpu-only build",SherlockNoMad,"Reland of #160532


Summary: 

To support exporting a cuda model on a CPU-only machine under fake tensor mode. 
User commonly need to move sample inputs to the cuda device with .to(""cuda:0"") or .to(""cuda"") call. 
This diff supports this.
I expect the following pattern to work
```
with FakeTensorMode(allow_non_fake_inputs=True):
    cuda_module = module.to(""cuda:0"")
    cuda_sample_inputs = tuple([x.to(""cuda:0"") for x in sample_inputs])
    with torch.no_grad():
        ep = torch.export.export(cuda_module, cuda_sample_inputs)
```
",2025-09-15 22:45:16+00:00,2025-09-17T22:27:24Z,,False,9,0,2,207,4,6,9,2025-09-17 22:27:24+00:00,113,536,False,False,False,False,False,False,6,8,2453,319,261,58,1,2,4.0,9.0,2025-09-15T23:55:05Z,pytorch
163015,open,Add native_layer_norm_backwards to opinfo,PaliC,,2025-09-15 22:39:50+00:00,2025-09-16T00:36:13Z,,False,2,0,1,72,2,1,2,,41,0,False,False,False,False,False,False,1,1,105,74,72,2,1,1,1.0,1.0,2025-09-15T22:39:51Z,pytorch
163014,closed,[ROCm][CI] update _rocm-test.yml based on _linux-test.yml,jeffdaily,"Fixes missing huggingface secrets and aligns _rocm-test.yml with other updates from _linux-test.yml that it was initially based on.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 22:37:04+00:00,2025-09-16T02:15:44Z,,False,3,1,2,26,11,3,4,2025-09-16 02:14:41+00:00,57,406,False,True,False,False,False,False,3,2,935,49,32,17,1,2,3.0,3.0,2025-09-15T23:10:17Z,pytorch
163012,closed,[MPS] Add `embedding_bag` forward pass,kurtamohler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163012

Part of #162270

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 22:04:53+00:00,2025-09-17T19:01:54Z,,False,8,4,6,423,5,8,12,2025-09-17 19:00:50+00:00,38,312,False,False,False,False,False,False,8,5,1698,4867,3498,1369,1,6,5.0,5.0,2025-09-16T07:21:05Z,pytorch
163011,closed,"[MPS] zeros like, narrow and enable tests",Isalia20,"zeros like, narrow and enable tests for SparseMPS

cc @kulinseth @malfet @DenisVieriu97 @jhavukainen",2025-09-15 21:53:15+00:00,2025-09-16T17:49:10Z,,False,4,0,1,10,9,2,4,2025-09-16 17:48:07+00:00,41,100,False,False,False,False,False,False,2,2,493,19,10,9,1,1,2.0,2.0,2025-09-16T17:40:04Z,pytorch
163010,open,Fix empty tensor list handling in torch._foreach_ functions,dsashidh,"Fixes #162911 

PR Description:

This PR addresses an issue where PyTorch's torch._foreach_* functions (e.g., torch._foreach_sin, torch._foreach_neg, etc.) would raise a RuntimeError when passed an empty list of tensors. Instead of throwing an error, these functions now return an empty tuple when the input tensor list is empty, as expected.

Changes:

- I updated foreach_tensor implementation to handle empty tensor lists smoothly by returning an empty list.

- Verified that functions like torch._foreach_sin, torch._foreach_neg, and torch._foreach_abs behave correctly when there provided with an empty list.

Testing:

- I ran unit tests locally to ensure the correct behavior for both non-empty and empty tensor lists.

- Verified that the functions now return an empty tuple (()) for empty inputs, instead of raising an error.",2025-09-15 21:29:43+00:00,2025-09-18T18:49:27Z,,False,2,0,1,2,1,1,2,,59,834,False,True,False,False,False,False,1,1,283,3,2,1,1,1,2.0,2.0,2025-09-15T21:29:51Z,pytorch
163009,open,"[dynamo, 3.14] support some bytecodes, fix CALL_FUNCTION_EX",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* #163110
* #163109
* __->__ #163009
* #161839
* #161555
* #161838



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 21:21:15+00:00,2025-09-25T00:04:06Z,,False,1,0,3,106,45,7,1,,59,356,False,True,False,False,False,False,7,0,0,13979,10436,3543,1,3,,,,pytorch
163008,closed,Add decomp rule to assert_tensor_metadata for BatchedTensors ,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163008


Whenever there is device move, export introduces assert_tensor_metadata aten operator to make sure to guard for device specialization. This aten op didn't work with Vmap because we didn't register explicit decomp rule saying we just skip BatchedTensor and call it on underlying tensor 

Differential Revision: [D82483979](https://our.internmc.facebook.com/intern/diff/D82483979)",2025-09-15 21:18:56+00:00,2025-09-19T19:50:29Z,,False,21,2,4,18,0,2,23,2025-09-17 03:49:44+00:00,61,473,False,False,False,False,False,False,2,18,3863,40,29,11,1,4,5.0,19.0,2025-09-15T21:20:25Z,pytorch
163007,closed,[DeviceMesh] Simplifying internal bookkeeping with CuTe layout,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #163007
* #163006
* #163005
* #163004
* #163003
* #163002



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 21:17:51+00:00,2025-09-15T21:18:44Z,,False,1,0,1,572,433,5,1,2025-09-15 21:18:44+00:00,62,239,False,False,False,False,False,False,5,0,0,1005,572,433,1,1,,,,pytorch
163006,closed,[DeviceMesh] Introduce CuTe layout into devicemesh code base for internal bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 21:17:49+00:00,2025-09-15T21:18:35Z,,False,1,0,1,338,1,2,1,2025-09-15 21:18:35+00:00,85,189,False,False,False,False,False,False,2,0,0,339,338,1,1,1,,,,pytorch
163005,closed,"[CuTe] Change the logic of coalesce, complement from co to lexico",fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163007
* #163006
* __->__ #163005
* #163004
* #163003
* #163002

",2025-09-15 21:17:46+00:00,2025-09-15T21:18:27Z,,False,1,0,1,237,33,6,1,2025-09-15 21:18:27+00:00,65,144,False,False,False,False,False,False,6,0,0,270,237,33,1,1,,,,pytorch
163004,closed,Make CuTe layout as mesh layout to be ready for using in DeviceMesh,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163007
* #163006
* #163005
* __->__ #163004
* #163003
* #163002

",2025-09-15 21:17:43+00:00,2025-09-15T21:18:20Z,,False,1,0,1,71,0,1,1,2025-09-15 21:18:20+00:00,67,144,False,False,False,False,False,False,1,0,0,71,71,0,1,1,,,,pytorch
163003,closed,[CuTe] Add type for CuTe layout via claude,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163007
* #163006
* #163005
* #163004
* __->__ #163003
* #163002

",2025-09-15 21:17:40+00:00,2025-09-15T21:18:13Z,,False,1,0,1,178,107,4,1,2025-09-15 21:18:13+00:00,42,144,False,False,False,False,False,False,4,0,0,285,178,107,1,1,,,,pytorch
163002,closed,[CuTe] Copy code from pycute for device mesh bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163007
* #163006
* #163005
* #163004
* #163003
* __->__ #163002

",2025-09-15 21:17:37+00:00,2025-09-15T21:18:07Z,,False,1,0,1,1483,0,11,1,2025-09-15 21:18:07+00:00,56,144,False,False,False,False,False,False,11,0,0,1483,1483,0,1,1,,,,pytorch
163001,open,[CUDA13][CUDA Graphs] Fix `test_graph_external_wait_and_record` to account for minimum supported compute-capability,eqy,"CUDA 13 requires SM 7.5

cc @ptrblck @msaroufim @jerryzh168 @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng",2025-09-15 21:03:10+00:00,2025-09-18T14:17:11Z,,False,1,0,1,5,1,3,1,,115,110,False,True,False,False,False,False,3,0,0,6,5,1,1,1,,,,pytorch
163000,open,torch.distrubuted: lazy import pdb only when calling breakpoint(),kelu-wandb,"**DRAFT NOT YET READY**

Fixes #113775

It makes sense to import debugging libraries when actually using debugging tools.

This also avoids the following chain of imports in Python 3.13:
`torch` -> `torch.distributed` -> `rlcompleter` -> `readline`

Importing `readline`, in turn, attempts to access `stdin`, which deadlocks if run from a `subprocess` launched with `process_group=0` or `preexec_fn=setpgrp` because it doesn't have access to stdin.

## Testing

On Mac:

```shell
make setup-env PYTHON=python3.12
git submodule update --init --recursive
pip install -r .github/requirements/pip-requirements-macOS.txt
USE_DISTRIBUTED=1 python setup.py develop --cmake
```



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 21:02:59+00:00,2025-09-17T17:01:00Z,,False,6,0,1,15,12,1,6,,65,774,False,True,False,False,False,False,1,3,612,27,15,12,1,1,2.0,4.0,2025-09-15T21:52:57Z,pytorch
162999,open,[RFC]: No Distributed Log Spew,msaroufim,"## Background
This PR is meant to be an RFC to solve the logging spew problem that often occurs in PyTorch distributed where the same log statement occurs on each rank. Right now the main way we detect these problems is we rely on @stas00 to yell at us and when he does we'll make a patch like so https://github.com/pytorch/pytorch/pull/162764

Not really looking for feedback on code style or linting, will be up front and say Claude did a lot of the heavy lifting here. I'm looking for feedback on whether we think monkey patching logging, warning and print statements is a terrible idea or not. And if it's not a terrible idea should this be on by default or not

The core idea of that patch is to do something along the lines of

```python
if not torch.distributed.is_available() or not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:
	logger.log_stuff()
```

That way we're guaranteed to log only once on rank 0.

But why do we keep spewing logs by accident? Well because often we can detect log spew within a rank fairly easily because we just run `pytest test.py` and if we get overwhelmed with log statements some good BE citizen will fix it.

However for the most part we don't run all pytorch tests using `torchrun` and so log spew problems that affect distributed often won't get caught since only PTD devs and people training large models will see it

So the solution so this exists in various forms in the code base there are statements such
1. warn_once
2. if conditions on rank 0

But ideally ""all"" logging statements should be distributed aware and not log spew and historically the best techniques we have to resolve this problem is to rely on code review and/or code linters

 ## This PR

This PR has a different more intrusive take which is that we want all logging statements to be distributed aware, we don't want to use linters and we don't want to send a large volume of small PRs to audit and fix the distributed log spew problem.

In particular when we know that a user is doing distributed stuff i.e when they run `init_process_group` we patch the pytorch logger, python warnings module and optionally print statements.

The core idea is simple and just relies on monkey patching

```python
    if _is_non_rank_zero():
        return
    return orig_func(msg, *args, **kwargs)
return distributed_logging_func
```

If a logging statement is already distributed aware by printing only on rank 0, the above simply no-ops

## Other options

This solves the problem but feels ""magical"" so some other option is 
1. We don't do it in init_process group but instead do it globally when users enable it and we mention it every time a user complains about log spew
2. We create an authoritative distributed aware logger, send PRs for all the existing log and warn statements in pytorch and force their usage using a new linter in lintrunner

## Test plan
1. I have a unit test but idk how useful it is since testing for low spew across ranks is a bit janky using pytest and torchrun, I can brinig it back
3. demo_no_spew.py is nice to look at and simple
4. I reverted the changes I made for cpp extension log spew to prove the point
5. This PR might have some nasty side effects like changing user loggers, we shouldn't do that for a real solution


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @dcci 
",2025-09-15 20:57:30+00:00,2025-09-23T20:04:53Z,,False,12,0,15,109,7,4,12,,30,3383,False,True,False,False,False,False,4,11,10598,1938,1020,918,1,15,6.0,11.0,2025-09-16T00:42:43Z,pytorch
162998,closed,[ROCm] Remove HIPBLASLT_ALLOW_TF32 from codebase,xinyazhang,"A few UT failures are caused by `HIPBLASLT_ALLOW_TF32`

Fixes #157094 
Fixes #157093
Fixes #157092 
Fixes #157091 
Fixes #157064
Fixes #157063
Fixes #157062
Fixes #157061 
Fixes #157042
Fixes #157041
Fixes #157039
Fixes #157004

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-15 20:47:40+00:00,2025-09-18T13:54:56Z,,False,12,0,8,48,196,10,12,2025-09-18 13:53:58+00:00,48,557,False,True,False,False,False,False,10,9,2239,268,60,208,2,8,4.0,9.0,2025-09-15T20:52:31Z,pytorch
162997,open,[FP8][H100][TF32] Disable tf32 for emulated reference computation in `test_scaled_mm_vs_emulated_block_wise`,eqy,"Fails with 2 mismatches otherwise

cc @ptrblck @msaroufim @jerryzh168 @zasdfgbnm",2025-09-15 20:46:21+00:00,2025-09-18T14:17:05Z,,False,1,0,1,6,2,4,1,,108,80,False,False,False,False,False,False,4,0,0,8,6,2,1,1,,,,pytorch
162996,closed,Inspect schedule IR comms,H-Huang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162996

Small change to util to allow us to see comms (e.g. `SEND`, `RECV`, etc.) in the schedule IR

cc @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 20:24:13+00:00,2025-09-16T17:00:14Z,,False,3,0,3,14,3,1,3,2025-09-16 16:59:09+00:00,25,280,False,False,False,False,False,False,1,2,493,21049,15600,5449,1,3,3.0,2.0,2025-09-16T06:12:49Z,pytorch
162995,closed,[CUDA] fix shared memory race in `reduce_kernel`,eqy,"Reported by compute-sanitizer, otherwise it looks like `block_y_reduce` and `block_x_reduce` both use `shared_memory` for temporaries without synchronization between them

reproduces in e.g.,

`compute-sanitizer --tool=racecheck python test/test_matmul_cuda.py -k test_scaled_mm_vs_emulated_block_wise_float32_lhs_block_128_rhs_block_1_cuda` (note that this test requires H100 to run unless only the non-emulated (cuBLAS impl.) is commented out)

cc @ptrblck @msaroufim @jerryzh168",2025-09-15 20:23:41+00:00,2025-09-16T07:54:27Z,,False,3,0,1,1,0,1,3,2025-09-16 07:53:24+00:00,48,481,False,True,False,False,False,False,1,2,493,1,1,0,1,1,2.0,2.0,2025-09-16T05:12:21Z,pytorch
162994,closed,Fix closure var in functional export,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162994
* #162993
* #162992
* #162682
* #162559
* #162558
* #162557
* #162556
* #162487

Differential Revision: [D82478645](https://our.internmc.facebook.com/intern/diff/D82478645)


**Why this change is needed**

When exporting a module with Dynamo, closure variables from the original forward are normally converted into fast-locals for guard building. For example:

```
class Foo(torch.nn.Module):
    def forward(self, x, y):
        return x + y

global_list = []

class ReferenceControl:
    def __init__(self, mod):
        def hacked_up_forward(self_, x, y):
            self.bank.append(x.clone())
            self.bank_dict[""x""] = x.clone()
            global_list.append(x.clone())
            return x + y
        mod.forward = hacked_up_forward.__get__(mod, Foo)
```


Dynamo would compile with a frame like:

```
f_locals = {
    ""self_"": self.mod,
    ""x"": x,
    ""y"": y,
    ""self"": reference_control,
    ""global_list"": global_list,
}

```

But with our new wrapper, we see a frame like:
```
f_locals = {
    ""self"": wrapper_mod,
    ""flat_args"": flat_args,
}
```

Here we see two issues:

1. Collisions: ""self"" from the wrapper vs. ""self"" from the outer ReferenceControl.
2. Lost closure vars: global_list is a nonlocal closure and disappears when we wrap the module in a flat tracing wrapper.
When using a wrapper, these closure variables no longer belong to the wrapper’s scope, so Dynamo cannot see them. But Dynamo’s guard builder requires that they are both in f_locals and referenced in the bytecode.

**What this PR does**

_Unify self naming_
Rename the wrapper’s first argument to match the inner module’s forward signature. This handles cases where users deliberately renamed self to avoid clashes.

_Re-inject closure variables_
Extend the wrapper’s co_varnames with the inner module’s freevars and insert a no-op LOAD_FAST; STORE_FAST prologue after the initial RESUME.
This guarantees they appear in the instruction stream, so Dynamo can build guards on them.

_Seed f_locals_
Populate the renamed self and the injected freevars with the actual values from the inner closure, ensuring correct execution.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 20:18:12+00:00,2025-09-16T22:03:03Z,,False,4,0,3,161,16,2,4,2025-09-16 22:03:03+00:00,36,2392,False,True,False,False,False,False,2,2,320,14407,10683,3724,1,3,1.0,2.0,2025-09-15T20:28:07Z,pytorch
162993,closed,"Move export_db to use new tracer, remove restriction on optional inputs",tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* __->__ #162993
* #162992
* #162682
* #162559
* #162558
* #162557

Differential Revision: [D82478644](https://our.internmc.facebook.com/intern/diff/D82478644)",2025-09-15 20:18:05+00:00,2025-09-18T00:44:38Z,,False,8,0,5,20,16,3,8,2025-09-18 00:43:35+00:00,71,265,False,False,False,False,False,False,3,6,1276,19328,14257,5071,1,5,3.0,6.0,2025-09-15T20:28:01Z,pytorch
162992,closed,Make dynamo preserving stack trace work with inlined nn modules,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* #162993
* __->__ #162992
* #162682
* #162559
* #162558
* #162557

Differential Revision: [D82478646](https://our.internmc.facebook.com/intern/diff/D82478646)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 20:18:00+00:00,2025-09-18T00:44:31Z,,False,10,0,5,10,5,2,10,2025-09-18 00:43:26+00:00,63,437,False,False,False,False,False,False,2,8,1366,19307,14247,5060,1,5,3.0,8.0,2025-09-15T20:27:54Z,pytorch
162991,closed,[Flight Recorder][WP] Added mismatch tail as an arg,VieEeEw,"Summary:
Mismatch tail is used as a fixed variable and there are cases that there are more than 10 mismatches FR gives up producing results (e.g. https://fburl.com/ai_infra/7gjl5ucb). This diff added the mismatch tail in the parsed args so make this configuarble.


Also tho the variable name is `mismatch_tail`(last 10) it is used as `mismatch_head` (the first 10). Updated it to be `num_mismatch_to_print`

Test Plan:
`buck2 run @//mode/opt //caffe2/fb/flight_recorder:fr_trace -- --mast_job_id aps-ctx_fm_pipeline_change-1c8ea38a94 --mast_job_version 0 --mast_job_attempt 2 --bucket tlcm_log_blob --world_size 128 --dump_file_name_offset 0 --allow-incomplete-ranks --num_mismatch_to_print 20 1>out 2>err`
Confirm no error and output 20 mismatches.

Rollback Plan:

Differential Revision: D82335995


",2025-09-15 19:48:37+00:00,2025-09-16T04:47:12Z,,False,8,0,1,9,3,2,8,2025-09-16 04:46:09+00:00,51,803,False,True,False,False,False,False,2,1,476,12,9,3,1,1,2.0,1.0,2025-09-15T20:36:44Z,pytorch
162990,open,DTensor: C++ compute_global_tensor_info,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163667
* __->__ #162990
* #163030
* #162968
* #162508
* #161695

compute_global_tensor_info is on the hot path for DTensor.{from,to}_local. More incremental progress toward C++.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-15 19:45:43+00:00,2025-09-24T02:49:45Z,,False,5,7,14,201,103,8,12,,39,571,False,False,False,False,False,False,8,4,2161,19446,14203,5243,1,14,3.0,4.0,2025-09-16T16:44:12Z,pytorch
162988,open,[CI][Distributed][CUDA][Symm-Mem] Enable B200 Symm Mem Test,nWEIdia,"Inspired by https://github.com/pytorch/pytorch/pull/162981 and motivated by https://github.com/pytorch/pytorch/pull/159323 taking a total of 20 hours to finish (and unlikely to make it in short time due to https://github.com/pytorch/pytorch/issues/162178 ) 

Creating this subtest to get *something distributed* on B200. 

cc @ptrblck @eqy @tinglvv @malfet @atalman @huydhn @ZainRizvi @kwen2501 ",2025-09-15 19:01:14+00:00,2025-09-17T10:20:51Z,,False,4,0,3,62,0,2,4,,59,395,False,False,False,False,False,False,2,3,548,64,63,1,1,3,3.0,3.0,2025-09-15T19:55:01Z,pytorch
162987,closed,[functional] Use the saved device on storage instead for device_custom,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163019
* __->__ #162987

Trying to reduce the number of __torch_dispatch__ calls of FakeTensorMode in the AOT metadata collection pass.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 19:00:48+00:00,2025-09-18T23:46:06Z,,False,3,0,3,13,10,2,3,2025-09-18 23:43:23+00:00,70,387,False,False,False,False,False,False,2,2,572,13289,9412,3877,1,3,5.0,4.0,2025-09-16T15:39:35Z,pytorch
162986,open,[functional] Use the saved device on storage instead for device_custom,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-09-15 19:00:02+00:00,2025-09-15T21:20:40Z,,False,2,0,1,4,1,1,2,,70,94,False,False,False,False,False,False,1,0,0,5,4,1,1,1,,,,pytorch
162984,closed,[BE] Adding aliases for CUDA and XPU API documentation,jiannanWang,"This PR reorganizes CUDA and XPU API documentation with additional aliases pages. Multiple entries of APIs under torch.cuda are thus removed.
",2025-09-15 18:25:35+00:00,2025-09-21T22:29:36Z,,False,8,0,1,89,39,5,8,2025-09-21 22:28:31+00:00,54,142,False,False,False,True,False,False,5,7,2464,128,89,39,1,1,3.0,8.0,2025-09-16T19:08:03Z,pytorch
162983,closed,[precompile] Store traced file information with CompileArtifacts.,zhxchen17,"Summary:
Add some metadata to CompileArtifacts, so that it contains the source code information about the original code while they are being traced.

For now, we will not provide a verification method to end user and instead we just provide which files are inlined. It's up to user to verify the content from these files are not changed (because it's optional for many users to validate source code changes anyway in aot precompile)

Test Plan:
buck run @mode/opt test/dynamo:test_dynamo -- -k test_file_change
buck run @mode/opt test/dynamo:test_dynamo -- -k test_aot_compile_source_info

Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 18:15:18+00:00,2025-09-16T18:28:54Z,,False,3,2,1,79,23,4,5,2025-09-16 18:27:51+00:00,65,782,False,True,False,False,False,False,4,2,493,102,79,23,1,1,3.0,2.0,2025-09-15T20:49:01Z,pytorch
162982,closed,[doc]: Small typos,joshuamarkovic,"Small typo fixes

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @mingfeima @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @SherlockNoMad @voznesenskym @penguinwu @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-15 18:14:18+00:00,2025-09-16T17:43:24Z,,False,6,0,3,43,43,38,6,2025-09-16 17:42:21+00:00,18,472,False,True,False,True,False,False,38,3,510,4735,3509,1226,2,3,3.0,3.0,2025-09-15T22:37:22Z,pytorch
162981,closed,[SymmMem] Not use run_test.py in CI,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162981

run_test.py can mute errors in test (mechanism unknown, issue filed #162978).
Replacing it with bare pytest in SymmMem CI commands.",2025-09-15 18:00:00+00:00,2025-09-16T01:17:21Z,,False,2,1,1,4,4,1,3,2025-09-16 01:17:21+00:00,35,225,False,False,False,False,False,False,1,1,392,8,4,4,1,1,3.0,2.0,2025-09-15T18:37:56Z,pytorch
162979,open,[POC] Print backtraces of where unwaited work was allocated,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162979

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-15 17:57:28+00:00,2025-09-15T20:33:25Z,,False,2,0,1,40,1,5,2,,59,245,False,False,False,False,False,False,5,1,82,41,40,1,1,1,1.0,1.0,2025-09-15T20:33:25Z,pytorch
162977,closed,fix deterministic scatter_add path for multi-d tensors,pytorchbot,PReviously for more than 2d tensor `select` didn't work correctly. ,2025-09-15 17:33:04+00:00,2025-09-17T00:17:36Z,2025-09-17T00:17:36Z,True,1,0,1,5,4,2,1,2025-09-17 00:17:36+00:00,54,67,False,True,False,False,False,False,2,0,0,9,5,4,1,1,1.0,0.0,2025-09-17T00:17:23Z,pytorch
162975,closed,Add to inductor provenance tracking doc,yushangdi,"As title 

cc @mlazos @desertfire @chenyang78 @penguinwu @benjaminglass1",2025-09-15 17:04:50+00:00,2025-09-16T19:10:13Z,,False,3,0,1,19,7,5,3,2025-09-16 19:09:09+00:00,39,72,False,False,False,True,False,False,5,2,495,26,19,7,1,1,4.0,2.0,2025-09-15T17:56:27Z,pytorch
162974,closed,Workaround for mtia double init issue in has_triton,srostrup,"Summary:
This change adds a new environment variable (`TORCHINDUCTOR_TRITON_DISABLE_DEVICE_DETECTION`) and configuration in `torch._inductor.config` which can be set to `""1""` to allow a user to disable triton's device detection logic in [torch/utils/_triton.py:has_triton()](https://github.com/pytorch/pytorch/blob/c9e57d7e9f326e427fc4ae5c318fd017cd4b75a9/torch/utils/_triton.py#L128). This function is used at import scope in several places but the function has a side effect of initializing the mtia device if it is available which is causing some of our autotuning workflows to crash. 

Worth noting that when enabled this configuration disables all device detection not just mtia and this is because the logic in has_triton will initialize the mtia device as a side effect even when checking for a cuda or other device via the [get_interface_for_device()](https://github.com/pytorch/pytorch/blob/c9e57d7e9f326e427fc4ae5c318fd017cd4b75a9/torch/_dynamo/device_interface.py#L570) function.

I've tagged it `topic: not user facing` since I don't anticipate any outside of meta users making use of this, however this is my first PR here, so please indicate if it should be handled differently.

Test Plan: This has been tested in the context of internal workflows.

Differential Revision: D82347853




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 17:02:46+00:00,2025-09-16T04:47:16Z,,False,8,0,1,10,0,2,8,2025-09-16 04:46:14+00:00,51,1503,False,False,False,False,False,False,2,1,476,10,10,0,1,1,2.0,1.0,2025-09-15T20:44:33Z,pytorch
162972,closed,compile_kernel enable pch,msaroufim,"Enabling automatic pre compiled headers per https://docs.nvidia.com/cuda/nvrtc/index.html#example-automatic-pch-cuda-12-8

I'm seeing large speedups in compilation times using PCH on average but the max compilation time with PCH is worst which is why I can't enable it by default. `load_inline()` also supports precompiled headers and does not enable them by default

```
Without PCH: 270.58 ms average
With PCH:    115.27 ms average
```

```
Without PCH: Max: 337.99 ms
With PCH: Max: 383.82 ms
```

```python
source) [marksaroufim@devgpu005]~/pytorch% python simple_pch_benchmark.py
============================================================
Simple PCH Compilation Benchmark
============================================================
Device: NVIDIA B200
Iterations: 100

Testing WITHOUT PCH:
------------------------------
Compiling kernel 100 times WITHOUT PCH...
  Completed 10/100 compilations
  Completed 20/100 compilations
  Completed 30/100 compilations
  Completed 40/100 compilations
  Completed 50/100 compilations
  Completed 60/100 compilations
  Completed 70/100 compilations
  Completed 80/100 compilations
  Completed 90/100 compilations
  Completed 100/100 compilations
Average: 270.58 ms (±6.99 ms)
Min: 264.09 ms
Max: 337.99 ms

Testing WITH PCH:
------------------------------
Compiling kernel 100 times WITH PCH...
  Completed 10/100 compilations
  Completed 20/100 compilations
  Completed 30/100 compilations
  Completed 40/100 compilations
  Completed 50/100 compilations
  Completed 60/100 compilations
  Completed 70/100 compilations
  Completed 80/100 compilations
  Completed 90/100 compilations
  Completed 100/100 compilations
Average: 115.27 ms (±27.32 ms)
Min: 110.65 ms
Max: 383.82 ms

```


## Benchmarking script

```python
#!/usr/bin/env python3
import argparse
import os
import sys
import time
from statistics import mean, stdev

import torch
from torch.cuda._utils import _nvrtc_compile


def benchmark_compilation(use_pch, iterations=100):
    """"""Compile the same kernel many times with or without PCH.""""""

    # CUB kernel that benefits from PCH
    kernel_source = """"""
    #include <cub/block/block_reduce.cuh>
    #include <cub/block/block_scan.cuh>
    #include <cub/warp/warp_reduce.cuh>

    extern ""C""
    __global__ void test_kernel(const float* input, float* output, int n) {
        using BlockReduce = cub::BlockReduce<float, 256>;
        using BlockScan = cub::BlockScan<float, 256>;
        using WarpReduce = cub::WarpReduce<float>;

        __shared__ union {
            typename BlockReduce::TempStorage reduce;
            typename BlockScan::TempStorage scan;
            typename WarpReduce::TempStorage warp[8];
        } temp_storage;

        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        float val = (idx < n) ? input[idx] : 0.0f;

        float sum = BlockReduce(temp_storage.reduce).Sum(val);
        __syncthreads();

        float scan_result;
        BlockScan(temp_storage.scan).ExclusiveSum(val, scan_result);
        __syncthreads();

        int warp_id = threadIdx.x / 32;
        float warp_sum = WarpReduce(temp_storage.warp[warp_id]).Sum(val);

        if (threadIdx.x == 0) {
            output[blockIdx.x] = sum + scan_result + warp_sum;
        }
    }
    """"""

    device = torch.cuda.current_device()
    major, minor = torch.cuda.get_device_capability(device)
    compute_capability = f""{major}{minor}""

    compile_times = []

    print(
        f""Compiling kernel {iterations} times {'WITH' if use_pch else 'WITHOUT'} PCH...""
    )

    for i in range(iterations):
        # Use unique kernel name to avoid caching between iterations
        kernel_name = f""test_kernel_{i}""
        unique_source = kernel_source.replace(""test_kernel"", kernel_name)

        start = time.perf_counter()

        ptx, mangled_name = _nvrtc_compile(
            unique_source,
            kernel_name,
            compute_capability,
            header_code="""",
            nvcc_options=[""-std=c++17""],
            auto_pch=use_pch,
        )

        elapsed = time.perf_counter() - start
        compile_times.append(elapsed * 1000)  # Convert to ms

        # Progress indicator
        if (i + 1) % 10 == 0:
            print(f""  Completed {i + 1}/{iterations} compilations"")

    return compile_times


def main():
    parser = argparse.ArgumentParser(description=""Simple PCH Compilation Benchmark"")
    parser.add_argument(""--pch"", action=""store_true"", help=""Test with PCH only"")
    parser.add_argument(""--no-pch"", action=""store_true"", help=""Test without PCH only"")
    parser.add_argument(
        ""--iterations"", type=int, default=100, help=""Number of compilations""
    )
    args = parser.parse_args()

    print(""="" * 60)
    print(""Simple PCH Compilation Benchmark"")
    print(""="" * 60)
    print(f""Device: {torch.cuda.get_device_name()}"")
    print(f""Iterations: {args.iterations}"")
    print()

    # Determine what to test
    test_both = not args.pch and not args.no_pch

    results = {}

    # Test without PCH
    if args.no_pch or test_both:
        print(""Testing WITHOUT PCH:"")
        print(""-"" * 30)
        times_no_pch = benchmark_compilation(use_pch=False, iterations=args.iterations)

        if times_no_pch:
            avg_no_pch = mean(times_no_pch)
            std_no_pch = stdev(times_no_pch) if len(times_no_pch) > 1 else 0
            print(f""Average: {avg_no_pch:.2f} ms (±{std_no_pch:.2f} ms)"")
            print(f""Min: {min(times_no_pch):.2f} ms"")
            print(f""Max: {max(times_no_pch):.2f} ms"")
            results[""no_pch""] = avg_no_pch
        print()

    # Test with PCH
    if args.pch or test_both:
        print(""Testing WITH PCH:"")
        print(""-"" * 30)
        times_with_pch = benchmark_compilation(
            use_pch=True, iterations=args.iterations
        )

        if times_with_pch:
            avg_with_pch = mean(times_with_pch)
            std_with_pch = stdev(times_with_pch) if len(times_with_pch) > 1 else 0
            print(f""Average: {avg_with_pch:.2f} ms (±{std_with_pch:.2f} ms)"")
            print(f""Min: {min(times_with_pch):.2f} ms"")
            print(f""Max: {max(times_with_pch):.2f} ms"")
            results[""pch""] = avg_with_pch
        print()

if __name__ == ""__main__"":
    main()

```

cc @gau-nernst ",2025-09-15 16:39:55+00:00,2025-09-15T22:56:43Z,,False,7,1,14,9,0,1,8,2025-09-15 22:55:42+00:00,25,6270,False,False,False,True,False,False,1,6,1430,1493,751,742,1,14,4.0,7.0,2025-09-15T19:49:58Z,pytorch
162969,closed,replace more // with FloorDiv in inductor code,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163124
* __->__ #162969


see this https://github.com/pytorch/pytorch/pull/162869 for more context, sympy div representation can make reasoning fail.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 16:09:41+00:00,2025-09-18T03:29:37Z,,False,7,0,5,8,6,1,7,2025-09-18 03:28:34+00:00,46,432,False,False,False,False,False,False,1,6,887,24750,18475,6275,1,5,5.0,6.0,2025-09-16T17:38:42Z,pytorch
162968,closed,Add basic tests for torch.distributed.tensor._utils.compute_global_tensor_info,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162990
* #163030
* __->__ #162968
* #162508
* #161695

Next PR writes a C++ implementation. Seems good to have tests first.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-15 16:03:06+00:00,2025-09-23T14:57:42Z,,False,6,0,7,54,1,1,6,2025-09-23 14:56:35+00:00,78,305,False,False,False,False,False,False,1,5,1401,18561,13596,4965,1,7,3.0,5.0,2025-09-18T04:09:03Z,pytorch
162967,open,Revise FindOpenBLAS.cmake logic to allow overriding location,directhex,"CMake has a specific resolution order for `find_library()`:

1. <PackageName>_ROOT variables specified via command line or environment, in various permutations, only when inside a `find_package()`
2. `CMAKE`-prefixed defines like `CMAKE_LIBRARY_PATH` passed via `-DCMAKE_LIBRARY_PATH`
3. same, but with env vars
4. paths specified by the `HINTS` option
5. some env vars like `PATH` and `LIB`
6. standard system paths like `/usr/lib/x86_64-linux-gnu` and `/usr/local/lib`
7. paths specified by the `PATHS` option

See https://cmake.org/cmake/help/latest/command/find_library.html

The first match is used, meaning a match discovered in step 2 will stop all further efforts to find a match, and within any option with multiple parameters (like paths specified in `HINTS`), the first matching location works.

All this means it is impossible to override a distro-provided OpenBLAS found in /usr/lib, with an optimized version in a location specified by `${OpenBLAS_HOME}`.

This change a) swaps the content of `Open_BLAS_LIB_SEARCH_PATHS` to consider the env var override before the hardcoded list of potential folders, and b) in the event that an env var override is specified, disables all default location searching such that system paths are not considered.

Similar rules apply for `find_path()`, used for finding the headers, and a similar resolution to allow overriding is applied.",2025-09-15 15:33:36+00:00,2025-09-18T01:31:39Z,,False,3,0,1,17,9,1,3,,60,1385,False,True,False,False,False,False,1,1,42,26,17,9,1,1,1.0,1.0,2025-09-15T15:45:09Z,pytorch
162966,closed,bb/test,roin-orca,,2025-09-15 15:33:00+00:00,2025-09-16T16:01:44Z,,False,3,0,1,1,2,2,3,2025-09-15 15:34:02+00:00,7,0,False,False,False,False,False,False,2,0,0,3,1,2,1,1,,,,pytorch
162965,closed,[ROCm][benchmark] Add HF LLM benchmark expected accuracy,amdfaa,"PR #156967 added HF LLM benchmarks but did not add the ci expected accuracy files for ROCm.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 15:20:57+00:00,2025-09-15T22:46:53Z,,False,7,0,1,200,0,10,7,2025-09-15 18:04:43+00:00,56,377,False,False,False,False,False,False,10,6,2378,200,200,0,1,1,2.0,6.0,2025-09-15T16:55:19Z,pytorch
162963,closed,[BE] Slight improvements to documentation in python_dispatch,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162963

I was briefly confused which way I should iterate stack, here's the
comments I wanted.

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-15 14:55:59+00:00,2025-09-21T01:46:53Z,,False,12,0,5,15,7,3,12,2025-09-21 01:45:49+00:00,60,333,False,False,False,True,True,False,3,11,3447,98163,61331,36832,1,5,4.0,12.0,2025-09-15T15:15:10Z,pytorch
162961,closed,[BE] Improve pytest summary display for OpInfo tests,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162961

pytest summarizes test failures by printing a truncated first line of the test of the OUTERMOST wrapped exception.

Prior to this PR, it looked like this:

```
FAILED [0.0454s] test/distributed/tensor/test_dtensor_ops.py::TestLocalDTensorOpsCPU::test_dtensor_op_db_H_cpu_float32 - Exception: Caused by sample input at index 0: SampleInput(input=Tensor[size=(12, 12), device=""cpu"", dtype=torch.float32], args=(), kwargs={}, ...
```

I argue this is not so useful.  If I have a lot of test failures, I look to the test summary to understand what /kind/ of errors I have, so I can assess which ones I should look at first.  In other words, this is better:

```
FAILED [0.1387s] test/distributed/tensor/test_dtensor_ops.py::TestLocalDTensorOpsCPU::test_dtensor_op_db__softmax_backward_data_cpu_float32 - Exception: Tensor-likes are not close!
```

Now I know specifically this is a numerics problem!

This PR does it by prepending the old exception text to the wrapped exception.  This is slightly redundant, as we are exception chaining, but it does the job.  Open to bikeshedding.

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-15 14:40:27+00:00,2025-09-15T20:21:01Z,,False,3,0,1,2,2,2,3,2025-09-15 19:58:22+00:00,52,1325,False,False,False,False,True,False,2,2,585,4,2,2,1,1,3.0,3.0,2025-09-15T15:22:10Z,pytorch
162960,closed,[BE] Use init_device_mesh over DeviceMesh,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162960

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-15 13:46:16+00:00,2025-09-17T06:13:25Z,,False,6,0,3,2,2,1,6,2025-09-17 06:12:21+00:00,41,245,False,False,False,False,False,False,1,5,1401,98185,61338,36847,1,3,6.0,5.0,2025-09-15T14:01:26Z,pytorch
162959,closed,[export] handling NamedTuple inputs,Raman-RH,"Fixes #160547
### Summary:
bug
```
    def test_namedtuple(self):
        from collections import namedtuple
        Point = namedtuple('Point', 'x y')
        
        class M(torch.nn.Module):
            def forward(self, x, y):
                return x + y 
        
        inp = Point(torch.ones(3), torch.ones(3))
        print(M()(*inp))
        
        # errors
        ep = torch.export.export(M(), inp, strict=False)
        print(ep)

        # succeeds
        ep = torch.export.export(M(), inp, strict=True)
        print(ep)

        # workaround could be to convert namedtuple to a kwarg
        inp_kwargs =  {field: getattr(inp, field) for field in inp._fields}
        ep = torch.export.export(M(), (), inp_kwargs)
        print(ep)
```
FIx :
namedtuple is subclass of tuple 
but namedtuple is not expected 
So, this change handles named tuple case

I have added 🧪 test case for this as well ",2025-09-15 13:32:32+00:00,2025-09-23T17:44:58Z,,False,6,2,2,23,0,2,8,2025-09-23 17:43:54+00:00,35,912,False,True,False,False,False,False,2,5,921,25,24,1,2,2,4.0,6.0,2025-09-22T12:19:49Z,pytorch
162956,closed,[ATen][CUDA] CUTLASS matmuls: add sm_103a flag ,Aidyn-A,"This PR adds an `sm_103a` flag for GroupMM and RowwiseScaledMM. Contrary to just #161399, this simply adds the flag as the support for `sm_103a` matmuls is going to be added to CUTLASS v4.2 (see https://github.com/pytorch/pytorch/pull/161399#issuecomment-3252892937).

cc @ptrblck @msaroufim @eqy @jerryzh168 @manuelcandales @SherlockNoMad @angelayi",2025-09-15 11:26:47+00:00,2025-09-16T10:31:03Z,,False,6,0,2,8,2,1,6,2025-09-16 10:30:00+00:00,47,349,False,False,False,False,False,False,1,5,1426,12,9,3,1,2,4.0,6.0,2025-09-15T15:53:23Z,pytorch
162955,open,use float as the intermidiate type in cpu rsqrt operator of float16 a…,wangtianxia-sjtu,"…nd bfloat16

Fixes #162907


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-15 11:17:11+00:00,2025-09-18T07:20:01Z,,False,4,3,3,27,2,3,7,,70,111,False,True,False,False,False,False,3,1,49,33,29,4,1,3,2.0,1.0,2025-09-16T08:25:11Z,pytorch
162954,open,[ZENDNN] Integrate ZenDNN binary-binary fusions,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162954
* #161512
* #161495
* #161158
* #161157
* #161156
* #161155

----

- Add zendnn binary-binary fusions support.
- Add a pass in optimize for the binary-binary ops.
- Update meta registration and shim files.
- Add a condition for typecast nodes.

Co-authored-by: Priyansh Jain <priyansh.jain2@amd.com>
Co-authored-by: Mrigank Srivastava <Mrigank.Srivastava@amd.com>
Co-authored-by: Dinesh Mareedu <Dinesh.Mareedu@amd.com>
Change-Id: Ibfafa31b82b2172ce049a5183d4833c6423d0ded

[RFC](https://github.com/pytorch/pytorch/issues/150296)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 10:26:59+00:00,2025-09-18T20:03:54Z,,False,5,0,2,383,8,11,5,,47,825,False,False,False,False,False,False,11,2,102,5742,4325,1417,1,2,2.0,2.0,2025-09-16T15:32:08Z,pytorch
162953,open,Fix Metadata object loses arbitrary attributes added dynamically,DamJanusz,"Fixes #162948 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @ankitageorge 
",2025-09-15 09:51:45+00:00,2025-09-16T18:49:40Z,,False,1,0,1,1,1,1,1,,64,133,False,True,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-15T14:42:53Z,pytorch
162951,closed,[Intel GPU][pre_compile] Add XPU toolkit version and hardware info in compiled model check.,etaf,"Following #162438, this PR generalized the origin CUDA only check, and add XPU check.

Fixes #162939, Fixes #162938, Fixes #163032，Fixes #163045





cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-15 08:52:17+00:00,2025-09-18T00:05:30Z,,False,3,0,1,50,46,5,3,2025-09-18 00:04:29+00:00,91,320,False,True,False,False,False,False,5,2,493,96,50,46,1,1,4.0,2.0,2025-09-15T15:05:40Z,pytorch
162950,closed,[CUDA] revert PR 130472,thenumberouscode,"This change may also resolve https://github.com/pytorch/pytorch/issues/161789, though verification is still needed.

PR #130472 would introduced the problem of  freeing the same address without clean metadata. according to the below discussion, reverted it.",2025-09-15 08:51:00+00:00,2025-09-22T22:53:25Z,,False,26,16,2,17,105,4,42,2025-09-19 19:50:48+00:00,23,257,False,False,False,False,False,False,4,24,27847,126,19,107,1,2,6.0,26.0,2025-09-15T08:52:51Z,pytorch
162947,closed,[xla hash update] update the pinned xla hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned xla hash.",2025-09-15 07:41:47+00:00,2025-09-15T11:43:07Z,,False,3,0,1,1,1,1,3,2025-09-15 11:42:04+00:00,44,155,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-15T07:41:48Z,pytorch
162946,closed,Update slow tests,pytorchupdatebot,"This PR is auto-generated weekly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/weekly.yml).
Update the list of slow tests.",2025-09-15 07:40:59+00:00,2025-09-15T11:32:49Z,,False,3,0,1,247,242,1,3,2025-09-15 11:31:41+00:00,17,156,False,False,False,False,False,False,1,2,493,489,247,242,1,1,2.0,2.0,2025-09-15T07:41:07Z,pytorch
162945,open,[DO NOT REVIEW] Test inductor,RUIJIEZHONG66166,"For test inductor ut
",2025-09-15 07:40:42+00:00,2025-09-19T12:22:44Z,,False,3,0,1,1,0,1,3,,29,21,False,False,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
162944,open,[Intel GPU] Fix conv1d precision error,yucai-intel,"Currently, conv1d converts the 3D view to 4D before calling onednn::convolution(). 
However, this function converts the 4D tensor to a channel-last memory format for computation, resulting in incorrect return results (the correct result should be channel-first).
This PR fixes this issue, ensuring that the output return value format is consistent with the expected format.
cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-15 07:17:42+00:00,2025-09-17T02:24:42Z,,False,2,0,3,7,2,3,2,,38,455,False,True,False,False,False,False,3,1,410,9,7,2,1,3,2.0,2.0,2025-09-15T07:19:15Z,pytorch
162943,open,[ROCM]enable TF32 config in MIOpen,yingluAMD,"MI30X series GPGPU support TF32 gemm compute while this feature is not introduced into conv now.
MIOpen is enabling this feature ([PR:1414](https://github.com/ROCm/rocm-libraries/pull/1414)). Corresponding config will be also applied to MIOpen backend just like cudnn does( Refer to  https://docs.pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices).

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-15 07:13:57+00:00,2025-09-15T07:20:51Z,,False,4,0,3,1725,946,11,4,,34,581,False,False,True,True,False,False,11,0,0,2713,1746,967,2,3,,,,pytorch
162942,closed,[torch][cuda][device_limits] Library for querying device hardware limits for flops and bandwidth,valentinandrei,"In various benchmarks scattered across the repo, the limits for flops/second and memory bandwidth are usually hardcoded for a single device. This utility could help in providing a more structured way to query the device capabilities. If this is approved, we can use it when reporting flops efficiency and bandwidth relative to peak in the benchmarks and tests. The intent is to add more devices, more parameters (e.g. L2 cache bandwidth, NVLink, etc.) for both CPUs and accelerators.

Testing:

```
import torch

if torch.cuda.is_available():
    device = torch.cuda.current_device()
    mod = torch.get_device_module('cuda')
    hw = mod._device_limits.GPULimits(device)

    print(hw.get_tflops_per_second(torch.float16))
    print(hw.get_tflops_per_second(torch.float32))
    print(hw.get_tflops_per_second(torch.float64))
    print(hw.get_tflops_per_second(torch.bfloat16))
    print(hw.get_tflops_per_second(torch.int8))
    print(hw.get_memory_bandwidth_Bps() / 1e9)
    print(hw.get_shared_memory_bandwidth_Bps() / 1e9)

# Output on an H100 GPU
1070.53056
535.26528
66.90816
1070.53056
2141.06112
4893.696
33454.08
```",2025-09-15 07:01:48+00:00,2025-09-23T14:48:03Z,,False,18,11,34,163,2,4,29,2025-09-23 04:48:23+00:00,96,1125,False,False,False,False,False,False,4,17,5209,25933,16271,9662,2,30,9.0,20.0,2025-09-15T07:03:26Z,pytorch
162941,open,[BE] refine import sympy,thenumberouscode,"I've consolidated all the scattered import sympy statements in sym_node.py into a single class. This makes the code cleaner and ensures sympy is only imported once, improving efficiency.


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-15 06:59:54+00:00,2025-09-25T07:48:52Z,,False,7,2,1,61,85,1,9,,24,245,False,False,False,False,False,False,1,6,2067,146,61,85,1,1,3.0,8.0,2025-09-15T18:48:45Z,pytorch
162936,closed,Add api info for torch._C._nn.pyi,orangeH25,"Fix part of #148404 

APis involved are as followed:

- silu
- silu_
- smooth_l1_loss
- soft_margin_loss",2025-09-15 03:53:16+00:00,2025-09-24T04:57:03Z,,False,9,0,1,41,0,1,9,2025-09-24 04:56:00+00:00,33,104,False,True,False,False,False,False,1,8,2084,41,41,0,1,1,3.0,8.0,2025-09-15T06:03:41Z,pytorch
162935,closed,[Release 2.9] Update torch-xpu-ops commit pin,CuiYifeng,"Update the torch-xpu-ops commit to [intel/torch-xpu-ops@f8408a](https://github.com/intel/torch-xpu-ops/commit/f8408a642da568051ab82e20f2947b89e491fbeb):

- Revert ""Roll back to original usage of sycl::get_kernel_bundle"" to fix a 70% performance regression for a particular model.

Refer https://github.com/intel/torch-xpu-ops/issues/2028#issuecomment-3284057754",2025-09-15 03:34:29+00:00,2025-09-17T00:19:31Z,2025-09-17T00:19:31Z,True,2,0,1,1,1,1,2,2025-09-17 00:19:31+00:00,45,361,False,True,False,False,False,False,1,1,15,2,1,1,1,1,3.0,1.0,2025-09-15T03:35:01Z,pytorch
162933,closed,[Fix XPU CI][Inductor UT] Fix test cases broken by community.,etaf,"Fixes #162937

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-15 02:51:43+00:00,2025-09-17T05:36:13Z,,False,9,0,1,17,7,4,9,2025-09-17 05:35:10+00:00,61,216,False,True,False,False,False,False,4,8,1850,24,17,7,1,1,4.0,8.0,2025-09-15T15:00:29Z,pytorch
162932,closed,"Skip empty tests, they don't make sense for numerics",ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162932

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-15 02:13:12+00:00,2025-09-15T06:21:33Z,,False,3,0,1,5,4,1,3,2025-09-15 06:20:29+00:00,52,245,False,False,False,False,False,False,1,2,493,9,5,4,1,1,3.0,2.0,2025-09-15T02:22:55Z,pytorch
162929,open,Bumping up FBGEMM submodule,sampathvic,We added this https://github.com/pytorch/FBGEMM/pull/4860 in FBGEMM repo. Hence syncing up the latest from FMGEMM repo to pytorch FBGEMM submodule.,2025-09-15 00:48:09+00:00,2025-09-20T18:34:59Z,,False,1,0,1,4,4,4,1,,27,147,False,False,False,False,False,False,4,0,0,8,4,4,1,1,,,,pytorch
162928,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-15 00:27:59+00:00,2025-09-16T04:26:08Z,,False,6,0,1,1,1,1,6,2025-09-16 04:25:06+00:00,46,156,False,False,False,False,False,False,1,5,1389,2,1,1,1,1,2.0,5.0,2025-09-15T00:28:00Z,pytorch
162927,closed,Fix: ShapeEnv not propagated properly to inductor SizeVars,laithsakka,"Summary:
I am really skeptical about inductor sizevars creating an empty shape env when not provided with one
i think we should fail there if the graph has dynamic shapes and no shape env is provided. 

however i wonder if there are actually use cases that depends on the shape env not being there?
Reasoning APIs depends on facts in the shape env. and assumes some stuff exists for specific symbols.

Test Plan:
Fix the bug reported in creating simple e2e unit test is not trivial
https://www.internalfb.com/diff/D82337184

Rollback Plan:

Differential Revision: D82412384




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-14 23:44:57+00:00,2025-09-18T00:57:27Z,,False,13,0,1,17,1,5,13,2025-09-18 00:56:25+00:00,58,779,False,True,False,False,False,False,5,2,696,18,17,1,1,1,5.0,4.0,2025-09-16T15:10:59Z,pytorch
162926,closed,[MTIA] [PyTorch] [Export] Allow setting names for lifted constants before tracing.,patrick-toulme,"Summary:
The lift_constants_pass will set the name of lifted to constants to module_path_{lifted_tensor_counter}. This makes it very difficult for a compiler or runtime to map certain constants to what the placeholder value is in the IR. 

This PR adds a new API to Torch Export allowing users to set the names of certain tensors. The lift_constants_pass then respects this name. 

This is not supported for strict=True or for naming FakeTensors.

```
        tensor = torch.tensor([1, 2, 3])
        result = torch.export._name_tensor_constants.set_tensor_name(tensor, ""my_custom_tensor"")
        self.assertIs(result, tensor)  # Should return the same tensor
```

Before:
```
 %model_layers_0_self_attn_attn_lifted_tensor_0 : bf16[2, 346732, 16, 8, 64][num_users=2] = placeholder[target='model_layers_0_self_attn_attn_lifted_tensor_0']
```

```
 kv_cache = torch.export._name_tensor_constants.set_tensor_name(kv_cache, ""kv_cache"")
```

After:


```
%model_layers_0_self_attn_attn_kv_cache : bf16[2, 346544, 16, 8, 64][num_users=2] = placeholder[target='model_layers_0_self_attn_attn_kv_cache']
```

Test Plan:
Added unit tests

Rollback Plan:

Differential Revision: D82387095


",2025-09-14 22:56:07+00:00,2025-09-18T22:11:10Z,,False,29,9,1,471,38,3,38,2025-09-18 22:11:10+00:00,82,1181,False,False,False,False,False,False,3,13,6839,509,471,38,1,1,5.0,19.0,2025-09-15T02:55:40Z,pytorch
162925,closed,[BE] Use `fmt::format` to define Conv key,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162925
* #162921

Also use `getArrayRefString` instead of having separate cases for 2D and 3D Conv",2025-09-14 21:41:17+00:00,2025-09-16T16:57:38Z,,False,5,3,1,26,65,1,8,2025-09-15 02:44:16+00:00,41,184,False,False,False,False,False,False,1,3,909,91,26,65,1,1,3.0,4.0,2025-09-14T21:56:05Z,pytorch
162924,closed,[6/n] Quantization with min & max bounds support - using fbgemm changes in ATen,sampathvic,"Summary:
This diff uses the FBGEMM changes made in D78181177 & D81858256 to support using the provided per row min/max values while quantizaing float/half to 8-bit, 4-bit & 2-bit in ATen library.

Please find more context on this here: https://fburl.com/gdoc/yutf32a0

Test Plan:
```
buck test mode/opt caffe2/torch/fb/model_transform/splitting/tests:split_dispatcher_test
```
https://www.internalfb.com/intern/testinfra/testrun/7881299640979446

Please refer to D80905814's test plan for integration testing.

Rollback Plan:

Differential Revision: D81327342




cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-14 21:06:13+00:00,2025-09-25T02:53:12Z,,False,20,0,1,223,40,4,20,2025-09-25 02:52:08+00:00,79,645,False,False,False,True,False,False,4,1,476,263,223,40,1,1,2.0,1.0,2025-09-22T23:52:41Z,pytorch
162921,closed,[BE] Delete [Ventura|Sonoma]Ops header,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162925
* __->__ #162921

Was a temp solution to make PyTorch+MPS buildable on MacOS-12, but it's no longer needed, as in 2.9+ MPS is only supported on MacOS Sonoma+",2025-09-14 20:55:53+00:00,2025-09-15T02:44:17Z,,False,2,0,2,2,296,18,2,2025-09-15 02:44:15+00:00,38,243,False,False,False,False,False,False,18,1,48,298,2,296,1,2,3.0,1.0,2025-09-14T21:59:26Z,pytorch
162920,closed,[easy] Fix unsigned long issue in static cuda launcher,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162920

Fixes https://github.com/pytorch/pytorch/issues/162430

It's a bit hard to come up with a unit test where the stream exceeds a C++ long, so just using existing unit tests for now. 
",2025-09-14 19:47:37+00:00,2025-09-15T15:01:39Z,,False,3,0,1,1,1,1,3,2025-09-15 15:00:36+00:00,54,275,False,True,False,False,False,False,1,2,493,2,1,1,1,1,4.0,2.0,2025-09-14T19:55:39Z,pytorch
162919,open,[Test] -ldl for nativert tests for release/2.9 (Ported #162643),kiszk,"Fixes #162824 in s390x

Original issue is #162640. Ported the fix #162643 to release/2.9
",2025-09-14 18:58:35+00:00,2025-09-25T16:21:51Z,,False,3,0,1,1,0,1,3,,63,89,False,True,False,False,False,False,1,2,123,1,1,0,1,1,1.0,2.0,2025-09-15T18:37:48Z,pytorch
162916,closed,[Triton] [Inductor] Add a Blackwell specific Template for persistent matmul,njriasan,"Summary:
This adds the Triton Tutorial Matmul persistent matmul with device side TMA for Blackwell and adds it as a template option for blackwell. This uses newer Triton features such as automatic warp specialization and loop flattening, which while still containing flaws can improve performance on blackwell. This does not include the Epilogue subtiling section, as that will be a followup PR.

This PR doesn't include any tuning. I am doing a larger benchmarking run to determine the best initial configs for tuning and will open a followup PR with better defaults soon.

Test Plan:
Tested on a Blackwell machine with test_max_autotune.py and confirmed the new tests pass.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-14 17:30:54+00:00,2025-09-15T23:24:09Z,,False,4,0,9,348,1,6,4,2025-09-15 23:23:07+00:00,75,888,False,False,True,False,True,False,6,3,527,369,358,11,1,9,3.0,4.0,2025-09-15T17:52:13Z,pytorch
162915,open,"20% CPU and 15% CUDA speedup of func.hessian for scalar functions (opt-in is_scalar argument), default behavior untouched",jmaczan,"When using a new opt-in `is_scalar` argument for `hessian`, there's a consistent 15-20% speedup for all dimensions, with some high variance on CPU in range 500-700 dims (see a sortable benchmark table in Google Sheet below)

Currently, we use `jacrev` for all types of functions. I checked that `grad` works better for scalar functions. Ideally, we'd have an auto-tuner inside `hessian` to pick which version it should run, but I didn't worked it out yet (since it's not obvious if `func` is a scalar function or not)

Benchmark results: https://docs.google.com/spreadsheets/d/1Ie2Nudasz0yewBMrEyKGtMyJdM9tSAcUh_EeMQ3sHnU/edit?usp=sharing

I benchmarked all tested combinations (device (cpu/cuda), function (quadratic/rosenbrock), dimension(2,5,10,50,100,200,500,700,1000), version (base/optimized)) for 1000 times on a low noise machine (Ubuntu 24.04.2, AMD Ryzen 7 9800X3D, 64 GB RAM, no background activity except Terminal with benchmark script)

Benchmark script: https://github.com/jmaczan/pytorch-benchmarks/blob/main/hessian/benchmark.py

I added unit tests to test against regression

<img width=""600"" height=""371"" alt=""Sum of CPU+CUDA runtimes"" src=""https://github.com/user-attachments/assets/7def4bad-389f-455e-b817-711c2a59d92b"" />
<img width=""600"" height=""371"" alt=""Average CPU+CUDA runtimes"" src=""https://github.com/user-attachments/assets/b28cd8e4-3706-4602-9ab7-092b12b60ca4"" />
<img width=""600"" height=""371"" alt=""Median CPU+CUDA runtimes"" src=""https://github.com/user-attachments/assets/a8df40f4-0338-4876-882c-79452f272b02"" />



cc @zou3519 @Chillee @samdow @kshitij12345",2025-09-14 16:13:02+00:00,2025-09-22T20:59:24Z,,False,5,0,6,205,3,4,5,,121,1589,False,False,False,True,False,False,4,4,441,990,596,394,1,6,1.0,4.0,2025-09-14T16:13:31Z,pytorch
162914,closed,Redirect all use of filesystem to c10/utils/FileSystem.h,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162914

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-14 15:51:49+00:00,2025-09-15T04:31:48Z,,False,7,1,4,34,13,8,8,2025-09-15 04:30:44+00:00,56,245,False,False,False,False,False,False,8,6,1614,49,35,14,1,4,5.0,6.0,2025-09-14T15:56:42Z,pytorch
162913,open,flip capture_dynamic_output_shape_ops=True,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162913



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 15:47:00+00:00,2025-09-14T18:35:02Z,,False,2,0,1,1,3,1,2,,42,266,False,False,False,False,False,False,1,0,0,4,1,3,1,1,,,,pytorch
162912,open,flip capture_scalar_outputs=True,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162912



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 15:45:32+00:00,2025-09-14T18:35:01Z,,False,2,0,1,1,1,1,2,,32,266,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162910,closed,[MPS] enable empty like and unsqueeze for SparseMPS,Isalia20,"Enable empty like and unsqueeze for SparseMPS

cc @kulinseth @malfet @DenisVieriu97 @jhavukainen",2025-09-14 14:43:27+00:00,2025-09-14T17:48:12Z,,False,4,0,1,5,5,2,4,2025-09-14 17:47:09+00:00,51,96,False,False,False,False,False,False,2,2,493,10,5,5,1,1,4.0,2.0,2025-09-14T14:48:41Z,pytorch
162906,open,Assoc scan mem eff,bohnstingl,"This PR replaces the matrix-based gradient computation of associative_scan with a backward associative_scan.
This enhances readability, maintainability and memory footprint. However, runtime needs to be checked.

cc @ydwu4",2025-09-14 08:20:58+00:00,2025-09-18T01:32:51Z,,False,2,0,3,139,184,1,2,,18,222,False,False,False,False,False,False,1,1,42,25675,19894,5781,1,3,1.0,1.0,2025-09-14T08:22:36Z,pytorch
162905,open,[dynamo] Remove retrieving objects by ID,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162905
* #162901
* #162904
* #162903
* #162902
* #162900
* #163028
* #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:55:55+00:00,2025-09-19T07:46:45Z,,False,1,0,8,0,17,1,1,,40,346,False,False,False,False,False,False,1,0,0,36319,27020,9299,1,6,,,,pytorch
162904,open,[user-streams] Track external/internal nodes for stream context,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* __->__ #162904
* #162903
* #162902
* #162900
* #163028
* #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:43:24+00:00,2025-09-20T05:23:33Z,,False,1,0,8,24,6,1,1,,63,346,False,False,False,False,False,False,1,0,0,36358,27044,9314,1,7,1.0,0.0,2025-09-20T05:23:33Z,pytorch
162903,open,[user-streams] update stream context to use fork/join,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* __->__ #162903
* #162902
* #162900
* #163028
* #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:43:21+00:00,2025-09-20T05:23:21Z,,False,1,1,8,41,24,1,2,,53,346,False,False,False,False,False,False,1,0,0,36390,27060,9330,1,6,1.0,0.0,2025-09-20T05:23:15Z,pytorch
162902,open,[user-streams] Add stream state manager,mlazos,"The stream state manager records tensors accessed from outside the stream context (external tensors) and tensors created within the stream context (internal tensors) in a stack of stream states. 

When entering a stream the new state is pushed onto the stack, initiating recording. When a stream context is exited, the fork stream function args are populated with all external tensors accessed from the stream context and the join stream function args are populated with all tensors/nodes created within the context. This allows us to demarcate stream regions. 


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* #162903
* __->__ #162902
* #162900
* #163028
* #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:43:17+00:00,2025-09-20T05:22:30Z,,False,1,0,8,75,2,1,1,,39,910,False,False,False,False,False,False,1,0,0,36387,27081,9306,1,6,1.0,0.0,2025-09-20T05:22:30Z,pytorch
162901,open,[user-cuda-streams] Add cuda streams test suite,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* __->__ #162901
* #162904
* #162903
* #162902
* #162900
* #163028
* #162899
* #163027

",2025-09-14 06:43:14+00:00,2025-09-19T09:46:07Z,,False,1,0,8,26,0,1,1,,47,174,False,False,False,False,False,False,1,0,0,36484,27173,9311,1,7,,,,pytorch
162900,open,[user-cuda-streams] Add fork/join custom ops,mlazos,"Creates the fork/join stream ops. These ops are passthrough ops which mutate all of their args (without actually performing any computation on them) so that during functionalization, implicit dependencies are added on all of their args. This allows us to prevent reordering during our pre/post grad graph passes. 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* #162903
* #162902
* __->__ #162900
* #163028
* #162899
* #163027

Make custom ops inplace

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:43:10+00:00,2025-09-20T05:21:11Z,,False,1,2,7,29,0,1,3,,44,684,False,False,False,False,False,False,1,0,116,36201,26979,9222,1,7,2.0,1.0,2025-09-19T21:26:53Z,pytorch
162899,open,[user-cuda-streams] Pass streams/events to the graph via lookup table,mlazos,"Stores streams in a global object look table that maps a dynamo selected index to objects. This index is generated during tracing, and at runtime, a helper function is called from the bytecode to populate this map. 

This differs from the previous implementation that simply mapped IDs to the associated objects. This required specialization on the IDs of the specific objects, while this new approach does not. 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162905
* #162901
* #162904
* #162903
* #162902
* #162900
* #163028
* __->__ #162899
* #163027



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-14 06:43:06+00:00,2025-09-20T05:20:14Z,,False,1,6,6,82,14,6,7,,69,760,False,False,False,False,False,False,6,0,76,36186,26982,9204,1,6,2.0,1.0,2025-09-19T21:15:20Z,pytorch
162896,closed,QoL: add pip to requirements-build.txt,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162896

uv venvs by default don't come with pip, but for example setup.py assumes it is available.

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-14 03:31:53+00:00,2025-09-15T13:19:11Z,,False,4,0,1,1,0,1,4,2025-09-14 17:08:07+00:00,38,242,False,False,False,False,False,False,1,3,566,1,1,0,1,1,4.0,3.0,2025-09-14T04:03:15Z,pytorch
162895,closed,[DTensor] Add _foreach_pow to sharding propagation list.,dcci,"Fixes #152696


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-14 03:28:07+00:00,2025-09-15T21:15:15Z,,False,9,0,1,2,0,1,9,2025-09-15 21:14:11+00:00,56,111,False,True,False,False,False,False,1,8,1873,2,2,0,1,1,3.0,8.0,2025-09-14T03:29:37Z,pytorch
162894,closed,"typo corrected from A[:,0].sum() to A[0,:].sum()",RajeshvShiyal,"Fixes #162882

typo corrected A[:,0].sum() to A[0,:].sum() 
",2025-09-14 02:13:39+00:00,2025-09-14T14:42:22Z,,False,4,0,1,1,1,1,4,2025-09-14 14:42:22+00:00,48,60,False,True,False,False,False,False,1,2,80,2,1,1,1,1,2.0,2.0,2025-09-14T02:14:44Z,pytorch
162893,closed,[AOTInductor] Use CudaCachingAllocator for memory allocation,muchulee8,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162893

Summary:
Use c10::CudaCachingAllocator for AOTInductor's initial constant buffer
allocation.

Test Plan:
Activate test under test/cpp/aoti_inference/test.cpp

Reviewers:

Subscribers:

Tasks:

Tags:

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @amjames @chauhang @aakhundov @coconutruben",2025-09-14 01:21:58+00:00,2025-09-17T17:09:28Z,,False,7,0,1,69,4,5,7,2025-09-17 17:08:23+00:00,60,484,False,False,False,False,False,False,5,5,1401,73,69,4,1,1,3.0,5.0,2025-09-16T00:32:44Z,pytorch
162892,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-14 00:28:12+00:00,2025-09-14T04:28:41Z,,False,3,0,1,1,1,1,3,2025-09-14 04:27:40+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-14T00:28:13Z,pytorch
162891,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-14 00:27:51+00:00,2025-09-14T04:28:39Z,,False,3,0,1,1,1,1,3,2025-09-14 04:27:37+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-14T00:27:52Z,pytorch
162890,closed,[DONT MERGE][WRONG] Dont use custom get_device,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162775
* __->__ #162890
* #162889

",2025-09-13 23:52:26+00:00,2025-09-15T18:31:14Z,,False,2,0,1,1,1,1,2,2025-09-15 18:31:14+00:00,46,114,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162889,closed,[functional] Avoid duplicate custom get_device call in constructor,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163019
* #162987
* __->__ #162889

Trying to reduce the number of `__torch_dispatch__` calls of FakeTensorMode in the AOT metadata collection pass.
",2025-09-13 23:52:22+00:00,2025-09-16T18:15:42Z,,False,5,0,2,1,1,1,5,2025-09-16 05:00:22+00:00,66,227,False,False,False,False,False,False,1,4,943,17414,13087,4327,1,2,5.0,5.0,2025-09-15T22:00:23Z,pytorch
162888,closed,[data foundation][vizard] Prevent checking the device type of numpy object in Tensorboard logger,clarkdykang,"Summary:
The check is introduced in D82262053
- `scalar_value` could be a numpy object
  - Move the check of `device.type` into `make_np` method where it happens only when it's a `torch.Tensor`.

Test Plan:
```
vizard launch -j 1x8 --launch=flow --config-path=pkg://vizard_projects.image_classification.configs --config-name=resnet50 ++flow.secure_group=ml_sensors ++flow.entitlement=ai_frameworks_pnb ++max_train_steps_per_epoch=10 ++max_epochs=5 ++log_every_n_steps=10 ++profiler=null ++max_eval_steps_per_epoch=10
```

Rollback Plan:

Differential Revision: D82383428


",2025-09-13 21:49:04+00:00,2025-09-14T16:55:38Z,,False,8,0,1,2,0,1,8,2025-09-14 08:09:12+00:00,96,573,False,False,False,False,False,False,1,1,483,2,2,0,1,1,2.0,2.0,2025-09-14T00:14:53Z,pytorch
162887,closed,[BE] [Triton] [Inductor] Add an assert for store_output val_shape to use a tuple,njriasan,"Summary: Updates the remaining tests to all ensure val_shapes is always passed a tuple and not a list. Enables adding an assert consistent with the other function arguments.

Test Plan:
Depends on CI.

Rollback Plan:

Differential Revision: D82383319




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-13 21:14:21+00:00,2025-09-18T04:31:41Z,,False,10,0,1,4,3,3,10,2025-09-18 04:30:39+00:00,80,456,False,False,False,False,False,False,3,5,1249,7,4,3,1,1,3.0,5.0,2025-09-17T23:53:08Z,pytorch
162886,closed,Simplify PrecompileContext to no longer be a CacheArtifactManager,jamesjwu,"Summary:
This diff does a big refactor of PrecompileContext to make it considerably simpler: instead of being a CacheArtifactManager and managing a bunch of bytes, it simply stores two things: dynamo cache entries and backend cache entries. When asked, it stitches them together into PrecompileCacheEntries, which are stored by DynamoCache. 

This structure then allows us to register DynamoCache to the regular Megacache API, instead of having two separate APIs that are confusing. It also lets us remove the autotune cache integration, since MegaCache API will automatically store autotune cache entries. 

The intent here is that users who want to use caching precompile will simply be able to use torch.compiler.save_cache_artifacts as before, just with `torch.dynamo.config.caching_precompile` set to True. They can also directly interact with PrecompileContext if they wish to specifically only load Precompile entries, using PrecompileContext.create_cache_entries(). 

Saving single entries and such with DynamoCache still works normally.

Test Plan:
All existing unit tests pass.

Rollback Plan:

Differential Revision: D82380307




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-13 20:45:00+00:00,2025-09-20T01:25:43Z,,False,9,1,1,148,441,10,10,2025-09-20 01:24:40+00:00,65,1356,False,False,False,False,False,True,10,2,612,589,148,441,1,1,3.0,3.0,2025-09-18T15:45:22Z,pytorch
162885,closed,[MPS] sparse mps any,Isalia20,"Add SparseMPS key for any op

cc @kulinseth @malfet @DenisVieriu97 @jhavukainen",2025-09-13 20:15:01+00:00,2025-09-14T18:58:59Z,,False,8,0,1,1,2,2,8,2025-09-14 18:57:56+00:00,20,79,False,False,False,False,False,False,2,6,1623,3,1,2,1,1,4.0,6.0,2025-09-13T20:19:55Z,pytorch
162883,closed,[PyTorch] Add SVE128 ISA (#158932),Nicoshev,"Summary:
Pull Request resolved: https://github.com/pytorch/pytorch/pull/158932

Importing https://github.com/pytorch/pytorch/pull/138388, as it improves SVE support for perfkernels

Test Plan: We will test it on AdFinder/AdRetriever/AdRanker offline tier

Reviewed By: r1mikey

Differential Revision: D70788867

Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-13 16:17:21+00:00,2025-09-13T16:17:30Z,2025-09-13T16:17:30Z,True,1,0,1,523,172,34,1,2025-09-13 16:17:30+00:00,34,591,False,True,False,False,True,False,34,0,0,695,523,172,1,1,,,,pytorch
162880,open,[Release Only] Increase default timeout for libtorch-rocm-shared-with-deps-release-build,atalman,"Increase timeout to be able to build libtorch
Related issue: https://github.com/pytorch/pytorch/issues/162601

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-13 14:18:21+00:00,2025-09-25T02:28:02Z,,False,1,0,1,1,1,1,1,,88,227,False,False,False,False,False,False,1,0,0,2,1,1,1,1,2.0,0.0,2025-09-24T17:31:58Z,pytorch
162879,open,Enable computing Hessian matrices of larger functions by chunking in torch.func.hessian via jacrev,jmaczan,"In func.hessian I added passing `chunk_size` to `jacrev` to enable chunking when computing Hessian, so we can use bigger functions as input

cc @zou3519 @Chillee @samdow @kshitij12345",2025-09-13 14:07:39+00:00,2025-09-18T01:32:25Z,,False,3,0,2,36,4,3,3,,98,183,False,False,False,False,False,False,3,2,82,46,39,7,1,2,1.0,2.0,2025-09-13T14:08:28Z,pytorch
162877,closed,[CI] Move libtorch-cpu-shared-with-deps-release-build to python 3.10,atalman,"Related to https://github.com/pytorch/pytorch/pull/162862
",2025-09-13 13:13:31+00:00,2025-09-15T15:28:32Z,,False,3,0,1,1,1,1,3,2025-09-15 15:27:29+00:00,68,58,False,False,False,False,False,False,1,2,871,2,1,1,1,1,2.0,3.0,2025-09-15T15:25:12Z,pytorch
162875,closed,[NVRTC] Enable compiling templated kernels,gau-nernst,"Per NVRTC doc - https://docs.nvidia.com/cuda/nvrtc/index.html#accessing-lowered-names, we can compile a templated kernel (e.g. `kernel<float>`) with the following steps

NVRTC side
- (new) `nvrtcAddNameExpression` -> C++ template e.g. `f<float>`
- `nvrtcCompileProgram`
- (new) `nvrtcGetLoweredName` -> get mangled name. need to do a copy since later this string is freed after NVRTC program is destroyed
- `nvrtcDestroyProgram`

CUDA side
- use mangled name instead of normal name -> profit
- `extern ""C""` is not even needed

cc @msaroufim ",2025-09-13 06:50:59+00:00,2025-09-16T11:56:00Z,,False,5,0,5,86,13,3,5,2025-09-14 06:17:39+00:00,42,541,False,False,False,True,False,False,3,4,558,115,94,21,1,5,4.0,4.0,2025-09-13T06:52:32Z,pytorch
162874,open,[MPS] Do not explicit wait for sync,oraluben,"Fixes #162872

The deleted line was waiting for this notify:

https://github.com/pytorch/pytorch/blob/aa41d3e49cbfef8117693ab80ec1ad57accfcb41/aten/src/ATen/mps/MPSEvent.mm#L28

But when we explicitly sync stream as in #162872, this will trigger a dead lock as no other threads will notify.

I think users are responsible for syncing, not here. Or we can add a full sync here, not just a wait.",2025-09-13 06:30:07+00:00,2025-09-23T01:40:48Z,,False,6,4,2,9,4,3,10,,35,393,False,True,False,False,False,False,3,5,360,13,9,4,1,2,3.0,5.0,2025-09-15T14:34:54Z,pytorch
162873,closed,[Graph Partition] allow sharing default device context,BoyuanFeng,"Entering a device context takes 30 us and exiting a device context takes 11 us. If all graph partitions and cudagraph-unsafe ops happen on the same device, we can share the device context.

## Trace

Use vLLM as an example. The first trace shows dynamo graph partition.
<img width=""1338"" height=""453"" alt=""image"" src=""https://github.com/user-attachments/assets/b81815fd-cdcb-4024-846a-5b64164f8bac"" />

The second trace shows inductor graph partition prior to this PR. 
<img width=""1331"" height=""270"" alt=""image"" src=""https://github.com/user-attachments/assets/8d98b127-2053-4eae-9a31-5491661f14d8"" />

Comparing with fx graph partition, we can see inductor graph partition shows extra overhead from enter/exit device contexts (13+6 us -> 30+11 us), but smaller runtime overhead (13 us -> 7 us). This motivates the PR to share default device context. 


The third trace shows Inductor graph partition after this PR. We observe that the extra overhead from enter/exit device contexts have been fixed. At the same time, we observe the smaller runtime overhead.
<img width=""1336"" height=""276"" alt=""image"" src=""https://github.com/user-attachments/assets/77be2237-34dd-4bac-ad9c-d9af3be36417"" />



cc @mcarilli @ezyang @eellison @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-13 06:28:24+00:00,2025-09-19T18:44:06Z,,False,11,7,3,150,21,4,18,2025-09-16 19:36:47+00:00,54,1431,False,True,False,False,False,False,4,10,5550,11001,8260,2741,1,3,3.0,11.0,2025-09-15T05:22:26Z,pytorch
162869,closed,Do not use // but use CleanDiv or FloorDiv instead,laithsakka,"Summary:
When rewriting sympy expressions in the compiler codebase we want to generate
FloorDiv(a, b) CleanDiv(a, b) directly and not a//b. since the later become floor(a*pow(b, -1))

For symnodes we automatically handle that conversions in the symnode op dispatch.
I will follow up with an issue to track all other usages of //.
Block internal Model.

Test Plan:
add test 
run existing tests. 
dakechen1993 testing on the model.

Rollback Plan:

Differential Revision: D82362241




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-13 02:42:30+00:00,2025-09-14T01:31:40Z,,False,5,0,1,19,1,2,5,2025-09-14 01:30:37+00:00,50,540,False,False,False,False,False,False,2,1,630,20,19,1,1,1,2.0,1.0,2025-09-13T13:55:48Z,pytorch
162867,closed,Do not use // but use CleanDiv or FloorDiv instead,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162867

When operating rewriting sympy expressions in the compiler code base we want to generate
FloorDiv(a, b) CleanDiv(a, b) directly and not a//b. since the later become floor(a*pow(b, -1))

For symnodes we automatically handle that conversions in the symnode op dispatch. 

I will follow up with an issue to track all other usages of //.

Block internal Model. 

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-13 01:26:45+00:00,2025-09-13T02:51:35Z,,False,2,0,2,19,2,2,2,2025-09-13 02:51:35+00:00,50,509,False,False,False,False,False,False,2,1,74,147,64,83,1,2,1.0,1.0,2025-09-13T02:49:50Z,pytorch
162866,closed,fix deterministic scatter_add path for multi-d tensors,ngimel,PReviously for more than 2d tensor `select` didn't work correctly. ,2025-09-13 01:18:28+00:00,2025-09-15T17:33:06Z,,False,5,0,1,5,4,2,5,2025-09-15 06:50:04+00:00,54,67,False,True,False,False,False,False,2,4,1073,9,5,4,1,1,4.0,4.0,2025-09-15T06:31:32Z,pytorch
162865,closed,[ONNX] Fix rotary_embedding_23 implementation,justinchuby,"The implementation of rotary_embedding_23 when input is 3D was incorrect.

## Tested

Locally with 

```py
import onnx_ir as ir
import onnx
import torch
import os
import numpy as np

base_path = ""/home/justinchu/dev/onnx/onnx/backend/test/data/node""
test_names = [
    ""test_rotary_embedding"",
    ""test_rotary_embedding_3d_input"",
    ""test_rotary_embedding_interleaved"",
    ""test_rotary_embedding_no_position_ids"",
    ""test_rotary_embedding_no_position_ids_interleaved"",
    ""test_rotary_embedding_no_position_ids_rotary_dim"",
    ""test_rotary_embedding_with_interleaved_rotary_dim"",
    ""test_rotary_embedding_with_rotary_dim"",
]
model_paths = [os.path.join(base_path, name) for name in test_names]


for path in model_paths:
    print(f""Checking {path} for issues..."")

    model = onnx.load(os.path.join(path, ""model.onnx""))
    input0 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_0.pb""))
    ).numpy()
    input1 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_1.pb""))
    ).numpy()
    input2 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_2.pb""))
    ).numpy()
    if os.path.exists(os.path.join(path, ""test_data_set_0"", ""input_3.pb"")):
        input3 = ir.from_proto(
            onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""input_3.pb""))
        ).numpy()
    else:
        input3 = None
    output0 = ir.from_proto(
        onnx.load_tensor(os.path.join(path, ""test_data_set_0"", ""output_0.pb""))
    ).numpy()

    m = ir.from_proto(model)

    node = m.graph[-1]
    print(node)
    assert node.op_type == ""RotaryEmbedding""

    interleaved = node.attributes.get_int(""interleaved"", 0)
    num_heads = node.attributes.get_int(""num_heads"", 0)
    rotary_embedding_dim = node.attributes.get_int(""rotary_embedding_dim"", 0)

    torch_out = torch.onnx.ops.rotary_embedding(
        torch.tensor(input0),
        torch.tensor(input1),
        torch.tensor(input2),
        position_ids=torch.tensor(input3) if input3 is not None else None,
        interleaved=bool(interleaved),
        num_heads=num_heads,
        rotary_embedding_dim=rotary_embedding_dim,
    )
    torch_out = torch_out.detach().cpu().numpy()
    np.testing.assert_allclose(torch_out, output0)
```

Fix https://github.com/pytorch/pytorch/issues/162848

cc @titaiwangms @kunal-vaishnavi",2025-09-13 00:48:01+00:00,2025-09-16T04:42:24Z,,False,12,8,12,119,19,2,20,2025-09-16 03:30:08+00:00,45,2395,False,True,False,False,False,False,2,11,3307,206,153,53,1,12,7.0,11.0,2025-09-13T01:30:48Z,pytorch
162864,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-13 00:23:22+00:00,2025-09-13T04:18:25Z,,False,3,0,1,1,1,1,3,2025-09-13 04:17:23+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-13T00:23:23Z,pytorch
162863,closed,[Inductor-FX] Support IndexPutFallback,blaine-rister,"# Feature

This PR supports lowering `IndexPutFallback` through Inductor's FX converter. The approach is very similar to the one taken in https://github.com/pytorch/pytorch/pull/162686.

Compared to `ScatterFallback`, this required one additional change: the value of `self.op_overload` for `IndexPutFallback` was inaccurate. Previously, it used `aten.index_put`, which would result in unsound FX IR. The existing Python/C++ codegen use `aten.index_put_`, since the fallback mutates its input. This PR changes `self.op_overload` to match that.

# Test plan
Added a CI test lowering deterministic index put via the FX converter.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 23:38:35+00:00,2025-09-16T08:53:52Z,,False,3,4,8,102,25,7,7,2025-09-16 08:52:49+00:00,38,830,False,False,True,False,False,False,7,2,493,35749,27214,8535,1,8,3.0,2.0,2025-09-15T17:15:08Z,pytorch
162862,closed,[CI] Move Windows build/tests to Python-3.10,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162310
* __->__ #162862
* #163341
* #163339

What supposed to be a very simple change end up being quite involved, as current Windows CI framework is quite inflexible, i.e. it takes a lots of argument, but later on ignores them, namely:
 - `PYTHON_VERSION` used to be a no-op that is simply ignored by the scripts
 - With this change, `setup-win` action will create an environment called `py_tmp` with specific python version + intel-openmp (that is hard runtime requirement, but for some reason not packaged into the wheel nor marked as such)
 - Copied test type dependencies from https://github.com/pytorch/test-infra/blob/be01a40157c36cd5a48391fdf44a7bc3ebd4c7e3/aws/ami/windows/scripts/Installers/Install-Pip-Dependencies.ps1#L16 into `win-test.sh`, but made some adjustments to be compatible with 3.10 runtime (scipy version update) and just make rerun-tests compatible with the rest of the deps

I think in the long run, one needs to update https://github.com/pytorch/test-infra/blob/4432e2cacd8a5bb7a46158e71d08c937c502e35a/aws/ami/windows/scripts/Installers/Install-Miniconda3.ps1 that currently pins Miniconda python to 3.9, but also figure out how CI can still create a new environment without having to download all the dependencies all the time",2025-09-12 23:24:50+00:00,2025-09-22T20:21:38Z,,False,13,0,32,32,11,8,13,2025-09-19 22:51:41+00:00,44,1335,False,False,False,False,False,False,8,12,3850,7024,4760,2264,1,30,6.0,13.0,2025-09-13T00:15:20Z,pytorch
162861,open,[FSDP][Replicate] tests replicate type casting behavior and edge cases in mixed precision,anshul-si,"**Summary:** Ensures that replicate can handle the same type casting behavior and edge cases that fully shard can when mixed precision is used

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_float16_on_one_submodule
2. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_submodules_with_external_inputs
3. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_norm_modules_bf16
4. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_norm_modules_fp16
5. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_clamp_reduce_dtype
6. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k test_dataclass_input

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631

",2025-09-12 22:38:38+00:00,2025-09-18T03:48:54Z,,False,1,0,4,240,1,1,1,,89,998,False,False,False,False,False,False,1,0,0,18311,13220,5091,1,4,,,,pytorch
162856,closed,[ROCm] fix conv relu fusion,xinyazhang,"Fixes #162816.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-12 22:03:13+00:00,2025-09-15T22:50:37Z,,False,4,0,2,80,188,2,4,2025-09-15 22:49:36+00:00,27,132,False,True,False,False,False,False,2,2,493,272,82,190,1,2,2.0,2.0,2025-09-12T22:04:08Z,pytorch
162855,open,[FSDP][Replicate] tests replicate core functionality with mixed precision,anshul-si,"**Summary:** Ensures that replicate functionality works the same as fully shard's when mixed precision is used

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_mixed_precision.py -k TestReplicateMixedPrecisionTraining

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* __->__ #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 22:02:31+00:00,2025-09-18T01:04:14Z,,False,1,0,4,387,0,1,1,,73,567,False,False,False,False,False,False,1,0,0,18455,13366,5089,1,4,,,,pytorch
162853,open,[FSDP][Replicate] tests replicate is composable with tp,anshul-si,"**Summary:** Proof that new replicate API is composable with TP

**Test Case**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_replicate_tp

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* __->__ #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 21:49:45+00:00,2025-09-18T01:04:08Z,,False,1,0,3,105,0,1,1,,55,494,False,False,False,False,False,False,1,0,0,18167,13081,5086,1,3,,,,pytorch
162851,open,[FSDP][Replicate] tests replicate with custom forward method,anshul-si,"**Summary: tests replicate works when users use custom forward methods** 

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_register_fsdp_forward_method

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* __->__ #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 21:44:21+00:00,2025-09-24T02:25:38Z,,False,1,0,3,54,1,1,1,,60,521,False,False,False,False,False,False,1,0,0,18117,13030,5087,1,3,,,,pytorch
162850,closed,Clean up 'torch.onnx' entries from public API allowlist,justinchuby,"Clean up entries related to 'torch.onnx' from the allowlist as the apis in onnx are properly configured.
",2025-09-12 21:00:22+00:00,2025-09-15T16:15:48Z,,False,3,0,1,0,51,1,3,2025-09-15 16:14:45+00:00,55,105,False,False,False,False,False,False,1,2,500,51,0,51,1,1,2.0,3.0,2025-09-15T13:28:04Z,pytorch
162849,closed,"make fullgraph_capture work on mod, args, kwargs",avikchaudhuri,"Summary:
Today `fullgraph_capture` takes a frame, but clients usually take a callable (`nn.Module`, function, or method) and example inputs (args and kwargs) and then explicitly set up the frame to pass. This is boilerplate—and potentially tricky to get right—that can be hidden inside the API.

The original `fullgraph_capture` now becomes `_fullgraph_capture_frame`.

Test Plan:
existing tests

Rollback Plan:

Differential Revision: D82339400


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-12 20:53:30+00:00,2025-09-20T22:49:13Z,,False,19,4,1,83,73,4,23,2025-09-20 22:48:10+00:00,48,618,False,False,False,False,False,False,4,3,589,156,83,73,1,1,4.0,3.0,2025-09-12T21:01:29Z,pytorch
162847,closed,unskip tests blocked by py3.12,ethanwee1,Skipped by #117853.  Let's see if the situation has improved since Feb 2024.,2025-09-12 20:40:23+00:00,2025-09-12T22:20:56Z,,False,2,0,1,0,12,1,2,2025-09-12 22:18:50+00:00,30,76,False,False,False,False,True,False,1,1,4,12,0,12,1,1,1.0,1.0,2025-09-12T22:18:50Z,pytorch
162846,closed,[MPS] Fix `[nan]median` output for empty tensors,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162846

It should be `NaN` rather than 0

Added respective checks to `test_empty_tensor`

Fixes https://github.com/pytorch/pytorch/issues/162798",2025-09-12 20:33:11+00:00,2025-09-12T22:27:36Z,,False,3,0,2,4,0,2,3,2025-09-12 22:26:33+00:00,48,230,False,True,False,False,False,False,2,2,787,59,4,55,1,2,3.0,2.0,2025-09-12T20:54:26Z,pytorch
162844,closed,[lint][CI] Don't checkout submodules for lintrunner-noclang,clee2000,"Shouldn't be needed?
",2025-09-12 20:15:58+00:00,2025-09-15T20:01:36Z,,False,7,0,1,1,1,1,7,2025-09-15 19:53:05+00:00,59,21,False,False,False,False,False,False,1,6,1374,2,1,1,1,1,5.0,7.0,2025-09-13T03:41:37Z,pytorch
162842,closed,Update placement utils and weights to handle meta device,georgiaphillips,"Summary:
This diff fixes two things which come up when testing a tgif-published pt2 model remote net:
1) Updates isSameDevice to handle meta device to avoid this error:
```
what():  Unsupported device typemeta and meta
Exception raised from isSameDevice at fbcode/caffe2/torch/nativert/executor/PlacementUtils.cpp:20
```

2. Updates xl weight v2 loading logic in Weights.cpp to handle non-TBE xl-weights. Today, we enforce the device is the same for an old weight and new weight when replacing with ModelRunnerAdapter.setAttr(). However, the way we replace non-TBE xl weights is to find any weights on ""meta"" device and then replace them with their correct weight with real device from xl_weights folder. Therefore, the new weight and old weight will always have different devices and the device check is invalid. I don't think we've run into this so far bc non-TBE xl weights have not been thoroughly tested until now.

Test Plan:
Run MRS you model merge net, which uses non-TBE xl weights. Confirm that before change #1 we get error:
```
Unsupported device typemeta and meta
```
Then after change #1 and before change #2 we get:
```
what():  Mismatched device for merge.user_tower.linear.weight: meta vs cpu
Exception raised from validateValue at fbcode/caffe2/torch/nativert/executor/Weights.cpp:374
```
After change run is successful 
Command:
```
MODEL_ENTITY_ID=921242082
SNAPSHOT_ID=1269
module_name=merge
SAMPLE_INPUT_DIR=/data/users/georgiaphillips/models/921242082/${SNAPSHOT_ID}/${module_name}_archive/package/data/sample_inputs
buck2 run mode/dev-nosan -c fbcode.nvcc_arch=h100,a100 -c fbcode.enable_gpu_sections=true caffe2/torch/fb/model_transform/fx2trt/packaging:load_net_predictor -- --loadMode=Benchmark --inputNetFile=/data/users/$USER/models/${MODEL_ENTITY_ID}/${SNAPSHOT_ID}/${MODEL_ENTITY_ID}_${SNAPSHOT_ID}.predictor.${module_name} --moduleName=${module_name} --submodToDevice=""merge|cuda0""  --benchmarkEnableProfiling=false --disableStaticRuntime=true --doNotRandomizeSampleInputs=true --benchmarkDontRebatchSamples=true --pytorch_predictor_sigmoid_static_dispatch_enable=false --pytorch_predictor_sigmoid_graph_passes_enable=false --sampleInputFilePath=${SAMPLE_INPUT_DIR}/${module_name}.pt
```

Rollback Plan:

Differential Revision: D80713052


",2025-09-12 19:55:55+00:00,2025-09-17T08:13:42Z,,False,7,0,2,378,12,5,7,2025-09-17 08:12:37+00:00,56,2272,False,True,False,False,False,False,5,2,518,390,378,12,1,2,3.0,2.0,2025-09-12T19:56:50Z,pytorch
162841,open,Add option to FakeProcessGroup to raise error if comms are invoked.,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162841

The current behavior is to do ""nothing"", which means you will corrupt
data.  If you're doing something similar to LocalTensor, where you're
overriding the behavior of collectives to do something numerically,
this can be unwelcome behavior.  If you can error when this happens
it can help prevent silent numerical incorrectness.

Authored with claude code.

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-12 19:51:02+00:00,2025-09-12T22:01:55Z,,False,1,0,1,83,1,3,1,,67,602,False,False,False,False,False,False,3,0,0,84,83,1,1,1,1.0,0.0,2025-09-12T20:53:42Z,pytorch
162840,open,[BE] Remove named tensor,PaliC,cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @mingfeima @XiaobingSuper @ashokei @jingxu10 @jerryzh168,2025-09-12 19:47:57+00:00,2025-09-13T21:17:14Z,,False,4,0,2,38,6488,108,4,,24,104,False,False,False,False,False,False,108,2,388,6526,38,6488,1,2,2.0,2.0,2025-09-12T19:47:58Z,pytorch
162839,open,[FSDP][Replicate] tests replicate gradient accumulation and 1f1b microbatching,anshul-si,"**Summary:** In order to ensure that replicate acts as intended (a specialized version of hsdp) we need to make sure that it can pass the same tests that fully_shard can for training. The first test verifies Replicate works with gradient accumulation properly. The second verifies that replicate works correctly with a One-Forward-One-Backward (1F1B) pipeline parallelism schedule

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_gradient_accumulation
2. pytest test/distributed/_composable/test_replicate_training.py -k test_1f1b_microbatching

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* __->__ #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 19:24:22+00:00,2025-09-18T01:04:01Z,,False,1,0,5,236,0,1,1,,78,914,False,False,False,False,False,False,1,0,0,18322,13229,5093,1,4,,,,pytorch
162836,open,[FSDP][Replicate] tests replicate parity for shared parameters,anshul-si,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* __->__ #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 19:07:00+00:00,2025-09-18T01:03:51Z,,False,1,0,5,56,0,1,1,,62,327,False,False,False,False,False,False,1,0,0,18154,13055,5099,1,5,,,,pytorch
162834,closed,[BE] Add Documentation for Device APIs,jiannanWang,"Added documentation for torch.cuda APIs. 
Fixed docstring for xpu and mtia is_bf16_supported API.

",2025-09-12 18:47:27+00:00,2025-09-16T17:02:13Z,,False,7,1,2,6,4,6,8,2025-09-16 17:01:09+00:00,38,99,False,True,False,True,False,False,6,5,1489,12,7,5,2,2,2.0,6.0,2025-09-12T18:59:03Z,pytorch
162830,open,[FSDP][Replicate] tests replicate parity with activation checkpointing,anshul-si,"**Summary:**  In order to ensure that replicate acts as intended (a specialized version of hsdp) we need to make sure that it can pass the same tests that fully_shard can for training. This tests that replicate function works correctly when combined with activation checkpointing

**Test Case**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_train_parity_with_activation_checkpointing

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* __->__ #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 18:29:12+00:00,2025-09-18T01:07:38Z,,False,1,0,4,134,0,1,1,,70,740,False,False,False,False,False,False,1,0,0,18198,13111,5087,1,4,,,,pytorch
162829,open,[Optimus] Skip dynamic shape for mutate_cat_pass,mengluy0125,"Summary: Context: https://fb.workplace.com/groups/1075192433118967/permalink/784057657844535/

We had a case there the split node has symint for its section size, need to skip the graph transformation for the node.

Test Plan:
before fix:

f794355491


after fix

Rollback Plan:

Differential Revision: D82326731




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-12 18:26:11+00:00,2025-09-17T18:43:23Z,,False,4,0,1,11,0,1,4,,48,526,False,True,False,False,False,False,1,1,310,11,11,0,1,1,3.0,1.0,2025-09-12T18:35:57Z,pytorch
162828,closed,[DCP] Add timeout for checkpoint background process join,kevinmtang,"Summary:
Cleaning up checkpoint background process can currently block trainer thread indefinitely if the process is hanging (notably due to Gloo pg init timeout).

This diff adds a 5s grace period for normal termination and sends SIGTERM if unable to shut down in that period.

Rollback Plan:

Differential Revision: D82268979




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 17:57:15+00:00,2025-09-16T02:33:59Z,,False,4,1,1,10,3,1,5,2025-09-16 02:32:54+00:00,56,433,False,False,False,False,False,False,1,1,476,13,10,3,1,1,3.0,1.0,2025-09-12T20:53:38Z,pytorch
162827,closed,"[ROCm] enable aoti tests, forward fix 162353",jeffdaily,"Forward fix for tests added by #162353.  Enables aoti tests on rocm.

cc @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-12 17:51:16+00:00,2025-09-12T20:06:57Z,,False,6,4,1,5,3,3,10,2025-09-12 20:05:54+00:00,44,175,False,True,False,False,False,False,3,5,1726,8,5,3,1,1,4.0,6.0,2025-09-12T18:03:59Z,pytorch
162826,closed,[OpenReg] Fix the docs of Accelerator Intergration,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162826
* #160101
* #161918
* #161917

----

- Fixed the redirect link about step 1
- Formatted the autoload and added necessary links",2025-09-12 17:41:38+00:00,2025-09-12T23:54:26Z,,False,3,0,2,6,12,3,3,2025-09-12 23:53:22+00:00,50,219,False,True,False,True,False,False,3,2,548,22,8,14,1,2,3.0,3.0,2025-09-12T20:50:54Z,pytorch
162825,open,[nativert] fix rocm usage for cmake build,dolpm,cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd,2025-09-12 17:37:57+00:00,2025-09-24T17:29:56Z,,False,7,4,1,7,5,3,11,,41,116,False,True,False,False,False,False,3,6,1641,12,7,5,1,1,3.0,6.0,2025-09-12T18:20:13Z,pytorch
162823,closed,Fix pipeline parallelism not correctly initializing backwards stages when evaluating before training.,BlueCrescent,"Previously, an eval() call before a training step() would not correctly initialize the backward pass of the pipeline stages, leading to errors during the subsequent training step. This PR ensures that the backward stages can still be initialized after an eval() call.

Fixes #162822


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-12 17:04:17+00:00,2025-09-25T15:14:27Z,,False,19,2,4,103,46,2,21,2025-09-25 15:13:23+00:00,101,598,False,True,False,False,False,False,2,15,4743,179,118,61,1,4,6.0,19.0,2025-09-12T17:07:01Z,pytorch
162821,closed,[triton] Update 3.5 pin to 5ae38bdb0dc066c5823e34dc9797afb9de42c866,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162821

Include @aakhundov's sam_fast patch, plus NVIDIA's sm88/sm110 patches (thanks @nWEIdia)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-12 16:33:19+00:00,2025-09-12T18:36:19Z,,False,3,0,1,2,4,2,3,2025-09-12 18:34:25+00:00,67,353,False,False,False,False,False,False,2,2,845,6,2,4,1,1,2.0,2.0,2025-09-12T18:31:41Z,pytorch
162819,closed,Fix typo in ONNX export error message,justinchuby,"Fix another ""summit"" 😅


cc @titaiwangms",2025-09-12 15:44:19+00:00,2025-09-12T16:35:56Z,,False,3,0,1,1,1,1,3,2025-09-12 16:34:53+00:00,37,40,False,True,False,False,False,False,1,2,786,2,1,1,1,1,4.0,2.0,2025-09-12T16:16:35Z,pytorch
162818,closed,compile_kernel: POC cpp outer template support,msaroufim,"Users would write templated code

```cpp
template<typename T>
__global__ void add_kernel(T* a, T* b, T* c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) c[i] = a[i] + b[i];
}
```

And we explicitly instantiate it

```cpp
template __global__ void add_kernel<float>(float* a, float* b, float* c, int n);

extern ""C"" {
__global__ void add_kernel_wrapper(float* a, float* b, float* c, int n) {
    add_kernel<float>(a, b, c, n);
}
}
```

NVRTC is happy 

It's not totally free since we expect users to change how they call compile_kernel

```python
kernel = torch.cuda._compile_kernel(
    template_code, ""add_kernel"",
    is_template=True,
    template_types=[""float""], 
    wrapper_signature=""float* a, float* b, float* c, int n"",
    wrapper_body=""add_kernel<float>(a, b, c, n);""
)
```

Inner templates that would show up in header libraries like CUB already works here https://github.com/pytorch/pytorch/pull/156380 so feels like both PRs are useful altho still not 100% sure",2025-09-12 15:24:22+00:00,2025-09-15T15:59:21Z,,False,3,0,14,351,4,4,3,2025-09-13 05:06:00+00:00,46,1004,False,False,False,False,False,False,4,2,112,2256,1300,956,1,14,1.0,2.0,2025-09-13T05:05:55Z,pytorch
162815,open,Support setting grad_dtype on leaf tensors,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162815

`grad_dtype` is a new attribute on Tensor to control gradient dtype:
- Access/setting is leaf-only.
- grad_dtype is respected when (1) when assigning to .grad, and (2) in the engine after the previous node produces incoming gradients for AccumulateGrad. (See table below for details)
- Not setting grad_dtype preserves the current behavior. Accessing it returns `t.dtype`
- `grad_dtype` cannot be set when there is already a `.grad` present and the dtypes conflict.

| `grad_dtype` setting | Setting `.grad` manually | Incoming gradient from autograd engine |
|-----------------------|--------------------------|-----------------------------------------|
| **Default (tensor’s dtype)** | `.grad` must match tensor’s dtype | Engine casts incoming grad to tensor’s dtype |
| **Set to specific dtype** | `.grad` must match that dtype | Engine casts incoming grad to the specified dtype |
| **Set to `None`** | `.grad` may be any dtype | Engine does not cast; accepts incoming grad dtype as-is |",2025-09-12 15:07:28+00:00,2025-09-22T23:28:58Z,,False,3,7,8,390,18,13,10,,42,1085,False,False,False,False,False,False,13,2,1101,674,523,151,1,8,2.0,2.0,2025-09-13T17:06:10Z,pytorch
162814,closed,[BE] [Inductor] Update NoValidChoicesError logic,njriasan,"Summary: Updates the NoValidChoicesError logic to include some additional context for if not choices exists or if no choices compiled.

Test Plan:
NFC. Depending on CI.

Rollback Plan:

Differential Revision: D82312035




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 15:05:48+00:00,2025-09-13T00:46:55Z,,False,6,0,1,7,4,1,6,2025-09-13 00:45:52+00:00,48,424,False,False,False,False,False,False,1,3,500,11,7,4,1,1,3.0,3.0,2025-09-12T17:20:31Z,pytorch
162811,closed,[ROCm][SymmMem] re-enable UTs,pragupta,"After the UT suite moved to `MultiProcContinuousTest`, `skipIfRocm` decorator started failing rather than skipping UTs because now we spawn multiple threads before the skip decorator is taken into account and the skip decorator was raising an exception to exit the process. But, the parent process treated the child process exiting as a crash rather than a skip. Additionally, in `MultiProcContinuousTest`, if one UT fails all subsequent ones are also skipped which makes sense since there's one setup for the entire suite. However, this showed up as many failing/skipped UTs in the parity.

I added multiprocess version of skip decorators for ROCm, including, `skip_if_rocm_arch_multiprocess` and
`skip_if_rocm_ver_lessthan_multiprocess`. These are needed as symmetric memory feature is only supported on MI300 onwards and we need to skip them for other archs and some UTs only work after ROCm7.0.

Fixes #161249
Fixes #161187
Fixes #161078
Fixes #160989
Fixes #160881
Fixes #160768
Fixes #160716
Fixes #160665
Fixes #160621
Fixes #160549
Fixes #160506
Fixes #160445
Fixes #160347
Fixes #160203
Fixes #160177
Fixes #160049
Fixes #159921
Fixes #159764
Fixes #159643
Fixes #159499
Fixes #159397
Fixes #159396
Fixes #159347
Fixes #159067
Fixes #159066
Fixes #158916
Fixes #158760
Fixes #158759
Fixes #158422
Fixes #158138
Fixes #158136
Fixes #158135

Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-12 14:49:23+00:00,2025-09-16T15:36:45Z,,False,3,0,4,123,38,3,3,2025-09-16 15:35:49+00:00,29,1576,False,True,True,False,False,False,3,2,493,235,160,75,1,4,2.0,2.0,2025-09-16T12:42:36Z,pytorch
162810,open,Remove .ci/aarch64_linux folder,robert-hardwick,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162810
* #160079
* #159737

",2025-09-12 14:48:10+00:00,2025-09-24T12:42:44Z,,False,2,0,6,0,1508,6,2,,31,114,False,False,False,False,False,False,6,0,0,47022,32813,14209,1,6,,,,pytorch
162809,open,[FakeTensor] Make view_copy meta copy-if-needed to match eager,YufengShi-dudu,"Eager `aten.view_copy` flattens broadcasted tensors by materializing when a view is not possible (e.g., expand -> flatten), returning a non-aliasing result. The FakeTensor meta kernel delegated to `_reshape_view_helper` with `allow_copy=False`, so the same call raised:

    x = torch.arange(7)
    y = x.expand(12, 7)  # strides (0, 1)
    aten.view_copy(y, [84])  # eager: OK, fake: ValueError

This change plumbs `allow_copy=True` for `aten.view_copy` in the FakeTensor impl while keeping `view` / `_unsafe_view` unchanged. This aligns FakeTensor with eager semantics.

Fixes #162817


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-12 14:45:58+00:00,2025-09-23T16:35:57Z,,False,5,0,1,6,3,1,5,,62,759,False,True,False,False,False,False,1,3,148,9,6,3,1,1,1.0,3.0,2025-09-15T11:45:55Z,pytorch
162808,closed,[BE] Preserve caller source location in the error message,albanD,"Summary:
Currently the C10_CUDA_CHECK only shows source location in CUDAException like below:
```
Exception raised from c10_cuda_check_implementation at fbcode/caffe2/c10/cuda/CUDAException.cpp:44
```
which is not terribly useful.

By checking the original diff D39619861 that introduced c10_cuda_check_implementation, it seems the original macro would show the source location correctly but c10_cuda_check_implementation broke it.

This diff will propagate caller source location to c10_cuda_check_implementation to fix the issue.

Test Plan:
CI

Observed desired error message after the change:
```
CUDA error: an illegal memory access was encountered
Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Device-side assertion tracking was not enabled by user.
Exception raised from operator() at fbcode/sigrid/predictor/aed/AedContainer.cpp:659 (most recent call first):
```

Note the last line reports actual caller location.

Rollback Plan:

Reviewed By: Raymo111

Differential Revision: D81880552


",2025-09-12 14:23:17+00:00,2025-09-15T13:30:52Z,,False,10,0,3,5,5,2,10,2025-09-15 13:29:47+00:00,57,1260,False,True,False,True,False,False,2,8,2362,18,9,9,2,3,3.0,8.0,2025-09-12T14:25:47Z,pytorch
162807,closed,Expunge use of filesystem header from distributed,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162807

The fbsource//arvr/mode/embedded/linux/clang-aarch64-debug internal build config is on too old a version of Clang to have full filesystem support. So until it is upgraded, lets have our own portability wrapper here.

Although the APIs mostly line up, I didn't bother implementing the entire path abstraction. So the use sites do need to change slightly.

Authored with claude code.

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-12 13:53:03+00:00,2025-09-14T15:51:59Z,,False,8,5,2,99,18,7,13,2025-09-14 15:51:58+00:00,49,628,False,True,False,False,False,False,7,7,1347,121,101,20,1,2,4.0,8.0,2025-09-12T14:11:22Z,pytorch
162806,closed,"Claude loves making these files in top level, ignore them for sanity.",ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162806

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-12 13:51:54+00:00,2025-09-13T05:00:06Z,,False,3,0,2,2,0,1,3,2025-09-13 04:59:03+00:00,69,150,False,False,False,False,False,False,1,2,493,6,4,2,1,2,3.0,2.0,2025-09-12T14:50:17Z,pytorch
162804,closed,Update torch-xpu-ops commit pin,CuiYifeng,"Update the torch-xpu-ops commit to [intel/torch-xpu-ops@d8c3ee](https://github.com/intel/torch-xpu-ops/commit/d8c3eefc297193cf9e0888a7d8ff32dc74da0793), includes:

- Optimize adaptive average pool for channel-last memory format
- Add unregister wait_tensor
- Replace deprecated `[[intel::reqd_sub_group_size(SgSize)]]` with `[[sycl::reqd_sub_group_size(SIMD)]]` and remove unnecessary attributes
- Revert ""Roll back to original usage of sycl::get_kernel_bundle""",2025-09-12 13:22:28+00:00,2025-09-16T06:31:56Z,,False,10,0,1,1,1,1,10,2025-09-16 06:30:52+00:00,31,461,False,False,False,False,False,False,1,9,2181,2,1,1,1,1,4.0,9.0,2025-09-12T14:26:14Z,pytorch
162803,open,"Revert ""[ROCm][CI] skip test_max_autotune until resolved (#162496)""",jataylo,"Job can be re-enabled after https://github.com/pytorch/pytorch/pull/162733 landed to resolve hanging issue.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd",2025-09-12 13:02:51+00:00,2025-09-22T15:34:17Z,,False,8,0,1,0,1,1,8,,67,216,False,False,False,False,False,False,1,6,934,1,0,1,1,1,2.0,6.0,2025-09-15T09:02:27Z,pytorch
162799,closed,[AOTI] raise PyTorchStreamWriter open failed error code on windows,xuhancn,"When I debug AOTI UT: `TestAOTInductorPackage_cpu::test_add`.  I found it didn't output the verbose error code, when PyTorchStreamWriter open failed.

This PR add the verbose error code output for debug. Local test shows as below:
<img width=""1124"" height=""653"" alt=""image"" src=""https://github.com/user-attachments/assets/01cb1a51-2982-4106-8b5b-c608ac26a075"" />

The error code is 32, we can check the Windows error code 32 at https://learn.microsoft.com/en-us/windows/win32/debug/system-error-codes--0-499- 
```
ERROR_SHARING_VIOLATION
32 (0x20)
The process cannot access the file because it is being used by another process.
```

This issue is caused by the file is opened by another process. I fixed same issue in zip open as PR: https://github.com/pytorch/pytorch/pull/162617 But still no idea how to open file with shared access in `std::ofstream`. I will continue to researching it. 

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",2025-09-12 11:17:50+00:00,2025-09-13T01:42:51Z,,False,6,3,4,30,12,1,9,2025-09-13 01:41:18+00:00,66,1023,False,True,False,False,False,False,1,5,1028,50,34,16,2,4,4.0,5.0,2025-09-12T11:43:43Z,pytorch
162796,closed,[Easy][AMP] Refactor the AMP logic for getting dtype,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162796

As the title stated.

cc @mcarilli @ptrblck @leslie-fang-intel @jgong5",2025-09-12 09:40:02+00:00,2025-09-21T06:33:42Z,,False,3,0,3,2,5,1,3,2025-09-21 06:32:38+00:00,52,164,False,False,False,False,False,True,1,2,559,15,6,9,1,3,2.0,3.0,2025-09-21T03:10:29Z,pytorch
162795,open,[inductor] optimize welford reduction for smaller reduction sizes,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162795
* #162709

**Summary:**

**Example:**
```
import torch
    
def fn(x):
    return torch.var_mean(x, dim=1, unbiased=False, keepdim=True)

compile_fn = torch.compile(fn)
x = torch.randn(32, 256)
with torch.no_grad():
    compile_fn(x)
```

**Generated code:**
- Before
```
cpp_fused_var_mean_0 = async_compile.cpp_pybinding(['float*', 'const float*', 'float*'], r'''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       const float* in_ptr0,
                       float* out_ptr0)
{
    auto out_ptr1 = in_out_ptr0;
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                Welford<at::vec::Vectorized<float>> masked_tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                static WelfordHelper<float, float, 4096> scalar_welford_helper0(static_cast<int64_t>(256L));
                static WelfordHelper<at::vec::Vectorized<float>, float, 4096> welford_helper0(static_cast<int64_t>(16L));
                static WelfordHelper<at::vec::Vectorized<float>, float, 4096> masked_welford_helper0(static_cast<int64_t>(0L));
                for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(256L); x1+=static_cast<int64_t>(16L))
                {
                    {
                        if(C10_LIKELY(x1 >= static_cast<int64_t>(0) && x1 < static_cast<int64_t>(256L)))
                        {
                            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<int64_t>(x1 + 256L*x0), static_cast<int64_t>(16));
                            tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0, &welford_helper0);
                        }
                    }
                }
                tmp_acc0 = welford_combine(tmp_acc0, &scalar_welford_helper0);
                tmp_acc0_vec = welford_combine(tmp_acc0_vec, &welford_helper0);
                masked_tmp_acc0_vec = welford_combine(masked_tmp_acc0_vec, &masked_welford_helper0);
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(masked_tmp_acc0_vec));
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec));
                out_ptr0[static_cast<int64_t>(x0)] = static_cast<float>(tmp_acc0.mean);
                in_out_ptr0[static_cast<int64_t>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
        }
    }
    {
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(16L))
        {
            {
                if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(32L)))
                {
                    auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr1 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                    auto tmp1 = static_cast<float>(256.0);
                    auto tmp2 = at::vec::Vectorized<float>(tmp1);
                    auto tmp3 = tmp0 / tmp2;
                    tmp3.store(in_out_ptr0 + static_cast<int64_t>(x0));
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, = args
        args.clear()
        assert_size_stride(arg0_1, (32, 256), (256, 1))
        buf0 = empty_strided_cpu((32, 1), (1, 1), torch.float32)
        buf1 = empty_strided_cpu((32, 1), (1, 32), torch.float32)
        buf3 = reinterpret_tensor(buf1, (32, 1), (1, 1), 0); del buf1  # reuse
        # [Provenance debug handles] cpp_fused_var_mean_0:1
        cpp_fused_var_mean_0(buf3, arg0_1, buf0)
        del arg0_1
        return (buf3, buf0, )
```

- After
```
cpp_fused_var_mean_0 = async_compile.cpp_pybinding(['float*', 'const float*', 'float*'], r'''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       const float* in_ptr0,
                       float* out_ptr0)
{
    auto out_ptr1 = in_out_ptr0;
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<at::vec::Vectorized<float>> tmp_acc0_vec = Welford<at::vec::Vectorized<float>>();
                for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(256L); x1+=static_cast<int64_t>(16L))
                {
                    {
                        if(C10_LIKELY(x1 >= static_cast<int64_t>(0) && x1 < static_cast<int64_t>(256L)))
                        {
                            auto tmp0 = at::vec::Vectorized<float>::loadu(in_ptr0 + static_cast<int64_t>(x1 + 256L*x0), static_cast<int64_t>(16));
                            tmp_acc0_vec = welford_combine(tmp_acc0_vec, tmp0);
                        }
                    }
                }
                tmp_acc0 = welford_combine(tmp_acc0, welford_vec_reduce_all(tmp_acc0_vec, false), false, false);
                tmp_acc0 = welford_combine_final_out(tmp_acc0);
                out_ptr0[static_cast<int64_t>(x0)] = static_cast<float>(tmp_acc0.mean);
                in_out_ptr0[static_cast<int64_t>(x0)] = static_cast<float>(tmp_acc0.m2);
            }
        }
    }
    {
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(16L))
        {
            {
                if(C10_LIKELY(x0 >= static_cast<int64_t>(0) && x0 < static_cast<int64_t>(32L)))
                {
                    auto tmp0 = at::vec::Vectorized<float>::loadu(out_ptr1 + static_cast<int64_t>(x0), static_cast<int64_t>(16));
                    auto tmp1 = static_cast<float>(256.0);
                    auto tmp2 = at::vec::Vectorized<float>(tmp1);
                    auto tmp3 = tmp0 / tmp2;
                    tmp3.store(in_out_ptr0 + static_cast<int64_t>(x0));
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, = args
        args.clear()
        assert_size_stride(arg0_1, (32, 256), (256, 1))
        buf0 = empty_strided_cpu((32, 1), (1, 1), torch.float32)
        buf1 = empty_strided_cpu((32, 1), (1, 32), torch.float32)
        buf3 = reinterpret_tensor(buf1, (32, 1), (1, 1), 0); del buf1  # reuse
        # [Provenance debug handles] cpp_fused_var_mean_0:1
        cpp_fused_var_mean_0(buf3, arg0_1, buf0)
        del arg0_1
        return (buf3, buf0, )
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-12 09:00:59+00:00,2025-09-19T12:23:02Z,,False,1,0,8,167,77,3,1,,65,7575,False,True,False,False,False,False,3,0,0,22512,15746,6766,1,8,,,,pytorch
162794,closed,[async_tp] Support mm+rs with scatter_dim matmul K by sharding B,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163069
* #163068
* __->__ #162794

Current state: Shape mismatch failure when mm+rs on the last mm scatter dim.

Adding separate path to handle lastdim for aten.mm, scaled_mm should be handled similarly, but needs additional PR.
So disabling scaled_mm case with filter matmul function.

Adding inductor.config for this change that is True by default for fast debuggability of new path.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 07:52:32+00:00,2025-09-25T12:19:46Z,,False,10,0,6,49,5,4,10,2025-09-25 12:18:42+00:00,64,766,False,True,False,False,False,False,4,9,2143,59088,41552,17536,1,6,3.0,10.0,2025-09-12T16:26:08Z,pytorch
162791,closed,To use cuSparseTl,chaochen1111,"Test Plan:
test

Rollback Plan:

Differential Revision: D82236062


",2025-09-12 06:14:58+00:00,2025-09-12T06:19:11Z,,False,4,0,1,24,0,1,4,2025-09-12 06:19:11+00:00,17,68,False,False,False,False,False,False,1,0,0,24,24,0,1,1,,,,pytorch
162789,open,draft multi-kernel nn,pianpwk,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 05:53:27+00:00,2025-09-13T01:06:51Z,,False,2,0,2,105,25,3,2,,21,223,False,True,False,False,False,False,3,0,0,140,110,30,1,2,,,,pytorch
162788,closed,Fix rebind_unbacked in torch.fx.experimental.symbolic_shapes,ColsonZhang,"## Description
Fix a float type handling in `torch.fx.experimental.symbolic_shapes` function. [#162480](https://github.com/pytorch/pytorch/issues/162480)

## Issue
When I use AOTInductor to compile the YOLOv10, I encounter the bug `'float' object has no attribute 'node'`.
[Torch AOTInductor Ahead-Of-Time Compilation Fail](https://github.com/opendatalab/DocLayout-YOLO/issues/177)

The problem is due to missing float type handling.
https://github.com/pytorch/pytorch/blob/main/torch/fx/experimental/symbolic_shapes.py#L597
```
            if isinstance(u1, int):
                log.info(
                    ""rebind_unbacked: discard %s %s %s -> %s"",
                    n.target,
                    raw_u0,
                    path,
                    u1,
                )
                continue
```

## Solution
Change the code `if isinstance(u1, float)` to `if isinstance(u1, (int,float))`

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-12 05:00:05+00:00,2025-09-14T17:08:21Z,,False,4,0,1,1,1,1,4,2025-09-14 17:07:18+00:00,60,958,False,True,False,True,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-14T14:21:39Z,pytorch
162787,closed,[ROCm] Enable test_fixed_striding,jagadish-amd,"Enable the distributed test test_fixed_striding on gfx arch which supports fp8. 
Test command: python test/distributed/test_c10d_functional_native.py -k test_fixed_striding


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-12 03:39:36+00:00,2025-09-15T20:24:51Z,,False,5,3,2,6,8,1,8,2025-09-15 20:23:47+00:00,33,390,False,True,False,False,False,False,1,4,1456,20,9,11,1,2,4.0,4.0,2025-09-12T15:24:47Z,pytorch
162785,closed,[FSDP][Replicate] tests replicate synchronization after optimizer states,anshul-si,"**Summary:**  In order to ensure that replicate acts as intended (a specialized version of hsdp) we need to make sure that it can pass the same tests that fully_shard can for training. Verify replicate correctly handles post-optimizer events.

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_post_optim_event

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* __->__ #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-12 01:46:52+00:00,2025-09-18T04:48:16Z,,False,3,0,3,50,0,1,3,2025-09-18 04:47:12+00:00,72,678,False,False,False,False,False,False,1,2,493,18112,13026,5086,1,3,3.0,2.0,2025-09-12T21:31:52Z,pytorch
162784,closed,Port shared_ptr optimization in std::shared_ptr to intrusive_ptr,mcfi,"Summary:
Please see D21021645 for details about the optimization and why it's beneficial.

A similar change has been added to libstdc++ as well, see https://github.com/gcc-mirror/gcc/commit/dbf8bd3c2f2cd2d27ca4f0fe379bd9490273c6d7

Rollback Plan:

Reviewed By: yfeldblum

Differential Revision: D81960754


",2025-09-12 01:36:42+00:00,2025-09-14T06:15:30Z,,False,20,10,1,48,16,1,30,2025-09-13 21:01:04+00:00,64,307,False,False,False,False,False,False,1,7,5060,64,48,16,1,1,5.0,7.0,2025-09-12T17:49:31Z,pytorch
162781,closed,[inductor][heuristics] add kernel template params,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163206
* #163209
* __->__ #162781

# why

- enable a clear interface for kernel templates to declare all their
  instantiation parameters and any potential defaults
- simplify KernelTemplateChoice to just have a single params, and not kwargs and extra_kwargs

# what

- KernelTemplateParams interface
- placeholder implementation where we just pass through a dict

# testing

- existing ci tests

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-12 00:36:19+00:00,2025-09-18T02:16:48Z,,False,10,6,4,70,21,4,16,2025-09-18 02:15:45+00:00,49,663,False,False,False,False,False,False,4,9,3285,21119,15945,5174,1,4,3.0,10.0,2025-09-12T21:38:55Z,pytorch
162779,closed,fix(mps): Handle non-contiguous tensors in linear op for v2.8.0,AnandAyush11,"This PR fixes an issue where `torch.nn.functional.linear` produces incorrect numerical results on the MPS backend when the weight tensor is non-contiguous.

The fix is a backport of the solution from commit `ee0ec211914dfb0fc5e8c4f355fe76eafa91c276` on the main branch. It adds a check to ensure all input tensors are contiguous before using the fast no-graph implementation, falling back to the robust graph-based path if not.

Fixes #162730 
",2025-09-12 00:29:11+00:00,2025-09-12T22:36:02Z,,False,3,0,2,3,1,1,3,2025-09-12 22:36:02+00:00,63,444,False,True,False,False,False,False,1,1,52,12,7,5,1,2,1.0,1.0,2025-09-12T22:36:02Z,pytorch
162778,closed,Fix the regression issue caused by non-arrch64 platforms not hitting the MKLDNN path.,pytorchbot,"This issue was introduced by the commit in issue #161065. Added an extra check to provide a proper path for other platforms.
",2025-09-12 00:25:22+00:00,2025-09-17T00:21:10Z,2025-09-17T00:21:10Z,True,1,0,1,4,6,1,1,2025-09-17 00:21:10+00:00,85,125,False,True,False,False,False,False,1,0,0,10,4,6,1,1,1.0,0.0,2025-09-17T00:21:02Z,pytorch
162777,closed,[pcache] Cache and AsyncCache implementations,nmacchioni,"Summary:
Implemented caching abstractions: `Cache` and `AsyncCache`.

`Cache` provides an abstraction for defining simple key -> value stores with get and put functionality. We propose using `Cache` for implementations with very low (microseconds) overhead, for example an in-memory cache.

`AsyncCache` provides an abstraction for defining simple key -> value stores with asynchronous get and put functionality. We propose using `AsyncCache` for implementations with medium to high (> millisecond) overhead, for example an on-disk cache.

We provide an initial extension of `Cache` in the form of `InMemoryCache`. `InMemoryCache` provides fast, in-memory caching that can be later used to memoize more expensive cache accesses. `InMemoryCache` also provides a custom constructor `InMemoryCache.from_env_var` that can be used to pre-populate the in-memory cache, which will be helpful for enabling determinism in the future.

We also provides extensions of `AsyncCache`. `OnDiskCache` subclasses `AsyncCache` and serves as a generic on-disk caching implementation with atomic, write-once guarantees. `OnDiskCache` is semi-generic, allowing subclassing to alter the output directory. `InductorOnDiskCache` subclasses `OnDiskCache` to create an Inductor-specific on-disk cache that outputs to Inductor's default caching directory.

Test Plan:
`Cache` Tests:
1. Get -> Set -> Get
- Checks that `get(key)` returns `None` when `key` is not cached, and that after calling `put(key, value)` subsequent `get(key)` calls return `value`
2. Set -> Set
- Checks that with duplicated `set(key, value)` calls only the initial call is successful
3. From env var
- Checks that constructing an `InMemoryCache` from an environment variable works.

`AsyncCache` Tests:
1. Get -> Set -> Get
- Same as `Cache` test, but checks both with synchronous and asynchronous execution
2. Set -> Set
- Same as `Cache` test, but checks both with synchronous and asynchronous execution
3. Set -> Set Concurrent
- Checks that of two concurrent `set(key, value)` operations, only one passes

```
cd ~/fbsource/fbcode && buck test mode/opt //caffe2/test/inductor:pcache
```

 {F1981926248}

Rollback Plan:

Differential Revision: D82269762




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 00:21:23+00:00,2025-09-16T04:24:07Z,,False,30,68,1,627,0,2,98,2025-09-16 04:07:16+00:00,45,2409,False,False,False,False,False,False,2,8,2241,627,627,0,1,1,5.0,9.0,2025-09-12T17:46:46Z,pytorch
162776,open,[MPS] Fix conv layout handling,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162776

What started as simple fix for `mps_convolution_backward_input` resulted in a pretty significant refactor/fixes:
- Updated `mps_conv_use_channels_last` to return channels last output if either input or weights are channels last
- Use the same primitive throughout `Convolution.mm` to determine wether output should be allocated in channels last format or not
- Do not alter the layout based on MacOS version, but rather do additional copies on MacOS-14 if inputs/output or weight are in channels last format ( done by defining `std::optional<Tensor> output_c;` that contains a contiguous copy of the output tensor

But doing only those two, resulted in crash in `test_memory_format_nn_Conv2d_mps_float32`, when weights were backward, and bias is present:
```
% python -c ""import torch;print(torch.nn.functional.conv2d(torch.rand(2, 4, 3, 4,device='mps'), torch.rand(5, 4, 3, 3,device='mps').to(memory_format=torch.channels_last), torch.rand(5,device='mps')))""
/AppleInternal/Library/BuildRoots/4~B5E4ugDCh2RsPWAjMEoPu8LC5w1yXEwd7XweDhg/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphExecutable.mm:3619: failed assertion `Error: MLIR pass manager failed'
zsh: abort      python -c 

``` 


As result, in addition to adding one more regression test this change removes `expectedFailures` from:
- `TestModule.test_memory_format` for `Conv2d`, `ConvTranspose2d`, `LazyConv1d`, `LazyConvTranspose1d`
- `test_require_stride_expanded_dynamic_shapes`
-  `test_mutable_custom_op_fixed_layout2` for MacOS-14

Fixes https://github.com/pytorch/pytorch/issues/161905



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-12 00:08:11+00:00,2025-09-25T13:19:24Z,,False,1,2,16,87,87,7,3,,30,1984,False,True,False,False,False,True,7,0,0,62261,43145,19116,1,16,2.0,0.0,2025-09-13T17:34:50Z,pytorch
162775,open,[DONT MERGE] Cold start experiments,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162775
* #162890
* #162889



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-12 00:01:31+00:00,2025-09-14T03:14:49Z,,False,2,0,3,533,20,5,2,,35,286,False,False,False,False,False,False,5,0,0,730,616,114,1,3,,,,pytorch
162774,open,Fix cpp build,svekars,"Fixes #ISSUE_NUMBER
",2025-09-11 23:58:33+00:00,2025-09-25T16:40:18Z,,False,7,1,21,45,3,3,8,,13,20,False,True,False,False,False,False,3,6,1116,196,119,77,1,21,6.0,7.0,2025-09-16T19:49:14Z,pytorch
162773,closed,Add basic tests for torch.distributed.tensor._utils.compute_global_tensor_info,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162773
* #162508
* #161695
* #162336
* #162298

Next PR writes a C++ implementation. Seems good to have tests first.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 23:51:56+00:00,2025-09-15T19:45:30Z,,False,3,0,1,55,1,1,3,2025-09-15 19:45:30+00:00,78,305,False,False,False,False,False,False,1,2,217,56,55,1,1,1,2.0,2.0,2025-09-12T03:03:29Z,pytorch
162772,closed,Fix boxcox to return same result for same input in one batch,Yuhta,"Summary:
The SIMD path is using SLEEF version of `pow` which is slightly different from `std::pow`.  The fix is to use the same vectorized code (with partial load and store) for the trailing data as well to ensure consistency between results.

Rollback Plan:

Differential Revision: D82265247


",2025-09-11 23:43:22+00:00,2025-09-22T02:33:31Z,,False,16,0,1,22,12,1,16,2025-09-22 02:33:30+00:00,60,295,False,True,False,False,False,False,1,7,1795,34,22,12,1,1,4.0,8.0,2025-09-12T19:15:17Z,pytorch
162771,closed,[ONNX] Support enable_gqa when dropout is non-zero,titaiwangms,"Fixes #162258
Related to https://github.com/microsoft/onnxscript/pull/2558
",2025-09-11 23:43:01+00:00,2025-09-12T04:02:02Z,,False,3,0,2,94,0,2,3,2025-09-12 04:01:00+00:00,50,75,False,True,False,False,False,False,2,2,548,108,101,7,1,2,3.0,3.0,2025-09-12T00:16:47Z,pytorch
162770,open,[c10d] destroy_process_group() works without default group created,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162571
* #162549
* #163058
* __->__ #162770
* #162545
* #162529



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 23:30:48+00:00,2025-09-21T03:00:36Z,,False,2,4,8,96,60,3,6,,66,259,False,False,False,False,False,False,3,1,28,344,219,125,1,8,2.0,1.0,2025-09-12T02:57:24Z,pytorch
162768,open,[dynamo] Implement iter with a polyfill,rtimpe,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162768

Currently most variable trackers implement `iter` via `_call_iter_tuple_list`.
This makes it difficult to customize the behavior of `iter` for different
variable types.  Instead, implement `iter` via a polyfill, which will delegate
to the appropriate `__iter__` method.

While this method is more flexible, it increases the overhead of dynamo tracing.
For example, `iter(x)` will generate 9x more instructions than the current
implementation for common iterable types.  Microbenchmarking shows a ~6x
slowdown for this operation.  I suspect this would be much less for realistic
workloads, but more work would be needed to get specific numbers.  If the
performance is a concern we could also consider adding a fast path for types
that are known to correctly implement `__iter__`.",2025-09-11 23:23:14+00:00,2025-09-23T23:54:09Z,,False,1,9,3,138,35,12,10,,39,872,False,False,False,False,False,False,12,0,148,175,139,36,1,3,2.0,1.0,2025-09-23T20:40:52Z,pytorch
162767,closed,[flatbuffer] Fix compile error due to discarded result,pulsejet,"Summary:
One of our builds fails because the return value of fread is discarded. Explicit cast to void fixes the build.

```log
In file included from fbcode/caffe2/torch/csrc/jit/mobile/import.cpp:15:
fbcode/caffe2/torch/csrc/jit/mobile/file_format.h:156:3: error: ignoring return value of function declared with 'warn_unused_result' attribute [-Werror,-Wunused-result]
  156 |   fread(data.get(), size, 1, f);
      |   ^~~~~ ~~~~~~~~~~~~~~~~~~~~~~
1 error generated.
...
BUILD FAILED
Failed to build 'fbcode//caffe2:libtorch (cfg:opt-linux-x86_64-clang19-no-san-opt-by-default#fef256f7ee896871)'
```

Test Plan:
No runtime behavior change. CI.

Rollback Plan:

Differential Revision: D82265002




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-11 23:22:52+00:00,2025-09-13T20:25:48Z,,False,10,6,1,2,1,1,16,2025-09-13 20:24:46+00:00,54,747,False,True,False,False,False,False,1,2,493,3,2,1,1,1,4.0,2.0,2025-09-12T16:21:45Z,pytorch
162766,closed,[ROCm][CI] unskip some test_memory_format tests,pragupta,"Fixes #70125.

Much of the work was done by #161687.
This PR is additional test cleanup.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-11 23:18:43+00:00,2025-09-12T15:03:48Z,,False,5,0,2,8,29,2,5,2025-09-12 15:02:45+00:00,47,206,False,True,False,False,False,False,2,2,494,37,8,29,1,2,3.0,2.0,2025-09-11T23:31:05Z,pytorch
162765,closed,[mxfp8 torch._scaled_grouped_mm] fix meta registration for 3d tensor,danielvegamyhre,"Meta registration checks for torch._scaled_grouped_mm has a bug for 3d ""B"" tensors. Namely, the scale shape for such a tensor should be 2d with shape (G, blocked_K * blocked_N), but it currently enforces an expected 3d shape of (G, blocked_K, blocked_N).

See Blas.cpp for correct validation logic [here](https://github.com/pytorch/pytorch/blob/8e217a9f6dc81e3d12697b04c3e611d82d9d866a/aten/src/ATen/native/cuda/Blas.cpp#L1622).

",2025-09-11 23:16:26+00:00,2025-09-12T03:52:57Z,,False,4,0,1,5,5,1,4,2025-09-12 03:51:54+00:00,68,430,False,True,False,False,False,False,1,3,512,10,5,5,1,1,3.0,3.0,2025-09-11T23:16:47Z,pytorch
162764,closed,fix cpp extension distributed warning spew,msaroufim,"With the new change we only log the warning if we're running non distributed code or if we're in rank 0. Unit testing that certain messages get printed on certain ranks only feels kinda jank so test plan is below instead


Test plan

```python
# torchrun --nproc_per_node=2 demo_fix.py 

import os
import logging

logging.getLogger('torch.utils.cpp_extension').setLevel(logging.DEBUG)

import torch
if 'RANK' in os.environ:
    torch.distributed.init_process_group('nccl')

from torch.utils.cpp_extension import _get_cuda_arch_flags
_get_cuda_arch_flags()

print(f""Rank {os.environ.get('RANK', '0')} done"")
```

Logs showing how how `TORCH_CUDA_ARCH_LIST`only shows up once if we explicitly set the the logging level to `logging.DEBUG`. It also improves the debug message to explain what the actual behavior will be

```
(source) [marksaroufim@devgpu005]~% torchrun --nproc_per_node=2 demo_fix.py

W0911 18:30:16.594000 1315439 /home/marksaroufim/pytorch/torch/distributed/run.py:814] 
W0911 18:30:16.594000 1315439 /home/marksaroufim/pytorch/torch/distributed/run.py:814] *****************************************
W0911 18:30:16.594000 1315439 /home/marksaroufim/pytorch/torch/distributed/run.py:814] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0911 18:30:16.594000 1315439 /home/marksaroufim/pytorch/torch/distributed/run.py:814] *****************************************
[rank0]:V0911 18:30:18.921000 1316753 pytorch/torch/utils/cpp_extension.py:2444] TORCH_CUDA_ARCH_LIST is not set, using TORCH_CUDA_ARCH_LIST='10.0+PTX' for visible GPU architectures. Set os.environ['TORCH_CUDA_ARCH_LIST'] to override.
Rank 0 done
Rank 1 done
```

But if we just use the default and comment out `logging.getLogger('torch.utils.cpp_extension').setLevel(logging.DEBUG)`

Then we get

```
(source) [marksaroufim@devgpu005]~% torchrun --nproc_per_node=2 demo_fix.py
W0911 18:14:33.926000 690759 /home/marksaroufim/pytorch/torch/distributed/run.py:814] 
W0911 18:14:33.926000 690759 /home/marksaroufim/pytorch/torch/distributed/run.py:814] *****************************************
W0911 18:14:33.926000 690759 /home/marksaroufim/pytorch/torch/distributed/run.py:814] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0911 18:14:33.926000 690759 /home/marksaroufim/pytorch/torch/distributed/run.py:814] *****************************************
Rank 0 done
Rank 1 done
(source) [marksaroufim@devgpu005]~% 
```

",2025-09-11 23:08:52+00:00,2025-09-12T15:21:25Z,,False,4,0,7,9,4,1,4,2025-09-12 06:12:49+00:00,42,2724,False,True,False,False,True,False,1,3,706,447,226,221,1,7,4.0,4.0,2025-09-12T03:12:05Z,pytorch
162763,closed,fix high=0 bug in nll_loss test,haifeng-jin,"Minor bug fix for the `nll_loss` test.
Before this PR, it runs `torch.randint(high=0)`, which will fail because it would try to generate a number that >= low and < high, i.e. x>=0 and x<0.

The test did not fail because that line is not run when testing on CPU because it failed earlier because of a unsupported dtype.
However, as we support TPUs at Google, this line is reached first before the dtype check, which triggers the bug.

To my understanding, these OpInfo should be general enough to support different hardware.
Fixing this obvious bug would make it more general cross different hardware.
",2025-09-11 23:05:20+00:00,2025-09-12T21:49:23Z,,False,3,1,2,2,1,1,4,2025-09-12 21:48:20+00:00,31,601,False,True,False,False,False,False,1,2,500,5,3,2,1,2,2.0,3.0,2025-09-12T12:55:41Z,pytorch
162761,closed,Fix windows path escape characters,ghostspiders,"Fixes #135954
Torch Inductor Windows Path Escape Characters

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-11 22:26:34+00:00,2025-09-17T23:40:46Z,,False,9,0,3,4,3,2,9,2025-09-17 23:39:43+00:00,34,270,False,True,False,False,False,False,2,8,1785,23598,17589,6009,2,3,4.0,9.0,2025-09-11T22:27:47Z,pytorch
162760,closed,[DCP] Decrease checkpoint background process Gloo pg init timeout,kevinmtang,"Summary:
Sometimes checkpoint background process creation times out during gloo pg init.
Attempting to destroy the process during that time can block the trainer thread until the timeout completes.

This diff reduces the pg init timeout from 30m -> 10m to reduce the cleanup time.

Test Plan:
CI

Rollback Plan:

Differential Revision: D81724668




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 21:36:34+00:00,2025-09-13T01:51:46Z,,False,13,0,1,4,1,1,13,2025-09-13 01:50:44+00:00,65,451,False,False,False,False,False,False,1,4,971,5,4,1,1,1,4.0,4.0,2025-09-12T03:19:45Z,pytorch
162759,closed,[Inductor] Fix ComboKernels failing due to missing helper functions,karthickai,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162759

Fixes: #162756 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos

Differential Revision: [D82257359](https://our.internmc.facebook.com/intern/diff/D82257359)",2025-09-11 21:21:26+00:00,2025-09-12T20:02:14Z,,False,5,0,2,27,0,2,5,2025-09-12 20:01:09+00:00,67,413,False,True,False,False,False,False,2,4,810,151,79,72,1,2,4.0,5.0,2025-09-11T21:34:15Z,pytorch
162758,closed,[ROCm] fix hardsigmoid op,dnikolaev-amd,"Currently std::min -> ::min did not work as expected on ROCm when input values >= 2147483648
It can be fixed by explicit typing std::min<opmath_t>

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-11 21:17:42+00:00,2025-09-12T15:08:21Z,,False,5,0,2,2,7,2,5,2025-09-12 15:07:17+00:00,25,264,False,True,False,False,False,False,2,2,814,15,5,10,1,2,3.0,2.0,2025-09-11T21:19:39Z,pytorch
162757,open,[inductor] partially type torch/_inductor/codegen/multi_kernel.py,pianpwk,"Test Plan:
None

Rollback Plan:

Differential Revision: D82253817




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 21:13:07+00:00,2025-09-14T17:36:56Z,,False,3,2,1,46,36,1,5,,65,271,False,False,False,False,False,False,1,0,0,82,46,36,1,1,1.0,0.0,2025-09-12T16:31:36Z,pytorch
162754,open,[scan] create fw and bw graphs via partitioning,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162754
* #161732
* #162025
* #161808
* #161664
* #161557


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 21:02:22+00:00,2025-09-23T22:49:59Z,,False,1,6,7,710,387,4,7,,47,338,False,False,False,False,False,False,4,0,0,27537,20686,6851,1,7,2.0,0.0,2025-09-19T12:24:17Z,pytorch
162753,open,[WIP] LocalTensor,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162753

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-11 20:58:16+00:00,2025-09-20T08:34:41Z,,False,1,17,7,1935,44,16,18,,17,245,False,False,False,False,False,False,16,0,0,93902,58244,35658,1,7,5.0,0.0,2025-09-11T21:33:08Z,pytorch
162752,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-11 20:58:13+00:00,2025-09-12T04:19:59Z,,False,6,0,1,1,1,1,6,2025-09-12 04:18:57+00:00,48,157,False,False,False,False,False,False,1,5,1401,2,1,1,1,1,2.0,5.0,2025-09-11T20:58:15Z,pytorch
162751,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-11 20:57:59+00:00,2025-09-13T04:17:56Z,,False,8,0,1,1,1,1,8,2025-09-13 04:16:54+00:00,46,156,False,False,False,False,False,False,1,7,2177,2,1,1,1,1,3.0,7.0,2025-09-11T20:58:00Z,pytorch
162750,open,add old way [test vllm],yangw-dev,"Fixes #ISSUE_NUMBER
",2025-09-11 20:53:56+00:00,2025-09-12T01:51:18Z,,False,1,0,1,2,1,1,1,,23,20,False,True,False,False,False,False,1,0,0,3,2,1,1,1,,,,pytorch
162749,closed,fix wait() missing in redistribute tensor,zpcore,"We notice that the wait() op is missing after collective op call: https://github.com/pytorch/pytorch/pull/162665#discussion_r2338460562.

The issue is that `_maybe_warp_tensor` calls AsyncCollectiveTensor in https://github.com/pytorch/pytorch/blob/3ad3bfe11df7bf542885a6dd2dc2ada4ab940e53/torch/distributed/_functional_collectives.py#L829 We need to check whether the wait() is required after collective op call.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 20:48:45+00:00,2025-09-17T16:25:34Z,,False,9,0,3,4,4,2,9,2025-09-17 16:24:31+00:00,41,515,False,True,False,False,False,False,2,8,3098,28,14,14,1,3,5.0,9.0,2025-09-11T22:29:28Z,pytorch
162747,closed,bring back the old vllm's use_existing_torch.py,yangw-dev,"vllm's pr will override our dependencies for torch.

quick fix to add the use_existing_torch.py. syncing with vllm now regarding the uv approach they have
",2025-09-11 20:41:51+00:00,2025-09-12T03:42:46Z,,False,3,0,7,41,2,4,3,2025-09-12 03:41:42+00:00,47,155,False,True,False,False,False,False,4,2,835,57,48,9,1,7,2.0,3.0,2025-09-11T22:23:16Z,pytorch
162744,closed,Fix operator benchmark issue#162708,jainapurva,"This PR skips memory metric calculation for ops which don't take tensor input, fixing the operator_benchmark bug

Fixes https://github.com/pytorch/pytorch/issues/162708
",2025-09-11 20:00:07+00:00,2025-09-12T06:55:09Z,,False,8,0,2,8,3,1,8,2025-09-12 06:55:09+00:00,35,169,False,True,False,False,False,False,1,6,1783,15,10,5,1,2,3.0,6.0,2025-09-12T04:01:23Z,pytorch
162740,closed,Refactor PrecompileContext to be considerably more debuggable,jamesjwu,"Summary:
This diff does a few things: 
- It refactors PrecompileContext to store DynamoCacheEntries directly on the context. This allows us at serialization time to check if the dynamo cache entry has all its backends ready for serialization, and if not, skip unnecessarily serializing it
- It also gives us the ability to print out a `debug` JSON, which contains a mapping for everything being serialized and deserialized. 

Here's an example of what that JSON looks like:

```
{
  ""artifacts"": {
    ""precompile_aot_autograd"": [
      ""__compiled_fn_8_306d538b_f7f8_4ab4_98a1_b5ff4493f99d""
    ],
    ""precompile_dynamo"": [
      {
        ""backend_ids"": [
          ""__compiled_fn_8_306d538b_f7f8_4ab4_98a1_b5ff4493f99d""
        ],
        ""fn_name"": ""TorchBenchmarkRunner.forward_and_backward_pass"",
        ""num_codes"": ""10"",
        ""python_version"": ""3.12.11+meta"",
        ""torch_version"": ""2.10.0a0+fb""
      }
    ]
  },
  ""num_entries"": 1
}
```

Test Plan:
Existing tests pass. 

NanoGPT tlparse showing the new debug:

https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/.tmpeIsL5G/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=10000

Note that there aren't compile ids since we're logging this in PrecompileContext.serialize() for now, where there isn't a compile yet. I think this is fine for now, as no compile ID makes sense here. If anything, these kind of belong in a ""Global"" compile ID, which I will not implement in this PR.

Rollback Plan:

Differential Revision: D82232574




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 18:34:51+00:00,2025-09-19T01:15:33Z,,False,18,0,1,322,152,4,18,2025-09-19 01:14:31+00:00,61,1720,False,True,False,False,False,True,4,1,476,474,322,152,1,1,2.0,1.0,2025-09-16T15:26:28Z,pytorch
162739,closed,[inductor] fix expand_shape when copy_shape is not a string,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162275
* __->__ #162739



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 18:26:27+00:00,2025-09-15T23:23:12Z,,False,5,0,2,1,1,1,5,2025-09-15 23:22:09+00:00,59,307,False,True,False,False,False,False,1,4,558,6451,4968,1483,1,2,4.0,4.0,2025-09-11T19:36:21Z,pytorch
162738,closed,Support vmap + custom autograd function/improve DTensor constructor inefficiency,pytorchbot,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162240

This makes gemma3 exportable on transformers=4.55.4

In HF, there is a torch funciton mode called TransformGetItemToIndex which internally calls custom autograd function. When this custom autograd function is called under vmap, It triggers CustomFunctionHigherOrderOP which error-ed because there was no pre-dispatch proxy mode implementation. 

Since there are number of requests lately to add various operators in pre-dispatch IR, I introduce a decorator in export that works similar to `allow_in_graph`. Basically:
1) We intercept custom_autograd_function.apply at pre-dispatch mode when this decorator is applied 
2) We apply `flat_apply` HOP to hide the pytree spec for this autograd function. Note that this adds restriction that this custom autograd function needs to take in fx-able types.
3) subclass constructor decorator is implemented similarly, so we just refactor it to use similar implementation as this new decorator. eventually we should delete the subclass constructor decorator. 
4) Move some code in subclass constructor decorator to exit early in non-export environment which should shave off some inefficiency (around 1% according to @swolchok 's benchmark) 


Fixes: https://github.com/pytorch/pytorch/issues/161563#issuecomment-3246309758 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D82141316](https://our.internmc.facebook.com/intern/diff/D82141316)",2025-09-11 18:22:39+00:00,2025-09-17T00:22:16Z,2025-09-17T00:22:16Z,True,1,0,1,212,43,5,1,2025-09-17 00:22:16+00:00,80,1622,False,True,False,False,True,True,5,0,0,255,212,43,1,1,1.0,0.0,2025-09-12T00:07:13Z,pytorch
162737,open,"[dynamo, nested graph breaks] fix nested step graph break related issues",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160611
* #163335
* __->__ #162737
* #160601
* #163503

Turns out codegen'ing a nested step graph break is significantly more complicated than first thought. The optimized function should actually do:
- call graph/load values/do side effects etc.
- call into the leaf's resume function, but skipped (this essentially step graph break function for just the leaf function)
- call into all the other resume functions, traced.

This PR also adds `torch._dynamo.step_unsupported()`, which can be used for internal testing purposes to better test step graph break handling.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 17:56:16+00:00,2025-09-23T00:01:38Z,,False,1,0,4,496,212,11,1,,72,817,False,True,False,False,False,False,11,0,0,28035,20601,7434,1,4,,,,pytorch
162736,closed,[async_tp] Do not apply mm+splt(k)+cat+rs when k is the last mm dim,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162736

```
       permute_7: ""bf16[1024, 4096][1, 1024]cuda:0"" = torch.ops.aten.permute.default(wait_tensor_9, [1, 0]);  wait_tensor_9 = None
        view_17: ""bf16[32768, 1024][1024, 1]cuda:0"" = torch.ops.aten.reshape.default(view_16, [32768, 1024]);  view_16 = None
        mm_3: ""bf16[32768, 4096][4096, 1]cuda:0"" = torch.ops.aten.mm.default(view_17, permute_7);  view_17 = permute_7 = None
        split_5 = torch.ops.aten.split.Tensor(mm_3, 1024, 1);  mm_3 = None
        getitem_25: ""bf16[32768, 1024][4096, 1]cuda:0"" = split_5[0]
        getitem_26: ""bf16[32768, 1024][4096, 1]cuda:0"" = split_5[1]
        getitem_27: ""bf16[32768, 1024][4096, 1]cuda:0"" = split_5[2]
        getitem_28: ""bf16[32768, 1024][4096, 1]cuda:0"" = split_5[3];  split_5 = None
        cat_3: ""bf16[131072, 1024][1024, 1]cuda:0"" = torch.ops.aten.cat.default([getitem_25, getitem_26, getitem_27, getitem_28]);  getitem_25 = getitem_26 = getitem_27 = getitem_28 = None
        reduce_scatter_tensor: ""bf16[32768, 1024][1024, 1]cuda:0"" = torch.ops._c10d_functional.reduce_scatter_tensor.default(cat_3, 'sum', 4, '5');  cat_3 = None
        wait_tensor_10: ""bf16[32768, 1024][1024, 1]cuda:0"" = torch.ops._c10d_functional.wait_tensor.default(reduce_scatter_tensor);  reduce_scatter_tensor = None
        view_18: ""bf16[4, 8192, 1024][8388608, 1024, 1]cuda:0"" = torch.ops.aten.reshape.default(wait_tensor_10, [4, 8192, 1024]);  wait_tensor_10 = None
```
->
```
        permute_7: ""bf16[1024, 4096][1, 1024]cuda:0"" = torch.ops.aten.permute.default(wait_tensor_9, [1, 0]);  wait_tensor_9 = None
        view_17: ""bf16[32768, 1024][1024, 1]cuda:0"" = torch.ops.aten.reshape.default(view_16, [32768, 1024]);  view_16 = None
        
        # No stacktrace found for following nodes
        inductor_force_stride_order_default_1 = torch.ops.prims.inductor_force_stride_order.default(view_17, (1, 32768));  view_17 = None
        fused_matmul_reduce_scatter_default = torch.ops.symm_mem.fused_matmul_reduce_scatter.default(inductor_force_stride_order_default_1, permute_7, 'sum', 1, '5');  inductor_force_stride_order_default_1 = permute_7 = None
        
         # File: /data/users/ivankobzarev/b/torchtitan-autoparallel/torchtitan/torchtitan/models/llama3/model/model.py:198 in forward, code: return self.wo(output)
        view_18: ""bf16[4, 8192, 1024][8388608, 1024, 1]cuda:0"" = torch.ops.aten.reshape.default(fused_matmul_reduce_scatter_default, [4, 8192, 1024]);  fused_matmul_reduce_scatter_default = None

```

will fail with mat muls mismatch:

```
    File ""/tmp/torchinductor_ivankobzarev/tmpexhi913l/6j/c6jlxwlb7pwqesmdblol7bqub6qq5ef7tibe5wvn37ojtwk5s4h2.py"", line 1618, in call
      buf72 = torch.ops.symm_mem.fused_matmul_reduce_scatter.default(buf71, reinterpret_tensor(buf68, (1024, 4096), (1, 1024), 0), 'sum', 1, '6')
    File ""/data/users/ivankobzarev/e/pytorch/torch/_ops.py"", line 841, in __call__
      return self._op(*args, **kwargs)
    File ""/data/users/ivankobzarev/e/pytorch/torch/distributed/_symmetric_memory/__init__.py"", line 1032, in _fused_matmul_reduce_scatter
      return _fused_matmul_reduce_scatter_impl(
    File ""/data/users/ivankobzarev/e/pytorch/torch/distributed/_symmetric_memory/__init__.py"", line 1097, in _fused_matmul_reduce_scatter_impl
      _pipelined_produce_and_all2all(
    File ""/data/users/ivankobzarev/e/pytorch/torch/distributed/_symmetric_memory/__init__.py"", line 419, in _pipelined_produce_and_all2all
      chunk_producer((rank + step) % group_size, p2p_buf)
    File ""/data/users/ivankobzarev/e/pytorch/torch/distributed/_symmetric_memory/__init__.py"", line 1093, in chunk_producer
      mm_out_op(A_shards[rank], B, **kwargs, out=out)
    File ""/data/users/ivankobzarev/e/pytorch/torch/_ops.py"", line 841, in __call__
      return self._op(*args, **kwargs)
  RuntimeError: mat1 and mat2 shapes cannot be multiplied (256x32768 and 1024x4096)
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 17:45:25+00:00,2025-09-24T14:20:39Z,,False,1,0,1,4,0,1,1,2025-09-24 14:20:39+00:00,67,4172,False,False,False,False,False,False,1,0,0,4,4,0,1,1,,,,pytorch
162735,closed,Refactor PrecompileContext to be considerably more debuggable,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162735

This diff does a few things:
- It refactors PrecompileContext to store DynamoCacheEntries directly on the context. This allows us at serialization time to check if the dynamo cache entry has all its backends ready for serialization, and if not, skip unnecessarily serializing it
- It also gives us the ability to print out a `debug` JSON, which contains a mapping for everything being serialized and deserialized.

Here's an example of what that JSON looks like:

```
{
  ""artifacts"": {
    ""precompile_aot_autograd"": [
      ""__compiled_fn_8_306d538b_f7f8_4ab4_98a1_b5ff4493f99d""
    ],
    ""precompile_dynamo"": [
      {
        ""backend_ids"": [
          ""__compiled_fn_8_306d538b_f7f8_4ab4_98a1_b5ff4493f99d""
        ],
        ""fn_name"": ""TorchBenchmarkRunner.forward_and_backward_pass"",
        ""num_codes"": ""10"",
        ""python_version"": ""3.12.11+meta"",
        ""torch_version"": ""2.10.0a0+fb""
      }
    ]
  },
  ""num_entries"": 1
}
```

Differential Revision: [D82232574](https://our.internmc.facebook.com/intern/diff/D82232574/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 17:35:36+00:00,2025-09-11T18:36:38Z,,False,1,0,1,233,67,3,1,2025-09-11 18:35:28+00:00,61,1304,False,True,False,False,False,True,3,0,0,300,233,67,1,1,,,,pytorch
162733,closed,[triton] Update 3.5 pin (AMD compilation fix + warp spec),davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162733

Fixes #162390

Also adds warp spec (thanks @manman-ren!)",2025-09-11 16:28:16+00:00,2025-09-11T18:20:23Z,,False,3,0,1,1,1,1,3,2025-09-11 18:19:19+00:00,57,150,False,True,False,False,False,False,1,2,1028,2,1,1,1,1,3.0,2.0,2025-09-11T17:45:50Z,pytorch
162732,closed,[MTIA Runtime] Add foreach_div ops to native_functions.yaml,nakuliyer,"Summary: Quick fix for runtime support on foreach_div, see D81274963. Fixed an issue that I created in that diff so that the CIs pass.

Test Plan:
CIs created in D81274963 and D81286593 pass.

Added some logs in [aten_mtia_ops.py](https://www.internalfb.com/code/fbsource/[c56272ba042c43c65517dcac254364cf732fcfa9]/fbcode/mtia/host_runtime/torch_mtia/aten_mtia_ops.cpp?lines=3676) to all the foreach_div ops. We can see that the correct MTIA kernels are being invoked in the tests.
https://www.internalfb.com/intern/testinfra/testrun/15481123829281588

Rollback Plan:


",2025-09-11 16:19:22+00:00,2025-09-17T18:01:00Z,,False,33,0,1,4,0,1,33,2025-09-17 17:44:06+00:00,59,570,False,True,False,False,False,False,1,19,3812,4,4,0,1,1,4.0,19.0,2025-09-11T16:26:14Z,pytorch
162729,open,Create alias c10 and c10_cuda to distinguish xplat and ovrsource.,ezyang,"Summary: #buildall

Test Plan:
sandcastle

This jobs in particular I found cause problems:

```
buck build --flagfile fbobjc/mode/buck2/ios-tests --config cxx.default_platform=iphonesimulator-arm64 fbsource//xplat/executorch/kernels/test:aten_op_avg_pool2d_testApple
buck build --flagfile fbsource//arvr/apps/hsr/mode/mac-arm/development --flagfile fbsource//arvr/projects/hsr/mode/avatarsdk/mainline --flagfile fbsource//arvr/apps/hsr/avatars_sandbox/flavor/codec fbsource//arvr/apps/hsr/avatars_sandbox:client_server_app_launcher
buck build --flagfile fbsource//arvr/mode/android/apk/vr_standalone/linux/dev fbsource//arvr/projects/codec_avatar/prod/client/androidTests:ca_clientlib_integration_tests
buck build --flagfile fbsource//arvr/mode/android/linux/dev fbsource//arvr/libraries/vega/colocation/inference:colocation_inference
buck build --flagfile fbsource//arvr/mode/android/linux/opt fbsource//arvr/projects/ariane/anon:simple_model_test
buck build --flagfile fbsource//arvr/mode/android/starlet/linux/release-stripped fbsource//arvr/projects/bolt/bolt/nn/tests:unit_test_torch_delegate
buck build --flagfile fbsource//arvr/mode/platform010/dev fbsource//xplat/supernova/arvrsync/supernova/palmvision:bolt_palm_vision_inference_test
buck build --flagfile fbsource//arvr/mode/win/vs2022/opt fbsource//arvr/libraries/vega/slam_mapper/semantics:semantic_inference_module
buck build --flagfile fbsource//arvr/mode/win/vs2022/opt fbsource//arvr/projects/nimble/prod/tools/HandReplay:HandReplay
buck2 build ""arvr/mode/fb-linux-nh/opt-stripped"" ""//arvr/projects/nimble/research/Ctrl-R/tests/...""
buck2 build ""arvr/mode/platform010/opt"" //arvr/libraries/ctrlr:ctrl-r -c ""python.ctrlr_version_override=3.12""
buck2 build arvr/mode/platform010/opt //arvr/projects/viper/viper/vega_mapper/simulator:vega_mapper_simulator
buck2 build fbsource//xplat/caffe2:torch_lib_ovrsource
buck2 run //arvr/tools/scripts/regression_tests:binary_size_increase_check -- --target_name //arvr/libraries/visioninterface/products/eureka:eureka_tracker_dev_shared --product_name eureka --build_mode release-strippedsource//arvr/mode/win/vs2022/opt fbsource//arvr/libraries/vega/slam_mapper/semantics:semantic_inference_module
```

Rollback Plan:

Differential Revision: D82224960


",2025-09-11 15:51:53+00:00,2025-09-11T18:47:47Z,,False,3,0,1,2,2,1,3,,65,2260,False,False,False,False,False,False,1,0,0,4,2,2,1,1,,,,pytorch
162726,closed,[ONNX] Set fallback=False by default,justinchuby,"This change addresses confusing error messages users encounter when using the ONNX exporter with default settings. Previously, `fallback=True` was the default, which would attempt to fall back to the TorchScript exporter when the dynamo path failed, leading to mixed error messages that obscured the actual issues.

## Problem

When `fallback=True` by default:
- Users get confusing error messages mixing dynamo and TorchScript export failures
- Error messages tell users to provide the `f` argument unnecessarily 
- Dynamo error messages get flushed with TorchScript errors when both paths fail
- Users expecting the dynamo path get unexpected fallback behavior

## Solution

Changed the default from `fallback=True` to `fallback=False` in both:
- `torch.onnx.export()` function 
- `torch.onnx._internal.exporter._compat.export_compat()` function

## Impact

**Before:**
```python
# Would fallback to TorchScript on dynamo failure, causing mixed error messages
torch.onnx.export(model, args)
```

**After:**  
```python
# Clean dynamo-only errors by default
torch.onnx.export(model, args)

# Advanced users can still opt-in to fallback behavior
torch.onnx.export(model, args, fallback=True)
```

Fixes #162697


cc @titaiwangms",2025-09-11 14:34:06+00:00,2025-09-15T22:24:04Z,,False,11,0,1,2,2,2,11,2025-09-11 18:10:02+00:00,36,1228,False,True,False,False,False,False,2,9,3822,4,2,2,1,1,5.0,9.0,2025-09-11T17:56:45Z,pytorch
162724,open,Used a fixed-length array to represent the interpreter stack,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #162724
* #162666
* #162521

CPython represents the stack of a frame as a fixed length C array + a stack pointer. The pointer points to the next free slot. When the interpreter executes a PUSH or a POP operation, it increments or decrements the pointer, without having to resize the underlying array. This allows fast O(1) append/pop operations.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 14:24:34+00:00,2025-09-17T17:33:59Z,,False,2,6,6,82,3,2,8,,60,614,False,True,False,False,False,False,2,0,25,15087,11370,3717,1,6,1.0,1.0,2025-09-13T18:16:07Z,pytorch
162721,closed,Remove tests-to-include from rocm-mi300 workflow,jithunnair-amd,"Accidentally introduced by https://github.com/pytorch/pytorch/pull/162288 (was meant to be a temporary change)


cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-11 12:56:00+00:00,2025-09-11T21:31:59Z,,False,6,0,1,0,1,1,6,2025-09-11 14:36:11+00:00,48,213,False,False,False,False,False,False,1,5,1772,1,0,1,1,1,2.0,5.0,2025-09-11T14:18:40Z,pytorch
162720,open,Adding check for step size=0 in unfold backward to avoid divide by 0 …,mansiag05,"…or FPE.

Fixes #142462 

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @malfet ",2025-09-11 12:25:35+00:00,2025-09-19T05:16:44Z,,False,4,2,2,11,0,2,6,,70,116,False,True,False,False,False,False,2,2,59,13,12,1,1,2,2.0,2.0,2025-09-11T13:25:30Z,pytorch
162718,open,Fix overflow in slow_conv3d when kernel size is too large.,mansiag05,"Also, adding check for padding to avoid segmentation fault caused by overflow.

Fixes #141846 


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @gujinghui @PenghuiCheng @jianyuh @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal @mcarilli @ptrblck @leslie-fang-intel @voznesenskym @penguinwu @EikanWang @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela @xmfan",2025-09-11 10:37:51+00:00,2025-09-22T18:12:30Z,,False,4,5,3,72,0,2,9,,58,593,False,True,False,False,False,False,2,3,633,98,85,13,1,3,4.0,6.0,2025-09-12T12:16:01Z,pytorch
162717,closed,fix: raise value error on init ParametrizationList if original.device != new.device,hanchchch,"raise value error on init `ParametrizationList`, if `original.device != new.device`.
currently `_maybe_set` will throw below error in such situations, which I think it's not convenient to debug.

```
[rank1]: RuntimeError: Attempted to set the storage of a tensor on device ""cuda:1"" to a storage on different device ""cpu"".  This is no longer allowed; the devices must match.
```",2025-09-11 10:18:53+00:00,2025-09-11T23:09:06Z,,False,7,0,2,24,0,2,7,2025-09-11 23:08:01+00:00,83,378,False,True,False,False,False,False,2,5,606,24,24,0,1,2,3.0,5.0,2025-09-11T10:21:42Z,pytorch
162716,open,"[FIX] Add negative number checking to `ADInterpreters.cpp`, aoviding out-of-bound array",shaoyuyoung,"If `front<0`, `front + arg_idx` may also be less than 0.",2025-09-11 10:12:18+00:00,2025-09-12T03:59:22Z,,False,4,1,3,1,0,1,5,,87,56,False,True,False,False,False,False,1,3,200,3931,2403,1528,1,3,3.0,3.0,2025-09-11T10:15:16Z,pytorch
162715,open,[FIX] add out-of-bound array check for `foreachTensorInplaceWithFlag`,shaoyuyoung,"Originally, we set three assert rules, but we have
https://github.com/pytorch/pytorch/blob/afdd4247a2251b3f4c2f4b402cb625f46d6784ba/aten/src/ATen/functorch/DynamicLayer.cpp#L303
The max size of `use_flag_relative` is 64.
AS a result, `end - begin` should be `<=64`

One more word: `end - begin` means the `number of return values`. Theoretically speaking, it seems unlikely that there will be more than 64 return values, so in general, this assertion error is not likely to be triggered to cause a crash.",2025-09-11 09:25:34+00:00,2025-09-14T06:26:33Z,,False,6,0,2,1,0,1,6,,69,504,False,True,False,False,False,False,1,5,1035,3945,2412,1533,1,2,3.0,6.0,2025-09-11T09:27:05Z,pytorch
162713,open,[FIX] [AOTI] improve `load_constants` bug fix,shaoyuyoung,"This is a supplement to #161887, which only fixes one place, but in fact, it may also cause problems during `MPS compilation`.
Please correct me if I am wrong.",2025-09-11 08:49:18+00:00,2025-09-14T01:14:05Z,,False,3,0,2,4,2,1,3,,45,159,False,True,False,False,True,False,1,2,341,8,5,3,1,2,2.0,3.0,2025-09-12T01:10:17Z,pytorch
162709,open,[Inductor] optimize scalar welford_reduce,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162795
* __->__ #162709


**Summary:**
Optimize scalar welford_reduce implementation, combining Welford algorithm with cascade sum to improve numerical stability. Specifically:

1. Use Welford algorithm to compute mean and variance.
2. Use cascade summation when computing sum over input for both mean and variance.

**Example:**
Take https://github.com/pytorch/pytorch/issues/141541 as an example:
```
import torch
import torch.nn as nn
torch.manual_seed(0)

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.gn = nn.GroupNorm(num_groups=32, num_channels=32)

    def forward(self, x):
        return self.gn(x)

model = Model().eval()
x = torch.randn(1, 32, 128, 128, 128)

with torch.no_grad():
    output = model(x)
    with torch._inductor.config.patch({""cpp.simdlen"": 0}):
        c_model = torch.compile(model)
        c_output = c_model(x)

print(torch.max(torch.abs(output - c_output)))
print(torch.allclose(output, c_output, 1.3e-6, 1e-5))
```
**logs**

- before
```
tensor(0.0005)
False
```
- After
```
tensor(1.4305e-06)
True
```

**Generated code:**
- before
```
cpp_fused_native_group_norm_0 = async_compile.cpp_pybinding(['float*', 'float*', 'const float*', 'const float*', 'const float*', 'float*'], '''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       float* in_out_ptr1,
                       const float* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr2)
{
    auto out_ptr1 = in_out_ptr0;
    auto out_ptr0 = in_out_ptr1;
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<float> tmp_acc0_arr[4];
                for (int i = 0; i < 4; i++)
                {
                    tmp_acc0_arr[i] = Welford<float>();
                }
                #pragma omp parallel num_threads(4)
                {
                    int tid = omp_get_thread_num();
                    Welford<float> tmp_acc0_local = Welford<float>();
                    #pragma omp for
                    for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(2097152L); x1+=static_cast<int64_t>(1L))
                    {
                        {
                            {
                                auto tmp0 = in_ptr0[static_cast<int64_t>(x1 + 2097152L*x0)];
                                tmp_acc0_local = welford_combine(tmp_acc0_local, tmp0);
                            }
                        }
                    }
                    tmp_acc0_arr[tid] = tmp_acc0_local;
                }
                for (int tid = 0; tid < 4; tid++)
                {
                    tmp_acc0 = welford_combine(tmp_acc0, tmp_acc0_arr[tid]);
                }
                in_out_ptr1[static_cast<int64_t>(x0)] = tmp_acc0.mean;
                in_out_ptr0[static_cast<int64_t>(x0)] = tmp_acc0.m2;
            }
        }
    }
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                {
                    auto tmp0 = out_ptr1[static_cast<int64_t>(x0)];
                    auto tmp6 = in_ptr1[static_cast<int64_t>(x0)];
                    auto tmp8 = out_ptr0[static_cast<int64_t>(x0)];
                    auto tmp11 = in_ptr2[static_cast<int64_t>(x0)];
                    auto tmp1 = static_cast<float>(2097152.0);
                    auto tmp2 = tmp0 / tmp1;
                    auto tmp3 = static_cast<float>(1e-05);
                    auto tmp4 = float(tmp2 + tmp3);
                    auto tmp5 = 1 / std::sqrt(tmp4);
                    auto tmp7 = float(tmp5 * tmp6);
                    auto tmp9 = decltype(tmp8)(-tmp8);
                    auto tmp10 = float(tmp9 * tmp7);
                    auto tmp12 = float(tmp10 + tmp11);
                    in_out_ptr0[static_cast<int64_t>(x0)] = tmp7;
                    in_out_ptr1[static_cast<int64_t>(x0)] = tmp12;
                }
            }
        }
    }
    #pragma omp parallel num_threads(4)
    {
        int tid = omp_get_thread_num();
        {
            #pragma omp for
            for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
            {
                #pragma GCC ivdep
                for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(2097152L); x1+=static_cast<int64_t>(1L))
                {
                    {
                        {
                            auto tmp0 = in_ptr0[static_cast<int64_t>(x1 + 2097152L*x0)];
                            auto tmp1 = in_out_ptr0[static_cast<int64_t>(x0)];
                            auto tmp3 = in_out_ptr1[static_cast<int64_t>(x0)];
                            auto tmp2 = float(tmp0 * tmp1);
                            auto tmp4 = float(tmp2 + tmp3);
                            out_ptr2[static_cast<int64_t>(x1 + 2097152L*x0)] = tmp4;
                        }
                    }
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, arg1_1, arg2_1 = args
        args.clear()
        assert_size_stride(arg0_1, (32, ), (1, ))
        assert_size_stride(arg1_1, (32, ), (1, ))
        assert_size_stride(arg2_1, (1, 32, 128, 128, 128), (67108864, 2097152, 16384, 128, 1))
        buf0 = empty_strided_cpu((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
        buf1 = empty_strided_cpu((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
        buf3 = reinterpret_tensor(buf1, (1, 32, 1, 1), (32, 1, 1, 1), 0); del buf1  # reuse
        buf4 = reinterpret_tensor(buf0, (1, 32, 1, 1), (32, 1, 1, 1), 0); del buf0  # reuse
        buf5 = empty_strided_cpu((1, 32, 128, 128, 128), (67108864, 2097152, 16384, 128, 1), torch.float32)
        # [Provenance debug handles] cpp_fused_native_group_norm_0:1
        cpp_fused_native_group_norm_0(buf3, buf4, arg2_1, arg0_1, arg1_1, buf5)
        del arg0_1
        del arg1_1
        del arg2_1
        return (buf5, )
```

- After
```
cpp_fused_native_group_norm_0 = async_compile.cpp_pybinding(['float*', 'float*', 'const float*', 'const float*', 'const float*', 'float*'], '''
#include <torch/csrc/inductor/cpp_prefix.h>
extern ""C""  void  kernel(float* in_out_ptr0,
                       float* in_out_ptr1,
                       const float* in_ptr0,
                       const float* in_ptr1,
                       const float* in_ptr2,
                       float* out_ptr2)
{
    auto out_ptr1 = in_out_ptr0;
    auto out_ptr0 = in_out_ptr1;
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                Welford<float> tmp_acc0 = Welford<float>();
                Welford<float> tmp_acc0_arr[4];
                for (int i = 0; i < 4; i++)
                {
                    tmp_acc0_arr[i] = Welford<float>();
                }
                #pragma omp parallel num_threads(4)
                {
                    int tid = omp_get_thread_num();
                    WelfordHelper<float, float, 4096> scalar_welford_helper0(static_cast<int64_t>(524288L));
                    Welford<float> tmp_acc0_local = Welford<float>();
                    #pragma omp for
                    for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(2097152L); x1+=static_cast<int64_t>(1L))
                    {
                        {
                            {
                                auto tmp0 = in_ptr0[static_cast<int64_t>(x1 + 2097152L*x0)];
                                tmp_acc0_local = welford_combine(tmp_acc0_local, tmp0, &scalar_welford_helper0);
                            }
                        }
                    }
                    tmp_acc0_local = welford_combine(tmp_acc0_local, &scalar_welford_helper0);
                    tmp_acc0_arr[tid] = tmp_acc0_local;
                }
                for (int tid = 0; tid < 4; tid++)
                {
                    tmp_acc0 = welford_combine(tmp_acc0, tmp_acc0_arr[tid]);
                }
                in_out_ptr1[static_cast<int64_t>(x0)] = tmp_acc0.mean;
                in_out_ptr0[static_cast<int64_t>(x0)] = tmp_acc0.m2;
            }
        }
    }
    {
        #pragma GCC ivdep
        for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
        {
            {
                {
                    auto tmp0 = out_ptr1[static_cast<int64_t>(x0)];
                    auto tmp6 = in_ptr1[static_cast<int64_t>(x0)];
                    auto tmp8 = out_ptr0[static_cast<int64_t>(x0)];
                    auto tmp11 = in_ptr2[static_cast<int64_t>(x0)];
                    auto tmp1 = static_cast<float>(2097152.0);
                    auto tmp2 = tmp0 / tmp1;
                    auto tmp3 = static_cast<float>(1e-05);
                    auto tmp4 = float(tmp2 + tmp3);
                    auto tmp5 = 1 / std::sqrt(tmp4);
                    auto tmp7 = float(tmp5 * tmp6);
                    auto tmp9 = decltype(tmp8)(-tmp8);
                    auto tmp10 = float(tmp9 * tmp7);
                    auto tmp12 = float(tmp10 + tmp11);
                    in_out_ptr0[static_cast<int64_t>(x0)] = tmp7;
                    in_out_ptr1[static_cast<int64_t>(x0)] = tmp12;
                }
            }
        }
    }
    #pragma omp parallel num_threads(4)
    {
        int tid = omp_get_thread_num();
        {
            #pragma omp for
            for(int64_t x0=static_cast<int64_t>(0L); x0<static_cast<int64_t>(32L); x0+=static_cast<int64_t>(1L))
            {
                #pragma GCC ivdep
                for(int64_t x1=static_cast<int64_t>(0L); x1<static_cast<int64_t>(2097152L); x1+=static_cast<int64_t>(1L))
                {
                    {
                        {
                            auto tmp0 = in_ptr0[static_cast<int64_t>(x1 + 2097152L*x0)];
                            auto tmp1 = in_out_ptr0[static_cast<int64_t>(x0)];
                            auto tmp3 = in_out_ptr1[static_cast<int64_t>(x0)];
                            auto tmp2 = float(tmp0 * tmp1);
                            auto tmp4 = float(tmp2 + tmp3);
                            out_ptr2[static_cast<int64_t>(x1 + 2097152L*x0)] = tmp4;
                        }
                    }
                }
            }
        }
    }
}
''')


async_compile.wait(globals())
del async_compile

class Runner:
    def __init__(self, partitions):
        self.partitions = partitions

    def recursively_apply_fns(self, fns):
        new_callables = []
        for fn, c in zip(fns, self.partitions):
            new_callables.append(fn(c))
        self.partitions = new_callables

    def call(self, args):
        arg0_1, arg1_1, arg2_1 = args
        args.clear()
        assert_size_stride(arg0_1, (32, ), (1, ))
        assert_size_stride(arg1_1, (32, ), (1, ))
        assert_size_stride(arg2_1, (1, 32, 128, 128, 128), (67108864, 2097152, 16384, 128, 1))
        buf0 = empty_strided_cpu((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
        buf1 = empty_strided_cpu((1, 32, 1, 1), (32, 1, 32, 32), torch.float32)
        buf3 = reinterpret_tensor(buf1, (1, 32, 1, 1), (32, 1, 1, 1), 0); del buf1  # reuse
        buf4 = reinterpret_tensor(buf0, (1, 32, 1, 1), (32, 1, 1, 1), 0); del buf0  # reuse
        buf5 = empty_strided_cpu((1, 32, 128, 128, 128), (67108864, 2097152, 16384, 128, 1), torch.float32)
        # [Provenance debug handles] cpp_fused_native_group_norm_0:1
        cpp_fused_native_group_norm_0(buf3, buf4, arg2_1, arg0_1, arg1_1, buf5)
        del arg0_1
        del arg1_1
        del arg2_1
        return (buf5, )
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-11 08:15:32+00:00,2025-09-19T12:23:09Z,,False,1,0,6,78,66,3,1,,41,12593,False,True,False,False,True,False,3,0,0,33051,24221,8830,1,6,,,,pytorch
162707,closed,Add api info for torch._C._nn.pyi,orangeH25,"Fix part of #148404 

APis involved are as followed:

- multilabel_margin_loss
- multi_margin_loss
- nll_loss_nd
- relu6
- relu6_
",2025-09-11 07:19:35+00:00,2025-09-21T06:21:36Z,,False,7,0,1,56,0,1,7,2025-09-21 06:17:17+00:00,33,130,False,True,False,False,False,False,1,6,996,56,56,0,1,1,3.0,6.0,2025-09-15T06:04:38Z,pytorch
162706,open,Fix decorators skipping NCCL tests,Flamefire,"Avoid failures caused by tests exiting via sys.exit instead of `unittest.skip`

In particular it will not try to start the test (causing forks into subprocess) just to stop them (killing the subprocess) which is done in the test setup

Using `unittest.skip` decorators avoids the starting of the test in the first place.

Reland of https://github.com/pytorch/pytorch/pull/158846 fixing a missing argument



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 06:59:29+00:00,2025-09-12T21:20:04Z,,False,10,0,8,55,103,6,10,,34,509,False,True,False,False,False,False,6,9,3156,176,64,112,1,8,3.0,9.0,2025-09-12T16:12:24Z,pytorch
162705,closed,Add a new API torch.xpu.can_device_access_peer for Intel GPU,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162705

# Motivation
Aligned with other backends, this PR introduces an new API `torch.xpu.can_device_access_peer`, which is used in vllm distributed [scenarios](https://github.com/vllm-project/vllm/blob/2048c4e37909a42847cd2f51c7e0cf92e3b63466/vllm/distributed/device_communicators/custom_all_reduce.py#L37)

",2025-09-11 06:52:40+00:00,2025-09-16T18:01:30Z,,False,10,0,9,55,0,7,10,2025-09-16 18:00:25+00:00,60,396,False,False,False,False,False,False,7,9,1158,84106,50377,33729,1,9,4.0,10.0,2025-09-12T02:14:19Z,pytorch
162702,closed,[dtensor] do not mutate specs when doing sharding prop,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163133
* __->__ #162702
* #161458

Because these specs are cached by reference. So by reusing them and mutating them, we're overwriting the cached specs of another op. I'm just fixing these 2, there are more instances, we'll need to do an audit separately.

This fixes a few opinfo tests, but side note that `PYTORCH_OPINFO_SAMPLE_INPUT_INDEX=0 python test/distributed/tensor/test_dtensor_ops.py TestDTensorOpsCPU.test_dtensor_op_db_nn_functional_multi_head_attention_forward_cpu_float32` fails for me locally even on the base commit, but it is not marked as xfail

NOTE: I am renaming `_wrap_output_spec_tensor_meta` so that external libraries will loudly fail. You should migrate to the functional `_create_output_spec_with_new_tensor_meta` or create your own mutation wrapper and take responsibility for the cache! This should be improved in https://github.com/pytorch/pytorch/issues/162731

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 06:29:57+00:00,2025-09-17T09:32:43Z,,False,5,3,14,19,10,2,8,2025-09-17 09:32:42+00:00,54,1075,False,True,False,False,True,False,2,4,996,83286,50685,32601,1,14,5.0,5.0,2025-09-13T14:54:34Z,pytorch
162701,closed,Bump setuptools from 72.1.0 to 78.1.1 in /.github/requirements,dependabot[bot],"Bumps [setuptools](https://github.com/pypa/setuptools) from 72.1.0 to 78.1.1.
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/pypa/setuptools/blob/main/NEWS.rst"">setuptools's changelog</a>.</em></p>
<blockquote>
<h1>v78.1.1</h1>
<h2>Bugfixes</h2>
<ul>
<li>More fully sanitized the filename in PackageIndex._download. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4946"">#4946</a>)</li>
</ul>
<h1>v78.1.0</h1>
<h2>Features</h2>
<ul>
<li>Restore access to _get_vc_env with a warning. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4874"">#4874</a>)</li>
</ul>
<h1>v78.0.2</h1>
<h2>Bugfixes</h2>
<ul>
<li>Postponed removals of deprecated dash-separated and uppercase fields in <code>setup.cfg</code>.
All packages with deprecated configurations are advised to move before 2026. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4911"">#4911</a>)</li>
</ul>
<h1>v78.0.1</h1>
<h2>Misc</h2>
<ul>
<li><a href=""https://redirect.github.com/pypa/setuptools/issues/4909"">#4909</a></li>
</ul>
<h1>v78.0.0</h1>
<h2>Bugfixes</h2>
<ul>
<li>Reverted distutils changes that broke the monkey patching of command classes. (<a href=""https://redirect.github.com/pypa/setuptools/issues/4902"">#4902</a>)</li>
</ul>
<h2>Deprecations and Removals</h2>
<ul>
<li>Setuptools no longer accepts options containing uppercase or dash characters in <code>setup.cfg</code>.</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pypa/setuptools/commit/8e4868a036b7fae3208d16cb4e5fe6d63c3752df""><code>8e4868a</code></a> Bump version: 78.1.0 → 78.1.1</li>
<li><a href=""https://github.com/pypa/setuptools/commit/100e9a61ad24d5a147ada57357425a8d40626d09""><code>100e9a6</code></a> Merge pull request <a href=""https://redirect.github.com/pypa/setuptools/issues/4951"">#4951</a></li>
<li><a href=""https://github.com/pypa/setuptools/commit/8faf1d7e0ca309983252e4f21837b73ee12e960f""><code>8faf1d7</code></a> Add news fragment.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/2ca4a9fe4758fcd39d771d3d3a5b4840aacebdf7""><code>2ca4a9f</code></a> Rely on re.sub to perform the decision in one expression.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/e409e8002932f2b86aae7b1abc8f8c2ebf96df2c""><code>e409e80</code></a> Extract _sanitize method for sanitizing the filename.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/250a6d17978f9f6ac3ac887091f2d32886fbbb0b""><code>250a6d1</code></a> Add a check to ensure the name resolves relative to the tmpdir.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/d8390feaa99091d1ba9626bec0e4ba7072fc507a""><code>d8390fe</code></a> Extract _resolve_download_filename with test.</li>
<li><a href=""https://github.com/pypa/setuptools/commit/4e1e89392de5cb405e7844cdc8b20fc2755dbaba""><code>4e1e893</code></a> Merge <a href=""https://github.com/jaraco/skeleton"">https://github.com/jaraco/skeleton</a></li>
<li><a href=""https://github.com/pypa/setuptools/commit/3a3144f0d2887fa37c06550f42a101e9eebd953a""><code>3a3144f</code></a> Fix typo: <code>pyproject.license</code> -&gt; <code>project.license</code> (<a href=""https://redirect.github.com/pypa/setuptools/issues/4931"">#4931</a>)</li>
<li><a href=""https://github.com/pypa/setuptools/commit/d751068fd2627d6d8f1729e39cbcd8119049998f""><code>d751068</code></a> Fix typo: pyproject.license -&gt; project.license</li>
<li>Additional commits viewable in <a href=""https://github.com/pypa/setuptools/compare/v72.1.0...v78.1.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=setuptools&package-manager=pip&previous-version=72.1.0&new-version=78.1.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/pytorch/pytorch/network/alerts).

</details>",2025-09-11 06:00:29+00:00,2025-09-12T04:03:35Z,2025-09-12T04:03:27Z,True,1,0,1,1,1,1,1,2025-09-12 04:03:27+00:00,62,5701,False,True,True,True,False,False,1,0,17,2,1,1,1,1,2.0,1.0,2025-09-11T23:21:14Z,pytorch
162699,closed,[Inductor][FP8] Add new scaled_mm and scaled_persistent_mm configs to Inductor FP8 Triton templates,jananisriram,"Summary:
Add new `scaled_mm` and `scaled_persistent_mm` configs to `template_heuristics.py` for Inductor FP8 Triton templates. These configs are a representative subset of the most performant configs generated from exhaustively autotuning FP8 Triton kernels with per-tensor and per-row scaling.

See this [spreadsheet](https://docs.google.com/spreadsheets/d/1Fal1vhFUJIUcLpM2kJect6IkgeUFvCY-nUr3RTupM_4/edit?gid=1732602731#gid=1732602731) for benchmarks and performance metrics.

Test Plan:
Verify that configs do not error, i.e.
```
CUDA_VISIBLE_DEVICES=0 TRITON_PRINT_AUTOTUNING=1 TRITON_ALWAYS_COMPILE=1 TORCH_LOGS=+i
nductor TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 ENABLE_PERSISTENT_TMA_MATMUL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 buck2 run mode/{opt,inplace} pytorch/tritonbench:run -- --op fp8_gemm --only pt2_fp8_gemm --metrics tflops,accuracy --input-loader={input_path} --output=""{output_csv}"" --atol=1e-2 --rtol=0.5 2>&1 | tee {log_file}
```

Rollback Plan:

Reviewed By: NikhilAPatel, PaulZhang12

Differential Revision: D81651226




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 05:42:11+00:00,2025-09-12T07:46:04Z,,False,4,0,1,8,0,1,4,2025-09-11 21:21:08+00:00,99,1243,False,False,False,True,False,False,1,2,523,8,8,0,1,1,3.0,3.0,2025-09-11T13:51:27Z,pytorch
162698,closed,[ONNX] Set fallback=False by default,Copilot,"This change addresses confusing error messages users encounter when using the ONNX exporter with default settings. Previously, `fallback=True` was the default, which would attempt to fall back to the TorchScript exporter when the dynamo path failed, leading to mixed error messages that obscured the actual issues.

## Problem

When `fallback=True` by default:
- Users get confusing error messages mixing dynamo and TorchScript export failures
- Error messages tell users to provide the `f` argument unnecessarily 
- Dynamo error messages get flushed with TorchScript errors when both paths fail
- Users expecting the dynamo path get unexpected fallback behavior

## Solution

Changed the default from `fallback=True` to `fallback=False` in both:
- `torch.onnx.export()` function 
- `torch.onnx._internal.exporter._compat.export_compat()` function

## Impact

**Before:**
```python
# Would fallback to TorchScript on dynamo failure, causing mixed error messages
torch.onnx.export(model, args)
```

**After:**  
```python
# Clean dynamo-only errors by default
torch.onnx.export(model, args)

# Advanced users can still opt-in to fallback behavior
torch.onnx.export(model, args, fallback=True)
```

## Compatibility

This change is well-aligned with existing test expectations:
- 10+ existing tests already explicitly use `fallback=False`
- Zero tests rely on `fallback=True` as default
- No test changes required beyond adding verification

Users who need fallback behavior can simply add `fallback=True` to their export calls.

Fixes #162697.

<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/pytorch/pytorch/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.


cc @justinchuby @titaiwangms",2025-09-11 05:34:32+00:00,2025-09-11T14:35:59Z,,False,4,2,4,2,2,2,6,2025-09-11 14:33:25+00:00,36,2099,False,True,False,True,False,False,2,1,58,62,31,31,2,3,1.0,1.0,2025-09-11T14:32:08Z,pytorch
162696,closed,[CUDAGraph][UX] warn many times for rerecording from dynamic shapes,BoyuanFeng,"Excessive re-recording CUDAGraphs lead to bad performance. We previously warns once if this happens.

However, the limit (=50) is too high and users may just observe bad performance before actually seeing the warning message. Even worse, users may not see the warning message when there are many other logs. @anijain2305 reported that he never saw this warning message when using transformer library, but he DOES observe slowdown due to cudagraph re-recording & needs to turn off cudagraph.

#162663 attempts to hard error when re-recording too many times due to dynamic shapes. But it is a bc-breaking change. Actually, hf-t5-generate model in torchbench failed due to 256 re-recordings.

This PR a) reduces to smaller limit (=8); and b) makes the warning more spam, i.e., warn once for every distinct shapes once the limit is reached.

Fixes #162299

cc @mcarilli @ezyang @eellison @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-11 05:30:43+00:00,2025-09-12T06:39:37Z,,False,3,2,2,1,3,2,5,2025-09-12 06:38:35+00:00,67,1090,False,True,False,False,False,False,2,2,493,52,25,27,1,2,5.0,2.0,2025-09-11T21:14:08Z,pytorch
162695,open,[dynamo][guards] Fail on an unknown framelocals to dict conversion,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162627
* __->__ #162695
* #162694



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 05:22:59+00:00,2025-09-19T12:23:32Z,,False,6,0,1,1,3,1,6,,66,286,False,False,False,False,False,False,1,5,906,4,1,3,1,1,3.0,5.0,2025-09-11T07:14:19Z,pytorch
162694,closed,[dynamo][guards] Do not consturct framelocals to dict on GlobalsGuardAccessor,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162627
* #162695
* __->__ #162694



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-11 05:22:56+00:00,2025-09-11T15:01:04Z,,False,2,0,1,5,0,1,2,2025-09-11 15:01:03+00:00,77,286,False,False,False,False,False,False,1,1,48,5,5,0,1,1,2.0,1.0,2025-09-11T07:14:59Z,pytorch
162693,closed,update the baseline data for the operator benchmark,LifengWang,"According to the results of the last four operator benchmark runs, we found that five models achieved more than a 30% improvement compared to the baseline. Therefore, we will update the operator benchmark baseline data. 
We use the average results from the four runs as the new baseline for the five models.

And add a pull request trigger for the operator benchmark workflow

Benchmarking   Framework | Benchmarking   Module Name | Case Name | tag | run_backward | baseline   old | r1 | r2 | r3 | r4 | avg | speedup
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
PyTorch | add | add_M1_N1_K1_cpu | short | FALSE | 3.9497 | 2.57 | 2.54 | 2.38 | 2.31 | 2.45 | 1.61
PyTorch | functional.hardtanh | functional.hardtanh_dims(512	512)_contigFalse_inplaceFalse_dtypetorch.quint8 | short | FALSE | 67.118 | 50.02 | 49.80 | 46.78 | 48.94 | 48.88 | 1.37
PyTorch | relu6 | relu6_dims(512	512)_contigFalse_inplaceFalse_dtypetorch.quint8 | short | FALSE | 68.739 | 51.17 | 51.19 | 48.07 | 50.42 | 50.21 | 1.37
PyTorch | relu6 | relu6_dims(256	1024)_contigFalse_inplaceFalse_dtypetorch.quint8 | short | FALSE | 69.1875 | 51.97 | 52.77 | 50.00 | 51.24 | 51.50 | 1.34
PyTorch | functional.hardtanh | functional.hardtanh_dims(256	1024)_contigFalse_inplaceFalse_dtypetorch.quint8 | short | FALSE | 67.436 | 50.98 | 51.69 | 49.06 | 49.87 | 50.40 | 1.34

@chuanqi129 @huydhn @desertfire @jainapurva 

",2025-09-11 05:03:58+00:00,2025-09-12T20:54:34Z,,False,11,0,2,10,6,2,11,2025-09-12 20:53:32+00:00,51,1393,False,False,False,False,True,False,2,9,1236,16,10,6,1,2,4.0,9.0,2025-09-11T05:06:46Z,pytorch
162691,open,compile_kernel: fix nvrtc discovery and force cuda init,msaroufim,"Untested since I don't have a docker available on dev gpu to test this https://github.com/pytorch/pytorch/issues/162367 - there's 2 additions
1. I borrow the nvrtc discovery logic from cuda-python https://github.com/NVIDIA/cuda-python/tree/main/cuda_pathfinder/cuda/pathfinder/_dynamic_libs
2. I was more explicit about initializing cuda


EDIT: So I talked with @tinglvv offline and it seems like the issue is her NVRTC passes are

```
/usr/local/lib/python3.12/site-packages/nvidia/cu13/lib/libnvrtc-builtins.so.13.0
/usr/local/lib/python3.12/site-packages/nvidia/cu13/lib/libnvrtc.so.13
```

So they're not global but instead in site-packages, this has 2 workarounds
1. I added an explicit nvrtc_path argument to compile_kernel that way people can pass in whatever they want
2. It's possible to fix discovery by using symlinks `sudo ln -s /usr/local/lib/python3.12/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12 /usr/local/lib/libnvrtc.so.12`

Both approaches above are fairly non intrusive

But if we really want discovery to work reliably it's kinda nasty and we could follow allmost exactly what cuda-python is doing below, testing this seems nasty but if CI passes and @tinglvv code passes then this might work. My vote is against this unless we expect site-packages to be one of. the main places users will have their cuda libraries

```python
def _get_nvrtc_library(nvrtc_path: Optional[str] = None) -> ctypes.CDLL:
    """"""Find and load NVRTC library following cuda-python's approach.
    
    Based on:
    https://github.com/NVIDIA/cuda-python/blob/main/cuda_bindings/cuda/bindings/_path_finder/find_nvidia_dynamic_library.py
    https://github.com/NVIDIA/cuda-python/blob/main/cuda_bindings/cuda/bindings/_path_finder/supported_libs.py
    """"""
    import os
    import glob
    import site
    
    if nvrtc_path is not None:
        try:
            return ctypes.CDLL(nvrtc_path)
        except OSError as e:
            raise OSError(f""Failed to load NVRTC library from provided path: {nvrtc_path}"") from e
    
    if sys.platform == ""win32"":
        nvrtc_lib_names = [
            ""nvrtc64_130_0.dll"",
            ""nvrtc64_120_0.dll"",
            ""nvrtc64_112_0.dll"",
            ""nvrtc64_111_0.dll"",
            ""nvrtc64_110_0.dll"",
        ]
    else:
        nvrtc_lib_names = [
            ""libnvrtc.so.13"",
            ""libnvrtc.so.12"",
            ""libnvrtc.so.11.2"",
            ""libnvrtc.so.11.1"",
            ""libnvrtc.so.11.0"",
            ""libnvrtc.so"",
        ]
    
    def _find_sub_dirs_all_sitepackages(sub_dirs):
        results = []
        site_packages_dirs = [site.getusersitepackages()] + site.getsitepackages()
        
        for base in site_packages_dirs:
            if not os.path.isdir(base):
                continue
                
            current_paths = [base]
            for sub_dir in sub_dirs:
                next_paths = []
                for current_path in current_paths:
                    if sub_dir == ""*"":
                        try:
                            for entry in os.listdir(current_path):
                                entry_path = os.path.join(current_path, entry)
                                if os.path.isdir(entry_path):
                                    next_paths.append(entry_path)
                        except OSError:
                            continue
                    else:
                        next_path = os.path.join(current_path, sub_dir)
                        if os.path.isdir(next_path):
                            next_paths.append(next_path)
                current_paths = next_paths
            
            results.extend(current_paths)
        return results
    
    def _try_load_from_paths(lib_names, search_paths):
        for lib_dir in search_paths:
            if not os.path.isdir(lib_dir):
                continue
                
            for lib_name in lib_names:
                lib_path = os.path.join(lib_dir, lib_name)
                if os.path.isfile(lib_path):
                    try:
                        return ctypes.CDLL(lib_path)
                    except OSError:
                        continue
                        
                if not lib_name.endswith(""*""):
                    pattern = lib_name + ""*""
                    for match in sorted(glob.glob(os.path.join(lib_dir, pattern)), reverse=True):
                        if os.path.isfile(match):
                            try:
                                return ctypes.CDLL(match)
                            except OSError:
                                continue
        return None
    
    for lib_name in nvrtc_lib_names:
        try:
            return ctypes.CDLL(lib_name)
        except OSError:
            continue
    
    if sys.platform == ""win32"":
        nvidia_sub_dirs = (""nvidia"", ""*"", ""bin"")
    else:
        nvidia_sub_dirs = (""nvidia"", ""*"", ""lib"")
    
    site_packages_paths = _find_sub_dirs_all_sitepackages(nvidia_sub_dirs)
    lib = _try_load_from_paths(nvrtc_lib_names, site_packages_paths)
    if lib is not None:
        return lib
    
    cuda_home = os.environ.get(""CUDA_HOME"") or os.environ.get(""CUDA_PATH"")
    if cuda_home:
        if sys.platform == ""win32"":
            cuda_subdirs = [""bin""]
        else:
            cuda_subdirs = [""lib64"", ""lib""]
            
        cuda_paths = [os.path.join(cuda_home, subdir) for subdir in cuda_subdirs]
        lib = _try_load_from_paths(nvrtc_lib_names, cuda_paths)
        if lib is not None:
            return lib
    
    conda_prefix = os.environ.get(""CONDA_PREFIX"")
    if conda_prefix:
        if sys.platform == ""win32"":
            conda_paths = [os.path.join(conda_prefix, ""Library"", ""bin"")]
        else:
            conda_paths = [os.path.join(conda_prefix, ""lib""), os.path.join(conda_prefix, ""lib64"")]
            
        lib = _try_load_from_paths(nvrtc_lib_names, conda_paths)
        if lib is not None:
            return lib
    
    if sys.platform != ""win32"":
        system_paths = [
            ""/usr/local/cuda/lib64"",
            ""/usr/local/cuda/lib"",
            ""/usr/lib/x86_64-linux-gnu"",
            ""/usr/lib64"",
            ""/usr/lib"",
            ""/opt/cuda/lib64"",
            ""/opt/cuda/lib"",
        ]
        lib = _try_load_from_paths(nvrtc_lib_names, system_paths)
        if lib is not None:
            return lib
    
    raise OSError(""Could not find any NVRTC library"")
```",2025-09-11 04:42:26+00:00,2025-09-23T16:02:34Z,,False,2,1,14,44,6,2,3,,55,6440,False,True,False,True,False,False,2,1,325,44619,34390,10229,1,14,2.0,1.0,2025-09-16T17:27:23Z,pytorch
162690,closed,"[CuTe] Change the logic of pycute manipulation ops like coalesce, complement from co-lex to lex",fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161106
* #161016
* __->__ #162690
* #162414
* #162534
* #162413

PyTorch tensor iteration (.view, contiguous, broadcasting) and NumPy array indexing all follow lexicographic (row-major) order. In Lexicographic (lex) on (i0, i1, …, i{k-1}): the leftmost index(stride is larger) changes fastest and the rightmost index changes slowest and usually last dim is contiguous.

However original pycute is all based on co-lex, after porting their code into pytorch and some cosmetic change, we now make it lex so that we can use it for use cases like device mesh internal bookkeeping and other stuff as well.

Changes included in this PR:
1. We changes all API ported in, included prefix_product(stride inferring and rename it to suffix_product), idx2crd, crd2idx, coalesce, composition, complement, right_inverse and left_inverse to make sure they are working in the lex way.
2. Added more unit test cases for some API mentioned above since existing unit tests do not have full coverage.
3. One bug fix inside composition, which will lead to infinite recursive call.

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 04:35:02+00:00,2025-09-16T19:54:54Z,,False,4,36,7,237,33,6,40,2025-09-16 19:53:48+00:00,95,1232,False,True,False,False,False,False,6,3,859,607,405,202,1,7,3.0,4.0,2025-09-11T04:37:00Z,pytorch
162689,closed,[TESTING ONLY] break scatter,blaine-rister,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 04:34:25+00:00,2025-09-11T22:19:38Z,,False,2,0,1,1,0,1,2,2025-09-11 22:19:38+00:00,28,223,False,True,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
162688,closed,[Triton] [Inductor] Restrict subprocess autotuning to just Triton,njriasan,"Summary: Restricts subprocess benchmarking to only `TritonTemplateCaller`, which is expected by the underlying `target` method. THhis triggered a bug with large K shapes because the decompose k is `SubgraphChoiceCaller`.

Test Plan:
mm autotuning with a large k and `TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1`

Rollback Plan:

Differential Revision: D82181924




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-11 04:02:01+00:00,2025-09-24T01:04:48Z,,False,23,0,2,12,7,1,23,2025-09-24 01:03:44+00:00,65,567,False,True,False,False,False,False,1,19,4902,33,19,14,1,2,6.0,20.0,2025-09-11T14:00:04Z,pytorch
162686,closed,[Inductor-FX] Support ScatterFallback,blaine-rister,"# Problem
Inductor has a `ScatterFallback` op with custom Python and C++ wrapper codegen macros. This is used in certain situations where the default Triton codegen doesn't apply, and especially for reductions which need to be deterministic. Since this op used direct Python/C++ codegen, it wasn't compatible with the FX backend.

# Feature
This PR refactors the associated wrapper codegen to support `ScatterFallback`. This follows the same basic steps that were used for other fallback ops including `MultiOutput` and `ExternKernel`:

1. Create a new wrapper IR op called `ScatterFallbackLine`. Move the logic in `ScatterFallback.cogeden` to `ScatterFallbackLine.codegen`, to prevent it from affecting the FX backend. This logic is unsafe for FX because it may generate Python or C++ strings with methods like `codegen_reference()`.
2. To eleminate the dependence on `V.graph`, move language-specific logic to the respective wrapper codegen subclasses. In this case, C++ codegen has some special logic, which is moved to `CppWrapperCpu`.
3. Create a new method in `FXWrapperCodegen` to handle `ScatterFallbackLine`. 

# Test plan
Added a couple of CI tests for the FX backend with scatter fallbacks.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 03:30:52+00:00,2025-09-12T08:42:56Z,,False,3,0,5,116,27,6,3,2025-09-12 08:41:53+00:00,37,1404,False,False,True,False,False,True,6,2,493,23043,18317,4726,1,5,3.0,2.0,2025-09-12T00:23:45Z,pytorch
162684,closed,paths to exclude shape guards,avikchaudhuri,"Summary: Easier to land than https://www.internalfb.com/diff/D82030581

Test Plan:
everything blamed by https://www.internalfb.com/diff/D80713603 (except some old exir tests)

Rollback Plan:

Differential Revision: D82180349


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-11 03:26:50+00:00,2025-09-11T15:35:11Z,,False,6,0,1,22,11,2,6,2025-09-11 15:34:09+00:00,29,283,False,False,False,False,False,False,2,2,493,33,22,11,1,1,3.0,2.0,2025-09-11T03:37:54Z,pytorch
162682,closed,Make inline tests to use new exporter and fix some issues around it,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* #162993
* #162992
* __->__ #162682
* #162559
* #162558
* #162557

inline_and_install_module export variant is our long term state so it is better to use the new tracer for this. It also uncovered bunch of minor bugs because with inline_and_install_module, the nn_module_stack generation is changed a bit. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D82478648](https://our.internmc.facebook.com/intern/diff/D82478648)",2025-09-11 03:18:11+00:00,2025-09-17T20:10:36Z,,False,9,0,7,93,19,2,9,2025-09-17 20:09:31+00:00,67,678,False,True,False,False,False,False,2,8,1627,27465,19880,7585,1,7,3.0,8.0,2025-09-15T20:27:48Z,pytorch
162681,open,[SymmMem] Add `num_active_allocations` and dealloc checks,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #162681
* #162680
* #163298

As titled.
`del tensor` should cause `num_active_allocations` to go down to 0.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 01:36:50+00:00,2025-09-18T23:31:06Z,,False,1,4,8,67,4,10,5,,57,307,False,False,False,False,False,False,10,0,0,381,243,138,1,8,2.0,0.0,2025-09-11T18:51:46Z,pytorch
162680,closed,[SymmMem] Fix memory allocation hold-up,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162681
* __->__ #162680
* #163298

Problem:
Without MemPool it looks like nvshmem backend never deallocates memory.

Cause:
Handles in `symm_mems_` (a map) keeps reference to memory allocations.

Solution:
- Remove reference to allocation from handles -- the reference is never used anyway.
- Use `unique_ptr` instead of `shared_ptr` to wrap allocation to ensure single ownership.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-11 01:36:46+00:00,2025-09-19T21:37:47Z,,False,10,7,7,18,13,2,17,2025-09-19 20:19:50+00:00,39,574,False,True,False,False,False,False,2,9,2065,296,168,128,1,7,6.0,10.0,2025-09-11T03:34:50Z,pytorch
162679,open,fp8-kv-cache-flex,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162679


```Py
import torch
from torch.nn.attention.flex_attention import flex_attention
from functools import partial
from typing import Callable
import logging
from transformer_nuggets.utils.benchmark import benchmark_cuda_function_in_microseconds


logging.getLogger(""transformer_nuggets"").setLevel(logging.INFO)
Tensor = torch.Tensor

def scaled_flex(
    query: Tensor,
    key: Tensor,
    value: Tensor,
    k_scale: float,
    v_scale: Tensor,
    **kwargs
):
    combined_scale =  k_scale * (1.0 / (query.size(-1) ** 0.5))
    out = flex_attention(query, key, value, scale=combined_scale, **kwargs)
    # out = out.to(torch.query.dtype)
    out *= v_scale
    return out


compiled_scale_flex = torch.compile(scaled_flex, fullgraph=True, dynamic=False)
compiled_flex = torch.compile(flex_attention, fullgraph=True, dynamic=False)

torch._dynamo.config.cache_size_limit = 1000

make_tensor = partial(torch.randn, device=""cuda"", dtype=torch.bfloat16)
B, H, seq_q, seq_k, D = 16, 16, 1, 1024, 64

query = make_tensor((B, H, seq_q, D))
key = make_tensor((B, H, seq_k, D))
value = make_tensor((B, H, seq_k, D))

q_scale = torch.tensor(1, device=""cuda"", dtype=torch.float32)
k_scale = torch.tensor(1, device=""cuda"", dtype=torch.float32).item()
v_scale = torch.tensor(1, device=""cuda"", dtype=torch.float32).item()

k_fp8 = key.to(torch.float8_e4m3fn)
v_fp8 = value.to(torch.float8_e4m3fn)



for _ in range(10):
    out = compiled_scale_flex(
        query,
        k_fp8,
        v_fp8,
        k_scale=k_scale,
        v_scale=v_scale,
    )
    out_bf16 = compiled_flex(query, key, value)


time_fp8 = benchmark_cuda_function_in_microseconds(compiled_scale_flex, query, k_fp8, v_fp8, k_scale=k_scale, v_scale=v_scale)
time_bf16 = benchmark_cuda_function_in_microseconds(compiled_flex, query, key, value)

print(f""fp8 time: {time_fp8} us"")
print(f""bf16 time: {time_bf16} us"")
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-09-11 01:15:08+00:00,2025-09-12T19:01:53Z,,False,2,0,2,11,8,3,2,,17,2205,False,False,False,False,False,False,3,0,0,19,11,8,1,2,,,,pytorch
162678,closed,[Inductor][FP8] Validate exhaustive autotuning for FP8 Inductor templates,jananisriram,"Summary: Validate exhaustive autotuning for FP8 Inductor templates: scaled MM templates require `block_k >= 32`. Before, exhaustive autotuning defaulted to a limited set of autotuning configs, as limitations for exhaustively autotuning on FP8 shapes had not been tested.

Test Plan:
```
CUDA_VISIBLE_DEVICES=0 TRITON_PRINT_AUTOTUNING=1 TRITON_ALWAYS_COMPILE=1 TORCH_LOGS=+inductor TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 ENABLE_PERSISTENT_TMA_MATMUL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_SEARCH_SPACE=DEFAULT buck2 run mode/{opt,inplace} pytorch/t
ritonbench:run -- --op fp8_gemm --only torch_fp8_gemm,pt2_fp8_gemm --metrics tflops,accuracy --input-loader=/home/jananisriram/personal/exhaustive_autotune_rowwise_persistent_tma/json_fi
les/rowwise_ptma_0.json --output=""/home/jananisriram/personal/exhaustive_autotune_rowwise_persistent_tma/autotune/gpu0_bench.csv"" --atol=1e-2 --rtol=0.5 2>&1 | tee ~/personal/exhaustive_
autotune_rowwise_persistent_tma/autotune/gpu0.log
```

Rollback Plan:

Differential Revision: D82174075




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-11 01:13:29+00:00,2025-09-12T02:13:39Z,,False,7,0,1,13,10,1,7,2025-09-12 02:12:37+00:00,73,1258,False,False,False,False,False,False,1,2,493,23,13,10,1,1,3.0,2.0,2025-09-11T18:09:52Z,pytorch
162673,closed,[Triton] [Inductor] Pruned failed compilations from Autotuning candidates,njriasan,"Summary:
When exahaustively autotuning a new template you may hit situations that lead to compilation failures. This template will still attempt to autotune because nothing was marking this as failed and in my experiments lead to a crash/segfault if I didn't set `TORCHINDUCTOR_AUTOTUNE_IN_SUBPROC=1`.

To help eliminate this issue this PR marks any template that fails to compile as ""failed"" and then removes all of the failed templates from the choice candidates. In the case where it would have just failed to compile twice, this should at least reduce compilation time.

Test Plan:
Tested locally when experminenting with the new blackwell templates and a Triton version that contains a bug related to `num_warps < 4`.

Rollback Plan:

Differential Revision: D82172207




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-11 00:43:45+00:00,2025-09-11T22:16:56Z,,False,6,2,1,20,2,2,8,2025-09-11 21:22:38+00:00,73,986,False,True,False,False,False,False,2,2,557,22,20,2,1,1,4.0,3.0,2025-09-11T14:01:10Z,pytorch
162672,closed,[inductor][ez] add ChoiceCaller annotations,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162672

# why

- enable ChoiceCaller generation to provide extra information that
  feedback_saver_fns (functions registered to run at the bench of
  benchmarking) can use afterwards
- users that extend ChoiceCaller creation e.g. by creating their own
  InductorChoices can use this to shuttle through information

# what

- add an annotations dictionary to ChoiceCaller class

# testing

n/a

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-09-11 00:29:27+00:00,2025-09-16T20:51:03Z,,False,6,0,3,4,0,1,6,2025-09-16 20:49:57+00:00,43,667,False,False,False,False,False,False,1,5,1401,8872,6805,2067,1,3,3.0,5.0,2025-09-11T19:00:40Z,pytorch
162670,open,fix some typos,co63oc,"Fixes #ISSUE_NUMBER

fix some typos

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-11 00:08:38+00:00,2025-09-12T12:39:15Z,,False,1,0,1,15,15,6,1,,14,118,False,True,False,False,False,False,6,0,0,30,15,15,1,1,,,,pytorch
162669,open,Start recording inductor provenance,c00w,"Summary:
This stores information on where fx graphs come from, which makes it
significantly easier to debug.

One outstanding question

1) I only stored the kernel stack traces, do we also want the node mappings?

Test Plan:
I wrote a explicit logging test which makes a module, fx traces it, compiles it, and makes sure the logging infomration shows up.

```
clr@devvm17763 ~/fbsource/fbcode/caffe2/test/dynamo
 % buck2 test @//mode/opt fbcode//caffe2/test/dynamo:test_dynamo -- test_utils

File changed: fbsource//xplat/caffe2/test/dynamo/test_utils.py
File changed: fbcode//caffe2/test/dynamo/test_utils.py
Buck UI: https://www.internalfb.com/buck2/528dea32-2416-4a62-a1ec-39f3c0efdd2e
Test UI: https://www.internalfb.com/intern/testinfra/testrun/13229324015574003
Network: Up: 0B  Down: 0B
Executing actions. Remaining     0/2      
Command: test.                                                                                 
Time elapsed: 17.3s
Tests finished: Pass 16. Fail 0. Fatal 0. Skip 0. Build failure 0
```

Rollback Plan:

Differential Revision: D82037582




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-11 00:08:28+00:00,2025-09-25T03:28:11Z,,False,7,0,1,30,1,4,7,,35,1291,False,True,False,False,False,False,4,0,0,31,30,1,1,1,,,,pytorch
162667,open,[DDP] Rebuild gradient buckets when computation graph changes,whwjiang,"This is a revival of an old PR that sought to rebuild gradient buckets when the computation graph changes between forwards and backwards passes when training using the distributed data parallel module. This can happen in very special cases, in which case the gradient buckets are static by default.

I and my partner were able to show that non-overlapped communication and computation on workers in the cluster was effectively reduced to zero, but that a majority of the time that was spent not overlapped previously was now spent rebuilding buckets and communicating them out across workers. Still, there was a net savings time spent in a training iteration (forward + gradient computation + backward pass). Our findings are documented here: https://www.overleaf.com/read/vrpzhtgwgwpb#9ed72b

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 23:43:08+00:00,2025-09-14T18:35:00Z,,False,3,0,4,56,16,3,3,,61,895,False,False,False,True,False,False,3,1,42,708489,475059,233430,2,4,1.0,1.0,2025-09-12T04:34:21Z,pytorch
162666,open,Cache most used constants in Dynamo,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162666
* #162521

This PR adds a cache for some constants like small integers, latin-1
one-char strings, True, False, None, NotImplemented. CPython caches a
bit more constants than this, which we might want to add in the future.
This list can also be extended to constants that are more common in ML
workloads.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-10 23:42:54+00:00,2025-09-18T04:34:51Z,,False,3,0,9,231,159,20,3,,35,568,False,False,False,False,False,False,20,2,283,15173,11405,3768,1,9,1.0,2.0,2025-09-12T02:08:25Z,pytorch
162665,closed,[DTensor] Introduce DebugMode,SherlockNoMad,"Introduce a lightweight TorchDispatchMode for understanding the magic behind DTensor. 

- Tracks redistribution, see `redistribute_input(input_idx, from_placement, to_placement)` 
- Optionally tracks torch-level functions, via `__torch_function__`
- Optionally tracks FakeTensor operations, which was needed for propagating tensor meta as a step of sharding propagation
- Optionally tracks real tensor operations, including functional c10d op, and regular ops
- Calls are shown in the hierarchical structure! 
- shorthand representation
  - dt: DTesnor, ft: FakeTensor, t: Tensor
  - DM(2, 2) == DeviceMesh(shape = [2, 2])
  - [R, P, S(0)] == Placement[Replicate, Partial, Shard(0)]
  - f32[8,8] == float32 with shape[8, 8]


```
  debug_mode = DTensorDebugMode(record_faketensor=False, record_realtensor=True)
  with debug_mode:
      torch.mm(x_dtensor, y_dtensor)
  print(debug_mode.debug_string())
```
produces: 
```
  torch.mm(dt: f32[8, 8][S(0)], dt: f32[8, 32][S(0)])
    aten::mm(dt: f32[8, 8][S(0)], dt: f32[8, 32][S(0)])
      redistribute_input(1, [S(0)], [R])
        _c10d_functional::all_gather_into_tensor(t: f32[1, 32], 8, 0)
        _c10d_functional::wait_tensor(t: f32[8, 32])
      aten::mm(t: f32[1, 8], t: f32[8, 32])
```


Another example, for torch.einsum
```
  torch.functional.einsum(bld,dnh->blnh, dt: f32[16, 6, 8][P, R], dt: f32[8, 4, 4][R, P])
    aten::unsqueeze(dt: f32[16, 6, 8][P, R], 3)
      aten::unsqueeze(t: f32[16, 6, 8], 3)
    aten::unsqueeze(dt: f32[16, 6, 8, 1][P, R], 4)
      aten::unsqueeze(t: f32[16, 6, 8, 1], 4)
    aten::permute(dt: f32[16, 6, 8, 1, 1][P, R], [0, 1, 3, 4, 2])
      aten::permute(t: f32[16, 6, 8, 1, 1], [0, 1, 3, 4, 2])
    aten::unsqueeze(dt: f32[8, 4, 4][R, P], 3)
      aten::unsqueeze(t: f32[8, 4, 4], 3)
    aten::unsqueeze(dt: f32[8, 4, 4, 1][R, P], 4)
      aten::unsqueeze(t: f32[8, 4, 4, 1], 4)
    aten::permute(dt: f32[8, 4, 4, 1, 1][R, P], [3, 4, 1, 2, 0])
      aten::permute(t: f32[8, 4, 4, 1, 1], [3, 4, 1, 2, 0])
    aten::permute(dt: f32[16, 6, 1, 1, 8][P, R], [0, 1, 4, 2, 3])
      aten::permute(t: f32[16, 6, 1, 1, 8], [0, 1, 4, 2, 3])
    aten::view(dt: f32[16, 6, 8, 1, 1][P, R], [1, 96, 8])
      aten::view(t: f32[16, 6, 8, 1, 1], [1, 96, 8])
    aten::permute(dt: f32[1, 1, 4, 4, 8][R, P], [4, 2, 3, 0, 1])
      aten::permute(t: f32[1, 1, 4, 4, 8], [4, 2, 3, 0, 1])
    aten::view(dt: f32[8, 4, 4, 1, 1][R, P], [1, 8, 16])
      aten::view(t: f32[8, 4, 4, 1, 1], [1, 8, 16])
    aten::bmm(dt: f32[1, 96, 8][P, R], dt: f32[1, 8, 16][R, P])
      redistribute_input(0, [P, R], [S(2), S(2)])
        aten::chunk(t: f32[1, 96, 8], 4, 2)
        aten::cat(['t: f32[1, 96, 2]', 't: f32[1, 96, 2]', 't: f32[1, 96, 2]', 't: f32[1, 96, 2]'])
        _c10d_functional::reduce_scatter_tensor(t: f32[4, 96, 2], sum, 4, 2)
        aten::clone(t: f32[1, 96, 1])
      redistribute_input(1, [R, P], [S(1), S(1)])
        aten::chunk(t: f32[1, 8, 16], 4, 1)
        aten::clone(t: f32[1, 2, 16])
        aten::chunk(t: f32[1, 2, 16], 2, 1)
        aten::cat(['t: f32[1, 1, 16]', 't: f32[1, 1, 16]'])
        _c10d_functional::reduce_scatter_tensor(t: f32[2, 1, 16], sum, 2, 3)
        _c10d_functional::wait_tensor(t: f32[1, 1, 16])
      aten::bmm(t: f32[1, 96, 1], t: f32[1, 1, 16])
    aten::view(dt: f32[1, 96, 16][P, P], [16, 6, 1, 4, 4])
      aten::view(t: f32[1, 96, 16], [16, 6, 1, 4, 4])
    aten::permute(dt: f32[16, 6, 1, 4, 4][P, P], [0, 1, 3, 4, 2])
      aten::permute(t: f32[16, 6, 1, 4, 4], [0, 1, 3, 4, 2])
    aten::view(dt: f32[16, 6, 4, 4, 1][P, P], [16, 6, 4, 4])
      aten::view(t: f32[16, 6, 4, 4, 1], [16, 6, 4, 4])
```



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 23:18:15+00:00,2025-09-22T15:55:57Z,,False,14,31,10,424,3,5,45,2025-09-16 07:30:09+00:00,29,3728,False,True,False,False,False,False,5,13,6614,901,661,240,2,10,10.0,17.0,2025-09-10T23:59:55Z,pytorch
162664,closed,Build vLLM aarch64 nightly wheels,huydhn,PyTorch has published its aarch64 nightly wheels for all CUDA version after https://github.com/pytorch/pytorch/pull/162364,2025-09-10 23:18:08+00:00,2025-09-13T03:46:07Z,,False,9,0,13,44,30,4,9,2025-09-13 03:43:59+00:00,33,122,False,False,False,False,False,False,4,8,1735,98,56,42,1,13,3.0,8.0,2025-09-12T19:18:43Z,pytorch
162663,closed,[CUDAGraph][UX] error instead of warn for excessive re-record,BoyuanFeng,"Excessive re-recording CUDAGraphs lead to bad performance. We previously warns if this happens.

However, the limit (=50) is too high and users may just observe bad performance before actually seeing the warning message. Even worse, users may not see the warning message when there are many other logs. @anijain2305 reported that he never saw this warning message when using transformer library, but he DOES observe slowdown due to cudagraph re-recording & needs to turn off cudagraph.

In practice, we usually observe either tons of re-recordings, or rare re-recording. This PR reduces the limit to 8 and hard error instead of warn. Also provides more error messages.

Fixes #162299


cc @mcarilli @ezyang @eellison @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela @mlazos",2025-09-10 23:17:59+00:00,2025-09-12T05:11:53Z,,False,5,0,4,92,145,7,5,2025-09-12 05:11:53+00:00,61,936,False,True,False,False,False,False,7,3,812,8891,5871,3020,1,4,3.0,4.0,2025-09-11T03:02:08Z,pytorch
162662,open,Back DTensor with Symmetric Memory ops,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

If user allocate inner of DTensor with Symmetric Memory, DTensor's `redistribute` op gets accelerated by SymmMem. For example:
```
local_tensor = symm_mem.empty((32, 32), device=self.device)
dtensor = DTensor.from_local(
    local_tensor, device_mesh, [Partial()],
)
dtensor.redistribute(device_mesh, [Replicate()])  # Two-shot all-reduce will be triggered
```

- Intra-node support only.
- Needs explicit enablement.

TODO:
- Use MemPool for creating output tensors

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 23:17:25+00:00,2025-09-11T18:47:20Z,,False,3,4,1,92,13,2,7,,38,663,False,False,False,False,False,False,2,2,414,105,92,13,1,1,3.0,2.0,2025-09-11T04:01:54Z,pytorch
162661,closed,[RecordFunction] Add Scope for Record Function Fast,sraikund16,"Differential Revision: D82164587


",2025-09-10 23:05:34+00:00,2025-09-15T21:02:54Z,,False,5,0,1,34,20,2,5,2025-09-15 21:01:51+00:00,51,35,False,False,False,False,False,False,2,1,476,54,34,20,1,1,2.0,1.0,2025-09-11T21:29:55Z,pytorch
162660,closed,[opaque obj] Initial OpaqueObject,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163278
* #163277
* #163276
* __->__ #162660

A big pain point ppl have with custom ops is that they do not accept arbitrary input/outputs. In this PR we create the concept of an ""OpaqueObject"" which allows users to pass arbitrary python objects into custom operators.

Some still slightly annoying parts with this implementation:
- The schema of the operator is `__torch__.torch.classes.aten.OpaqueObject` instead of whatever python type
- `@torch.library.custom_op` doesn't work.. yet?

UX:
```python
from torch._library.opaque_object import make_opaque, get_payload

# your custom python class
class OpaqueQueue:
    def __init__(self, queue: list[torch.Tensor], init_tensor_: torch.Tensor) -> None:
        super().__init__()
        self.queue = queue
        self.init_tensor_ = init_tensor_

    def push(self, tensor: torch.Tensor) -> None:
        self.queue.append(tensor)

    def pop(self) -> torch.Tensor:
        if len(self.queue) > 0:
            return self.queue.pop(0)
        return self.init_tensor_

    def size(self) -> int:
        return len(self.queue)

queue = OpaqueQueue([], torch.zeros(3))
obj: torch._C.ScriptObject = make_opaque(queue)

# obj.payload stores a direct reference to this python queue object
self.assertEqual(get_payload(obj), queue)

# This is able to be passed through the dispatcher
torch.ops._TestOpaqueObject.queue_push(obj, torch.ones(3))
self.assertTrue(queue.size(), 1)
```

Authoring a custom op:

```python
lib = torch.library.Library(""_TestOpaqueObject"", ""FRAGMENT"")

torch.library.define(
    f""_TestOpaqueObject::queue_push"",
    ""(__torch__.torch.classes.aten.OpaqueObject a, Tensor b) -> ()"",
    tags=torch.Tag.pt2_compliant_tag,
    lib=lib,
)


@torch.library.impl(f""{libname}::queue_push"", ""CompositeExplicitAutograd"", lib=lib)
def push_impl(q: torch._C.ScriptObject, b: torch.Tensor) -> None:
    # We can get the payload directly by get_payload(q)
    queue = get_payload(q)
    assert isinstance(queue, OpaqueQueue)
    queue.push(b)
```

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-10 23:03:22+00:00,2025-09-22T18:30:32Z,,False,8,9,8,156,0,5,17,2025-09-22 18:30:31+00:00,33,2132,False,False,False,False,False,False,5,7,1952,51899,37447,14452,1,8,4.0,8.0,2025-09-11T00:48:35Z,pytorch
162659,closed,[BE] Make PyObjectSlot use a global PyInterpreter,PaliC,"This pr gets rid of the pyobj_interpreter_ variable from PyObjectSlot and saves a word in the process

Gonna ask for review from @huydhn as there are some changes to CI.

Testing: imported internally and the failed android build seems to work now!

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @EikanWang @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-10 23:01:40+00:00,2025-09-25T08:54:28Z,,False,42,0,17,45,73,10,42,2025-09-25 08:53:24+00:00,49,618,False,False,False,False,False,False,10,29,8401,305,124,181,2,15,6.0,31.0,2025-09-12T20:25:51Z,pytorch
162658,closed,[FSDP][Replicate] tests replicate with prefetching,anshul-si,"**Summary:** Prefetching tests validate that distributed training systems can correctly overlap communication and computation by pre-loading parameters or data before they're needed. This test ensures the prefetching mechanism doesn't break training correctness while potentially improving performance by reducing idle time where computation waits for communication to complete.

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_explicit_prefetching

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* __->__ #162658
* #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 23:01:00+00:00,2025-09-18T01:07:35Z,,False,3,0,4,49,0,1,3,2025-09-18 01:05:19+00:00,50,818,False,False,False,False,False,False,1,2,493,23709,17173,6536,1,4,3.0,2.0,2025-09-12T21:29:40Z,pytorch
162657,closed,[ez][CI] Fix docs push in nightly workflow,clee2000,"HUD metrics page says docs push hasn't happened in 21 days
<img width=""293"" height=""142"" alt=""image"" src=""https://github.com/user-attachments/assets/f930aab8-0503-4bf2-b962-8c375dec6b78"" />

I guess main branch docs just haven't been updated?  Did anyone notice?  Do we care?

Either way I think this should fix it

Likely started after https://github.com/pytorch/pytorch/pull/161182",2025-09-10 22:59:29+00:00,2025-09-16T18:59:51Z,,False,8,0,1,1,1,1,8,2025-09-11 16:45:45+00:00,42,383,False,True,False,True,False,False,1,6,1448,2,1,1,1,1,5.0,7.0,2025-09-10T23:35:46Z,pytorch
162656,closed,[FSDP][Replicate] tests replicate module functionality when used multiple times in a forward pass,anshul-si,"**Summary:** Verifies that Replicate works correctly when a module is used multiple times in a single forward pass.

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_multi_forward_module

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* __->__ #162656
* #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 22:50:16+00:00,2025-09-18T01:03:15Z,,False,3,0,4,51,0,1,3,2025-09-18 01:02:11+00:00,97,555,False,False,False,False,False,False,1,2,493,23711,17175,6536,1,4,3.0,2.0,2025-09-17T21:20:49Z,pytorch
162655,closed,[aoti] add config for libtorch free so,yushangdi,"Users can specify the following to get a libtorch_free `.so`. 

""aot_inductor.use_libtorch"": False, 

The following config is only used for torchnative (see https://github.com/meta-pytorch/torchnative/pull/110). It's not intended to be used by executorch. The reason we need it for torchnative is because a lot of the symbol definitions in torchnative repo is only in header files. 

""aot_inductor.libtorch_free_header"": ""/data/users/shangdiy/torchnative/standalone,/data/users/shangdiy/torchnative/"" (or their custom headers)

The main motivating use case is for executorch to produce a libtorch free `.so`.

TODO for follow-up PR: this flag should be consolidated with the `compile_standalone` flag. 
  

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 22:42:53+00:00,2025-09-12T07:32:10Z,,False,3,11,1,107,21,4,14,2025-09-12 07:31:07+00:00,38,908,False,False,False,False,False,False,4,2,493,128,107,21,1,1,4.0,2.0,2025-09-11T18:34:43Z,pytorch
162654,closed,[FSDP][Replicate] tests replicate runs forward/backward for root and non-root module,anshul-si,"**Summary:** Verifies that Replicate correctly handles the scenario where forward and backward passes are run through both the root module and a non-root module. 

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_non_root_forward_backward


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* __->__ #162654
* #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 22:38:44+00:00,2025-09-18T00:48:27Z,,False,3,0,4,46,0,1,3,2025-09-18 00:47:23+00:00,84,608,False,False,False,False,False,False,1,2,493,23706,17170,6536,1,4,3.0,2.0,2025-09-12T21:28:09Z,pytorch
162653,closed,type error,kinto0,"Fixes #ISSUE_NUMBER
",2025-09-10 22:34:55+00:00,2025-09-10T22:35:07Z,,False,1,0,1,1,10882,1,1,2025-09-10 22:35:07+00:00,10,20,False,True,False,False,False,False,1,0,0,10883,1,10882,1,1,,,,pytorch
162652,closed,add support for hint_override in mark_unbacked,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162652

Very similar to https://github.com/pytorch/pytorch/pull/161007 except now for mark_unbacked.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-10 22:30:22+00:00,2025-09-17T22:31:01Z,,False,10,4,14,92,12,4,14,2025-09-17 22:29:56+00:00,46,425,False,False,False,False,False,False,4,9,3717,194,137,57,1,14,4.0,10.0,2025-09-10T23:02:58Z,pytorch
162651,closed,[DTensor] Add guide for what to do about mixed torch.Tensor and DTensor operations,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162651
* #162307
* #162117

Also updates the error message to point to the guide.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 22:30:17+00:00,2025-09-18T06:42:10Z,,False,6,3,3,72,0,2,9,2025-09-18 06:41:06+00:00,82,270,False,False,False,False,False,False,2,5,900,84,78,6,1,2,3.0,5.0,2025-09-11T03:56:17Z,pytorch
162650,closed,[FSDP][Replicate] tests replicate parity for single and multigroup,anshul-si,"**Summary:** The parity tests train two identical models with the same inputs - one using a reference approach and one using the test approach (replicate) - then check that both models produce identical losses. This ensures the distributed training methods don't change the mathematical results compared to standard training.

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_train_parity_single_group
2. pytest test/distributed/_composable/test_replicate_training.py -k test_train_parity_multi_group
3. pytest test/distributed/_composable/test_replicate_training.py -k test_train_parity_multi_group_cpu_offload_eager
 
Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* __->__ #162650
* #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 22:28:09+00:00,2025-09-18T00:39:54Z,,False,3,0,4,230,4,1,3,2025-09-18 00:38:53+00:00,66,987,False,False,False,False,False,False,1,2,493,23884,17346,6538,1,4,3.0,2.0,2025-09-17T22:30:23Z,pytorch
162649,closed,[ROCm][CI] benchmark must patch fbgemm_gpu with tbb dep,jithunnair-amd,"fbgemm adds tbb as a dep only for rocm to avoid missing tbb symbols at import.  But the way it was done was in setup.py to add the linker flag to CMAKE_CXX_FLAGS and it wasn't working for reasons unknown to me.  But what did work was to add tbb as a dep in the cmake file.  [We have a PR against upstream fbgemm](https://github.com/pytorch/FBGEMM/pull/4859) for that.  Meanwhile, a much smaller patch is applied here in this PR until the fbgemm rocm ci commit hash is moved forward to include the tbb patch from upstream.

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-10 22:22:49+00:00,2025-09-11T14:34:18Z,,False,3,0,1,13,5,1,3,2025-09-11 14:10:55+00:00,55,623,False,False,False,False,False,False,1,2,817,18,13,5,1,1,2.0,2.0,2025-09-10T22:25:08Z,pytorch
162648,closed,[ROCm] Build FBGEMM_GENAI for gfx942 only,jithunnair-amd,"Fixes build timeouts >4h on libtorch build jobs: https://hud.pytorch.org/hud/pytorch/pytorch/75e7f49f9c70116d7c4f8f86c3d0688ade306284/1?per_page=50&name_filter=inux-binary-libtorch%20%2F%20libtorch-rocm&mergeEphemeralLF=true

Brings back code to narrow down CK compilation targets from https://github.com/pytorch/pytorch/commit/69a25f68884a168550695fdb1a7c310c54d29536#diff-ce80f3115ab2f6be5142f0678a1fc92c6b2d7727766ce44f48726c99e720f777

gfx942 supports fp8

Don't enable gfx950 for now, until more optimizations are in place as per https://github.com/pytorch/pytorch/pull/162648/files#r2369588738

Validation:
[rocm6.4](https://github.com/pytorch/pytorch/actions/runs/17944766350/job/51028483128) and [rocm6.3](https://github.com/pytorch/pytorch/actions/runs/17944766350/job/51028483093) libtorch builds finished within 3.9h.

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @danielvegamyhre (since their [change](https://github.com/pytorch/pytorch/commit/b6d0a9ea9056ede4f7024dbf3bd6c43be3aff49c) had removed this snippet, causing ROCm builds to increase >4h)",2025-09-10 22:20:56+00:00,2025-09-23T18:56:43Z,,False,13,11,5,11,1,2,24,2025-09-23 18:55:39+00:00,41,1116,False,True,False,False,False,False,2,12,3167,44,27,17,2,5,5.0,12.0,2025-09-10T22:30:47Z,pytorch
162647,closed,compile_kernel large shared memory fix,msaroufim,"Alternate solution to https://github.com/pytorch/pytorch/pull/162328

cc @gau-nernst ",2025-09-10 22:16:05+00:00,2025-09-11T05:53:52Z,,False,7,1,11,135,7,2,8,2025-09-11 05:52:49+00:00,38,85,False,True,False,False,False,False,2,6,1418,445,278,167,1,11,4.0,6.0,2025-09-11T00:08:09Z,pytorch
162646,closed,Smoke tests don't run nvshmem on Windows,atalman,"Only available for linux x86 and aarch64 :
https://pypi.org/project/nvidia-nvshmem-cu13/#files


nvshmem is available only on linux:
``
""nvidia-nvshmem-cu12==3.3.24; platform_system == 'Linux' and platform_machine == 'x86_64' | ""
``
https://github.com/pytorch/pytorch/blob/main/.github/scripts/generate_binary_build_matrix.py#L57",2025-09-10 22:11:01+00:00,2025-09-11T16:10:28Z,,False,4,4,6,5,3,1,8,2025-09-11 16:09:25+00:00,40,329,False,False,False,False,False,False,1,3,989,18,10,8,1,6,4.0,4.0,2025-09-10T22:12:26Z,pytorch
162645,open,[ONNX] Implement while_loop,justinchuby,"Fixes #ISSUE_NUMBER
",2025-09-10 22:05:59+00:00,2025-09-24T21:20:27Z,,False,1,0,1,132,0,1,1,,27,20,False,True,False,False,False,False,1,0,0,132,132,0,1,1,,,,pytorch
162644,closed,Fix protobuf test comparison by parsing proto instead of raw strings,0xjeffro,"The tests were comparing raw exported strings for protobuf comparison, which is not backward/forward compatible with different versions of protobuf.

This PR parses the strings into protobuf and compares the protobufs directly, similar to what we did in assertImageProto.

Our test failed because we used a different version of protobuf, which output 44100.0 instead of 44100, which resulted in an error. However, they are equal, but only different in the exported strings.

cc @haifeng-jin 
",2025-09-10 21:38:35+00:00,2025-09-12T16:28:02Z,,False,5,2,1,6,5,1,7,2025-09-12 16:26:58+00:00,68,492,False,True,False,False,False,False,1,4,970,11,6,5,1,1,4.0,4.0,2025-09-10T23:30:37Z,pytorch
162643,closed,-ldl for nativert tests,dolpm,"Fixes #162640
",2025-09-10 21:22:28+00:00,2025-09-11T00:37:01Z,,False,4,0,1,1,0,1,4,2025-09-11 00:36:00+00:00,23,14,False,True,False,False,False,False,1,3,661,1,1,0,1,1,4.0,3.0,2025-09-10T21:24:57Z,pytorch
162642,open,skip erroring at known issues and add debug mode,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162642
* #162562
* #160655

",2025-09-10 21:18:06+00:00,2025-09-11T03:26:51Z,,False,2,0,2,243,46,1,2,,48,114,False,True,False,False,False,False,1,0,0,291,244,47,1,2,,,,pytorch
162641,open,[DO NOT MERGE][B200] remove xfail of `grouped_gemm_compiled_op` in `test_matmul_cuda.py`,eqy,"@nWEIdia observed that this test appears to consistently pass, but only on the first?! invocation? Removing the `xfail` and running with `--repeat` causes failures to show up reliably on the second run",2025-09-10 21:16:05+00:00,2025-09-12T12:39:30Z,,False,1,0,1,6,1,2,1,,88,201,False,False,False,False,False,False,2,0,0,7,6,1,1,1,,,,pytorch
162639,open,Fix Windows CI missing pwlf dependency for sac_ilp tests,aran-yogesh,"Fixes #162453 
   ## Problem
   The test `test_sac_ilp_case1` was failing in Windows CI because the `pwlf` dependency was missing, while other CI environments already had it.

   ## Solution
   Added `pwlf==2.2.1` to the Windows CI dependencies in `.ci/pytorch/win-test.sh`.

   ## Testing
   - Verified both `pwlf` and `pulp` dependencies work correctly
   - No linting errors introduced
   - Fix addresses the exact issue described in #162453",2025-09-10 20:49:31+00:00,2025-09-11T21:16:00Z,,False,3,0,1,2,1,1,3,,56,444,False,True,False,False,False,False,1,1,42,3,2,1,1,1,1.0,1.0,2025-09-10T21:07:00Z,pytorch
162637,closed,[ONNX] Update export docstring & Set fallback=False by default,pytorchbot,"Update export docstring to reflect the latest configuration.

cc @titaiwangms",2025-09-10 20:35:51+00:00,2025-09-17T00:23:47Z,2025-09-17T00:23:47Z,True,1,0,3,62,90,2,1,2025-09-17 00:23:47+00:00,62,77,False,False,False,True,False,False,2,0,0,152,62,90,1,3,1.0,0.0,2025-09-17T00:23:39Z,pytorch
162636,closed,[FSDP][Replicate] tests replicate casting module after init,anshul-si,"**Summary:** In order to ensure that replicate acts as intended (a specialized version of hsdp) we need to make sure that it can pass the same tests that fully_shard can for training. This test is important as it verifies we can cast a replicated module to a different type after initialization, and import feature for enabling mixed precision,

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_to_float64_after_init

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* __->__ #162636
* #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 20:30:58+00:00,2025-09-17T20:37:20Z,,False,3,0,3,63,2,1,3,2025-09-17 20:36:16+00:00,59,785,False,False,True,False,False,False,1,2,512,23695,17169,6526,1,3,3.0,3.0,2025-09-11T00:23:30Z,pytorch
162635,closed,Don't include cuh header when USE_NVSHMEM is off,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* (to be filled)

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-10 20:28:59+00:00,2025-09-11T00:25:56Z,,False,3,0,1,3,0,1,3,2025-09-11 00:24:53+00:00,48,245,False,False,False,False,False,False,1,2,500,3,3,0,1,1,2.0,3.0,2025-09-10T20:29:34Z,pytorch
162634,closed,Add cuda headers automatically for compile_kernel,msaroufim,"Issue was pointed out before by @ngimel and more recently by https://gau-nernst.github.io/nvrtc-matmul/#missing-cuda-and-c-headers- by @gau-nernst 

Benefit is now we can add 

`#include <cuda_fp16.h>` without crapping out",2025-09-10 20:21:34+00:00,2025-09-11T00:21:37Z,,False,6,0,4,40,0,2,6,2025-09-11 00:20:36+00:00,49,222,False,False,False,False,False,False,2,5,1591,132,86,46,1,4,3.0,5.0,2025-09-10T20:39:42Z,pytorch
162633,closed,fix var args for shape guards,avikchaudhuri,"Summary: Fixes #162599

Test Plan:
added test based on repro

Rollback Plan:

Differential Revision: D82144520
",2025-09-10 20:17:46+00:00,2025-09-12T00:34:42Z,,False,11,4,1,105,26,2,15,2025-09-12 00:33:39+00:00,29,111,False,True,False,False,False,False,2,2,493,131,105,26,1,1,3.0,2.0,2025-09-10T20:38:02Z,pytorch
162632,closed,cuda include paths fix,msaroufim,"- **Compile kernel fixes**
- **update**
- **lint**
- **update**
- **Add cuda include dir for compile_kernel**

Fixes #ISSUE_NUMBER
",2025-09-10 20:17:00+00:00,2025-09-10T20:17:19Z,,False,1,0,5,80,7,3,1,2025-09-10 20:17:19+00:00,22,131,False,True,False,False,False,False,3,0,0,579,326,253,1,5,,,,pytorch
162631,closed,[FSDP][Replicate] tests replicate parameter registration,anshul-si,"**Summary**
Tests parameter state management after forward and backward passes for single and multiple replicate groups

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_param_registration_after_forward 
2. pytest test/distributed/_composable/test_replicate_training.py -k test_param_registration_after_backward

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* __->__ #162631



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 20:12:45+00:00,2025-09-17T02:47:36Z,,False,6,0,3,121,1,1,6,2025-09-17 02:46:33+00:00,56,680,False,False,False,False,False,False,1,5,1401,23752,17227,6525,1,3,3.0,5.0,2025-09-11T00:25:44Z,pytorch
162629,closed,[FSDP][Replicate] tests replicate input device movements,anshul-si,"**Summary:** This test verifies that the replicate function automatically moves forward pass inputs to the correct device.

**Test Cases**
1. pytest test/distributed/_composable/test_replicate_training.py -k test_root_move_forward_input_to_device


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162861
* #162855
* #162853
* #162851
* #162839
* #162836
* #162830
* #162785
* #162658
* #162656
* #162654
* #162650
* #162636
* #162631
* __->__ #162629



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 19:52:23+00:00,2025-09-16T17:36:36Z,,False,3,0,2,53,0,1,3,2025-09-16 17:35:30+00:00,56,586,False,False,False,False,False,False,1,2,493,5623,4184,1439,1,2,3.0,2.0,2025-09-11T00:26:22Z,pytorch
162628,closed,Fix provenance tracking kernel name for fallback kernels,yushangdi,"Summary:
as title

`kernel.cpp_kernel_name` is something like `at::_ops::_scaled_dot_product_efficient_attention::call`, but the actual kernel name we want is `aoti_torch_cuda__scaled_dot_product_efficient_attention`

Differential Revision: D82142287




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 19:49:34+00:00,2025-09-16T06:57:08Z,,False,7,0,1,2,1,1,7,2025-09-16 06:56:05+00:00,56,456,False,True,False,False,False,False,1,2,486,3,2,1,1,1,3.0,2.0,2025-09-15T18:25:37Z,pytorch
162627,open,[profile trace] Fill gaps in compile time profile traces,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162775
* __->__ #162627
* #162695
* #162694

<img width=""697"" height=""295"" alt=""image"" src=""https://github.com/user-attachments/assets/ad83bfa0-c5bc-4d18-862a-d0335555e654"" />


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-10 19:47:14+00:00,2025-09-12T00:01:32Z,,False,1,0,3,30,22,4,1,,56,427,False,False,False,False,False,False,4,0,0,1811,986,825,1,3,,,,pytorch
162626,closed,compile_kernel: Handle python floats as c double,msaroufim,"This was an open todo in the code and probably a footgun in waiting


cc @ptrblck @eqy @jerryzh168",2025-09-10 19:34:37+00:00,2025-09-12T20:15:07Z,,False,17,0,10,42,8,2,17,2025-09-11 06:03:29+00:00,48,98,False,False,False,False,False,False,2,16,4370,4513,3318,1195,1,10,3.0,16.0,2025-09-10T21:58:18Z,pytorch
162625,closed,[Inductor] Add force fallback config for repeat_interleave op to address cuda device asserts on certain block sizes,karthickai,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162759
* __->__ #162625



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos

Differential Revision: [D82151822](https://our.internmc.facebook.com/intern/diff/D82151822)",2025-09-10 19:16:23+00:00,2025-09-11T21:52:36Z,,False,5,0,4,39,0,3,5,2025-09-11 21:52:36+00:00,115,408,False,False,False,False,False,False,3,4,670,41366,28032,13334,1,4,2.0,4.0,2025-09-10T21:12:41Z,pytorch
162622,closed,[ONNX] Update export docstring,justinchuby,"Update export docstring to reflect the latest configuration.

cc @titaiwangms",2025-09-10 18:32:15+00:00,2025-09-10T20:35:53Z,,False,8,2,1,60,88,1,10,2025-09-10 20:29:50+00:00,30,77,False,False,False,True,False,False,1,7,2272,148,60,88,1,1,4.0,7.0,2025-09-10T18:44:32Z,pytorch
162621,closed,[FSDP][Replicate] composable replicate function using HSDP instead of DDP,anshul-si,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162621



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 18:29:47+00:00,2025-09-10T18:33:35Z,,False,1,0,2,1,4,2,1,2025-09-10 18:33:35+00:00,73,197,False,False,False,False,False,False,2,0,0,1383,690,693,1,2,,,,pytorch
162620,closed,bf16 support for fused_moving_avg_obs_fake_quant() op,liangel-02,"enabling bf16 support for `torch.fused_moving_avg_obs_fake_quant()` op on cuda

**testing**
`python test/quantization/pt2e/test_quantize_pt2e.py`

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-10 18:22:22+00:00,2025-09-16T21:23:52Z,,False,8,4,1,140,83,3,12,2025-09-16 21:22:48+00:00,53,263,False,False,False,False,False,False,3,2,556,223,140,83,1,1,4.0,4.0,2025-09-10T20:37:55Z,pytorch
162619,closed,Placement: make is_shard/is_replicate/is_partial more straightforward,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #162619

We already have method dispatch based on actual type, so just provide appropriate base class and subclass method implementations. (This is not motivated by any particular performance profiling, just seems more straightforward to me.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 18:17:20+00:00,2025-09-15T22:55:12Z,,False,5,0,2,17,9,1,5,2025-09-15 22:54:09+00:00,69,441,False,False,False,False,False,False,1,4,988,17513,13047,4466,1,2,5.0,6.0,2025-09-11T04:11:22Z,pytorch
162618,closed,Fix torch export with dict input nested in args,supercharleszhu,"Investigated together with @pyemma and @taotaohuang001 

## Problem
when calling exported module with dict nested in the args tuple, it will make following complaits
```
Traceback (most recent call last):
  File ""/home/chzhu/infinitrain/test_torch_export.py"", line 32, in <module>
    print(exported_model({""a2"": torch.randn(10), ""a1"": torch.randn(10)}))
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/fx/graph_module.py"", line 848, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/fx/graph_module.py"", line 424, in __call__
    raise e
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/fx/graph_module.py"", line 411, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1879, in _call_impl
    return inner()
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/nn/modules/module.py"", line 1806, in inner
    args_kwargs_result = hook(self, args, kwargs)  # type: ignore[misc]
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py"", line 929, in _fn
    return fn(*args, **kwargs)
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/export/_unlift.py"", line 81, in _check_input_constraints_pre_hook
    flat_args_with_path = _check_inputs_match(args, kwargs, self._in_spec)
  File ""/home/chzhu/infinitrain/build/infinitrain/environments/development-venv/lib/python3.10/site-packages/torch/export/_unlift.py"", line 64, in _check_inputs_match
    raise ValueError(  # noqa: B904
ValueError: Trying to flatten user inputs with exported input tree spec: 
TreeSpec(tuple, None, [TreeSpec(tuple, None, [TreeSpec(dict, ['a1', 'a2'], [*,
      *])]),
  TreeSpec(dict, [], [])])
but actually got inputs with tree spec of: 
TreeSpec(tuple, None, [TreeSpec(tuple, None, [TreeSpec(dict, ['a2', 'a1'], [*,
      *])]),
  TreeSpec(dict, [], [])]).
Please check that the inputs have the same number and type of args and kwargs as the ones you used when tracing.

```

## How to reproduce the issue
```python
import torch


# create a nn.Module with data_batch as input and output as output
class MyModel(torch.nn.Module):
   def __init__(self):
       super(MyModel, self).__init__()
       self.linear = torch.nn.Linear(10, 1)


   def forward(self, data_batch):
       h1 = self.linear(data_batch[""a1""])
       h2 = self.linear(data_batch[""a2""])
       return h1 + h2




# torch export this module
model = MyModel()
example_args_forward = (
   {
       ""a1"": torch.randn(10),
       ""a2"": torch.randn(10),
   },
)
exported_model = torch.export.export(model, example_args_forward, strict=True)


# save the exported model
torch.export.save(exported_model, ""exported_model.pt2"")


# load the exported model
exported_model = torch.export.load(""exported_model.pt2"").module()


# run the exported model
print(exported_model({""a2"": torch.randn(10), ""a1"": torch.randn(10)}))


```

## Root Cause
Input spec is encoded as [TreeSpec](https://github.com/pytorch/pytorch/blob/582d278983b28a91ac0cedd035183f2495bb6887/torch/utils/_pytree.py#L1059) in torch export. With (args, kwargs) at the top level. When we call the exported model, it has a pre-execution [hook](https://github.com/pytorch/pytorch/blob/582d278983b28a91ac0cedd035183f2495bb6887/torch/export/_unlift.py#L66) to check the input TreeSpec matches the received TreeSpec, where in Treespec, the dict key order is preserved. Something like 

TreeSpec(dict, ['a2', 'a1'], [*,*])

To workaround this, the input check reorders [kwargs](https://github.com/pytorch/pytorch/blob/582d278983b28a91ac0cedd035183f2495bb6887/torch/export/_unlift.py#L67), that is why kwargs can be out of order. But the dict nested in the args is not re-ordered, so any re-ordering of the keys will throw errors.

## Solution
Update eq_spec to handle the dict case, where we only guarantee that key set is the same without ordering constraints.",2025-09-10 18:03:34+00:00,2025-09-13T03:25:37Z,,False,3,3,7,36,1,2,6,2025-09-13 03:24:32+00:00,47,4628,False,True,False,False,False,False,2,2,513,107468,68602,38866,1,7,3.0,3.0,2025-09-10T18:48:04Z,pytorch
162617,closed,[AOTI] Fix Windows fail to zip opened file.,xuhancn,"Original issue:
<img width=""1767"" height=""544"" alt=""Image"" src=""https://github.com/user-attachments/assets/9de90d50-217f-4049-8f19-77ff1660c8b0"" />

reproducer:
```cmd
pytest test\inductor\test_aot_inductor.py -v -k test_weight_on_disk_legacy_cpu
```

Fixed list:
1. `WritableTempFile`'s `__exit__` function auto unlink opened file, when the file was opened, it should raise error. Ignore it on Windows.
2. When open zip file, if the file is opened, it would be failed. Switch to `_wfsopen` with shared access flag, which can open file with shared access.

Local test passed:
<img width=""1101"" height=""233"" alt=""image"" src=""https://github.com/user-attachments/assets/935cbf2e-52db-41f1-80fa-617569b92a96"" />


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben ",2025-09-10 17:43:23+00:00,2025-09-11T06:57:29Z,,False,10,0,1,14,4,2,10,2025-09-11 06:22:24+00:00,43,1018,False,True,False,False,False,False,2,9,1527,18,14,4,1,1,3.0,9.0,2025-09-10T17:50:13Z,pytorch
162616,closed,[torch][c10d] propagate timeout to ProcessGroupGloo creation,suo,"Oops, my refactor didn't thread through this variable, causing at least one test failure.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 17:38:34+00:00,2025-09-11T17:36:24Z,,False,4,0,1,4,2,1,4,2025-09-11 17:36:16+00:00,60,193,False,False,False,False,False,True,1,3,1515,6,4,2,1,1,3.0,3.0,2025-09-10T17:42:49Z,pytorch
162615,closed,[standalone_compile] fix multiprocess compile,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162615
* #162432

If multiple processes standalone_compile the same exact thing, this
resulted in two issues:
1) We were always deleting the directory. This leads to one process
   trying to read from something that is no longer there
2) there is a line that iterates through all inductor cache artifacts.
   Another process might have put a temporary file in this directory,
   and we don't want the current process to iterate through temporary
   files, just the new file.

For (2), we can't change `write_atomic` to use a temporary directory.
If the temporary directory is still a subdirectory of the current
directory, then we have the same problem. If we put the temporary
directory elsewhere (like /tmp), we run into issues: if the Inductor
cache directory is not on the same filesystem as the /tmp directory,
then we cannot move files between them without a copy.

Test Plan:
```
CUDA_VISIBLE_DEVICES=2 vllm serve --gpu-memory-utilization 0.1 &
CUDA_VISIBLE_DEVICES=3 vllm serve --gpu-memory-utilization 0.1 &
CUDA_VISIBLE_DEVICES=4 vllm serve --gpu-memory-utilization 0.1 &
CUDA_VISIBLE_DEVICES=5 vllm serve --gpu-memory-utilization 0.1 &
CUDA_VISIBLE_DEVICES=6 vllm serve --gpu-memory-utilization 0.1 &
CUDA_VISIBLE_DEVICES=7 vllm serve --gpu-memory-utilization 0.1 &
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 17:29:43+00:00,2025-09-10T20:42:16Z,,False,2,0,1,10,5,2,2,2025-09-10 20:42:16+00:00,45,1569,False,True,False,False,False,False,2,0,42,15,10,5,1,1,1.0,1.0,2025-09-10T17:35:06Z,pytorch
162614,closed,added example for torch.is_storage,rohit-kumar-manav,"Fixes #162613 
",2025-09-10 17:27:46+00:00,2025-09-11T20:26:32Z,,False,8,2,4,8,0,1,10,2025-09-11 20:25:29+00:00,34,15,False,True,False,False,False,False,1,6,1108,20,14,6,2,4,3.0,7.0,2025-09-10T17:29:44Z,pytorch
162611,closed,Update triton pin to tip of release/3.5.x,jataylo,"Sync to tip of tree, notably resolves a hanging issue observed in ROCm with https://github.com/triton-lang/triton/commit/615ab03c354be9fa18ccc058dbf3cb882598eb77",2025-09-10 16:44:25+00:00,2025-09-12T12:54:51Z,,False,8,0,1,1,1,1,8,2025-09-12 12:54:51+00:00,41,161,False,False,False,False,False,False,1,6,918,2,1,1,1,1,2.0,6.0,2025-09-11T09:00:11Z,pytorch
162610,closed,[hop] Support non-node outputs (constants) in subgraph output,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162702
* __->__ #162610
* #161458

eg when tracing this graph:
```python
def forward(self, boosted_weight : torch._subclasses.fake_tensor.FakeTensor, to : torch._subclasses.fake_tensor.FakeTensor):
    t = boosted_weight.t();  boosted_weight = None
    matmul = torch.matmul(to, t);  to = t = None
    return (matmul,)
```

when partitioning its joint, and arg1_1 is a fw_input that doesn't require gradients:
```python
(Pdb++) joint_hop_gm.print_readable()
class <lambda>(torch.nn.Module):
    def forward(self, arg0_1: ""bf16[6144, 6144]"", arg1_1: ""bf16[64, 256, 6144]"", arg2_1: ""bf16[64, 256, 6144]""):
        # No stacktrace found for following nodes
        t: ""bf16[6144, 6144]"" = torch.ops.aten.t.default(arg0_1);  arg0_1 = None
        view: ""bf16[16384, 6144]"" = torch.ops.aten.view.default(arg1_1, [16384, 6144]);  arg1_1 = None
        mm: ""bf16[16384, 6144]"" = torch.ops.aten.mm.default(view, t);  t = None
        _unsafe_view: ""bf16[64, 256, 6144]"" = torch.ops.aten._unsafe_view.default(mm, [64, 256, 6144]);  mm = None
        view_1: ""bf16[16384, 6144]"" = torch.ops.aten.view.default(arg2_1, [16384, 6144]);  arg2_1 = None
        t_1: ""bf16[6144, 16384]"" = torch.ops.aten.t.default(view_1);  view_1 = None
        mm_1: ""bf16[6144, 6144]"" = torch.ops.aten.mm.default(t_1, view);  t_1 = view = None
        t_2: ""bf16[6144, 6144]"" = torch.ops.aten.t.default(mm_1);  mm_1 = None
        t_3: ""bf16[6144, 6144]"" = torch.ops.aten.t.default(t_2);  t_2 = None
        return (t_3, None, _unsafe_view)
```
",2025-09-10 16:38:13+00:00,2025-09-15T17:13:10Z,,False,4,0,7,3,1,1,4,2025-09-15 17:13:10+00:00,61,1594,False,False,False,False,False,False,1,3,442,83184,50627,32557,1,7,2.0,3.0,2025-09-13T03:59:23Z,pytorch
162609,closed,[ez] add docstring/typing for codegen_kernel_benchmark,ColinPeppler,"```
lintrunner init && lintrunner -m origin/main
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162609
* #162442



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 16:25:53+00:00,2025-09-10T20:50:46Z,,False,6,0,2,21,1,2,6,2025-09-10 20:49:42+00:00,54,361,False,False,False,True,False,False,2,5,1822,24,22,2,1,2,3.0,5.0,2025-09-10T16:34:47Z,pytorch
162608,closed,Update SECURITY.md with reporting guidelines,malfet,"Added clarification that all reports will be disclosed within 90 days
",2025-09-10 16:21:19+00:00,2025-09-11T16:31:35Z,,False,3,0,3,2,0,1,3,2025-09-11 16:30:32+00:00,44,70,False,False,False,False,False,False,1,2,784,8,5,3,1,3,4.0,3.0,2025-09-10T16:24:31Z,pytorch
162605,open,Add support for 16 KB page size alignment,vivascu,"Align shared library ELF segments to support the 16 KB page size requirement for apps targeting Android 15+ on Google Play.

The project is constrained by older NDK and CMake versions, preventing a direct upgrade to the latest Android Gradle Plugin (AGP). To achieve compatibility, the following workarounds were implemented:

- Upgraded to AGP 4.2.0 to enable the `useLegacyPackaging` flag. This ensures native libraries are stored uncompressed in the APK, a prerequisite for 16 KB alignment.

- Added linker flags (`-Wl,-z,max-page-size=16384`) to the build. Since the project's minimum CMake version (3.5) is too old to support the modern `target_link_options` command (requires 3.13+), the flags are set globally using the legacy `CMAKE_SHARED_LINKER_FLAGS` variable.

Fixes #154449
",2025-09-10 15:57:59+00:00,2025-09-24T04:35:06Z,,False,2,0,1,13,1,5,2,,41,787,False,True,False,False,False,False,5,0,39,14,13,1,1,1,1.0,1.0,2025-09-24T04:25:26Z,pytorch
162603,closed,[release/2.4] add triton name,ethanwee1,"Cherry pick of https://github.com/ROCm/pytorch/pull/2518

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @mingfeima @XiaobingSuper @ashokei @jingxu10 @jerryzh168 @mcarilli @ptrblck @leslie-fang-intel @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-10 15:36:07+00:00,2025-09-10T15:36:32Z,,False,2,0,243,6631,3331,350,2,2025-09-10 15:36:18+00:00,29,579,False,False,False,False,False,False,350,0,0,2938,1415,1523,12,30,,,,pytorch
162600,closed,[ROCm] rocblas Aten GEMM overload for FP32 output from FP16/BF16 inputs,jagadish-amd,"Fix ROCm GEMM helper to set output type (C/D) based on C_Dtype template parameter.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-10 15:21:11+00:00,2025-09-11T03:39:10Z,,False,7,0,1,10,8,2,7,2025-09-11 03:34:11+00:00,71,201,False,True,False,False,False,False,2,5,1476,18,10,8,1,1,5.0,5.0,2025-09-10T15:22:12Z,pytorch
162597,closed,fix typo: summit -> submit,pytorchbot,cc @justinchuby @titaiwangms,2025-09-10 14:50:13+00:00,2025-09-12T15:41:11Z,2025-09-12T15:41:11Z,True,1,0,1,1,1,1,1,2025-09-12 15:41:11+00:00,26,28,False,True,False,False,False,False,1,0,0,2,1,1,1,1,1.0,0.0,2025-09-12T15:41:01Z,pytorch
162596,closed,[CD] Aarch64 Fix packaging ``libarm_compute.so`` and other libraries to the aarch64 CUDA wheels,pytorchbot,"Fixes aarch64 linux packaging, following error:
https://github.com/pytorch/vision/actions/runs/17612462583/job/50037380487#step:15:62
```
Traceback (most recent call last):
  File ""/__w/vision/vision/pytorch/vision/setup.py"", line 13, in <module>
    import torch
  File ""/__w/_temp/conda_environment_17612462583/lib/python3.11/site-packages/torch/__init__.py"", line 415, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libarm_compute.so: cannot open shared object file: No such file or directory
```
Due to missing dependencies.

Current Error:
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl is extracted 
File is repackaged as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl renamed as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
Hence the repackaging does not take any effect.

This PR does following
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl is extracted 
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl  deleted
File is repackaged as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl

Looks like after migrating from zipping the wheel to wheel pack renaming the wheel is no longer necessary. Hence removing renaming and deleting old file.
```
2025-09-10T10:10:05.9652454Z Using nvidia libs from pypi - skipping CUDA library bundling
2025-09-10T10:10:05.9656595Z Copying to /pytorch/dist/tmp/torch/lib/libgomp.so.1
2025-09-10T10:10:05.9873843Z Copying to /pytorch/dist/tmp/torch/lib/libgfortran.so.5
2025-09-10T10:10:06.0410041Z Copying to /pytorch/dist/tmp/torch/lib/libarm_compute.so
2025-09-10T10:10:06.2869242Z Copying to /pytorch/dist/tmp/torch/lib/libarm_compute_graph.so
2025-09-10T10:10:06.4385740Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_lapack_lp64_gomp.so.0
2025-09-10T10:10:06.5461372Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_blas_lp64_gomp.so.0
2025-09-10T10:10:06.5728970Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_lapack_core.so.0
2025-09-10T10:10:06.6231872Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_blas_core.so.0
2025-09-10T10:10:14.1503110Z Updated tag from Tag: cp310-cp310-linux_aarch64
2025-09-10T10:10:14.1503482Z  to Tag: cp310-cp310-manylinux_2_28_aarch64
2025-09-10T10:10:14.1503682Z 
2025-09-10T10:10:41.6498892Z Repacking wheel as /pytorch/dist/torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl...OK
2025-09-10T10:10:41.9394460Z Renaming torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl wheel to torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
```

Test Plan, Executed on local file:
```
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/WHEEL  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/entry_points.txt  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/top_level.txt  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/RECORD  
Bundling CUDA libraries with wheel
Updated tag from Tag: cp310-cp310-manylinux_2_28_aarch64
 to Tag: cp310-cp310-manylinux_2_28_aarch64

Repacking wheel as ubuntu/dist/torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl...OK
Copying torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl to artifacts
Build Complete. Created torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl..
```
",2025-09-10 14:43:16+00:00,2025-09-10T16:22:02Z,2025-09-10T16:22:02Z,True,1,0,1,3,8,1,1,2025-09-10 16:22:02+00:00,95,3498,False,True,False,False,False,False,1,0,0,11,3,8,1,1,1.0,0.0,2025-09-10T16:21:52Z,pytorch
162595,closed,"Don't unconditionally import torch._dynamo, it's slow",ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162595

A trivial test on OS X.

Before:

```
real	0m6.550s
user	0m2.532s
sys	0m3.359s
```

After:

```
real	0m2.607s
user	0m1.898s
sys	0m3.344s
```

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-10 14:35:31+00:00,2025-09-10T17:22:10Z,,False,3,1,1,2,1,1,4,2025-09-10 17:21:06+00:00,53,292,False,False,False,False,False,False,1,2,620,3,2,1,1,1,3.0,3.0,2025-09-10T14:36:35Z,pytorch
162594,open,[RELAND] Always build USE_DISTRIBUTED (#160449) and Make distributed modules importable even when backend not built (#159889),ezyang,"Summary:
Original: D81957844 and D81957923

Also, https://github.com/pytorch/pytorch/pull/162142 is patched in as well

#buildall

Test Plan:
sandcastle and oss ci

Rollback Plan:

Reviewed By: H-Huang


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-10 14:10:48+00:00,2025-09-25T17:01:09Z,,False,28,0,1,778,458,52,28,,125,342,False,False,False,False,False,False,52,22,6562,1236,778,458,1,1,7.0,22.0,2025-09-10T14:22:33Z,pytorch
162593,open,Fix for torch.inf.to(torch.int32) produces different values on CPU vs…,vishalgoyal316,"Code Changes for:

Mapping from +inf to max, -inf to min and Nan to 0, when we type cast to integer type.
Updating for respective test case.

Fixes #154311
",2025-09-10 14:04:34+00:00,2025-09-12T13:07:39Z,,False,3,0,1,37,6,2,3,,70,156,False,True,False,False,False,False,2,2,92,43,37,6,1,1,1.0,2.0,2025-09-10T14:06:05Z,pytorch
162590,closed,[ROCm] Bump FBGEMM commit to avoid CK errors,pragupta,"Trying to solve the following CK build errors:

<img width=""2531"" height=""1193"" alt=""image"" src=""https://github.com/user-attachments/assets/db5a7d9c-e736-460a-98e3-c726688463a0"" />



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-10 13:38:09+00:00,2025-09-22T16:02:57Z,,False,22,0,2,1,1,1,22,2025-09-22 16:01:18+00:00,44,300,False,False,False,False,False,False,1,16,3042,4,2,2,1,2,8.0,16.0,2025-09-12T11:40:24Z,pytorch
162589,open,Update doc: Dev infra not available,Raman-RH,"updating `CONTRIBUTING.md`
since now Dev Infra Office Hours is not hosted",2025-09-10 13:25:33+00:00,2025-09-11T19:57:55Z,,False,2,1,1,1,1,1,3,,35,73,False,False,False,True,False,False,1,1,64,2,1,1,1,1,1.0,1.0,2025-09-11T14:14:27Z,pytorch
162587,closed,fix typo: summit -> submit,crcrpar,cc @justinchuby @titaiwangms,2025-09-10 13:06:01+00:00,2025-09-10T14:50:16Z,,False,6,0,1,1,1,1,6,2025-09-10 14:43:58+00:00,26,28,False,True,False,False,False,False,1,5,1323,2,1,1,1,1,3.0,5.0,2025-09-10T14:40:52Z,pytorch
162583,open,KernelOracle (POC),AmdSampsa,"The idea of this POC is to test a new idea:
A ML model for predicting nice XBLOCK, YBLOCK, etc. values for kernel launch and CachingAutotuner.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 12:44:44+00:00,2025-09-10T14:20:21Z,,False,2,0,3,439,0,2,2,,18,346,False,False,False,False,False,False,2,0,0,441,440,1,1,3,,,,pytorch
162581,open,[CI] Remove the unnecessary workflow related functorch,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162581

The [docs](https://docs.pytorch.org/functorch/stable/) about `functorch` has been migrated into [PyTorch Doc](https://docs.pytorch.org/docs/stable/func.html) since PyTorch 2.0, so I think we can remove it right now to reduce the compute resources usages.",2025-09-10 11:28:47+00:00,2025-09-11T04:06:15Z,,False,2,0,1,0,101,3,2,,54,348,False,False,False,True,False,False,3,1,24,101,0,101,1,1,1.0,1.0,2025-09-11T04:06:15Z,pytorch
162580,open,WIP Profiler Performance Visualization,exclamaforte,"This PR creates a compact visualization of all the kernels in a model trace, with bandwidth, flops, and roofline calculation (mem bound vs comp bound) +  call-stack for each kernel. An analogy here would be, if the profile is an audio waveform (samples), then this visualization is a Spectrogram (frequency domain.) You can provide many different traces, and they will be smashed together into the same visualization. The kernels are color coded to match which profile it belongs to. This allows easy comparison of the performance between eager vs compile, or between hardware vendors. The coloring of the kernel nodes provides quick identification of the important kernels by different stats (runtime by default.)

Note: this doesn't work on cudagraphed traces because they lack the cpu events through which to build the graph!

## `--visualize <trace_name_1> <trace_name_2> <dtype> <output>`

Examples:
```
# Single trace visualization 
python profile_analysis.py --visualize trace.json float32 output.svg 
```
![a2a84271-test_output2](https://github.com/user-attachments/assets/ae6322ae-722e-41d0-86e0-37009b8f562b)

^ click to open in a new window

```
# Multi-trace comparison 
python profile_analysis.py --visualize baseline.json current.json float32 comparison.svg --compact False
```
![945b0061-test_output2](https://github.com/user-attachments/assets/d146ff1b-95db-4f01-8be4-8286b6893441)


^ click to open in a new window

## --color
Controls how kernel nodes are colored to highlight different performance characteristics. 

Examples:
```
# Color by kernel runtime (default) - darker = longer runtime
python profile_analysis.py --visualize trace.json float32 output.png --color time

# Highlight memory bandwidth utilization = utilization %
python profile_analysis.py --visualize trace.json float32 output.png --color mem-utilization

# Show compute utilization = FLOPS utilization %
python profile_analysis.py --visualize trace.json float32 output.png --color compute-utilization

# Roofline percentage: % away from mem optimal if mem bound, % away from compute optimal if compute bound
python profile_analysis.py --visualize trace.json float32 output.png --color roofline
```
TODO Layer stuff

## --height
Limits visualization complexity by showing only the specified number of operation levels above kernel nodes.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-10 11:11:37+00:00,2025-09-16T12:04:37Z,,False,1,0,7,3469,331,6,1,,38,2538,False,False,False,False,False,False,6,0,0,6556,4847,1709,1,7,,,,pytorch
162579,open,xpu: enable Sycl CPP extension on Windows,tbohutyn,"PR for enabling #153265 on Windows
",2025-09-10 11:08:29+00:00,2025-09-17T19:23:35Z,,False,3,17,2,88,19,2,20,,41,35,False,False,False,False,False,False,2,2,427,155,112,43,1,2,4.0,2.0,2025-09-10T14:18:54Z,pytorch
162578,closed,[BUG] Fix nonzero_static crash on CUDA when the input is a empty tensor,can-gaa-hou,"Fixes #162473


cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-10 10:23:49+00:00,2025-09-15T06:00:54Z,,False,10,0,1,20,0,2,10,2025-09-15 05:44:19+00:00,71,55,False,True,False,False,False,False,2,8,1640,20,20,0,1,1,3.0,8.0,2025-09-11T01:42:52Z,pytorch
162577,closed,[ROCm/Windows] Support load_inline on windows,jammm,"Supports `torch.utils.cpp_extension.load_inline` on Windows with ROCm.
Tested on Windows with gfx1201.

Note that it currently only works when CC and CXX are set to `clang-cl`. This is also needed when building extensions via. `setuptools` due to linker errors when using `cl` directly.



cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-10 10:18:42+00:00,2025-09-12T12:37:13Z,,False,12,2,1,15,7,1,14,2025-09-12 08:10:10+00:00,45,468,False,False,False,False,False,False,1,11,1831,22,15,7,1,1,4.0,12.0,2025-09-10T10:18:53Z,pytorch
162576,open,[Fix] remove legacy function `_jinja2_env()` in inductor,shaoyuyoung,"`_jinja2_env()` in select_algorithm.py is legacy. Because when rendering the template below.
https://github.com/pytorch/pytorch/blob/26b3ae58908becbb03b28636f7384d2972a8c9a5/torch/_inductor/select_algorithm.py#L1431
`self._template_from_string(source)` comes from common.py
https://github.com/pytorch/pytorch/blob/26b3ae58908becbb03b28636f7384d2972a8c9a5/torch/_inductor/codegen/common.py#L2347-L2349


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-10 09:43:38+00:00,2025-09-18T06:30:55Z,,False,3,0,2,0,12,1,3,,56,604,False,True,False,False,False,False,1,2,67,4848,3497,1351,1,2,2.0,2.0,2025-09-10T09:45:20Z,pytorch
162575,open,Add rfft declaration in pyi file,zeshengzong,"Fixes #162373

## Test Result

No warning about `not callable`

```python
# test.py
import torch

if __name__ == ""__main__"":
    t = torch.arange(4)
    V = torch.fft.rfft(t)

pylint test.py 
************* Module test
test.py:1:0: C0114: Missing module docstring (missing-module-docstring)

------------------------------------------------------------------
Your code has been rated at 7.50/10 (previous run: 7.50/10, +0.00)

```


```python
# test2.py
import torch

signal = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0])

stft_result = torch.stft(
    signal,
    n_fft=4,
    hop_length=2,
    return_complex=True
)

recovered = torch.istft(
    stft_result,
    n_fft=4,
    hop_length=2
)

pylint test2.py 
************* Module test2
test2.py:1:0: C0114: Missing module docstring (missing-module-docstring)
test2.py:12:0: C0103: Constant name ""recovered"" doesn't conform to UPPER_CASE naming style (invalid-name)

------------------------------------------------------------------
Your code has been rated at 5.00/10 (previous run: 5.00/10, +0.00)

```",2025-09-10 09:19:34+00:00,2025-09-18T14:14:32Z,,False,2,0,2,31,0,1,2,,32,1065,False,True,False,True,False,False,1,1,42,33,32,1,1,2,1.0,1.0,2025-09-11T02:25:17Z,pytorch
162573,closed,use torch.accelerator and device_module instead of cuda to make DataParallel more device agnostic.,sunjiabin17,"use torch.accelerator and `_get_device_module` instead of cuda to make DataParallel more device agnostic.

Fixes #162152

recently, I've done some works to support my own privateuse1 backend in DataParallel module, but I found some cuda related APIs exist in parallel_apply.py file, that makes me have to monkey patch DataParallel module to support DP on my own backend.

so I make some small changes to replace cuda.xxx to accelerator.xxx, and acquire device module by `_get_device_module`.

this is my first time to contribute to pytorch, please let me know if there is any problem about the change.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 08:45:01+00:00,2025-09-11T10:05:32Z,,False,15,7,6,10,6,1,22,2025-09-11 10:04:30+00:00,98,704,False,True,False,False,False,False,1,13,3192,40,22,18,4,6,4.0,13.0,2025-09-10T08:57:28Z,pytorch
162572,closed,[WIP] Small Wheel on SBSA: fix broken paths,Aidyn-A,A follow up to #160720,2025-09-10 08:38:23+00:00,2025-09-15T11:08:45Z,,False,3,0,2,3,1,1,3,2025-09-15 11:08:45+00:00,43,22,False,True,False,False,False,False,1,1,67,4,3,1,1,2,1.0,1.0,2025-09-15T11:08:45Z,pytorch
162571,open,[c10d] Differentiate participate requirements between init_device_mesh and direct construction,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #162571
* #162549
* #163058
* #162770
* #162545
* #162529

1. `init_device_mesh` is a collective call across World.
Reason: it supports implicit `dist.init()` which is a world-wide call.
```
# Valid
dist.init()
init_device_mesh(...)

# Valid
init_device_mesh(...)
```

2. `DeviceMesh` direct construction is more flexible, can be called on a subset of ranks. But, prior to `DeviceMesh` construction, *the* distributed environment must have been initialized. We don't want to take the risk to perform `dist.init()` implicitly here due to the flexibility of the constructor API. 
```
# Valid
dist.init()
dm = DeviceMesh(...)

# Invalid, will throw environment uninitialized error
dm = DeviceMesh(...)
```

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 07:01:35+00:00,2025-09-18T03:49:02Z,,False,3,4,17,40,16,2,7,,94,902,False,False,False,False,False,False,2,2,69,821,518,303,1,17,3.0,2.0,2025-09-10T17:16:36Z,pytorch
162569,closed,[FlexAttention][Easy] turn off TMA when cannot use it,BoyuanFeng,cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben,2025-09-10 04:27:14+00:00,2025-09-11T19:52:28Z,,False,3,3,4,0,4,1,6,2025-09-11 19:51:23+00:00,53,201,False,False,False,False,False,False,1,2,493,18,7,11,1,4,3.0,2.0,2025-09-10T04:27:27Z,pytorch
162568,closed,"Revert ""Make distributed modules importable even when backend not built (#159889)""",ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162568

This reverts commit a0d026688cd69583d5a4e0c6f3e5fda141a7f4a9.

Revert ""Always build USE_DISTRIBUTED. (#160449)""

This reverts commit d80297a6846f1f2c36fd4f19e22919f2abe8fcea.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-10 04:24:15+00:00,2025-09-10T04:30:50Z,,False,3,0,1,451,774,50,3,2025-09-10 04:29:46+00:00,82,420,False,False,False,False,False,False,50,2,863,1225,451,774,1,1,3.0,3.0,2025-09-10T04:27:14Z,pytorch
162567,closed,"Back out ""Make distributed modules importable even when backend not built (#159889)""",ezyang,"Summary:
Original commit changeset: a4350155ba02

Original Phabricator Diff: D81957923

Test Plan:
Sandcastle

Rollback Plan:

Differential Revision: D82084671




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-10 04:15:31+00:00,2025-09-11T04:36:45Z,,False,2,0,1,235,648,21,2,2025-09-11 04:36:45+00:00,84,257,False,False,False,False,False,False,21,0,0,883,235,648,1,1,,,,pytorch
162566,closed,[CD] Aarch64 Fix packaging ``libarm_compute.so`` and other libraries to the aarch64 CUDA wheels,atalman,"Fixes aarch64 linux packaging, following error:
https://github.com/pytorch/vision/actions/runs/17612462583/job/50037380487#step:15:62
```
Traceback (most recent call last):
  File ""/__w/vision/vision/pytorch/vision/setup.py"", line 13, in <module>
    import torch
  File ""/__w/_temp/conda_environment_17612462583/lib/python3.11/site-packages/torch/__init__.py"", line 415, in <module>
    from torch._C import *  # noqa: F403
    ^^^^^^^^^^^^^^^^^^^^^^
ImportError: libarm_compute.so: cannot open shared object file: No such file or directory
```
Due to missing dependencies.

Current Error:
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl is extracted 
File is repackaged as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl renamed as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
Hence the repackaging does not take any effect.

This PR does following
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl is extracted 
File torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl  deleted
File is repackaged as torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl

Looks like after migrating from zipping the wheel to wheel pack renaming the wheel is no longer necessary. Hence removing renaming and deleting old file.
```
2025-09-10T10:10:05.9652454Z Using nvidia libs from pypi - skipping CUDA library bundling
2025-09-10T10:10:05.9656595Z Copying to /pytorch/dist/tmp/torch/lib/libgomp.so.1
2025-09-10T10:10:05.9873843Z Copying to /pytorch/dist/tmp/torch/lib/libgfortran.so.5
2025-09-10T10:10:06.0410041Z Copying to /pytorch/dist/tmp/torch/lib/libarm_compute.so
2025-09-10T10:10:06.2869242Z Copying to /pytorch/dist/tmp/torch/lib/libarm_compute_graph.so
2025-09-10T10:10:06.4385740Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_lapack_lp64_gomp.so.0
2025-09-10T10:10:06.5461372Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_blas_lp64_gomp.so.0
2025-09-10T10:10:06.5728970Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_lapack_core.so.0
2025-09-10T10:10:06.6231872Z Copying to /pytorch/dist/tmp/torch/lib/libnvpl_blas_core.so.0
2025-09-10T10:10:14.1503110Z Updated tag from Tag: cp310-cp310-linux_aarch64
2025-09-10T10:10:14.1503482Z  to Tag: cp310-cp310-manylinux_2_28_aarch64
2025-09-10T10:10:14.1503682Z 
2025-09-10T10:10:41.6498892Z Repacking wheel as /pytorch/dist/torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl...OK
2025-09-10T10:10:41.9394460Z Renaming torch-2.10.0.dev20250910+cu130-cp310-cp310-linux_aarch64.whl wheel to torch-2.10.0.dev20250910+cu130-cp310-cp310-manylinux_2_28_aarch64.whl
```

Test Plan, Executed on local file:
```
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/WHEEL  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/entry_points.txt  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/top_level.txt  
  inflating: ubuntu/dist/tmp/torch-2.9.0.dev20250909+cu130.dist-info/RECORD  
Bundling CUDA libraries with wheel
Updated tag from Tag: cp310-cp310-manylinux_2_28_aarch64
 to Tag: cp310-cp310-manylinux_2_28_aarch64

Repacking wheel as ubuntu/dist/torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl...OK
Copying torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl to artifacts
Build Complete. Created torch-2.9.0.dev20250909+cu130-cp310-cp310-manylinux_2_28_aarch64.whl..
```
",2025-09-10 04:11:46+00:00,2025-09-10T14:43:18Z,,False,7,0,5,3,8,1,7,2025-09-10 14:22:45+00:00,95,3498,False,True,False,False,False,False,1,5,1383,23,9,14,2,5,5.0,5.0,2025-09-10T12:34:48Z,pytorch
162565,open,[1/N] Fix recursive typing in torch/_C/__init__.pyi.in,cyyever,"Write forward references of types as strings, and use `Self` when possible. These changes are syntax only and shouldn't break the semantics.",2025-09-10 04:01:26+00:00,2025-09-18T09:30:16Z,,False,2,7,1,145,145,1,9,,54,140,False,True,False,False,False,False,1,1,44,290,145,145,1,1,2.0,1.0,2025-09-10T04:05:35Z,pytorch
162564,open,Add XPU support on torch.accelerator.get_memory_info,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162564
* #156812

# Motivation
Support XPU for `torch.accelerator.get_memory_info`.

",2025-09-10 03:44:26+00:00,2025-09-15T11:00:54Z,,False,3,0,3,37,23,4,3,,52,171,False,False,False,False,False,False,4,2,44,61,38,23,1,3,2.0,2.0,2025-09-15T01:41:15Z,pytorch
162563,open,Fix typing for current_accelerator,cyyever,"Change `torch.device` to `torch.Device`. While the former is also valid, linters report errors due to recursive initialisation in ``torch/__init__.py`` and `torch.device` is not ready at that time.",2025-09-10 03:11:18+00:00,2025-09-10T06:23:03Z,,False,5,0,1,1,1,1,5,,34,197,False,True,False,False,False,False,1,4,868,2,1,1,1,1,2.0,4.0,2025-09-10T03:19:06Z,pytorch
162562,open,add optins to run given seed single and print test output,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162642
* __->__ #162562
* #160655

Two main changes:
1. Add args to run the fuzzer 
```
# Single execution with specific seed and capture full error output
python /home/lsakka/pytorch/fuzzer.py --single --seed 42

# Test loop with starting seed (gets full error details on failure)  
python /home/lsakka/pytorch/fuzzer.py --test --seed 100

# Test with both seed and max depth
python /home/lsakka/pytorch/fuzzer.py --test --seed 42 --max-depth 5
```

2. fuzz_end_execute returns std out/err at faluire.
",2025-09-10 03:03:12+00:00,2025-09-10T21:18:11Z,,False,2,0,1,501,330,1,2,,57,582,False,False,False,False,False,False,1,0,0,831,501,330,1,1,,,,pytorch
162560,closed,[DTensor] add op support for aten.unbind.int,tianyu-l,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162560

As titled.

It seems unbind returns views of the original tensor. E.g. see https://stackoverflow.com/questions/78910951/does-unbind-return-the-views-of-tensors-in-pytorch

So we error out when `shard_dim == unbind_dim`. This is similar to why we error out in view ops.
https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/_ops/_view_ops.py#L544-L546

This PR also refactors some other tensor ops code, by creating two utils function `shift_shard_dims_after_insert`, `shift_shard_dims_after_remove`.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-10 02:39:09+00:00,2025-09-11T00:59:28Z,,False,4,2,4,111,37,3,6,2025-09-11 00:58:25+00:00,44,713,False,False,False,False,False,True,3,3,2267,240,157,83,1,4,3.0,5.0,2025-09-10T17:18:38Z,pytorch
162559,closed,Fix set_grad_enabled HOP in strict mode with new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* #162993
* #162992
* #162682
* __->__ #162559
* #162558
* #162557

previous graph seems wrong probably because dynamo bytecode running might be changing the grad state unintentionally.

Differential Revision: [D82478643](https://our.internmc.facebook.com/intern/diff/D82478643)",2025-09-10 02:32:09+00:00,2025-09-17T17:14:10Z,,False,8,1,10,29,23,1,9,2025-09-17 17:13:06+00:00,55,384,False,True,False,False,False,False,1,7,1585,27389,19806,7583,1,9,4.0,7.0,2025-09-10T16:31:03Z,pytorch
162558,closed,Fix inconsistent test and add new tracer as config,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* #162993
* #162992
* #162682
* #162559
* __->__ #162558
* #162557

It is better to have the new tracer as global config that can be manipulated easily. Also I believe dynamo-like config infra is useful instead of relying on custom way of patching stuff.

Differential Revision: [D82478649](https://our.internmc.facebook.com/intern/diff/D82478649)",2025-09-10 02:32:03+00:00,2025-09-17T17:02:56Z,,False,8,0,10,61,60,7,8,2025-09-17 17:01:51+00:00,50,453,False,True,False,False,False,False,7,7,1585,27404,19810,7594,1,9,3.0,7.0,2025-09-10T16:30:49Z,pytorch
162557,closed,Don't skip register_dataclass unflatten in dynamo,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163137
* #163136
* #163107
* #162993
* #162992
* #162682
* #162559
* #162558
* __->__ #162557


We changed how we are tracing, as a result, we need to trace into register_data_class now.

Differential Revision: [D82478651](https://our.internmc.facebook.com/intern/diff/D82478651)",2025-09-10 02:31:56+00:00,2025-09-17T16:54:09Z,,False,8,6,9,37,8,2,14,2025-09-17 16:53:05+00:00,49,358,False,False,False,False,False,False,2,7,1585,27311,19766,7545,1,8,4.0,7.0,2025-09-10T15:32:49Z,pytorch
162556,closed,Prefer_deferred_runtime_asserts should be propagated to new tracer,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162994
* #162993
* #162992
* #162682
* #162559
* #162558
* #162557
* __->__ #162556
* #162487



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D82478650](https://our.internmc.facebook.com/intern/diff/D82478650)",2025-09-10 02:31:48+00:00,2025-09-16T21:26:07Z,,False,5,0,7,190,180,3,5,2025-09-16 21:25:03+00:00,66,439,False,False,False,False,False,False,3,4,1105,22603,16230,6373,1,5,3.0,4.0,2025-09-12T15:25:03Z,pytorch
162555,closed,Skip test_ind_worker_queue on Windows and macOS (flaky),huydhn,"Fixes https://github.com/pytorch/pytorch/issues/68643

It was closed by the bot yesterday and the issue was still there https://github.com/pytorch/pytorch/actions/runs/17595694816/job/49989589647.  It's better to just skip it directly in the code as this test has been disabled on Windows and MacOS since 2021 O_o",2025-09-10 01:50:05+00:00,2025-09-19T20:06:04Z,,False,5,2,2,5,0,1,7,2025-09-10 07:05:17+00:00,55,313,False,True,False,False,False,False,1,4,1017,7,6,1,1,2,4.0,4.0,2025-09-10T03:02:35Z,pytorch
162552,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-10 00:24:32+00:00,2025-09-10T04:25:44Z,,False,3,0,1,1,1,1,3,2025-09-10 04:24:42+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-10T00:24:33Z,pytorch
162551,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-10 00:24:18+00:00,2025-09-11T04:57:10Z,,False,9,0,1,1,1,1,9,2025-09-11 04:56:07+00:00,46,156,False,False,False,False,False,False,1,8,2514,2,1,1,1,1,3.0,8.0,2025-09-10T00:24:19Z,pytorch
162550,closed,[torchao][pt2e] Make prepare and convert faster by caching,navsud,"Summary: D79674759 tried to fix the expensive prepare and convert steps, as `assert_and_get_unique_device` was called multiple times. This change fixes that issue by using `functools.cache` decorator.

Test Plan:
Verified on llm export to QNN.
LLM Quantization prepare time of ~20min reduced to ~3min.

Rollback Plan:

Differential Revision: D82073679




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-10 00:05:58+00:00,2025-09-11T08:00:57Z,,False,6,0,1,2,0,1,6,2025-09-11 07:59:25+00:00,58,412,False,True,False,False,False,False,1,1,496,2,2,0,1,1,2.0,2.0,2025-09-10T14:08:52Z,pytorch
162549,open,[c10d] Remove init_process_group from DeviceMesh,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162571
* __->__ #162549
* #163058
* #162770
* #162545
* #162529

As titled.
So that `DeviceMesh` creation will no longer implicitly create a global process group, which is time consuming and most-of-time not used.
`init_process_group` is replaced with `dist.init()` -- just for `get_rank()` and `get_world_size()` to work.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 23:59:04+00:00,2025-09-18T03:49:01Z,,False,2,1,15,174,178,5,3,,48,516,False,False,False,False,False,False,5,1,57,996,603,393,1,15,2.0,1.0,2025-09-10T02:37:40Z,pytorch
162548,closed,[FSDP][experiment] CI error test1,anshul-si,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162548



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 23:53:52+00:00,2025-09-10T19:40:44Z,,False,1,0,1,1,1,1,1,2025-09-10 19:40:44+00:00,33,197,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162547,closed,test2,anshul-si,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162547



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 23:52:44+00:00,2025-09-10T19:39:59Z,,False,1,0,1,38,24,3,1,2025-09-10 19:39:59+00:00,5,197,False,False,False,False,False,False,3,0,0,62,38,24,1,1,,,,pytorch
162546,closed,xpu: test py_limited_api with SyclExtension,dvrogozh,"Commit extends existing CUDA test to cover XPU SyclExtension case for the same feature - `py_limited_api`. Commit required a fix for xpu to install some Aten header files (#145902) which got resolved after the merge of #159621.

See: https://github.com/pytorch/pytorch/issues/145902
Requires: https://github.com/pytorch/pytorch/pull/159621
Requires: https://github.com/intel/torch-xpu-ops/pull/1743

CC: @guangyey, @EikanWang 
",2025-09-09 23:35:46+00:00,2025-09-12T21:58:06Z,,False,3,9,2,59,13,5,12,2025-09-12 21:57:04+00:00,43,427,False,True,True,False,False,False,5,2,506,206,126,80,1,2,5.0,4.0,2025-09-10T02:34:04Z,pytorch
162545,open,[c10d] new_group works without world group created,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162571
* #162549
* #163058
* #162770
* __->__ #162545
* #162529

As titled.
Unblocking DeviceMesh use case where DeviceMesh may only want to create sub-groups along dimensions, without creating the world group.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 23:32:15+00:00,2025-09-17T08:36:26Z,,False,1,1,11,59,25,2,2,,50,404,False,False,False,False,False,False,2,0,0,846,483,363,1,11,1.0,0.0,2025-09-12T03:08:37Z,pytorch
162544,closed,Build fbgemm_gpu for TORCH_CUDA_ARCH_LIST=10.0 and CUDA 12.8 and 12.9,danielvegamyhre,"## Summary
- pytorch is not built for *a variants of SM architectures, due to non-portability. However, we need fbgemm_gpu kernels built for sm100a (see #162209)

## Changes
- **Setting USE_FBGEMM_GENAI for CUDA builds**: fbgemm_gpu builds for sm100a if using CUDA 12.8 or 12.9 ([source](https://github.com/pytorch/FBGEMM/blob/2033a0a08fbc08f83a1c8da5717546407f9bd972/.github/scripts/nova_dir.bash#L29-L32)), so I follow the same rule here.
- **Extra nvcc flags**: if USE_FBGEMM_GENAI and USE_CUDA are set, we add extra nvcc flags for sm100a

## Test plan

Test build:  
```
echo $CUDA_HOME
/usr/local/cuda-12.9

export TORCH_CUDA_ARCH_LIST=10.0
python -m pip install --no-build-isolation -v -e .
```

Check build logs:
```
  CMake Warning at CMakeLists.txt:901 (message):
    Setting USE_FBGEMM_GENAI to ON, doing CUDA build for SM100a
```

Run unit tests:
- `pytest test/test_matmul_cuda.py  -k test_mxfp8_scaled_grouped_mm`


cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-09 23:27:51+00:00,2025-09-10T23:00:46Z,,False,4,3,3,11,3,2,7,2025-09-10 22:59:43+00:00,69,968,False,False,False,False,False,False,2,3,788,30,19,11,1,3,3.0,3.0,2025-09-10T19:10:30Z,pytorch
162543,closed,Fix inconsistent clock types in `ProcessGroupNCCL::runHookLoop`,0xjeffro,"## Summary
This PR fixes an inconsistency in `ProcessGroupNCCL::runHookLoop` when computing `timeStarted`. Both `timeFinished` and `timeStarted` in `WorkInfo` are expected to use `std::chrono::system_clock`, but previously the code was casting a duration from `steady_clock`.

Reviewers suggested using `steady_clock` consistently for time measurement since it is appropriate for durations (see #153135 ). This PR updates both `timeStarted` and `timeFinished` in `WorkInfo`, and corresponding code in `runHookLoop`, to use `std::chrono::steady_clock`.

## Error message:
```
libcxx/include/__memory/allocator_traits.h:302:5: error: no matching function for call to '__construct_at'
  302 |     std::__construct_at(__p, std::forward<_Args>(__args)...);
      |     ^~~~~~~~~~~~~~~~~~~
libcxx/include/__memory/shared_ptr.h:162:33: note: in instantiation of function template specialization 'std::allocator_traits<std::allocator<c10d::WorkInfo>>::construct<c10d::WorkInfo, c10d::OpType, unsigned long, std::chrono::time_point<std::chrono::system_clock, std::chrono::duration<long long, std::ratio<1, 1000000000>>> &, std::chrono::time_point<std::chrono::system_clock> &, std::chrono::duration<float, std::ratio<1, 1000>>, 0>' requested here
  162 |     allocator_traits<_TpAlloc>::construct(__tmp, __get_elem(), std::forward<_Args>(__args)...);
      |                                 ^
libcxx/include/__memory/shared_ptr.h:736:51: note: in instantiation of function template specialization 'std::__shared_ptr_emplace<c10d::WorkInfo, std::allocator<c10d::WorkInfo>>::__shared_ptr_emplace<c10d::OpType, unsigned long, std::chrono::time_point<std::chrono::system_clock, std::chrono::duration<long long, std::ratio<1, 1000000000>>> &, std::chrono::time_point<std::chrono::system_clock> &, std::chrono::duration<float, std::ratio<1, 1000>>, std::allocator<c10d::WorkInfo>, 0>' requested here
  736 |   ::new ((void*)std::addressof(*__guard.__get())) _ControlBlock(__a, std::forward<_Args>(__args)...);
      |                                                   ^
libcxx/include/__memory/shared_ptr.h:744:15: note: in instantiation of function template specialization 'std::allocate_shared<c10d::WorkInfo, std::allocator<c10d::WorkInfo>, c10d::OpType, unsigned long, std::chrono::time_point<std::chrono::system_clock, std::chrono::duration<long long, std::ratio<1, 1000000000>>> &, std::chrono::time_point<std::chrono::system_clock> &, std::chrono::duration<float, std::ratio<1, 1000>>, 0>' requested here
  744 |   return std::allocate_shared<_Tp>(allocator<__remove_cv_t<_Tp> >(), std::forward<_Args>(__args)...);
      |               ^
torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2674:32: note: in instantiation of function template specialization 'std::make_shared<c10d::WorkInfo, c10d::OpType, unsigned long, std::chrono::time_point<std::chrono::system_clock, std::chrono::duration<long long, std::ratio<1, 1000000000>>> &, std::chrono::time_point<std::chrono::system_clock> &, std::chrono::duration<float, std::ratio<1, 1000>>, 0>' requested here
 2674 |         onCompletionHook_(std::make_shared<WorkInfo>(
      |                                ^
libcxx/include/__memory/construct_at.h:44:58: note: candidate template ignored: substitution failure [with _Tp = c10d::WorkInfo, _Args = <c10d::OpType, unsigned long, std::chrono::time_point<std::chrono::system_clock, std::chrono::duration<long long, std::ratio<1, 1000000000>>> &, std::chrono::time_point<std::chrono::system_clock> &, std::chrono::duration<float, std::ratio<1, 1000>>>]: no matching constructor for initialization of 'c10d::WorkInfo'
   43 | template <class _Tp, class... _Args, class = decltype(::new(std::declval<void*>()) _Tp(std::declval<_Args>()...))>
      |                                                                                    ~~~
   44 | _LIBCPP_HIDE_FROM_ABI _LIBCPP_CONSTEXPR_SINCE_CXX20 _Tp* __construct_at(_Tp* __location, _Args&&... __args) {
      |                                                          ^
1 error generated.

```
cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @haifeng-jin 
",2025-09-09 23:19:28+00:00,2025-09-12T16:51:51Z,,False,12,2,2,5,5,2,14,2025-09-12 16:50:46+00:00,63,4148,False,True,False,False,False,False,2,10,1835,14,7,7,1,2,6.0,11.0,2025-09-10T02:52:07Z,pytorch
162542,open,[CP][WIP] Introduce ContextParallal plan for parallelize_module(),fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* __->__ #162542
* #163231
* #163131
* #163115
* #162541
* #162540
* #162539

**Motivation**

Since FlexAttention and SDPA are both functions, not modules, we have tried numerous mechanisms to dispatch FlexAttention and SDPA to customized call paths so that we can inject the CP logic. Unfortunately, all of these approaches have their own composability issues with different techniques.

**Candidate Approaches**

1. Ask users to write a module to wrap FlexAttention/SDPA and use `parallelize_module` to install a forward hook.

   - Pros: This is similar to how we do TP.
   - Cons: 1) It is cumbersome for users as they need to create a new module. 2) We need two places to parallelize the CP, as a context_parallel context manager is still required for splitting the inputs.

2. Provide a function wrapper.

   - Pros: Users just need to replace their FlexAttention/SDPA calls with the wrapper.
   - Cons: It is not the same API, though we can maintain the API signatures to be the same as the core API.

**Summary**

~~This PR implements approach 2 and refactor the code in such a way that most code can be used by option approach 1, which will be introduced in another PR.~~

We changed this PR to implement option 1 as people like option 1 due to the consistency with the existing parallelisms. But this PR can also serve the foundation to implement option 2, which was the early version of this PR.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 22:47:33+00:00,2025-09-18T03:48:41Z,,False,2,9,16,255,114,2,11,,65,1588,False,False,False,False,False,True,2,1,1485,30046,22457,7589,1,15,3.0,2.0,2025-09-17T04:00:29Z,pytorch
162541,closed,[CP][BE] Correct the names of some tests,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* #163231
* #163131
* #163115
* __->__ #162541
* #162540
* #162539

We are not doing ring attention but only using allgather to do CP for Flex.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 22:47:30+00:00,2025-09-19T00:39:13Z,,False,3,0,7,8,8,1,3,2025-09-19 00:38:07+00:00,40,335,False,False,False,False,False,False,1,2,800,28988,21853,7135,1,7,6.0,2.0,2025-09-10T02:48:52Z,pytorch
162540,closed,[CP] Remove the need of recording cp_dim in the global var,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* #163231
* #163131
* #163115
* #162541
* __->__ #162540
* #162539

This information can be obtained during the dispatching.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 22:47:27+00:00,2025-09-18T23:41:56Z,,False,3,0,6,3,21,2,3,2025-09-18 23:40:51+00:00,58,316,False,False,False,False,False,False,2,2,569,28996,21848,7148,1,6,5.0,3.0,2025-09-10T02:48:41Z,pytorch
162539,closed,[CP][BE] Remove _AttentionContextParallel,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163185
* #162542
* #163231
* #163131
* #163115
* #162541
* #162540
* __->__ #162539

This is not an API we want to support.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 22:47:23+00:00,2025-09-18T06:21:27Z,,False,13,0,6,3,298,2,13,2025-09-18 06:20:20+00:00,41,298,False,False,False,False,False,False,2,12,3454,29283,21857,7426,1,6,4.0,13.0,2025-09-10T02:48:16Z,pytorch
162538,closed,[nativert] AOTI delegate with flat inputs and outputs,yiming0416,"Summary: `executorch_call_delegate` should have flattened inputs and outputs. So that it can be correctly serialized and the input/output specs are consistent with runtime.

Test Plan:
CI

Rollback Plan:

Differential Revision: D82064354


",2025-09-09 22:43:30+00:00,2025-09-10T11:36:54Z,,False,7,0,1,55,4,2,7,2025-09-10 11:35:49+00:00,53,240,False,False,False,False,False,False,2,1,476,59,55,4,1,1,2.0,1.0,2025-09-09T23:27:22Z,pytorch
162537,closed,[inductor] Add FLOAT_IS_NAN and COMPLEX_IS_NAN guards,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162537
* #162528



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-09-09 22:31:19+00:00,2025-09-12T04:34:10Z,,False,3,0,3,65,10,3,3,2025-09-12 04:32:50+00:00,53,284,False,False,False,False,False,False,3,2,493,8771,6083,2688,1,2,4.0,2.0,2025-09-11T18:00:02Z,pytorch
162536,closed,[Cutlass] Add exp and sigmoid activations,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162536
* #162535



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 22:26:25+00:00,2025-09-10T22:01:49Z,,False,3,0,1,6,2,2,3,2025-09-10 21:44:30+00:00,41,307,False,False,False,False,False,False,2,2,493,8,6,2,1,1,4.0,2.0,2025-09-10T17:23:00Z,pytorch
162535,closed,[Cutlass] Add tanh activation and test case for activations,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162536
* __->__ #162535



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 22:26:22+00:00,2025-09-10T21:44:30Z,,False,2,0,1,28,2,2,2,2025-09-10 21:44:29+00:00,59,307,False,False,False,False,False,False,2,1,48,30,28,2,1,1,2.0,1.0,2025-09-10T17:21:51Z,pytorch
162534,closed,[CuTe] Add type for CuTe layout via claude,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161106
* #161016
* #162690
* #162414
* __->__ #162534
* #162413

This PR mostly is a cosmetic change using Claude to add types for copied PyCute code.
We removed all suppressions of linters and add type checker, type alias and mypy ignore(if needed) so that the pycute code will be checked by linter.


cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 22:22:44+00:00,2025-09-12T05:00:28Z,,False,5,41,8,178,107,4,46,2025-09-12 04:59:23+00:00,42,475,False,False,False,False,False,False,4,4,965,589,326,263,1,8,4.0,6.0,2025-09-10T03:05:48Z,pytorch
162533,closed,[FlexAttn][Minor] Update FlexConfig doc,BoyuanFeng,cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben,2025-09-09 22:16:43+00:00,2025-09-10T02:04:53Z,,False,3,0,2,2,1,1,3,2025-09-10 02:03:50+00:00,39,201,False,False,False,True,False,False,1,2,493,5,3,2,1,2,3.0,2.0,2025-09-09T22:58:42Z,pytorch
162532,closed,forward fix shape guards handling for executorch,avikchaudhuri,"Summary:
X-link: https://github.com/pytorch/ao/pull/2969

D80713603 introduces a call module to a generated `_guards_fn` by export that ET pass infra cannot handle right now.

It is possible to omit this generation by using `ep.module(check_guards=False)` but there are too many places to update, so meanwhile we can explicitly remove the generated instruction as in here.

Test Plan:
broken tests should pass

This diff has been blamed as the cause of 27 test breakages:
tests:test_pt2e - test_pt2e_quant_joiner (pyspeech.tests.models.test_pt2e_compatibilities.JoinerNumericalEval)
MS Request 60655532 ⋅ Sep 9, 2025, 2:25 AM
tests:test_pt2e - test_pt2e_quant_encoder (pyspeech.tests.models.test_pt2e_compatibilities.EncoderNumericalEval)
MS Request 60655534 ⋅ Sep 9, 2025, 2:05 AM
tests:test_pt2e - test_pt2e_fp32_joiner (pyspeech.tests.models.test_pt2e_compatibilities.JoinerNumericalEval)
MS Request 60655533 ⋅ Sep 9, 2025, 1:51 AM
tests:test_model - test_qat_model_et (vizard_projects.ml_depth.tests.test_model.TestModel)
MS Request 60654585 ⋅ Sep 9, 2025, 1:45 AM
tests:test_load_fbl - test_load_qat (vizard_projects.ml_depth.tests.test_load_fbl.TestData)
MS Request 60654613 ⋅ Sep 9, 2025, 1:26 AM
tests:test_pt2e - test_pt2e_quant_predictor (pyspeech.tests.models.test_pt2e_compatibilities.PredictorNumericalEval)
MS Request 60655536 ⋅ Sep 9, 2025, 1:20 AM
tests:test_pt2e - test_pt2e_fp32_predictor (pyspeech.tests.models.test_pt2e_compatibilities.PredictorNumericalEval)
MS Request 60655535 ⋅ Sep 9, 2025, 12:57 AM
tests:trainer - test_hello_world_qat_executorch (motion_algos.deepmotion.tests.trainer_tests.TrainerTests)
MS Request 60654651 ⋅ Sep 9, 2025, 12:45 AM
tests:test_model - test_qat_model_et_per_channel (vizard_projects.ml_depth.tests.test_model.TestModel)
MS Request 60654576 ⋅ Sep 9, 2025, 12:39 AM
tests:test_model - test_qat_uncertainty_model_et (vizard_projects.ml_depth.tests.test_model.TestModel)
MS Request 60654572 ⋅ Sep 9, 2025, 12:08 AM
test:e2e_test_cpu - test_interpreter (sigmoid.inference.test.e2e_test.E2ETestGraphInputAsStr2)
MS Request 60654118 ⋅ Sep 9, 2025, 12:05 AM
tests:test_model - test_qat_uncertainty_model_et_per_channel (vizard_projects.ml_depth.tests.test_model.TestModel)
MS Request 60654573 ⋅ Sep 9, 2025, 12:03 AM
test:e2e_test_cpu - test_package_reader (sigmoid.inference.test.e2e_test.E2ETestGraphInputAsStr2)
MS Request 60654108 ⋅ Sep 8, 2025, 11:55 PM
test:e2e_test_cpu - test_aotinductor_simple (sigmoid.inference.test.e2e_test.E2ETestGraphInputAsStr2)
MS Request 60654105 ⋅ Sep 8, 2025, 11:39 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_added_node_gets_unique_id (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654198 ⋅ Sep 8, 2025, 11:27 PM
quantization:test_turing_quantizer - test_masked_attention_turing_3_0 (on_device_ai.helios.quantization.test_turing_quantizer.TestPatterns)
MS Request 60654829 ⋅ Sep 8, 2025, 11:25 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_run_decompositions_same_handle_id (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654203 ⋅ Sep 8, 2025, 11:21 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_simple (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654209 ⋅ Sep 8, 2025, 11:18 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_copy_preserve_handle (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654202 ⋅ Sep 8, 2025, 11:11 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_re_export_preserve_handle (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654197 ⋅ Sep 8, 2025, 11:10 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_run_decompositions_map_handle_to_new_nodes (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654204 ⋅ Sep 8, 2025, 11:10 PM
test:torchao_test_quantization_pt2e_quantization_pt2e_test_numeric_debugger - test_deepcopy_preserve_handle (pytorch.ao.test.quantization.pt2e.test_numeric_debugger.TestNumericDebuggerInfra)
MS Request 60654201 ⋅ Sep 8, 2025, 11:06 PM
tests:test_ptq - test_convert_model (vizard_projects.ego_ocr_recognition.tests.test_ptq.TestPTQ)
MS Request 60644273 ⋅ Sep 8, 2025, 9:01 PM
test:test_backends_lifted - test_backend_with_compiler_delegate_and_operator (executorch.exir.backend.test.test_backends_lifted.TestBackends)
MS Request 60644108 ⋅ Sep 8, 2025, 8:10 PM
test:test_backends_lifted - test_backend_with_compiler_delegate_and_operator_with_two_modules (executorch.exir.backend.test.test_backends_lifted.TestBackends)
MS Request 60644092 ⋅ Sep 8, 2025, 8:04 PM
test:test_backends - test_backend_with_compiler_delegate_and_operator_with_two_modules (executorch.exir.backend.test.test_backends.TestBackends)
MS Request 60644086 ⋅ Sep 8, 2025, 7:56 PM
test:test_backends - test_backend_with_compiler_delegate_and_operator (executorch.exir.backend.test.test_backends.TestBackends)
MS Request 60644083 ⋅

Rollback Plan:

Differential Revision: D82030581


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-09 21:31:33+00:00,2025-09-11T15:40:58Z,,False,2,0,1,17,2,3,2,2025-09-11 15:40:58+00:00,48,5364,False,True,False,False,False,False,3,0,62,19,17,2,1,1,1.0,1.0,2025-09-10T20:05:49Z,pytorch
162531,closed,Update Nvidia driver to CUDA 13.0 compatible 580.82.07,atalman,"Please see: https://docs.nvidia.com/datacenter/tesla/tesla-release-notes-580-65-06/

Looks like our CUDA 13.0 CI  tests are passing because however the driver was not updated in CI:
https://github.com/pytorch/pytorch/actions/runs/17577200229/job/49928053389#step:13:577

Please see:
```
/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 12080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org/ to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /var/lib/jenkins/workspace/c10/cuda/CUDAFunctions.cpp:119.)
  return torch._C._cuda_getDeviceCount() > 0
```",2025-09-09 21:29:00+00:00,2025-09-17T14:10:50Z,,False,14,0,3,2,5,2,14,2025-09-17 14:10:50+00:00,54,865,False,False,False,True,False,False,2,13,6865,9,3,6,1,3,5.0,13.0,2025-09-09T22:33:44Z,pytorch
162530,open,Add operator benchmarking run to CI nightly,jainapurva,"This PR introduces a new ""operator microbenchmark"" CI workflow and GitHub Actions for operator microbenchmarks, updating test scripts and job matrices to support new parameters, and broadening the operator benchmark tests to include more data types, larger shapes, and gradient tests. The benchmark configurations now focus more on different cuda hardware and multiple dtypes (bf16, fp16, fp32), for both compile and eager mode.

**Benchmark Configuration and Coverage:**

* Expanded operator benchmark configurations in `addmm_test.py`, `bmm_test.py`, `matmul_test.py`, and `mm_test.py` to benchmark multiple dtypes on CUDA devices, in eager and compile mode, for forward and backward run. The configs with tag ""long"" for the above mentioned files are being run in CI.
* The CI benchmarking is running on various hardwares: H100, B200, A100.
* The CI job also uploads the microbenchmarking outputs to a [HUD](https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fpytorch&benchmarkName=PyTorch+operator+microbenchmark) dashboard.



",2025-09-09 21:27:09+00:00,2025-09-25T04:48:54Z,,False,7,7,45,269,92,8,14,,43,1037,False,False,False,False,False,False,8,6,2940,118884,75943,42941,2,30,4.0,8.0,2025-09-12T17:29:45Z,pytorch
162529,open,[c10d] Add dist.init(),kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #162571
* #162549
* #163058
* #162770
* #162545
* __->__ #162529

Mainly to support a case where user don't want to call `init_process_group` which instantiates a real group that may not be used.

What `dist.init()` does:
```
    Initialize a distributed environment for current job. Once this API is
    called, `get_world_size()` is guaranteed to return the number of processes
    in current job, and `get_rank()` is guaranteed to return the rank of current
    process in the job.
```

With the proposed API, user can get group of interest via, e.g. DeviceMesh creation:
```
dist.init()
mesh = dist.init_device_mesh(...)
group = mesh.get_dim(...)
```

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 21:20:53+00:00,2025-09-23T20:28:42Z,,False,6,14,10,254,88,11,20,,22,847,False,False,False,False,False,False,11,5,1047,404,285,119,1,10,6.0,5.0,2025-09-10T00:05:06Z,pytorch
162528,closed,[dynamo] Add DUAL_LEVEL_MATCH C++ guard,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162537
* __->__ #162528



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-09-09 21:08:21+00:00,2025-09-12T04:32:50Z,,False,2,0,4,64,8,3,2,2025-09-12 04:32:49+00:00,39,284,False,False,False,False,False,False,3,1,48,8786,6092,2694,1,4,2.0,1.0,2025-09-11T17:57:40Z,pytorch
162527,closed,Allow aot_module_simplified to return a serializable output,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162527

This PR refactors AOTAutograd slightly: 

- It adds `simple_wraps` to various wrappers so that the reference to inner functions is stored in the output of AOTAutograd. 
- It saves a `serialize()` method on the result of `aot_stage2`, in the event of an eager backward compile. 

I discussed the lazy backward case with @bdhirsh, and we agreed that serialization in that case would probably use a different, more AOT API anyway, so we do not implement a serialize function for the lazy backward case. AOT precompile, at least initially, will always eagerly compile the backward. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 20:18:16+00:00,2025-09-16T15:23:12Z,,False,4,5,6,147,60,7,9,2025-09-16 15:22:08+00:00,59,844,False,False,False,False,False,True,7,3,1540,26024,20378,5646,1,6,4.0,4.0,2025-09-10T19:36:09Z,pytorch
162526,closed,"[Release 2.9] Add compatibility matrix, Version Bump",atalman,"Release 2.9
1. Add release compatibility matrix
2. Add version bump for 2.10",2025-09-09 20:05:44+00:00,2025-09-09T21:31:39Z,,False,3,0,2,2,1,2,3,2025-09-09 20:38:19+00:00,52,76,False,False,False,False,False,False,2,2,805,5,3,2,1,2,3.0,2.0,2025-09-09T20:35:03Z,pytorch
162525,open,[dynamo][guards] Do not construct entire framelocals dict for LAMBDA_GUARD,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162525
* #162509



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 19:59:24+00:00,2025-09-19T12:23:28Z,,False,7,2,8,128,19,7,9,,74,276,False,False,False,False,False,False,7,6,1800,4042,2344,1698,1,8,3.0,6.0,2025-09-10T00:06:30Z,pytorch
162524,closed,Export d82049754,Nicoshev,"Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 19:53:33+00:00,2025-09-09T19:53:42Z,2025-09-09T19:53:42Z,True,2,0,9,4957,187,43,2,2025-09-09 19:53:42+00:00,16,279,False,True,False,False,False,False,43,0,0,5158,4964,194,1,9,,,,pytorch
162523,open,[PyTorch] Add OSS SVE128 detection,Nicoshev,"Summary:
Modify FindARM.cmake to enable SVE128 detection at compile time

Modify codegen.cmake to add proper c++ compile flags

Test Plan:
Github nightly pipeline

Rollback Plan:

Differential Revision: D82049754


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 19:51:40+00:00,2025-09-25T17:00:59Z,,False,32,1,9,4751,221,39,33,,34,472,False,False,False,False,False,False,39,0,0,4984,4757,227,1,9,,,,pytorch
162521,open,Avoids calling builtin `iter` if object is a generator,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162666
* __->__ #162521

The `iter(gen)` call will return the given `gen` object. So, we just avoid this call and shaves off a few ms of tracing time

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 19:06:42+00:00,2025-09-17T21:47:18Z,,False,2,0,5,2,0,1,2,,54,400,False,False,False,False,False,False,1,0,0,39130,27342,11788,1,5,,,,pytorch
162520,closed,[serialization] Add pte file to archive,lucylq,"Summary:
Add _package_executorch_files to archive apis. Allow us to package a PTE file into the archive.

I don't think there's a use-case to have more than one PTE file at the moment, but left it as `EXECUTORCH_FILES` just in case.

Test Plan:
Tested in D81992612

Rollback Plan:

Differential Revision: D81977483


",2025-09-09 19:00:22+00:00,2025-09-11T08:00:18Z,,False,5,0,1,20,0,4,5,2025-09-11 07:59:14+00:00,39,317,False,False,False,False,False,False,4,1,476,20,20,0,1,1,2.0,1.0,2025-09-09T20:09:08Z,pytorch
162519,closed,[dynamo][refactor] Move get_framelocals_idx to a helper,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162525
* #162509
* __->__ #162519



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 18:59:20+00:00,2025-09-10T00:36:15Z,,False,3,0,3,15,14,1,3,2025-09-10 00:35:12+00:00,55,286,False,False,False,False,False,True,1,2,493,170,25,145,1,3,3.0,2.0,2025-09-09T21:16:46Z,pytorch
162518,closed,[CP] Fix the CP FlexAttention test,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162542
* #162541
* #162540
* #162539
* __->__ #162518



cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 18:48:06+00:00,2025-09-16T00:13:32Z,,False,6,0,2,9,6,1,6,2025-09-16 00:12:29+00:00,34,230,False,True,False,False,False,False,1,5,1395,18894,14351,4543,1,2,4.0,6.0,2025-09-09T18:53:07Z,pytorch
162516,closed,[BE] Fix `'_WIN32' is not defined` warning,malfet,"Summary: As indeed it is not defined neither on  Linux nor on MacOS platforms

Test Plan:
CI

Rollback Plan:

Differential Revision: D82044853


",2025-09-09 18:39:26+00:00,2025-09-10T04:22:46Z,,False,4,0,1,1,1,1,4,2025-09-10 04:21:42+00:00,42,145,False,True,False,False,False,False,1,1,476,2,1,1,1,1,2.0,1.0,2025-09-09T18:48:49Z,pytorch
162515,closed,[c10d][nvshmem] fix override function modifier,jushg,"Summary: Fix compilation error in fbsource by missing override modifier

Differential Revision: D82038876




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 18:35:25+00:00,2025-09-10T11:36:56Z,,False,4,0,1,1,1,1,4,2025-09-10 11:35:53+00:00,46,211,False,True,False,False,False,False,1,1,476,2,1,1,1,1,3.0,1.0,2025-09-09T18:38:41Z,pytorch
162514,open,[aoti] Add mtime to precompiled header hash,benjaminglass1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162514

Fixes GCC errors when a header to be precompiled is touched, but the preprocessed content remains the same. This is apparently not allowed, so add an additional field to the deduplication hash with modification time.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 18:20:50+00:00,2025-09-16T22:03:52Z,,False,4,0,1,20,8,1,4,,43,513,False,True,False,False,False,False,1,3,932,28,20,8,1,1,3.0,5.0,2025-09-09T18:21:24Z,pytorch
162513,closed,[inductor] Add shape to load_input in matmul templates,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162275
* __->__ #162513
* #162426



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 18:20:09+00:00,2025-09-11T01:52:23Z,,False,6,0,2,31,17,3,6,2025-09-11 01:51:18+00:00,54,317,False,False,False,False,False,False,3,5,1600,50,32,18,1,2,3.0,5.0,2025-09-10T01:43:00Z,pytorch
162511,open,Add torch compile check for ZeroBubble,H-Huang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162511

Fix https://github.com/pytorch/pytorch/issues/161904


cc @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 18:14:02+00:00,2025-09-12T02:24:20Z,,False,1,2,3,44,14,2,3,,38,241,False,True,False,False,False,False,2,0,0,82,56,26,1,3,2.0,0.0,2025-09-10T22:13:12Z,pytorch
162510,closed,[ROCm] Support torch.cuda._compile_kernel,jammm,"Supports `torch.cuda._compile_kernel` on ROCm. Related to https://github.com/pytorch/pytorch/pull/151484
Tested on Windows with gfx1201. Testing on Linux pending.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-09 18:11:39+00:00,2025-09-12T00:19:53Z,,False,33,17,1,102,29,2,50,2025-09-12 00:19:52+00:00,41,280,False,False,False,False,False,False,2,29,8551,131,102,29,1,1,4.0,31.0,2025-09-09T18:13:55Z,pytorch
162509,closed,[dynamo][guards] Prevent framelocals to dict conversion for not required LAMBDA_GUARD,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162525
* __->__ #162509

This is a smaller PR to reduce framelocals to dict conversion.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 18:06:08+00:00,2025-09-19T23:30:28Z,,False,5,3,5,135,9,3,8,2025-09-10 18:52:18+00:00,85,338,False,False,False,False,False,False,3,4,1526,3978,2321,1657,1,5,3.0,4.0,2025-09-10T00:35:21Z,pytorch
162508,closed,Fully native DTensor.__new__,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163030
* #162968
* __->__ #162508
* #161695

Move the entirety of `__new__` into C++, saving a layer of disable_dynamo and making progress toward all-C++.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 17:44:41+00:00,2025-09-21T18:37:12Z,,False,4,6,10,129,103,4,10,2025-09-21 18:36:08+00:00,28,336,False,False,False,False,False,False,4,3,1591,35717,26725,8992,1,10,4.0,4.0,2025-09-09T20:54:25Z,pytorch
162506,closed,[triton] enable int64 indexing in convolution and mm template,yuguo68,"Summary: hitting illegal memory access issue when compiling conv and addmm kernels with the change in https://github.com/pytorch/pytorch/pull/157767

Differential Revision: D81995664




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 17:16:04+00:00,2025-09-10T01:54:33Z,,False,4,0,1,15,15,5,4,2025-09-10 01:53:30+00:00,61,388,False,False,False,False,False,False,5,2,949,30,15,15,1,1,3.0,3.0,2025-09-09T18:53:14Z,pytorch
162505,closed,Make torch.cuda.rng_set_state() and torch.cuda.rng_get_state() work during stream capture.,galv,"Note that this works only in a limited case, where you *don't* change the seed, but change only the offset of the philox generator. This captures the main use case we're interested in: Rewinding the RNG to a previous state. This is done by torch.utils.checkpoint.checkpoint in particular.

Calls to increase() change only the offset, not the seed. Thus, we allow for ""no-op"" calls to set_seed where the new seed is the same as the old seed. If a user does happen to try to change the seed during stream capture, they will receive an error.

Fixes #162504


cc @pbelevich @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng",2025-09-09 17:07:58+00:00,2025-09-17T03:58:42Z,,False,8,4,4,70,12,2,12,2025-09-17 03:57:38+00:00,90,621,False,True,False,False,False,False,2,7,1901,90,74,16,1,4,7.0,8.0,2025-09-09T22:25:27Z,pytorch
162503,open,Fix MPS AOTI benchmarks,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162503



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 16:37:03+00:00,2025-09-11T07:46:18Z,,False,7,1,2,19,6,3,8,,23,266,False,True,False,False,False,False,3,6,3740,102118,64771,37347,1,2,3.0,7.0,2025-09-09T20:45:01Z,pytorch
162502,closed,canary capture scalar outputs = True,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162502
* #161843



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 16:29:55+00:00,2025-09-17T03:32:42Z,,False,2,0,1,1,1,1,2,2025-09-17 03:32:42+00:00,36,276,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162501,closed,CUDA 13.0 Windows Nvidia Driver Update to 580.88,pytorchbot,"Related to https://github.com/pytorch/pytorch/issues/162333
https://github.com/pytorch/pytorch/issues/159779
",2025-09-09 16:24:43+00:00,2025-09-09T18:27:57Z,2025-09-09T18:27:57Z,True,1,0,1,5,5,1,1,2025-09-09 18:27:57+00:00,48,109,False,False,False,False,False,False,1,0,0,10,5,5,1,1,1.0,0.0,2025-09-09T18:27:46Z,pytorch
162500,open,Add support for scaled_grouped_mm in AOTI,yyetim,"Summary:
Currently, this aten on errors out with
error: use of undeclared identifier 'aoti_torch_cuda__scaled_grouped_mm'

Test Plan:
CUDA_VISIBLE_DEVICES=3 buck test  mode/{opt,inplace} -c fbcode.enable_gpu_sections=true -c fbcode.nvcc_arch=h100a -c fbcode.use_link_groups=True  caffe2/test/inductor:test_aot_inductor -- test_scaled_grouped_mm

https://www.internalfb.com/intern/testinfra/testrun/8444249593285549

Rollback Plan:

Differential Revision: D81959389




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 16:23:20+00:00,2025-09-22T08:01:16Z,,False,21,0,1,77,2,5,21,,41,670,False,False,False,False,False,False,5,4,1239,79,77,2,1,1,4.0,4.0,2025-09-10T00:37:56Z,pytorch
162499,closed,[nativert][triton] improve hardware registration,dolpm,"Summary: att

Test Plan:
ci

Rollback Plan:

Differential Revision: D82031814


",2025-09-09 16:18:03+00:00,2025-09-10T04:54:03Z,,False,14,0,1,142,88,8,14,2025-09-10 04:53:00+00:00,48,80,False,False,False,False,True,False,8,2,498,230,142,88,1,1,3.0,2.0,2025-09-09T16:33:32Z,pytorch
162498,open,"mul(sparse, dense): enable proper broadcasting",nikitaved,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162498
* #162490

",2025-09-09 16:13:45+00:00,2025-09-11T16:51:58Z,,False,1,2,2,21,14,1,3,,46,104,False,False,False,False,False,False,1,0,0,670,550,120,1,2,2.0,0.0,2025-09-09T17:54:23Z,pytorch
162496,closed,[ROCm][CI] skip test_max_autotune until resolved,BLOrange-AMD,"many tests taking >30 min and causing timeouts


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-09 16:05:05+00:00,2025-09-09T16:35:08Z,,False,3,0,1,1,0,1,3,2025-09-09 16:34:05+00:00,48,165,False,False,False,False,False,False,1,2,832,1,1,0,1,1,2.0,2.0,2025-09-09T16:07:55Z,pytorch
162495,closed,[ONNX] Expose the testing module,justinchuby,"* Created a new module `torch/onnx/testing.py` that exposes the `assert_onnx_program` function for testing exported ONNX models.
* Updated the ONNX documentation (`docs/source/onnx.md`) to include `onnx_testing` in the list of relevant modules.


cc @titaiwangms",2025-09-09 16:03:59+00:00,2025-09-10T01:41:29Z,,False,9,2,3,18,0,3,11,2025-09-10 01:40:27+00:00,32,262,False,False,False,True,False,False,3,8,2621,20,19,1,1,3,4.0,8.0,2025-09-09T16:10:54Z,pytorch
162494,closed,fix Dtensor doc link,H-Huang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162511
* __->__ #162494

Small fix for https://docs.pytorch.org/docs/main/distributed.tensor.parallel.html
<img width=""890"" height=""274"" alt=""image"" src=""https://github.com/user-attachments/assets/6ee7fc7c-e0fe-4f5e-ab7e-a895bb3fa79f"" />

now it is:

<img width=""909"" height=""320"" alt=""image"" src=""https://github.com/user-attachments/assets/8b2c41ef-1684-4597-8dae-144b49723796"" />
",2025-09-09 15:59:49+00:00,2025-09-09T22:11:43Z,,False,3,0,2,2,2,1,3,2025-09-09 22:10:40+00:00,20,461,False,True,False,True,False,False,1,2,493,131735,89742,41993,1,2,3.0,2.0,2025-09-09T20:25:31Z,pytorch
162493,closed,[Release 2.9] Release only changes,atalman,"Release 2.9 release only changes
",2025-09-09 15:58:35+00:00,2025-09-09T18:15:21Z,2025-09-09T18:15:21Z,True,2,0,1,343,466,101,2,2025-09-09 18:15:21+00:00,34,33,False,False,False,False,False,False,101,1,135,809,343,466,1,1,3.0,1.0,2025-09-09T17:30:05Z,pytorch
162492,open,[DRAFT] Test triton 3.5 CP,jataylo,"Tests triton fix
",2025-09-09 15:47:33+00:00,2025-09-10T12:58:17Z,,False,2,0,2,2,2,2,2,,26,17,False,True,False,False,False,False,2,0,0,6,3,3,1,2,,,,pytorch
162491,closed,sparse_broadcast_to: fix false-positive coalesced with inputs of shape (),nikitaved,"
ghstack-source-id: 3a3a79a3bc5ff093d4c462f1d38688d3db253364
Pull-Request: https://github.com/pytorch/pytorch/pull/162490

Fixes #162482
",2025-09-09 14:46:14+00:00,2025-09-09T14:49:56Z,,False,2,0,1,27,16,2,2,2025-09-09 14:49:56+00:00,73,137,False,True,False,False,False,False,2,0,0,43,27,16,1,1,,,,pytorch
162490,open,sparse_broadcast_to: fix false-positive coalesced with inputs of shape (),nikitaved,"Fixes https://github.com/pytorch/pytorch/issues/162482

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162498
* __->__ #162490



cc @alexsamardzic @pearu @cpuhrsch @amjames @bhosmer @jcaip",2025-09-09 14:45:13+00:00,2025-09-10T14:34:00Z,,False,1,0,4,30,16,2,1,,73,221,False,True,False,False,False,False,2,0,0,729,583,146,1,4,,,,pytorch
162488,open,Shake up CI with new export,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162488
* #162487
* #162190
* #162183



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 14:30:24+00:00,2025-09-09T20:22:44Z,,False,1,0,3,8,14,3,1,,27,296,False,False,False,False,False,False,3,0,0,23438,15541,7897,1,3,,,,pytorch
162487,closed,Fix error message,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162994
* #162993
* #162992
* #162682
* #162559
* #162558
* #162557
* #162556
* __->__ #162487


More proper fix here should be that we directly replace shape_env with correct sources but it is bit involved as we have to manually construct dynamo sources by hand (need to handle list/dict etc) but it is quite easy if we are operating on a string so i do this as post-processing step for now. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D82478647](https://our.internmc.facebook.com/intern/diff/D82478647)",2025-09-09 14:30:16+00:00,2025-09-16T19:07:39Z,,False,7,1,11,63,9,3,8,2025-09-16 19:06:34+00:00,17,736,False,True,False,False,False,False,3,6,1578,47536,32538,14998,1,10,4.0,6.0,2025-09-10T16:29:09Z,pytorch
162485,open,Fix CachingAutotuner argument validation,ghostspiders,"Fixes #146018


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-09 14:12:44+00:00,2025-09-24T12:00:47Z,,False,9,0,1,78,0,2,9,,40,225,False,True,False,False,False,False,2,6,319,78,78,0,1,1,2.0,6.0,2025-09-09T14:56:09Z,pytorch
162484,open,Fix torch.linalg.eig inductor stride mismatch,parsshar-RH,"Fixes #159445

### Summary

- Fixed a stride layout issue in the `torch.linalg.eig` meta kernel that prevented successful compilation with the inductor backend. The meta kernel was producing incorrect row-major strides.

- LAPACK/BLAS libraries (underlying implementation) expect column-major layout

",2025-09-09 13:20:23+00:00,2025-09-19T16:39:23Z,,False,5,2,1,9,0,2,7,,45,301,False,True,False,False,False,False,2,4,559,9,9,0,1,1,3.0,4.0,2025-09-09T17:30:26Z,pytorch
162479,closed,[CPU][GEMM Template] Improve A16W8 performance,Xia-Weiwen,"**Summary**
Improve A16W8 performance by
1. supporting GQA concat linear
2. using smaller cache blocking size
3. improving code for dequantization of weight (reducing instructions and adding prefetch)

We saw > 5% E2E next token performance gain when running Llama3.1-8B-instruct.

**Test plan**
Already covered by UT


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 10:04:05+00:00,2025-09-18T01:29:45Z,,False,9,9,4,37,38,4,18,2025-09-18 01:28:41+00:00,46,565,False,False,False,False,True,False,4,8,1459,22916,17259,5657,1,4,5.0,8.0,2025-09-10T01:32:11Z,pytorch
162478,open,[DO NOT MERGE] Test AMD capacity.,saienduri,"This PR is purely to test AMD capacity.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-09 09:40:54+00:00,2025-09-17T15:01:05Z,,False,9,0,10,48,48,7,9,,33,157,False,False,False,False,False,False,7,0,0,96,48,48,1,1,,,,pytorch
162477,closed,update ideep submodule,Ryo-not-rio,"Update ideep hash to include primitive cache remove patch: https://github.com/intel/ideep/pull/357

cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal",2025-09-09 09:31:13+00:00,2025-09-10T01:03:04Z,,False,3,0,1,1,1,1,3,2025-09-10 01:03:04+00:00,22,265,False,False,False,False,False,False,1,2,121,2,1,1,1,1,2.0,2.0,2025-09-09T10:09:57Z,pytorch
162475,open,[CI] Upgrade Ubuntu 24.04 for XPU CI tests,chuanqi129,"Fixes #ISSUE_NUMBER
",2025-09-09 08:46:55+00:00,2025-09-12T06:01:04Z,,False,1,0,1,25,26,4,1,,42,20,False,True,False,False,False,False,4,0,0,51,25,26,1,1,,,,pytorch
162474,open,[CD] Upgrade GCC version to 13 for XPU build,chuanqi129,Follow #152426,2025-09-09 08:39:11+00:00,2025-09-09T18:22:11Z,,False,1,0,1,2,7,2,1,,44,14,False,False,False,False,False,False,2,0,0,9,2,7,1,1,,,,pytorch
162472,closed,Fix flaky AOTFxirTestCase,huydhn,"Fixes https://github.com/pytorch/pytorch/issues/162357
Fixes https://github.com/pytorch/pytorch/issues/160970
Fixes https://github.com/pytorch/pytorch/issues/161038
Fixes https://github.com/pytorch/pytorch/issues/160951
Fixes https://github.com/pytorch/pytorch/issues/161698

These tests were introduced in https://github.com/pytorch/pytorch/pull/160765 and they are all flaky when `torch._inductor.aot_compile` uses multiple threads (the default option).  The issue could be reproduced by running them locally multiple times.  For example,

```
pytest --flake-runs 10 --flake-finder -v inductor/test_fxir_backend.py -k test_aoti_fx_add
(output logs at P1938386961)
...
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 2), ('async_compile_cache_hit', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 2), ('async_compile_cache_hit', 1)]
graph_break []
--------------------------------------------------------------------------------------------------------------------------------------------------- Captured stdout call ---------------------------------------------------------------------------------------------------------------------------------------------------
inductor [('async_compile_cache_miss', 2), ('async_compile_cache_hit', 1)]
graph_break []
================================================================================================================================================= short test summary info ==================================================================================================================================================
FAILED [0.4834s] inductor/test_fxir_backend.py::AOTFxirTestCase::test_aoti_fx_add - AttributeError: 'NoneType' object has no attribute '__code__'
FAILED [0.4576s] inductor/test_fxir_backend.py::AOTFxirTestCase::test_aoti_fx_add - AttributeError: 'NoneType' object has no attribute '__code__'
FAILED [0.4613s] inductor/test_fxir_backend.py::AOTFxirTestCase::test_aoti_fx_add - AttributeError: 'NoneType' object has no attribute '__code__'
=============================================================================================================================================== 3 failed, 7 passed in 12.89s ===============================================================================================================================================
```

Setting `compile_threads` to 1 will get rid of the test flakiness, but there might be underlying issues from https://github.com/pytorch/pytorch/pull/160765.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:16:20+00:00,2025-09-09T19:40:29Z,,False,4,0,1,1,1,1,4,2025-09-09 19:39:28+00:00,25,5952,False,True,False,False,False,False,1,3,513,2,1,1,1,1,4.0,3.0,2025-09-09T15:51:58Z,pytorch
162471,closed,[inductor][bucketing][WIP] Experimenting with Collectives Trie bucketing,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162471



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:10:04+00:00,2025-09-24T14:20:31Z,,False,3,0,2,1576,186,11,3,2025-09-24 14:20:31+00:00,72,390,False,False,False,False,False,False,11,1,52,948,887,61,1,2,1.0,1.0,2025-09-09T14:00:18Z,pytorch
162470,open,[inductor][bucketing] Fx collectives bucketing of multiple dtypes,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162470




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:09:59+00:00,2025-09-09T15:21:52Z,,False,1,1,4,238,44,2,2,,65,391,False,False,False,False,False,False,2,0,0,958,388,570,1,4,,,,pytorch
162469,open,[inductor][reordering] Use runtime estimations in reorder/sink passes,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162469



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:09:53+00:00,2025-09-09T13:23:16Z,,False,1,0,2,486,102,5,1,,69,390,False,False,False,False,False,False,5,0,0,596,490,106,1,2,,,,pytorch
162468,closed,[inductor][bucketing][WIP] Bucket multidtype,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:08:42+00:00,2025-09-24T14:19:47Z,,False,2,0,1,222,46,3,2,2025-09-24 14:19:47+00:00,44,390,False,False,False,False,False,False,3,0,0,268,222,46,1,1,,,,pytorch
162467,closed,[inductor][reordering] Use runtime estimations in reorder/sink passes,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 08:08:36+00:00,2025-09-24T14:45:28Z,,False,2,0,1,490,102,5,2,2025-09-24 14:45:28+00:00,69,390,False,False,False,False,False,False,5,0,0,592,490,102,1,1,,,,pytorch
162465,closed,Improve doc examples for softmax Issue#161252,vishalgoyal316,"Fixes #161252
",2025-09-09 07:13:48+00:00,2025-09-18T06:14:11Z,,False,4,0,1,39,0,1,4,2025-09-18 06:14:11+00:00,45,14,False,True,False,True,True,False,1,3,240,39,39,0,1,1,2.0,3.0,2025-09-09T07:18:38Z,pytorch
162460,closed,[DTensor] fix copy_ strategy to support linearity,tianyu-l,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162460

Fixing issue introduced in https://github.com/pytorch/pytorch/pull/158538
where `aten.copy_.default` is registered as a pointwise op, but without linearity.

In particular, when both `src` and `dst` tensors have same `Partial` placements, direct copy should happen without redistribute, instead of redistributing both to `Replicate` before making the copy.

This was discovered from silent incorrect results e.g. on `torch.einsum` backward.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-09 06:11:59+00:00,2025-09-10T04:37:11Z,,False,4,2,1,17,20,3,6,2025-09-10 00:47:16+00:00,49,637,False,True,False,False,False,False,3,3,980,37,17,20,1,1,4.0,4.0,2025-09-09T16:33:53Z,pytorch
162459,closed,[inductor] keep weakdep for mutation with different index,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162459

Fix https://github.com/pytorch/pytorch/issues/162410 .

This is a bug exposed by https://github.com/pytorch/pytorch/pull/162355 . 

After each round of fusion (fuse_nodes_once  method), the scheduler will prune redundant dependencies. Some of the WeakDep represents mutation for buffer with different access indexes for read/write. Pruning those deps would cause incorrect fusion in later round.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 05:47:07+00:00,2025-09-19T23:33:41Z,,False,6,0,1,21,0,2,6,2025-09-19 23:33:41+00:00,57,693,False,True,False,False,False,False,2,4,634,21,21,0,1,1,4.0,5.0,2025-09-09T05:53:26Z,pytorch
162457,open,[caffe2][uenv] check if get_build_rule() is not None,AishwaryaSivaraman,"Test Plan:
CI

Rollback Plan:

Reviewed By: lty1308

Differential Revision: D81570876




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 04:43:27+00:00,2025-09-09T19:31:28Z,,False,5,0,1,12,6,1,5,,52,260,False,False,False,False,False,False,1,0,0,18,12,6,1,1,,,,pytorch
162455,closed,[CD] CUDA 13 specific followup changes,tinglvv,"Follow up for CUDA 13 bring up https://github.com/pytorch/pytorch/issues/159779
sm50-70 should not be added to sbsa build arch list, as previous archs had no support for arm. 
remove platform_machine from PYTORCH_EXTRA_INSTALL_REQUIREMENTS

cc @atalman @malfet @ptrblck @nWEIdia ",2025-09-09 04:13:23+00:00,2025-09-23T16:55:09Z,,False,11,2,3,95,95,6,13,2025-09-11 00:03:51+00:00,38,279,False,False,False,False,False,False,6,10,4123,252,126,126,1,3,4.0,10.0,2025-09-09T16:49:58Z,pytorch
162454,open,[Intel GPU] Integrate OneDNN SDPA training forward/backward into XPU OVERRIDEABLE Backend,LuFinch,"This is the second PR split from https://github.com/pytorch/pytorch/pull/156272
cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-09 03:37:47+00:00,2025-09-22T20:32:34Z,,False,15,4,4,237,50,13,19,,89,161,False,False,False,False,False,False,13,11,1187,303,245,58,1,4,4.0,11.0,2025-09-10T02:37:32Z,pytorch
162450,closed,[Inductor][UT] Fix flex attention related inductor cases,hoshibara,"## Motivation
Fixes #162435, Fixes #162436

UT failures:
* https://github.com/pytorch/pytorch/actions/runs/17523991468/job/49772651636
* https://github.com/pytorch/pytorch/actions/runs/17523991468/job/49772651637

To fix flex attention related cases.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 02:45:13+00:00,2025-09-10T07:01:15Z,,False,4,0,2,6,2,1,4,2025-09-10 06:48:04+00:00,56,453,False,True,False,False,False,False,1,2,493,8,6,2,1,2,3.0,2.0,2025-09-10T00:15:04Z,pytorch
162448,closed,[test][do not merge] test with pull requests from IDEEP by ARM folks,yanbing-j,"Fixes #ISSUE_NUMBER


cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @Guobing-Chen @Xia-Weiwen @snadampal",2025-09-09 01:31:53+00:00,2025-09-10T01:09:29Z,,False,2,0,2,4,4,4,2,2025-09-10 01:09:29+00:00,68,176,False,True,False,False,False,False,4,1,351,8,4,4,2,2,1.0,1.0,2025-09-10T00:59:10Z,pytorch
162447,closed,Add less warps config to inner reductions,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162447

Add less warps to ensure proper vectorization + memory coalescing for inner reductions, prefer more work per thread

<img width=""1717"" height=""731"" alt=""Screenshot 2025-09-17 at 10 03 25 AM"" src=""https://github.com/user-attachments/assets/7b1f4a30-62f2-4bee-bb9c-122501bde63e"" />


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-09 01:23:02+00:00,2025-09-24T19:10:10Z,,False,9,0,16,18,3,1,9,2025-09-24 19:09:05+00:00,41,577,False,False,False,False,False,False,1,8,2830,84935,57743,27192,1,16,5.0,8.0,2025-09-19T15:52:29Z,pytorch
162446,closed,Add num_store to inductor_meta and use it to scale persistent reduction x block,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162447
* __->__ #162446
* #162296

Scale up XBLOCK for contiguous persistent reductions based on rnumel and number of loads + stores

<img width=""928"" height=""656"" alt=""Screenshot 2025-09-18 at 5 02 57 PM"" src=""https://github.com/user-attachments/assets/ec3c561f-2a3f-4459-9e14-653715898da3"" />


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)",2025-09-09 01:20:34+00:00,2025-09-23T20:37:47Z,,False,11,11,10,26,3,3,22,2025-09-23 20:36:42+00:00,79,727,False,False,False,False,False,False,3,10,2261,44008,30775,13233,1,10,5.0,11.0,2025-09-15T18:59:13Z,pytorch
162443,closed,[Optimus] Add batch dropout pattern,mengluy0125,"Summary: We observe dropout pattern in AFOC, such add a new pattern to Optimus

Test Plan:
```
buck2 test 'fbcode//mode/dev-nosan' fbcode//caffe2/test/inductor:group_batch_fusion -- test_batch_dropout_pre_grad_fusion
```

Buck UI: https://www.internalfb.com/buck2/2c899fb5-6e8b-43eb-8fb3-b53abfbfa6d9
Test UI: https://www.internalfb.com/intern/testinfra/testrun/15762598805248688
Network: Up: 0B  Down: 0B  (reSessionID-bfbb9e6a-7e2a-425a-a027-b44282cef419)
Executing actions. Remaining     0/3                                                                                                     1.3s exec time total
Command: test.     Finished 2 local
Time elapsed: 1:22.3s
Tests finished: Pass 2. Fail 0. Fatal 0. Skip 0. Build failure 0



### E2E

baseline
f791163796

proposal
f793225207

Rollback Plan:

Differential Revision: D81981264




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-09 00:38:01+00:00,2025-09-10T09:50:08Z,,False,6,0,1,56,0,2,6,2025-09-10 09:49:05+00:00,35,1055,False,False,False,False,False,False,2,1,476,56,56,0,1,1,3.0,1.0,2025-09-09T20:56:23Z,pytorch
162442,closed,[inductor] Enable combo kernels with unbacked inputs,ColinPeppler,"Internal user tried enabling combo kernels, but ran into ""Cannot convert symbols to int"". This PR is to enable combo kernels on inputs with data-dependent shapes.

### Example exception

```
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton.py"", line 4997, in benchmark_combo_kernel
    kernel_code_list = self.generate_combo_kernel_code(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/codegen/simd.py"", line 1849, in generate_combo_kernel_code
    src_code = kernel.codegen_kernel()
               ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton_combo_kernel.py"", line 802, in codegen_kernel
    code.splice(self.codegen_kernel_benchmark(num_gb=0))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton_combo_kernel.py"", line 852, in codegen_kernel_benchmark
    var_names.extend(self.kernel_benchmark_extra_args())
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/codegen/triton_combo_kernel.py"", line 733, in kernel_benchmark_extra_args
    extra_args.append(str(V.graph.sizevars.size_hint(tree.numel)))
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/colinpeppler/pytorch/torch/_inductor/sizevars.py"", line 584, in size_hint
    return int(out)
           ^^^^^^^^
  File ""/home/colinpeppler/.conda/envs/pytorch/lib/python3.12/site-packages/sympy/core/expr.py"", line 307, in __int__
    raise TypeError(""Cannot convert symbols to int"")
torch._inductor.exc.InductorError: TypeError: Cannot convert symbols to int
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162609
* __->__ #162442



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D82042230](https://our.internmc.facebook.com/intern/diff/D82042230)",2025-09-09 00:37:26+00:00,2025-09-10T20:49:42Z,,False,5,3,2,99,12,5,8,2025-09-10 20:49:41+00:00,52,2132,False,False,False,False,False,False,5,4,378,154,105,49,1,2,4.0,4.0,2025-09-09T00:44:08Z,pytorch
162441,closed,Use upper bound for persistent rblock,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162441

Previously, we were using 128 and increasing to upper bound. We should be setting at the upper bound and raising to next power of 2.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81984103](https://our.internmc.facebook.com/intern/diff/D81984103)",2025-09-09 00:32:12+00:00,2025-09-10T22:30:10Z,,False,5,0,3,43,5,2,5,2025-09-10 22:29:05+00:00,37,522,False,False,False,False,False,False,2,4,801,58,48,10,1,3,3.0,4.0,2025-09-09T00:38:02Z,pytorch
162440,closed,"Reland #161649, vectorize stored in cat for all dtypes",ngimel,"Per title
",2025-09-09 00:31:58+00:00,2025-09-18T13:51:52Z,,False,5,3,9,148,11,2,8,2025-09-18 13:50:47+00:00,54,10,False,False,False,False,False,False,2,2,493,681932,452344,229588,1,9,4.0,2.0,2025-09-09T16:34:33Z,pytorch
162439,closed,DeviceMesh: support _rank for use with non-global PGs,d4l3k,"Summary: This adds a `_rank` field to DeviceMesh init that allows for instantiating a DeviceMesh without depending on `dist.get_rank()` which requires a global PG to be instantiated.

Test Plan:
```
buck2 test mode/opt -c fbcode.enable_gpu_sections=true  //caffe2/test/distributed:device_mesh -- init_backend
```

Rollback Plan:

Differential Revision: D81981777




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @pragupta @ezyang @msaroufim @dcci",2025-09-09 00:31:09+00:00,2025-09-10T03:01:26Z,,False,12,2,1,12,3,2,14,2025-09-10 01:19:33+00:00,53,461,False,False,False,False,False,False,2,9,2481,15,12,3,1,1,5.0,10.0,2025-09-09T00:39:12Z,pytorch
162438,closed,[pre_compile] Add check for cuda and hardware version,yushangdi," if we detect compiled model is using cuda in meaningful way, we should store information about cuda + hardware
 
 Example: `SystemInfo(python_version='3.12.9', torch_version='2.9.0a0+gite02b0e6', cuda_version='12.6', triton_version=(3, 4), gpu_name='NVIDIA PG509-210')`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-09 00:30:09+00:00,2025-09-12T01:43:13Z,,False,5,4,1,155,15,5,9,2025-09-12 01:42:10+00:00,53,442,False,False,False,False,False,False,5,4,1056,170,155,15,1,1,3.0,5.0,2025-09-09T17:29:48Z,pytorch
162437,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-09 00:26:02+00:00,2025-09-09T04:42:39Z,,False,3,0,1,1,1,1,3,2025-09-09 04:41:36+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-09T00:26:03Z,pytorch
162434,open,[poc] Add config to allow gradient dtype mismatch,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162434
* #160883
* #160888

",2025-09-08 23:45:57+00:00,2025-09-09T15:46:19Z,,False,3,0,2,63,21,6,3,,49,114,False,False,False,False,False,False,6,1,268,86,64,22,1,2,2.0,2.0,2025-09-09T15:34:25Z,pytorch
162433,closed,Export d80005963-2,Nicoshev,"Pull changes

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 23:36:59+00:00,2025-09-08T23:37:14Z,2025-09-08T23:37:14Z,True,2,0,10,5348,428,48,2,2025-09-08 23:37:14+00:00,18,271,False,False,False,False,False,False,48,0,0,6428,5674,754,1,10,,,,pytorch
162432,closed,[standalone_compile] binary format write should be atomic,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162432

We update it to call write_atomic instead of file.write

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-08 23:33:08+00:00,2025-09-09T18:44:20Z,,False,5,0,2,4,2,1,5,2025-09-09 18:43:15+00:00,57,360,False,False,False,False,False,False,1,4,955,8,5,3,1,2,3.0,4.0,2025-09-09T00:13:59Z,pytorch
162431,closed,Export d80005963,Nicoshev,"Pull latest changes

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @mingfeima @XiaobingSuper @ashokei @jingxu10 @jerryzh168 @SherlockNoMad @voznesenskym @penguinwu @Guobing-Chen @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-08 23:29:40+00:00,2025-09-08T23:35:15Z,,False,2,0,163,18000,7631,376,2,2025-09-08 23:35:15+00:00,16,513,False,False,False,False,False,False,376,0,0,4872,1109,3763,21,30,,,,pytorch
162428,closed,Fix missing moves in initJITBindings,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162428

Per @Skylion007 on #162219

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-08 22:45:51+00:00,2025-09-09T09:00:56Z,,False,8,0,2,3,1,1,8,2025-09-09 08:47:36+00:00,36,169,False,True,False,False,False,False,1,6,1672,6,4,2,1,2,2.0,6.0,2025-09-09T06:05:40Z,pytorch
162427,open,"Revert ""simplify nvrtc discovery login in compile_kernel (#156674)""",atalman,"This reverts commit 4e8dd11. Also removing cuda 11 logic.
Since we have cuda 12 and cuda 13 builds now. Lib name should be: ``lib_name = f""nvrtc64_{version}_0.dll""`` or ``f""lib64/libnvrtc.so.{version}""``

",2025-09-08 22:43:28+00:00,2025-09-09T03:27:38Z,,False,2,0,2,52,4,1,2,,67,205,False,False,False,False,False,False,1,1,348,88,68,20,1,2,3.0,3.0,2025-09-08T22:53:24Z,pytorch
162426,closed,[inductor] Add shape for store_output in matmul templates,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162275
* #162513
* __->__ #162426



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 22:13:20+00:00,2025-09-11T01:51:18Z,,False,3,0,2,21,15,8,3,2025-09-11 01:51:17+00:00,57,317,False,False,False,False,False,False,8,2,102,38,22,16,1,2,2.0,3.0,2025-09-10T00:18:20Z,pytorch
162425,closed,CUDA 13.0 Windows Nvidia Driver Update to 580.88,atalman,"Related to https://github.com/pytorch/pytorch/issues/162333
https://github.com/pytorch/pytorch/issues/159779
",2025-09-08 22:06:38+00:00,2025-09-09T16:24:46Z,,False,9,0,1,5,5,1,9,2025-09-09 14:40:37+00:00,48,109,False,False,False,False,False,False,1,7,1698,10,5,5,1,1,5.0,8.0,2025-09-08T22:07:58Z,pytorch
162424,closed,[torch][c10d] fix split_group in mixed backend case,suo,"Today we can initialize a mixed-backend process group (e.g. ""cpu:gloo,cuda:nccl"") but we can only pass one set of process group options.

However, when we call `split_group`, we retrieve that set of options from the parent PG and pass it to the ProcessGroup::groupSplit C++ API, which then attempts to propagate that set of options to all backends.

This leads to an assert on some user code, where ProcessGroupGloo::split is expecting gloo options but receives nccl options instead.

Arguably the APIs as currently designed are just broken; we should not ever expect a single set of backend options to apply across multiple backends. However, fixing this would require changing quite a few public APIs.

As a quick fix, since user-provided options really only exist for NCCL, just warn and fall-back to defaulted options for Gloo if non-gloo options are detected.



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-08 22:02:37+00:00,2025-09-11T16:30:40Z,,False,12,0,5,109,30,6,12,2025-09-11 16:29:36+00:00,51,969,False,True,False,False,False,False,6,11,3319,149,114,35,1,5,6.0,14.0,2025-09-08T22:21:03Z,pytorch
162423,closed,Update docs for quantile to be clearer for nearest,janeyx99,"Correct the rounding scheme for nearest in quantile.
",2025-09-08 21:47:25+00:00,2025-09-09T18:05:17Z,,False,3,4,2,1,1,1,7,2025-09-09 18:04:15+00:00,50,53,False,False,False,True,False,False,1,2,493,6,3,3,1,2,4.0,2.0,2025-09-08T21:55:52Z,pytorch
162421,closed,Add missing fstream include to fix std::ofstream compilation error,0xjeffro,"## Summary
This PR adds a missing `#include <fstream>` to fix a compilation error that occurred with the clang compiler on the standard *Google internal compile setup* (built with bazel).

## Details
The `std::ofstream` type was implicitly instantiated, which can cause compilation to fail with certain compilers. In this case, the clang compiler within the Google internal compile setup failed with an implicit instantiation error of `std::basic_ofstream<char>`. By explicitly including the `<fstream>` header, this PR resolves the error and ensures proper compilation in a wider range of setups and compilers.

## Error message:
```
torch/csrc/distributed/c10d/FlightRecorder.cpp:8:17: error: implicit instantiation of undefined template 'std::basic_ofstream<char>'
8 | std::ofstream file(filename_, std::ios::binary);
| ^
libcxx/include/__fwd/fstream.h:26:7: note: template is declared here
26 | class basic_ofstream;
| ^
1 error generated.
```

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @haifeng-jin ",2025-09-08 21:41:57+00:00,2025-09-09T05:15:37Z,,False,6,0,3,1,0,1,6,2025-09-09 05:14:35+00:00,66,1058,False,True,False,False,False,False,1,5,1304,2,2,0,1,2,2.0,5.0,2025-09-09T02:37:10Z,pytorch
162420,open,[dynamic shapes] guard_or_false numel checks in _refs/__init__.py,pianpwk,"Fixes #ISSUE_NUMBER
",2025-09-08 21:35:15+00:00,2025-09-09T01:07:27Z,,False,2,0,1,18,6,1,2,,65,20,False,True,False,False,False,False,1,0,0,24,18,6,1,1,,,,pytorch
162419,closed,[ROCm] enable grouped gemm fallback,jagadish-amd,"Enables bf16 group gemm alternative path as described in #161366
Fast path will be enabled in future through CK integration.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-08 21:31:37+00:00,2025-09-09T20:06:02Z,,False,4,0,1,8,18,2,4,2025-09-09 20:04:59+00:00,35,242,False,False,False,False,False,False,2,3,866,26,8,18,1,1,3.0,3.0,2025-09-08T21:32:27Z,pytorch
162418,closed,[precompile] Fix issues with guard serialization on distributed types.,zhxchen17,"Summary: Add more support for torch internal distributed data structures.

Test Plan:
test_guard_serialization.py

Rollback Plan:

Differential Revision: D81927732




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-08 21:19:19+00:00,2025-09-11T23:10:59Z,,False,14,5,1,178,23,4,19,2025-09-11 23:09:57+00:00,70,437,False,True,False,False,False,False,4,2,656,201,178,23,1,1,4.0,2.0,2025-09-09T02:18:45Z,pytorch
162417,open,[WIP] repro 161807,avikchaudhuri,"Summary:
FX tracing does not preserve dict types.

Fixes #161807

Test Plan:
added test based on repro

Rollback Plan:

Differential Revision: D81961130


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-08 21:17:34+00:00,2025-09-08T23:01:57Z,,False,2,0,1,57,1,2,2,,18,211,False,True,False,False,False,False,2,0,0,58,57,1,1,1,,,,pytorch
162416,closed,repro 161902,avikchaudhuri,"Summary:
Sometimes `ShapeEnv.create_symbol` can return a `sympy.Integer`. This messes up our phantom symbol infra for derived dims.

Fixes #161902

Test Plan:
added test based on repro

Rollback Plan:

Differential Revision: D81960709


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-08 21:14:49+00:00,2025-09-11T16:36:28Z,,False,11,0,1,32,10,2,11,2025-09-11 16:35:27+00:00,12,293,False,True,False,False,False,False,2,2,493,42,32,10,1,1,3.0,2.0,2025-09-10T20:40:16Z,pytorch
162414,closed,[DeviceMesh] Make CuTe layout as mesh layout to be ready for using in DeviceMesh,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161106
* #161016
* #162690
* __->__ #162414
* #162534
* #162413

We create a wrapper class named ""_MeshLayout"" acting as a layout for device mesh so that we can add new methods more specific to DeviceMesh and keep the core logic of CuTe manipulation inside pycute module. This PR create the main body of the code and then next PR will come with actual implementation and unit test for device mesh layout. (Actual implementation can be found in https://github.com/pytorch/pytorch/pull/161016)

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-08 20:56:24+00:00,2025-09-15T17:05:50Z,,False,19,15,15,71,0,1,34,2025-09-15 17:04:45+00:00,80,665,False,False,False,False,False,False,1,18,4387,2908,2136,772,1,15,7.0,19.0,2025-09-09T02:23:26Z,pytorch
162413,closed,[CuTe] Copy code from pycute for device mesh bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161106
* #161016
* #162690
* #162414
* #162534
* __->__ #162413


We copied the whole module and its unit test into pytorch codebase. (https://github.com/NVIDIA/cutlass/blob/main/python%2Fpycute%2Flayout.py).

We did change the indentation of code from 2 spaces to 4 spaces. And add lint suppressor to make mypy happy.

Also we need to make changes to unit test to include ownership and use `run_tests, TestCase` so that the test gets picked up by CI.

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-08 20:56:22+00:00,2025-09-12T04:29:10Z,,False,14,7,6,1483,0,11,21,2025-09-12 04:28:06+00:00,56,625,False,False,False,False,False,False,11,13,2461,2681,2082,599,1,6,4.0,14.0,2025-09-09T02:21:47Z,pytorch
162412,closed,[CUDA-13] Implement workaround for cudaErrorNotSupported,malfet,See https://github.com/pytorch/pytorch/issues/162333#issuecomment-3267929585,2025-09-08 20:47:13+00:00,2025-09-09T04:13:16Z,,False,4,1,2,12,0,1,5,2025-09-09 04:12:14+00:00,56,76,False,False,False,False,False,False,1,3,1185,26,19,7,1,2,4.0,3.0,2025-09-08T20:50:37Z,pytorch
162411,open,Give linear an explicit autograd formula always,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162411

Signed-off-by: Edward Z. Yang <ezyang@meta.com>",2025-09-08 20:45:25+00:00,2025-09-12T15:10:56Z,,False,5,8,1,53,15,6,13,,47,141,False,False,False,False,False,False,6,2,233,68,53,15,1,1,3.0,3.0,2025-09-08T20:48:55Z,pytorch
162409,closed,[ROCm] Remove scheduling MI2xx workflows,amdfaa,"Removed commented-out schedule entries from the workflow.

Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-08 20:24:30+00:00,2025-09-09T01:01:30Z,,False,2,0,3,4,18,2,2,2025-09-09 01:01:29+00:00,40,197,False,True,False,False,False,False,2,1,110,22,4,18,1,3,1.0,1.0,2025-09-09T01:01:29Z,pytorch
162407,closed,[fx] fix qualified name for methods of torch.Tensor,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162407

This fixes an error in the previous PR.

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 19:14:37+00:00,2025-09-09T05:15:50Z,,False,3,0,1,2,2,2,3,2025-09-09 05:14:46+00:00,51,359,False,True,False,False,False,False,2,2,505,4,2,2,1,1,3.0,3.0,2025-09-09T02:33:30Z,pytorch
162406,open,[inductor] Small refactor of CachingAutotuner,kundaMwiza,"This is a simple refactor that just moves some logic in `_precompile_config` to two new functions for separation of concerns. This will allow subclasses e.g. out of tree to configure options and metadata for triton.compile.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 18:48:11+00:00,2025-09-24T19:31:51Z,,False,3,0,2,60,35,1,3,,45,426,False,False,False,False,False,True,1,2,137,103,64,39,1,2,1.0,2.0,2025-09-08T18:51:04Z,pytorch
162405,open,fix input_put_ for torch.compile,parsshar-RH,"Fixes #162146 

### Summary

Torch.compile was not synchronizing the mutations properly.

- Without realize(), the inductor creates inconsistent execution plans where mutations to tensor slices don't propagate correctly
- This leads to differences between eager mode and compiled mode results

Impacts:
module: inductor




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 18:22:15+00:00,2025-09-12T05:45:51Z,,False,4,0,2,21,0,2,4,,32,525,False,True,False,False,False,False,2,2,90,29,25,4,1,2,1.0,2.0,2025-09-08T18:25:03Z,pytorch
162404,open,Validate file and handle exceptions for weights_only unpickler,apach301,"Fixes #127525

Fixing a lot of unhandled exceptions raised from weights_only model loading:

 - Validate file size to be able to contain at least magic number and protocol version
 - Also catch all exceptions from unpickler due to inconsistent operators interpretation.

A previous PR #127526 was closed, but the changes didn't get into main.
",2025-09-08 18:14:33+00:00,2025-09-11T21:06:34Z,,False,1,0,1,29,1,3,1,,62,343,False,True,False,False,False,False,3,0,0,30,29,1,1,1,,,,pytorch
162401,closed,Add fp16-overflow regression test,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162401

Discovered while debugging https://github.com/pytorch/pytorch/issues/160841 where sdpa returned NaNs, because during the computation intermediate values were cast back to fp16 before normalization, which was fixed by https://github.com/pytorch/pytorch/pull/161999 )",2025-09-08 17:44:03+00:00,2025-09-08T20:34:29Z,,False,3,0,1,5,0,1,3,2025-09-08 20:33:26+00:00,33,359,False,True,False,False,False,False,1,2,503,5,5,0,1,1,4.0,3.0,2025-09-08T17:49:08Z,pytorch
162400,closed,Fix markdown link syntax in graph breaks index,williamwen42,,2025-09-08 17:36:36+00:00,2025-09-12T19:30:54Z,,False,3,0,1,1,1,1,3,2025-09-12 19:29:52+00:00,46,0,False,True,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-12T16:54:27Z,pytorch
162399,open,[torchgen] Fix parsing YAML file,apach301,"Fixes #162395, #162397, #162398
",2025-09-08 17:15:49+00:00,2025-09-11T21:06:28Z,,False,1,2,1,6,3,1,3,,32,32,False,True,False,False,False,False,1,0,97,9,6,3,1,1,3.0,1.0,2025-09-08T17:49:52Z,pytorch
162396,closed,Replace export_for_training with export,tugsbayasgalan,"Summary: replace export_for_training with epxort

Test Plan:
CI

Rollback Plan:

Differential Revision: D81935792


We need to get this in to 2.9 as we are publicly deprecating an API so it would be nice if there is no reference to it in pytorch repo by the time deprecation shows up. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 17:04:38+00:00,2025-09-10T15:32:04Z,,False,17,0,1,180,215,24,17,2025-09-10 14:19:38+00:00,39,602,False,False,False,False,False,False,24,5,1744,395,180,215,1,1,5.0,5.0,2025-09-08T17:13:26Z,pytorch
162394,closed,[SymmMem] Use global pe for put and get,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162394
* #162320

NVSHMEM put/get APIs take global PE instead of local counterpart. So we'd need to do a translation within the kernel.

Also added a sub-group test for dispatch and combine mimic'ing the Expert Parallel cases.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-08 16:58:55+00:00,2025-09-09T03:59:55Z,,False,3,2,3,80,30,2,5,2025-09-09 03:58:51+00:00,39,409,False,False,False,False,False,False,2,2,493,114,82,32,1,3,5.0,2.0,2025-09-08T17:23:50Z,pytorch
162393,open,In-tree registration for MTIA kl_div kernel,trirpi,"Summary: Updated kl_div to be registered correctly.

Differential Revision: D81819070




cc @egienvalue",2025-09-08 16:52:12+00:00,2025-09-17T17:05:30Z,,False,9,6,1,3,0,1,15,,43,104,False,False,False,False,False,False,1,2,97,3,3,0,1,1,4.0,2.0,2025-09-08T16:55:49Z,pytorch
162392,closed,[nn] Assert parsed iterable arguments are an appropriate length,benjaminglass1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162392

Fixes #162327",2025-09-08 16:46:12+00:00,2025-09-08T16:48:12Z,,False,2,0,1,5,3,2,2,2025-09-08 16:46:56+00:00,63,107,False,True,False,False,False,False,2,0,0,8,5,3,1,1,,,,pytorch
162391,closed,Comment this one out for now,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162391
* #162136

",2025-09-08 16:40:47+00:00,2025-09-08T18:15:27Z,,False,2,0,1,1,1,1,2,2025-09-08 18:15:27+00:00,28,104,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162389,closed,[precompile] Fix inlined source tracking with generators.,zhxchen17,"Summary:
When compiled code has generator, code.co_firstlineno will be inconsistent with the result from inspect.getsource, which returns the toplevel enclosing code source rather than the inner code location.

In this case, it seems simpler to just use the toplevel enclosing code location rather than the co_firstlineno field.

Test Plan:
test_package.py -k test_code_with_generator

Rollback Plan:

Differential Revision: D81929751




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-08 15:47:47+00:00,2025-09-09T00:15:00Z,,False,4,1,1,26,6,2,5,2025-09-09 00:13:57+00:00,57,609,False,True,False,False,False,False,2,1,476,32,26,6,1,1,3.0,1.0,2025-09-08T16:36:29Z,pytorch
162388,closed,[associative_scan] partial gradient support,bohnstingl,"This PR tests the partial gradient support of the `associative_scan` operation. It replaces https://github.com/bohnstingl/pytorch/pull/6

cc @ydwu4 
",2025-09-08 15:32:18+00:00,2025-09-09T23:53:35Z,,False,4,0,1,140,21,2,4,2025-09-09 23:52:32+00:00,43,149,False,False,False,False,False,False,2,3,535,161,140,21,1,1,3.0,3.0,2025-09-08T15:44:06Z,pytorch
162387,closed,[CUDA][float8][TF32] Disable tf32 for vs. emulated rowwise comparison,eqy,cc @csarofeen @ptrblck @xwang233 @zasdfgbnm,2025-09-08 15:22:29+00:00,2025-09-09T17:05:15Z,,False,3,0,1,2,0,1,3,2025-09-09 17:04:11+00:00,69,43,False,False,False,False,False,False,1,2,498,2,2,0,1,1,3.0,2.0,2025-09-08T17:22:13Z,pytorch
162385,closed,[ROCm][CI] update fbgemm nightly benchmark hash,jataylo,"fbgemm_gpu was failing to clone due to missing submodule commit.
```
+ pushd fbgemm/fbgemm_gpu
~/pytorch/fbgemm/fbgemm_gpu ~/pytorch
+ git checkout 7f1de94a4c2d14f59ad4ca84538c36084ea6b2c8 --recurse-submodules
fatal: failed to unpack tree object b1281b8b08d973a7064f864f47eeb30f3e2596e9
error: Submodule 'external/composable_kernel' could not be updated.
error: Cannot update submodule:
	external/composable_kernel
```
Log File
[inductor-periodic · pytorch/pytorch@5babb4d](https://github.com/pytorch/pytorch/actions/runs/17536630806/job/49802458834) 

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd",2025-09-08 14:42:06+00:00,2025-09-09T15:45:47Z,,False,3,0,3,1,1,1,3,2025-09-09 15:44:43+00:00,47,660,False,False,False,False,False,False,1,2,846,12,6,6,2,3,2.0,2.0,2025-09-08T16:05:18Z,pytorch
162384,closed,check what woild fail,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162384
* #162099

",2025-09-08 14:02:28+00:00,2025-09-08T18:28:28Z,,False,2,0,1,5,5,1,2,2025-09-08 18:28:28+00:00,21,104,False,False,False,False,False,False,1,0,0,10,5,5,1,1,,,,pytorch
162383,closed,CD Windows CUDA 13.0 build - fix packaging of cuda dlls,atalman,"Trying to fix https://github.com/pytorch/pytorch/issues/162333

CUDA 13.0 file structure changed. Instead of keeping most of dlls in bin folder its now in ``bin\x64`` except for cudnn dll. See attached picture :
<img width=""511"" height=""361"" alt=""Screenshot 2025-09-08 at 9 46 26 AM"" src=""https://github.com/user-attachments/assets/d2e630ee-930f-4da6-9b81-f9ef48fde7ce"" />
<img width=""490"" height=""333"" alt=""Screenshot 2025-09-08 at 9 46 34 AM"" src=""https://github.com/user-attachments/assets/194cbf43-b6ef-4218-b516-db37b91302be"" />
",2025-09-08 13:50:19+00:00,2025-09-08T17:58:30Z,,False,3,1,2,15,12,1,4,2025-09-08 17:57:27+00:00,55,534,False,True,False,False,False,False,1,2,888,27,15,12,1,2,5.0,4.0,2025-09-08T15:33:02Z,pytorch
162380,open,[Test] Add more instructions for installing CI requirements on local machine,can-gaa-hou,"# Motivation

When I run `python test/run_tests.py` on my Mac, I get a message ""Missing pip dependency: matplotlib, please run `pip install -r .ci/docker/requirements-ci.txt`"". However, `cuda-bindings` is a mandatory package in this file. 
https://github.com/pytorch/pytorch/blob/32911ff541e8a60693412f575b4363c546d251df/.ci/docker/requirements-ci.txt#L385

I guess most CI machines have CUDA installed, but for our users, `cuda-bindings` is unnecessary unless we are operating on a CUDA machine. Additionally, there is no compatible version of `cuda-bindings` available for Mac users. Therefore, this pull request aims to provide clearer instructions for users who do not intend to test CUDA on their local machines.

# Changes

The original suggestion only instructs users to run `pip install -r .ci/docker/requirements-ci.txt`. In contrast, this pull request adds another command: `pip install -r <(grep -v 'cuda-bindings' .ci/docker/requirements-ci.txt)`. This command removes `cuda-bindings` from the installation requirements.

# Advantages

- Removing `cuda-bindings` from the requirements for Mac users will help them avoid errors related to the installation of an incompatible `cuda-bindings` package.
- For Linux and Windows users who also do not plan to test on CUDA, this way of installation will save time, bandwidth, and storage space by eliminating the need to download `cuda-bindings`.


cc @mruberry",2025-09-08 12:23:11+00:00,2025-09-09T17:35:26Z,,False,2,0,1,2,0,1,2,,76,1416,False,False,False,True,False,False,1,1,33,2,2,0,1,1,1.0,1.0,2025-09-09T01:44:55Z,pytorch
162377,closed,[inductor] Runtime estimations: use nccl estimator; mm only benchmark mode,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162377



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 11:31:21+00:00,2025-09-24T14:45:32Z,,False,3,0,1,324,32,8,3,2025-09-24 14:45:32+00:00,74,390,False,False,False,False,False,False,8,1,56,356,324,32,1,1,1.0,1.0,2025-09-09T02:38:00Z,pytorch
162372,closed,[xla hash update] update the pinned xla hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned xla hash.",2025-09-08 07:41:55+00:00,2025-09-08T11:32:21Z,,False,3,0,1,1,1,1,3,2025-09-08 11:31:19+00:00,44,155,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-08T07:41:56Z,pytorch
162371,closed,Repackage vLLM nightlies,huydhn,"I suspected that I would need to repack vLLM wheels from https://github.com/pytorch/pytorch/pull/162000 because I renamed the wheel, and it turns out to be true.  The error is as follows:

```
$ uv pip install --pre xformers --index-url https://download.pytorch.org/whl/nightly/cu129
Using Python 3.12.11+meta environment at: venv/py3.12
Resolved 28 packages in 759ms
error: Failed to install: xformers-0.0.33.dev20250901+cu129-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (xformers==0.0.33.dev20250901+cu129)
  Caused by: Wheel version does not match filename: 0.0.33+5d4b92a5.d20250907 != 0.0.33.dev20250901+cu129
```
",2025-09-08 07:29:50+00:00,2025-09-10T04:03:42Z,,False,3,0,8,107,45,2,3,2025-09-10 04:02:38+00:00,24,633,False,False,False,False,False,False,2,2,789,192,127,65,1,8,3.0,2.0,2025-09-09T17:29:00Z,pytorch
162370,open,[WIP] port test/test_nn.py to Intel GPU,daisyden,"For https://github.com/pytorch/pytorch/issues/114850, we will port aten unit tests to Intel GPU. This PR will work on test/test_nn.py. We could enable Intel GPU with following methods and try the best to keep the original code styles:

1. Replaced onlyCUDA with onlyOn(['cuda', 'xpu']) for supported tests
2. Added allow_xpu=True for supported test class in test parameterization.
3. Use torch.accelerator to extend cude specific test to XPU.
4. Added skipIfXPU decorator for cases with known issues on Intel GPU
5. Enabled 'xpu' for some test pathes
6. Make get_all_device_types general for accelerators. 

",2025-09-08 07:16:01+00:00,2025-09-25T03:14:41Z,,False,1,9,3,483,412,3,10,,39,608,False,False,False,False,False,False,3,0,194,989,530,459,1,3,3.0,2.0,2025-09-10T10:04:19Z,pytorch
162369,closed,[WIP] Port test_nn.py to Intel GPU,daisyden,"For https://github.com/pytorch/pytorch/issues/114850, we will port aten unit tests to Intel GPU. This PR will work on test/test_nn.py. We could enable Intel GPU with following methods and try the best to keep the original code styles:

1. Replaced onlyCUDA with onlyOn(['cuda', 'xpu']) for supported tests
2. Added allow_xpu=True for supported test class in test parameterization.
3. Use torch.accelerator to extend cude specific test to XPU.
4. Added skipIfXPU decorator for cases with known issues on Intel GPU
5. Enabled 'xpu' for some test pathes
6. Make get_all_device_types general for accelerators. 


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-08 07:02:53+00:00,2025-09-08T07:20:57Z,,False,1,0,97,8224,6649,219,1,2025-09-08 07:16:27+00:00,34,1045,False,False,False,False,False,False,219,0,0,8172,2739,5433,18,30,,,,pytorch
162368,closed,Improved error lr last epoch,krastogi-in,"Fixes #160626
",2025-09-08 06:38:07+00:00,2025-09-15T23:34:20Z,,False,9,0,1,14,2,2,9,2025-09-15 23:33:17+00:00,28,14,False,True,False,False,True,False,2,5,591,16,14,2,1,1,3.0,5.0,2025-09-08T08:27:32Z,pytorch
162364,closed,"[CD] [aarch64] Add CUDA 12.6 and 12.8 to build matrix, remove 12.9 build",tinglvv,"https://github.com/pytorch/pytorch/issues/159779

Add the full CUDA support matrix to sbsa build (12.6, 12.8)
Same arch support as x86 build
Remove 12.9 sbsa build

cc @ptrblck @nWEIdia @atalman @malfet ",2025-09-08 05:41:28+00:00,2025-09-08T22:25:26Z,,False,4,0,4,653,9,4,4,2025-09-08 20:00:27+00:00,72,203,False,False,False,False,False,False,4,3,1795,672,658,14,1,4,3.0,4.0,2025-09-08T17:19:18Z,pytorch
162363,open,[Conv1d] Check overflow before we compute padding size.,thenumberouscode,"Fixes https://github.com/pytorch/pytorch/issues/161877
also fixes https://github.com/pytorch/pytorch/issues/161875


cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-08 05:40:26+00:00,2025-09-25T02:16:51Z,,False,20,0,1,51,0,2,20,,55,258,False,True,False,False,False,False,2,19,6145,51,51,0,1,1,2.0,19.0,2025-09-08T05:45:38Z,pytorch
162361,closed,Add api info for torch._C._nn.pyi,orangeH25,"Fix part of #148404 

APis involved are as followed:

- im2col
- l1_loss
- mish
- mish_
- mse_loss",2025-09-08 02:45:30+00:00,2025-09-12T05:57:27Z,,False,8,0,1,53,0,1,8,2025-09-12 05:56:25+00:00,33,98,False,True,False,False,False,False,1,5,582,53,53,0,1,1,3.0,5.0,2025-09-09T08:02:15Z,pytorch
162360,closed,[optim] avoid aliasing lr and initial_lr in `SequentialLR.__init__`,filipviz,"Prevents a SequentialLR edge case which corrupted base_lrs in chained schedulers when using an optimizer with a Tensor learning rate.

Fixes #162359",2025-09-08 02:22:41+00:00,2025-09-16T20:15:55Z,,False,5,6,2,25,1,2,11,2025-09-16 20:15:55+00:00,67,148,False,True,False,False,False,False,2,4,1398,34,29,5,1,2,2.0,5.0,2025-09-08T02:52:10Z,pytorch
162356,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-08 00:26:57+00:00,2025-09-09T04:41:30Z,,False,6,0,1,1,1,1,6,2025-09-09 04:40:28+00:00,46,156,False,False,False,False,False,False,1,5,1537,2,1,1,1,1,2.0,5.0,2025-09-08T00:26:58Z,pytorch
162355,closed,[Inductor] do loop reordering in a separate final round,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* __->__ #162355
* #162126
* #162101


Previous LOAF after fusion algorithm is not guaranteed to create more fusion opportunities even if loop reordering happens. I can not find an example that LOAF reduce the amount of fusion, but here is an example that reordering loops does not add more fusions:

https://github.com/pytorch/pytorch/blob/a1f7639922ee0470bd7109bab6fe62989cf5000d/test/inductor/test_loop_ordering.py#L612-L641

Move LOAF to a separate final round of fusion so that we are guaranteed to not reducing the amount of fusions. Hopefully this also helps compilation time since LOAF kicks in when there are less nodes.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-08 00:26:15+00:00,2025-09-19T20:22:45Z,,False,4,0,5,55,11,2,4,2025-09-19 20:21:38+00:00,55,929,False,False,False,False,False,False,2,3,2758,80293,46778,33515,1,5,4.0,5.0,2025-09-08T21:28:23Z,pytorch
162354,closed,are_strides_like_channels_last_or_false,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162354


Note this could change suggest_memory_format behaviour for unbacked

we used to return True for are_strides_like_channels_last sometimes even when results undecided
now when its not decided we return False.
",2025-09-07 23:09:40+00:00,2025-09-16T00:50:12Z,,False,16,1,6,18,8,2,17,2025-09-16 00:49:08+00:00,39,302,False,False,False,False,False,False,2,15,6910,57172,40013,17159,1,6,4.0,16.0,2025-09-08T22:59:50Z,pytorch
162353,closed,[nativert] aoti,dolpm,"Summary: att

Test Plan:
ci

Rollback Plan:

Differential Revision: D81731425


",2025-09-07 22:26:48+00:00,2025-09-12T18:17:13Z,,False,65,0,1,581,5,18,65,2025-09-12 05:56:28+00:00,15,80,False,False,False,False,False,False,18,0,0,586,581,5,1,1,1.0,0.0,2025-09-08T20:10:16Z,pytorch
162351,open,[BE]: Update NCCL to 2.28.3 and fix build runtime CUDA13 mismatch,Skylion007,@eqy New NCCL has some a bunch of bugfixes for features including reducing the number SMs needed by NVLINK collectives as well as some very useful new APIs for SymmetricMemory.  Also allows FP8 support for non-reductive operations on pre-sm90 devices.,2025-09-07 19:36:20+00:00,2025-09-23T22:45:10Z,,False,4,0,1,48,48,6,4,,65,251,False,True,True,False,False,False,6,3,324,96,48,48,1,1,5.0,3.0,2025-09-09T02:55:45Z,pytorch
162349,closed,[MPS] mps sparse mul op implementation,Isalia20,"Implements mps sparse mul operation as well as enables other operations such as:
1. copy_
2. div
3. sum
4. floor
5. power
6. sub
7. floor_divide

cc @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer @jcaip @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen",2025-09-07 19:27:30+00:00,2025-09-11T18:37:33Z,,False,11,17,7,436,31,4,28,2025-09-11 18:36:29+00:00,38,271,False,False,False,False,False,False,4,8,1975,26553,16928,9625,2,7,5.0,12.0,2025-09-08T07:56:25Z,pytorch
162348,open,[BE]: Update CUTLASS submodule to 4.2rc,Skylion007,Let's see what breaks.,2025-09-07 19:08:32+00:00,2025-09-16T19:44:29Z,,False,2,0,1,1,1,1,2,,39,22,False,False,False,False,False,False,1,1,68,2,1,1,1,1,1.0,1.0,2025-09-16T19:44:29Z,pytorch
162347,closed,[BE]: Update cudnn frontend submodule to 1.14.1,Skylion007,Fixes a few bugs introduced to CUDNN 1.11 which affects all our CUDA13 builds. Also adds support for new CUDNN features whenever we choose to update. @eqy pretty sure this addresses the concern you had over the previous upgrade since that bugfix is now merged. This is a simple header only update.,2025-09-07 19:05:33+00:00,2025-09-08T20:04:28Z,,False,6,0,1,3,3,2,6,2025-09-08 20:03:26+00:00,47,297,False,True,True,False,False,False,2,5,712,6,3,3,1,1,4.0,5.0,2025-09-07T19:15:08Z,pytorch
162345,open,distributed autotuning,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162345



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-07 05:01:01+00:00,2025-09-10T18:38:48Z,,False,2,0,3,278,33,9,2,,22,403,False,False,False,False,False,False,9,0,0,375,310,65,1,3,,,,pytorch
162344,open,Fix/fsdp gloo grad avg,Vinayak-Pawar,"Fixes #162338 

This pull request addresses a bug in the composable FSDP implementation where gradients were being summed instead of averaged when using the gloo backend for CPU-based training. This led to incorrect gradient values that were scaled by the world size.
The root cause was an incorrect assumption that the gloo backend supported ReduceOp.AVG for reduce_scatter operations. The fix explicitly checks for the gloo backend and forces the use of ReduceOp.SUM followed by a manual division. This ensures that the gradient reduction is mathematically correct and consistent with the nccl backend's behavior.

Test Plan
A new unit test, test_fsdp_gradient_reduction_cpu, has been added to test_fsdp_core.py. This test verifies that the gradient averaging is performed correctly on the CPU by comparing the FSDP-computed gradients against a non-distributed reference model, ensuring the issue is resolved and preventing future regressions.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-07 04:46:56+00:00,2025-09-18T14:17:44Z,,False,12,0,8,0,0,0,12,,22,1048,False,True,False,False,False,False,0,11,1790,143775,96253,47522,2,7,5.0,12.0,2025-09-07T19:41:03Z,pytorch
162342,open,fix-functorch-experimental-import-error,ghostspiders,"Fixes #162283
Maintain deprecation warnings and backward compatibility


cc @zou3519 @Chillee @samdow @kshitij12345",2025-09-06 23:46:24+00:00,2025-09-17T14:26:19Z,,False,5,11,2,5,1,1,16,,39,115,False,True,False,False,False,False,1,4,226,8,6,2,1,2,2.0,4.0,2025-09-06T23:48:16Z,pytorch
162341,closed,[inductor] fix 3d tiled online softmax,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162355
* #162126
* #162101
* __->__ #162341
* #162311

The online_softmax_reduce runtime helper previously assumes the input tl.Tensor's are 2d tensors. But with tiled reduction, they can be 3d (y, x, r). 



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-06 23:08:58+00:00,2025-09-09T03:00:58Z,,False,3,0,3,14,1,2,3,2025-09-09 02:59:55+00:00,38,509,False,True,False,False,False,False,2,2,493,69476,38657,30819,1,3,4.0,2.0,2025-09-07T23:36:36Z,pytorch
162340,open,[nn] Assert parsed iterable arguments are an appropriate length,benjaminglass1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162340

Fixes #162327",2025-09-06 21:37:17+00:00,2025-09-16T02:08:57Z,,False,11,6,7,31,24,6,17,,63,107,False,True,False,False,False,False,6,10,4355,5750,4362,1388,1,7,4.0,10.0,2025-09-06T21:39:03Z,pytorch
162339,open,Add LazyLayerNorm,vsey,"Fixes #158832
",2025-09-06 21:05:19+00:00,2025-09-11T21:30:09Z,,False,2,0,55,398,2,3,2,,17,14,False,True,False,False,False,False,3,1,37,14534,8855,5679,1,30,1.0,1.0,2025-09-06T22:55:35Z,pytorch
162337,closed,Remove __torch_dispatch__ check in THPVariable_make_dtensor,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162337
* #161596
* #162218
* #162220
* #162219
* #161692
* #161634
* #161633
* #161595
* #161591

We control DTensor, so we can just guarantee there isn't a programming error with __torch_dispatch__. (The guard is already less-than-perfect; see the note that the deleted comment refers to.)",2025-09-06 17:37:00+00:00,2025-09-11T06:59:43Z,,False,3,0,4,6,2,1,3,2025-09-11 06:58:40+00:00,59,376,False,False,False,False,False,False,1,2,493,30244,18890,11354,1,4,3.0,2.0,2025-09-06T18:05:48Z,pytorch
162336,closed,Fix TODO in make_tensor_for_subclass_helper,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163031
* #163030
* #162990
* #162968
* #162508
* #161695
* __->__ #162336
* #162298

The constructor does accept a DataPtr (had to fix the DataPtr variant not accepting a SymInt, though).",2025-09-06 17:36:56+00:00,2025-09-17T07:01:05Z,,False,8,0,5,17,2,2,8,2025-09-17 06:46:37+00:00,43,266,False,True,False,False,False,False,2,7,1723,47134,31912,15222,1,5,3.0,7.0,2025-09-13T15:10:58Z,pytorch
162334,closed,Add std::any_of to ConvParams struct,benjaminglass1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162334

Removes some for-loops that didn't short-circuit in favor of std::any_of.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-06 16:12:42+00:00,2025-09-08T20:13:27Z,,False,6,6,1,22,38,1,12,2025-09-08 20:12:23+00:00,36,250,False,False,False,False,False,False,1,5,1401,60,22,38,1,1,3.0,5.0,2025-09-06T16:17:23Z,pytorch
162332,closed,Improve typing of ONNX decorators with ParamSpec,Vinayak-Pawar,"## Summary
This PR improves typing in ONNX-related modules by replacing TypeVar bound to Callable[..., Any] with ParamSpec to preserve parameter types and avoid type erasure in decorator functions.

## Changes
- `torch/onnx/_internal/exporter/_flags.py`: Replace TCallable TypeVar with ParamSpec
- `torch/onnx/ops/_impl.py`: Replace _T TypeVar with ParamSpec for _onnx_op decorator  
- `torch/onnx/_internal/exporter/_torchlib/_torchlib_registry.py`: Replace _T TypeVar with ParamSpec

## Motivation
The previous implementation used TypeVar bound to Callable which erased parameter type information to Any. ParamSpec preserves the exact parameter types and return types, providing better type safety and IDE support.

## Testing
- Verified all changes compile and import correctly
- Created comprehensive test suite to validate ParamSpec functionality
- No linting errors introduced
- Maintains backward compatibility

Fixes #142306",2025-09-06 10:32:50+00:00,2025-09-07T18:21:08Z,,False,8,0,2,24,14,3,8,2025-09-07 18:06:07+00:00,48,932,False,True,False,False,True,False,3,6,1075,40,25,15,1,2,4.0,8.0,2025-09-06T13:37:00Z,pytorch
162330,closed,[ROCm/Windows] Support aotriton for scaled_dot_product_attention on Windows.,jammm,"Enables flash attention and/or memory efficient attention on Windows with scaled_dot_product_attention via. aotriton.
Already tested to be working on Windows with TheRock.

Steps to enable: simply set `USE_FLASH_ATTENTION=1` and `USE_MEM_EFF_ATTENTION=1` as usual. See https://github.com/ROCm/TheRock/blob/main/external-builds/pytorch/build_prod_wheels.py#L578-L604

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-06 09:34:11+00:00,2025-09-15T16:14:10Z,,False,45,21,4,179,44,5,66,2025-09-15 16:13:07+00:00,76,545,False,False,False,False,False,False,5,28,5542,225,180,45,1,4,7.0,29.0,2025-09-06T09:34:20Z,pytorch
162329,open,Eliminate setup.py install/develop in the codebose,XuehaiPan,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* #156049
* #158104
* __->__ #162329



cc @malfet @seemethere",2025-09-06 08:07:35+00:00,2025-09-19T23:01:22Z,,False,2,0,2,2,3,2,2,,50,150,False,False,False,False,False,False,2,1,25,42854,33533,9321,1,2,1.0,1.0,2025-09-11T03:41:50Z,pytorch
162328,closed,`_compile_kernel()`: handle smem > 48kb,gau-nernst,"When shared memory is more than 48kb, we need to manually set `cudaFuncAttributeMaxDynamicSharedMemorySize` to the required shared memory size. This is documented in CUDA C++ API guide https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x. And we can also see it in CUTLASS https://github.com/NVIDIA/cutlass/blob/76c96b0be35cb263debe3e3d8418b80911a544ab/include/cutlass/transform/device/transform_universal_adapter.hpp#L143-L148

When the requested shared memory is more than what is supported, `CUDA Invalid arguments` will be thrown. I decided not to check against `torch.cuda.get_device_properties().shared_memory_per_block_optin` at kernel invocation time to avoid overhead.

@msaroufim ",2025-09-06 05:41:32+00:00,2025-09-11T00:09:18Z,,False,9,0,4,65,0,2,9,2025-09-11 00:09:18+00:00,39,720,False,False,False,True,False,False,2,8,2652,73,69,4,1,4,3.0,9.0,2025-09-06T05:42:52Z,pytorch
162326,open,RFC varlen_api op,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162326


```Py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.attention import varlen_attn
from rich import print

# Set seed for reproducibility
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)

class AttentionBlock(nn.Module):
    def __init__(self, embed_dim: int, num_heads: int, device: torch.device, dtype: torch.dtype):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False, device=device, dtype=dtype)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, device=device, dtype=dtype)
    
    def forward_varlen(self, x_packed: torch.Tensor, cu_seq: torch.Tensor, max_len: int, is_causal: bool = False):
        qkv = self.qkv_proj(x_packed)
        q, k, v = qkv.chunk(3, dim=-1)
        
        q = q.view(-1, self.num_heads, self.head_dim)
        k = k.view(-1, self.num_heads, self.head_dim)
        v = v.view(-1, self.num_heads, self.head_dim)
        
        attn_out = varlen_attn(q, k, v, cu_seq, cu_seq, max_len, max_len, is_causal=is_causal)
        attn_out = attn_out.view(-1, self.embed_dim)
        
        return self.out_proj(attn_out)
    
    def forward_sdpa(self, x_padded: torch.Tensor, is_causal: bool = False):
        batch_size, seq_len, _ = x_padded.shape
        
        qkv = self.qkv_proj(x_padded)
        q, k, v = qkv.chunk(3, dim=-1)
        
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        attn_out = F.scaled_dot_product_attention(q, k, v, is_causal=is_causal)
        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)
        
        return self.out_proj(attn_out)


def create_variable_length_batch(batch_size: int, max_seq_len: int, embed_dim: int, device: torch.device, dtype: torch.dtype):
    seq_lengths = []
    for _ in range(batch_size):
        length = torch.randint(1, max_seq_len // 64 + 1, (1,)).item() * 64
        seq_lengths.append(min(length, max_seq_len))
    
    seq_lengths = torch.tensor(seq_lengths, device=device)
    total_tokens = seq_lengths.sum().item()
    
    x_packed = torch.randn(total_tokens, embed_dim, device=device, dtype=dtype)
    
    cu_seq = torch.zeros(batch_size + 1, device=device, dtype=torch.int32)
    cu_seq[1:] = seq_lengths.cumsum(0)
    
    max_len = seq_lengths.max().item()
    x_padded = torch.zeros(batch_size, max_len, embed_dim, device=device, dtype=dtype)
    
    start_idx = 0
    for i, seq_len in enumerate(seq_lengths):
        end_idx = start_idx + seq_len
        x_padded[i, :seq_len] = x_packed[start_idx:end_idx]
        start_idx = end_idx
    
    return {
        'seq_lengths': seq_lengths,
        'cu_seq': cu_seq,
        'x_packed': x_packed,
        'x_padded': x_padded,
        'max_len': max_len,
        'total_tokens': total_tokens
    }


def test_varlen_vs_sdpa():    
    if not torch.cuda.is_available():
        print(""CUDA not available, skipping test"")
        return True
    
    batch_size = 16
    max_seq_len = 4096
    embed_dim = 2048
    num_heads = 32
    
    device = torch.device('cuda')
    dtype = torch.bfloat16
    
    print(f""Device: {device}, dtype: {dtype}, max_seq_len: {max_seq_len}"")
    
    attention_block = AttentionBlock(embed_dim, num_heads, device, dtype)
    
    for is_causal in [False, True]:
        print(f""Testing is_causal={is_causal}"")
        
        data = create_variable_length_batch(batch_size, max_seq_len, embed_dim, device, dtype)
        print(f""Sequence lengths: {data['seq_lengths'].tolist()}"")
        print(f""Total tokens: {data['total_tokens']}"")
        
        varlen_output = attention_block.forward_varlen(data['x_packed'], data['cu_seq'], data['max_len'], is_causal)
        sdpa_output = attention_block.forward_sdpa(data['x_padded'], is_causal)
        
        start_idx = 0
        for i, seq_len in enumerate(data['seq_lengths']):
            end_idx = start_idx + seq_len
            
            varlen_seq = varlen_output[start_idx:end_idx]
            sdpa_seq = sdpa_output[i, :seq_len]
            
            torch.testing.assert_close(varlen_seq, sdpa_seq, atol=9e-2, rtol=5e-2)
            print(f""Sequence {i} (len={seq_len}): match"")
            
            start_idx = end_idx
        
        print(f""All sequences match for is_causal={is_causal}"")
    
    return True


def test_basic_functionality():
    print(""Testing basic functionality..."")
    
    if not torch.cuda.is_available():
        print(""CUDA not available, skipping test"")
        return True
        
    device = torch.device('cuda')
    dtype = torch.bfloat16
    
    embed_dim = 2048
    num_heads = 16
    batch_size = 2
    seq_len = 1024
    
    attention_block = AttentionBlock(embed_dim, num_heads, device, dtype)
    
    total_tokens = batch_size * seq_len
    x_packed = torch.randn(total_tokens, embed_dim, device=device, dtype=dtype)
    cu_seq = torch.tensor([0, seq_len, total_tokens], device=device, dtype=torch.int32)
    
    output = attention_block.forward_varlen(x_packed, cu_seq, seq_len, is_causal=False)
    
    print(f""Basic test passed - Output shape: {output.shape}"")
    return True


if __name__ == ""__main__"":
        test_basic_functionality()
        test_varlen_vs_sdpa()
        print(""ALL GOOD BROTHA"")
```
",2025-09-06 04:22:22+00:00,2025-09-20T19:45:56Z,,False,3,0,5,306,1,3,3,,17,5748,False,False,False,False,False,False,3,1,2026,43945,33614,10331,1,5,1.0,1.0,2025-09-08T14:37:30Z,pytorch
162325,open,[BE] simplify .github/scripts/generate_binary_build_matrix.py,XuehaiPan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162325
* #162324

",2025-09-06 03:54:20+00:00,2025-09-08T21:46:52Z,,False,1,0,3,89,72,1,1,,61,104,False,False,False,False,False,False,1,0,0,1744,1540,204,1,3,,,,pytorch
162324,open,[BE][Easy] add CUDA 13.0 to nightly tool setup source,XuehaiPan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162325
* __->__ #162324

",2025-09-06 03:54:12+00:00,2025-09-06T05:47:42Z,,False,1,0,2,6,0,1,1,,53,104,False,False,False,False,False,False,1,0,0,1581,1453,128,1,2,,,,pytorch
162323,closed,Move inductor jobs 3.9->3.10,atalman,"Related to: https://github.com/pytorch/pytorch/issues/161167


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-06 03:36:17+00:00,2025-09-12T04:01:51Z,,False,13,1,3,40,46,20,14,2025-09-12 03:43:09+00:00,28,233,False,False,False,False,False,False,20,12,4314,86,40,46,2,3,4.0,13.0,2025-09-06T08:08:44Z,pytorch
162322,closed,[CUDA 13][cuDNN][Windows] Roll back cuDNN upgrade from 9.13 to 9.12 on Windows,eqy,"Forward fix for #162268

CC @atalman 

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @csarofeen @ptrblck @xwang233 @msaroufim @jerryzh168",2025-09-06 03:26:33+00:00,2025-09-06T13:33:13Z,,False,3,0,1,1,1,1,3,2025-09-06 13:32:10+00:00,78,156,False,True,False,False,False,False,1,2,803,2,1,1,1,1,3.0,2.0,2025-09-06T03:37:52Z,pytorch
162321,open,forward fix mypy type errors in torch when linalg is type checked,stmcgovern,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

----

- Fix cholesky_ex tuple access in distributions/constraints.py
- Fix multi_dot argument type in nn/utils/spectral_norm.py
- Fix list to tuple conversions for linalg functions in _refs/__init__.py
- Fix ord parameter handling in masked/_ops.py
- Fix NumberType conversion in _refs/nn/functional/__init__.py
- Add TorchScript-compatible type ignores in functional.py

Resolves 14 MyPy type errors while maintaining TorchScript compatibility.",2025-09-06 01:50:21+00:00,2025-09-17T19:40:03Z,,False,1,11,2,55,17,6,12,,65,539,False,True,False,False,False,False,6,0,0,72,55,17,1,1,2.0,0.0,2025-09-06T17:44:21Z,pytorch
162320,closed,[SymmMem] Add team pool to hold duplicated teams for the same rank group,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162243
* __->__ #162320

When multiple threadblocks call device-side collectives concurrently, NVSHMEM requires each call being made on a separate team struct, see [Collective operations scopes and active sets](https://docs.nvidia.com/nvshmem/api/gen/api/collectives.html?highlight=nvshmem_barrier_all#collective-operations-scopes-and-active-sets).

This PR adds a util `get_n_teams` for creating duplicated nvshmem teams for the same rank group, i.e. team pool. So that we can use them on device side.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-06 01:01:58+00:00,2025-09-09T04:05:36Z,,False,5,18,8,175,33,2,23,2025-09-09 03:58:50+00:00,72,678,False,False,False,True,False,False,2,4,751,16032,9408,6624,1,8,4.0,5.0,2025-09-06T03:30:27Z,pytorch
162319,closed,forward fix mypy type errors in torch when linalg is type checked,stmcgovern,"This the continuation of [PR160750](https://github.com/pytorch/pytorch/pull/160750)
Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

----

- Fix cholesky_ex tuple access in distributions/constraints.py
- Fix multi_dot argument type in nn/utils/spectral_norm.py
- Fix list to tuple conversions for linalg functions in _refs/__init__.py
- Fix ord parameter handling in masked/_ops.py
- Fix NumberType conversion in _refs/nn/functional/__init__.py
- Add TorchScript-compatible type ignores in functional.py

Resolves 14 MyPy type errors while maintaining TorchScript compatibility.",2025-09-06 00:50:32+00:00,2025-09-06T01:47:25Z,,False,1,0,2,55,17,6,1,2025-09-06 01:46:30+00:00,65,623,False,True,False,False,False,False,6,0,0,72,55,17,1,1,,,,pytorch
162318,closed,[dynamo] fix resume_execution.py KeyError in Python 3.11+,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162318

Fixes https://github.com/pytorch/pytorch/issues/162313

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos

Differential Revision: [D81938289](https://our.internmc.facebook.com/intern/diff/D81938289)",2025-09-06 00:47:03+00:00,2025-09-12T18:35:15Z,,False,9,0,2,37,6,2,9,2025-09-08 20:26:28+00:00,57,421,False,True,False,False,False,False,2,8,2756,43,37,6,1,2,6.0,9.0,2025-09-08T15:31:23Z,pytorch
162317,closed,[AOTI-FX] Support registering custom FX backends,blaine-rister,"# Feature
Currently, `torch._inductor.compile_aot` always uses the `WrapperFxCodegen` class. In contrast, Python and C++ codegen allow users to register custom backends. This PR brings that feature to FX codegen.

# Test plan
Added a CI test registering a custom FX backend.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-06 00:42:55+00:00,2025-09-06T07:33:08Z,,False,3,0,7,56,4,3,3,2025-09-06 07:32:06+00:00,48,477,False,False,True,False,False,False,3,2,493,7279,2341,4938,1,6,3.0,2.0,2025-09-06T03:38:36Z,pytorch
162316,closed,[inductor] bugfix: keep WeakDeps (WAR deps) during fusion,v0i0,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162316

fixes #159855, was not triggered in other tests since it took
more than one round of fusion to get to the problematic code
which prunes WeakDeps. The WeakDeps are important to inhibit
fusion of kernels that read/write data into mutated buffers
with different indexing.

We modify the code to a) always prune before fusion, rather
than after, which improves its coverage and makes our basic
vertical fusion tests surface this issue as well and b)
check whether the weak dep is fusable before eliminating it
(which basically means checking that the producing code and
the consuming code are sufficiently compatible).

The tests that trigger this with change (a) is:
test_fusing_write_into_disjoint_read introduced in #118210.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-06 00:29:38+00:00,2025-09-21T13:09:18Z,,False,13,6,12,40,12,2,19,2025-09-21 13:08:14+00:00,57,1020,False,True,False,False,True,False,2,12,3429,47047,34108,12939,1,11,5.0,14.0,2025-09-06T00:31:10Z,pytorch
162315,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-06 00:24:32+00:00,2025-09-08T18:27:38Z,,False,14,0,1,1,1,1,14,2025-09-08 18:26:36+00:00,48,157,False,False,False,False,False,False,1,13,3953,2,1,1,1,1,3.0,13.0,2025-09-06T00:24:32Z,pytorch
162314,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-06 00:24:15+00:00,2025-09-07T04:30:27Z,,False,6,0,1,1,1,1,6,2025-09-07 04:29:25+00:00,46,156,False,False,False,False,False,False,1,5,1552,2,1,1,1,1,2.0,5.0,2025-09-06T00:24:16Z,pytorch
162312,open,[DTensor] DTensor.item() should Replicate() inputs,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162312
* #162307
* #162117

Previously, DTensor.item() didn't replicate inputs. This caused silent
incorrectness in the case of DTensor.max().item() -- .max() returns
DTensors with Partial(max) placement and .item() just returned the local
values.

This PR changes it so that DTensor.item() replicates the inputs before
calling .item().

Test Plan:
- new tests

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @gchanan",2025-09-05 23:35:31+00:00,2025-09-10T14:20:31Z,,False,4,2,3,53,3,3,6,,50,558,False,False,False,False,False,False,3,3,2499,58,54,4,1,3,4.0,4.0,2025-09-06T01:36:37Z,pytorch
162311,closed,[inductor] fuse for scalar shared data,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162355
* #162126
* #162101
* #162341
* __->__ #162311

LOAF previously may skip these fusion opportunities and cause some tests fail.

Test:
- TORCHINDUCTOR_LOOP_ORDERING_AFTER_FUSION=1 python test/inductor/test_torchinductor_strided_blocks.py TritonBlockPointerTestGPU.test_2d_reduction_odd_shapes_view_size4_num_block_pointers_1_num_triton_kernels_1_reduction_op4_cuda


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 23:31:53+00:00,2025-09-08T17:21:53Z,,False,10,0,5,52,16,2,10,2025-09-08 17:20:49+00:00,38,673,False,False,False,False,False,False,2,9,1822,69630,38752,30878,1,5,4.0,9.0,2025-09-06T03:39:23Z,pytorch
162310,closed,[BE] Update Python min version to 3.10,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162310

",2025-09-05 22:56:37+00:00,2025-09-23T15:16:34Z,,False,20,0,23,16,13,5,20,2025-09-22 17:04:24+00:00,38,94,False,False,False,False,False,False,5,18,5432,63436,44741,18695,1,23,7.0,20.0,2025-09-05T23:04:53Z,pytorch
162309,closed,[inductor][triton] support static cuda launcher after triton # 7866,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162278
* __->__ #162309
* #162244

Fixes static cuda launcher after https://github.com/triton-lang/triton/pull/7866.

Static cuda launcher checks to make sure that no hook knobs are set (and if they are, it throws an error). But Triton has changed the semantics of hooks so that ""empty hooks"" are now represented by empty `HookChain`s instead of being represented by `None`. This PR changes the way we define ""empty hooks"" to account for HookChains.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-05 22:49:02+00:00,2025-09-08T08:01:00Z,,False,3,0,3,13,1,1,3,2025-09-08 07:57:52+00:00,67,739,False,True,False,False,False,False,1,2,542,20,16,4,1,3,3.0,3.0,2025-09-05T23:21:55Z,pytorch
162308,closed,[export] Update PT2 archive docs,yiming0416,"Summary: Minor updates based on the recent refactoring for weight saving and loading

Test Plan:
doc change only

Rollback Plan:

Differential Revision: D81821994


",2025-09-05 22:40:25+00:00,2025-09-09T02:09:20Z,,False,8,0,1,17,12,1,8,2025-09-09 02:08:17+00:00,32,165,False,False,False,True,False,True,1,4,1526,29,17,12,1,1,3.0,4.0,2025-09-08T16:45:06Z,pytorch
162307,closed,[DTensor] fix F.one_hot,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162312
* __->__ #162307
* #162117

F.one_hot(dtensor) used to run into a mixed DTensor-Tensor operation due
to an arange call creating a new Tensor (not DTensor). This PR fixes it
by allowing implicit replication of Tensors for the arange call and the
one consumer of the arange call (the at::eq call).

Test Plan:
- new test. Also, F.one_hot(num_classes=-1) is broken so we skip that.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-05 22:29:03+00:00,2025-09-08T19:38:16Z,,False,3,0,4,34,9,2,3,2025-09-08 19:37:11+00:00,23,561,False,True,False,False,False,False,2,2,493,55,40,15,1,4,3.0,2.0,2025-09-06T02:00:23Z,pytorch
162306,closed,remove deprecated vllm test,yangw-dev,"Fixes https://github.com/pytorch/pytorch/issues/162274

the test is removed from vllm side
",2025-09-05 22:11:47+00:00,2025-09-06T01:28:18Z,,False,5,0,1,0,1,1,5,2025-09-06 01:27:16+00:00,27,91,False,True,False,False,False,False,1,4,1206,1,0,1,1,1,3.0,5.0,2025-09-05T22:12:53Z,pytorch
162305,open,Move pytorch checkout before setup-rocm,jithunnair-amd,"Fixes issues like this: https://github.com/pytorch/pytorch/actions/runs/17469551170/job/49630020383

Need to run on MI325 runners

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-05 22:01:09+00:00,2025-09-06T01:49:45Z,,False,1,0,1,1,1,1,1,,39,231,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162304,closed,Disable autocast when running joint graph passes,yf225,"Fixes #159469. See https://github.com/pytorch/pytorch/issues/159469#issuecomment-3221474027 for root-cause analysis.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 21:38:49+00:00,2025-09-06T00:59:04Z,,False,3,0,1,111,1,2,3,2025-09-06 00:58:01+00:00,48,320,False,True,False,False,False,False,2,2,500,112,111,1,1,1,5.0,3.0,2025-09-05T21:42:53Z,pytorch
162303,closed,[inductor] rename deps during refreshing,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162126
* #162101
* #162311
* __->__ #162303
* #162221
* #162028

Skiping renaming cause wrong dependencies when mutations are involved. 

Test:

CUDA_VISIBLE_DEVICES=4,5,6 TORCHINDUCTOR_LOOP_ORDERING_AFTER_FUSION=1 python test/distributed/test_compute_comm_reordering.py TestComputeCommReorderingMultiProc.test_reorder_compute_for_overlap

Both all-reduce and wait-tensor ir node contains a MutationBuffer for this test.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 21:35:06+00:00,2025-09-06T20:39:38Z,,False,3,0,2,16,2,1,3,2025-09-06 20:38:32+00:00,40,723,False,False,False,False,False,False,1,2,698,118,75,43,1,2,4.0,3.0,2025-09-05T22:13:20Z,pytorch
162301,closed,[export] Move example inputs in move_to_device_pass,yiming0416,"Summary:
If i have a EP that's exported on CPU and want to AOTI compile it for CUDA. I need to use `move_to_device_pass`.

But in `torch._inductor.aoti_compile_and_package()`, it directly uses the `example_inputs` attached to the EP, so we should move the example inputs as well if applicable.

Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_move_device_example_inputs

Rollback Plan:

Differential Revision: D81812366


",2025-09-05 21:21:47+00:00,2025-09-06T23:56:01Z,,False,6,0,1,47,0,2,6,2025-09-06 23:54:58+00:00,51,445,False,False,False,False,False,False,2,1,476,47,47,0,1,1,2.0,1.0,2025-09-05T21:23:25Z,pytorch
162300,closed,[inductor] estimate peak memory in codegen only when buffer reuse,ruisizhang123,"As titled, this PR ensures peak memory is estimated only when buffer reuse is enabled. Without this config, some nodes' successor nodes are eliminated from memory estimation after inductor bucketing, which can cause errors. 

The original codegen peak memory estimation code is from this PR: https://github.com/pytorch/pytorch/pull/159530

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 21:18:48+00:00,2025-09-06T01:31:44Z,,False,6,0,1,2,1,1,6,2025-09-06 01:30:41+00:00,65,541,False,False,False,False,False,False,1,5,1216,3,2,1,1,1,4.0,5.0,2025-09-05T22:19:45Z,pytorch
162298,closed,Add DISABLE_JUSTKNOBS to torch/_utils_internal.py and use it for dynamo _maybe_set_eval_frame,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162990
* #162968
* #162508
* #161695
* #162336
* __->__ #162298

If JustKnobs is disabled (as it always is in OSS), we can easily avoid an extra layer of Python function call.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-05 20:58:19+00:00,2025-09-15T23:01:46Z,,False,3,0,5,18,11,2,3,2025-09-15 23:00:42+00:00,93,426,False,False,False,False,False,False,2,2,493,46897,31903,14994,1,5,3.0,2.0,2025-09-13T15:09:40Z,pytorch
162297,closed,[CD][EZ] Update libtorch python version to 3.10,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162136
* #162265
* __->__ #162297

Not sure why it was at 3.9",2025-09-05 20:46:46+00:00,2025-09-05T22:47:41Z,,False,6,0,3,36,36,8,6,2025-09-05 22:46:39+00:00,47,140,False,False,False,False,False,False,8,5,2078,15775,8341,7434,1,3,4.0,5.0,2025-09-05T20:50:45Z,pytorch
162296,closed,Allow add_persistent_r_block to scale up rblock up to a limit,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162447
* #162446
* __->__ #162296

<img width=""654"" height=""392"" alt=""Screenshot 2025-09-18 at 4 22 53 PM"" src=""https://github.com/user-attachments/assets/975650ec-f769-43a6-bdf5-2885a8d40d3c"" />





cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 20:39:46+00:00,2025-09-22T22:01:31Z,,False,12,4,4,5,6,2,16,2025-09-22 21:41:48+00:00,61,481,False,False,False,False,False,False,2,11,2241,43945,30733,13212,1,4,3.0,12.0,2025-09-10T18:16:53Z,pytorch
162295,closed,Fix some edge cases,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162295


``` Summary
🔝 Top 5 Performance Differences (by absolute %):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 56.937931         ┆ 58.960459            ┆ 1.035522                  ┆ 3.552163  │
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 128) ┆ 89.221306         ┆ 86.295642            ┆ 0.967209                  ┆ -3.27911  │
│ causal         ┆ torch.bfloat16 ┆ (2, 16, 4096, 4, 4096, 128) ┆ 111.552594        ┆ 114.380841           ┆ 1.025353                  ┆ 2.535349  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16, 1024, 64) ┆ 74.830149         ┆ 76.685445            ┆ 1.024793                  ┆ 2.479344  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 55.279932         ┆ 56.369312            ┆ 1.019707                  ┆ 1.97066   │
└────────────────┴────────────────┴─────────────────────────────┴───────────────────┴──────────────────────┴───────────────────────────┴───────────┘

🔺 Top 5 Cases Where no_peel (change) is Faster than base (baseline):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 56.937931         ┆ 58.960459            ┆ 1.035522                  ┆ 3.552163  │
│ causal         ┆ torch.bfloat16 ┆ (2, 16, 4096, 4, 4096, 128) ┆ 111.552594        ┆ 114.380841           ┆ 1.025353                  ┆ 2.535349  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16, 1024, 64) ┆ 74.830149         ┆ 76.685445            ┆ 1.024793                  ┆ 2.479344  │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 64)  ┆ 55.279932         ┆ 56.369312            ┆ 1.019707                  ┆ 1.97066   │
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 4096, 4, 4096, 64)  ┆ 111.08814         ┆ 112.447047           ┆ 1.012233                  ┆ 1.22327   │
└────────────────┴────────────────┴─────────────────────────────┴───────────────────┴──────────────────────┴───────────────────────────┴───────────┘

🔻 Top 5 Cases Where no_peel (change) is Slower than base (baseline):
shape: (5, 7)
┌────────────────┬────────────────┬─────────────────────────────┬───────────────────┬──────────────────────┬───────────────────────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)       ┆ TFlops BWD (base) ┆ TFlops BWD (no_peel) ┆ no_peel_speedup_over_base ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                         ┆ ---               ┆ ---                  ┆ ---                       ┆ ---       │
│ str            ┆ str            ┆ str                         ┆ f64               ┆ f64                  ┆ f64                       ┆ f64       │
╞════════════════╪════════════════╪═════════════════════════════╪═══════════════════╪══════════════════════╪═══════════════════════════╪═══════════╡
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 4, 1024, 128) ┆ 89.221306         ┆ 86.295642            ┆ 0.967209                  ┆ -3.27911  │
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 1024, 4, 1024, 64)  ┆ 78.23082          ┆ 76.693169            ┆ 0.980345                  ┆ -1.965531 │
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 2048, 4, 2048, 128) ┆ 96.95663          ┆ 95.573333            ┆ 0.985733                  ┆ -1.426717 │
│ alibi          ┆ torch.bfloat16 ┆ (4, 16, 2048, 4, 2048, 64)  ┆ 93.373473         ┆ 92.294147            ┆ 0.988441                  ┆ -1.155924 │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 2048, 4, 2048, 128) ┆ 96.95147          ┆ 96.105389            ┆ 0.991273                  ┆ -0.872685 │
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos @Chillee @yanboliang @BoyuanFeng",2025-09-05 20:33:42+00:00,2025-09-16T21:30:07Z,,False,5,5,14,162,133,3,10,2025-09-10 21:33:48+00:00,19,5353,False,True,False,False,False,False,3,4,1025,22706,13804,8902,1,14,6.0,4.0,2025-09-06T18:11:19Z,pytorch
162294,open,Support of DCP with device order,zpcore,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163772
* __->__ #162294
* #160903
* #160266



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-05 20:26:18+00:00,2025-09-24T20:21:12Z,,False,3,1,4,16,3,3,4,,32,221,False,False,False,False,False,False,3,2,83,139647,89865,49782,1,4,3.0,2.0,2025-09-06T01:58:22Z,pytorch
162293,closed,[inductor][choices] rename get_mm_configs to get_template_configs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* __->__ #162293
* #161350
* #161351

# why

- eventually we want all templates to go through this
- we're exposing this through diode as a sort of interface/API
- avoid later renaming

# what

- rename get_mm_configs to get_template_configs
- rename _finalize_mm_configs to _finalize_template_configs

# testing

- lintrunner
- ci

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81820641](https://our.internmc.facebook.com/intern/diff/D81820641)",2025-09-05 20:03:26+00:00,2025-09-12T21:10:49Z,,False,17,0,9,22,14,4,17,2025-09-12 21:10:49+00:00,65,719,False,False,False,False,False,False,4,16,2822,27884,20498,7386,1,9,3.0,16.0,2025-09-05T22:18:44Z,pytorch
162292,closed,Add more tests for vllm and clean out the old vllm test,yangw-dev,"
Test failure coverage from pytorch 2.8 release issues
[internal access only](https://docs.google.com/document/d/1zvK1eUAHubHGGHg9jKxd-QlP89fzgfqOBvE2m9mUs90/edit?tab=t.0
)

See coverage mapping
| Given test / pattern | Suite ID (from config) |
|---|---|
| pytest -v -s basic_correctness/test_cumem.py | vllm_basic_correctness_test |
| pytest -v -s entrypoints/openai/test_sleep.py | vllm_entrypoints_test |
| pytest -v -s entrypoints/openai/test_translation_validation.py::test_long_audio_request | vllm_entrypoints_test |
| pytest -v -s lora/test_quant_model.py | vllm_lora_28_failure_test |
| pytest -v -s -x tests/lora/test_llama_tp.py | vllm_lora_tp_test_distributed |
| pytest -v -s distributed/test_sequence_parallel.py -k test_tp_sp_generation |vllm_distributed_test_28_failure_test |
| pytest -v -s distributed/test_sequence_parallel.py::test_tp_sp_generation[...] | vllm_distributed_test_28_failure_test |
| pytest models/language/generation/test_mistral.py::test_models[...] | vllm_languagde_model_test_extended_generation_28_failure_test |
| pytest models/multimodal/pooling/test_jinavl_reranker.py::test_model_text_image[...] | vllm_multi_model_test_28_failure_test |
| tests/lora/test_qwen2vl.py::test_qwen2vl_lora | vllm_lora_test |
| tests/lora/test_qwen2vl.py::test_qwen25vl_lora | vllm_lora_test |
| tests/lora/test_qwen2vl.py::test_qwen2vl_lora_beam_search | vllm_lora_test |
| tests/lora/test_phi.py::test_phi2_lora | DIDN'T FIND IT IT IN VLLM |
| models/multimodal/generation/test_voxtral.py::test_models_with_multiple_audios[5-128-half] | vllm_multi_model_test_28_failure_test |
| models/test_initialization.py::test_can_initialize[VoxtralForConditionalGeneration] | vllm_basic_models_test |
| pytest -v -s -x lora/test_chatglm3_tp.py -k test_chatglm3_lora_tp4_fully_sharded_loras | vllm_lora_tp_test_distributed |

",2025-09-05 19:47:02+00:00,2025-09-09T05:54:53Z,,False,4,4,14,71,20,3,8,2025-09-09 05:53:49+00:00,55,1838,False,False,False,True,False,False,3,3,924,17008,13002,4006,1,14,4.0,4.0,2025-09-05T19:49:27Z,pytorch
162291,open,[AI Codemod][DevmateTestDeletion] fbsource//xplat/caffe2/android:test_host,facebook-github-bot,"Reviewed By: malfet

Differential Revision: D81429090


",2025-09-05 19:41:14+00:00,2025-09-11T21:31:39Z,,False,5,0,1,0,621,1,5,,74,56,False,False,False,False,False,False,1,0,0,621,0,621,1,1,,,,pytorch
162289,closed,replace expect_true with guard_or_true in maybe_reduce,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162289

Fixes #161276

Tested within repo via `python test/dynamo/test_misc.py -k test_validate_outputs_unbacked`

Tested in torchtitan via `LOG_RANK=0,1 TORCHINDUCTOR_COMPILE_THREADS=1 NGPU=2 CONFIG_FILE=""./torchtitan/experiments/llama4/train_configs/debug_model.toml"" tlp ./run_train.sh --parallelism.data_parallel_shard_degree=2 --parallelism.expert_parallel_degree=2`

<img width=""1796"" height=""403"" alt=""image"" src=""https://github.com/user-attachments/assets/822aec7d-524f-4e6c-8375-57353be5d32d"" />


",2025-09-05 19:26:31+00:00,2025-09-06T21:27:03Z,,False,2,0,1,1,1,1,2,2025-09-06 21:27:03+00:00,54,593,False,True,False,False,False,False,1,1,58,2,1,1,1,1,1.0,1.0,2025-09-06T21:27:03Z,pytorch
162288,closed,[ROCm][Inductor][CK backend] Install rocm-composable-kernel python package on ROCm Linux CI docker images,tenpercent,"Reopened from #158747 which got reverted since without setuptools-scm in pytorch index URL the wheel cannot be built

We reconsider the original PR idea of introducing CK as a pytorch dependency on ROCm Linux and install the CK python package in CI only -- since (1) rocm-composable-kernel depends on setuptools-scm which depends on tomli and the existing index URLs need to be modified to host the new packages and (2) there also is a packaging [bug](https://github.com/pypa/setuptools/issues/3269#issuecomment-1254507377) in Ubuntu 22.04 which prevents correct dynamic version calculation with default system pip.

Extras:

 ->   this PR reconsiders how TORCHINDUCTOR_CK_DIR env variable is used; previously, this var was used to point to rocm-composable-kernel package installation path on the filesystem; now, the path is inferred by trying to import ck4inductor
 ->   the tests are updated to reflect this change
 ->   since in CI clang points to a bash script which invokes sccache, we cannot patch PATH to not contain sccache, this logic is removed from the testing code
->    scaled_mm test crashes during the benchmarking when the benchmarking happens in the main process, and times out benchmarking when it happens in a subprocess, on gfx942, so it is disabled

TBD: roll back rocm-mi300 workflow before merging

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 19:14:31+00:00,2025-09-10T19:34:47Z,,False,6,0,9,39,36,8,6,2025-09-10 19:33:44+00:00,105,1638,False,True,False,True,False,False,8,5,981,225,114,111,1,9,3.0,5.0,2025-09-08T03:59:59Z,pytorch
162286,closed,[upstream triton] bump version number from 3.4 to 3.5,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162286
* #162278
* #162244

",2025-09-05 19:00:09+00:00,2025-09-08T06:11:22Z,,False,1,0,1,2,2,2,1,2025-09-08 06:11:22+00:00,53,114,False,False,False,False,False,False,2,0,0,4,2,2,1,1,,,,pytorch
162285,closed,[nativert] AOTI lowering and packaging as NativeRT delegate,yiming0416,"Summary:
A demo for creating AOTI delegate for NativeRT in OSS.

- It supports full graph lowering only.
- It leverages `executorch_call_delegate` HOP but doesn't rely on `executorch`.
- The delegate graph is obtained by tracing a `LoweredBackendModule` whose forward function calls `executorch_call_delegate`.
- The main difference between `executorch_call_delegate` and `aoti_call_delegate` is that the delegate graph from `executorch_call_delegate` doesn't have weights lifted as inputs.
- original_ep and delegate_ep are treated as flat EP dictionary and there is no nested structure.
- The naming contract is enforced by `model_name` and `backend_id`

Test Plan:
CI

Rollback Plan:

Differential Revision: D81641157


",2025-09-05 18:53:39+00:00,2025-09-07T01:31:00Z,,False,11,9,1,103,0,5,20,2025-09-07 01:29:57+00:00,59,723,False,False,False,False,False,False,5,1,489,103,103,0,1,1,4.0,2.0,2025-09-05T19:53:36Z,pytorch
162284,closed,Handle f([]) vs. f() in fake tensor caching,angelayi,Fixes https://github.com/pytorch/pytorch/issues/162279,2025-09-05 18:43:41+00:00,2025-09-08T18:29:12Z,,False,3,9,1,14,0,2,12,2025-09-08 18:28:08+00:00,43,54,False,True,False,False,False,False,2,2,508,14,14,0,1,1,4.0,3.0,2025-09-05T19:12:22Z,pytorch
162281,open,Make stride argument to Tensor.set_ optional,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161635
* __->__ #162281

Long-standing fixme. Seems to be blocking #161635.",2025-09-05 18:16:23+00:00,2025-09-11T04:13:07Z,,False,9,3,4,38,43,12,12,,44,154,False,True,False,False,False,False,12,7,1252,30611,19403,11208,1,4,3.0,7.0,2025-09-05T18:20:06Z,pytorch
162278,closed,[upstream triton] update triton pin to triton 3.5,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162278
* #162309
* #162244

Update PyTorch to the latest Triton release candidate branch (release/3.5.x in triton-lang/triton)

Notably:
* this does *not* include the version number bump from 3.4 -> 3.5 (we'll do that in a follow-up PR)
* sam_fast is still failing, so we've disabled it temporarily https://github.com/pytorch/pytorch/issues/162282 and we are committed to fixing it, ideally before the branch cut but possibly as a cherry-pick into the release branch.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela @mlazos",2025-09-05 18:09:38+00:00,2025-09-08T14:30:32Z,,False,6,0,5,6,3,4,6,2025-09-08 14:29:27+00:00,49,777,False,True,False,False,False,False,4,3,861,31,23,8,1,5,3.0,3.0,2025-09-05T18:46:37Z,pytorch
162277,closed,Fix `DeviceMesh._flatten` docstring example,mariosasko,"Fix the `DeviceMesh._flatten` docstring example of use. Alternative fix would be to replace `mesh_3d[""dp"", ""cp""]` with `mesh_3d[""cp"", ""tp""]`.

(I verified the fix using the `gloo` backend)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-05 17:10:07+00:00,2025-09-06T05:01:05Z,,False,5,0,1,2,2,1,5,2025-09-06 05:00:03+00:00,43,285,False,True,False,True,False,False,1,3,1058,4,2,2,1,1,2.0,3.0,2025-09-06T02:18:38Z,pytorch
162276,closed,inductor: add helper for size==1 checks,ColinPeppler,"## Summary
- document guard behavior in `SizeVarAllocator.is_size_one`
- use `is_size_one` for broadcast/expand checks.
- This diff is a no-op since we'd use `shape_env.evaluate_expr(... fallback_value=False)`

https://github.com/pytorch/pytorch/blob/a4f9132a178ce8c41b17ff6e89aa382ec1dbe701/torch/_inductor/sizevars.py#L450-L453
------
https://chatgpt.com/codex/tasks/task_e_68b8d0d1f2c48328b2d38c00e738bc8c

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 17:05:41+00:00,2025-09-05T17:07:07Z,,False,2,0,1,15,17,3,2,2025-09-05 17:07:07+00:00,39,611,False,False,False,True,False,False,3,0,0,32,15,17,1,1,,,,pytorch
162275,open,[inductor] require shape in TritonCSEVariable,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162275
* #163023



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-05 16:41:22+00:00,2025-09-16T16:48:22Z,,False,6,0,8,1,2,1,6,,45,315,False,False,False,False,False,False,1,4,1115,43719,30218,13501,1,8,3.0,4.0,2025-09-14T06:43:46Z,pytorch
162273,open,Fix NestedTensor max/min operations for integer dtypes.,adabeyta,"Fixes: https://github.com/pytorch/pytorch/issues/162049

### Summary

max_dim and min_dim functions incorrectly used torch.finfo()
for all dtypes, causing TypeError for integer tensors. 

### Changes

- Use torch.iinfo() for integer dtypes instead of torch.finfo().
- Add CPU test: `test_jagged_max_min_dtypes` covering `int8, int16, int32, int64, uint8, float16, bfloat16, float32 and float64`

### Testing

Before Fix:

`python -m pytest test/test_nestedtensor.py -k ""test_jagged_max_min_dtypes"" -v`

Output:

```
FAILED [0.0006s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_bfloat16 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0006s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float16 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0006s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float32 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0006s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float64 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0006s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int16 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0005s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int32 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0005s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int64 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0004s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int8 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
FAILED [0.0004s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_uint8 - TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'
```

After Fix:

`python -m pytest test/test_nestedtensor.py -k ""test_jagged_max_min_dtypes"" -v`

Output:

```
Running 9 items in this shard

test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_bfloat16 PASSED [0.0086s]                                                                                                                   [ 11%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float16 PASSED [0.0011s]                                                                                                                    [ 22%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float32 PASSED [0.0011s]                                                                                                                    [ 33%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_float64 PASSED [0.0011s]                                                                                                                    [ 44%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int16 PASSED [0.0009s]                                                                                                                      [ 55%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int32 PASSED [0.0010s]                                                                                                                      [ 66%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int64 PASSED [0.0010s]                                                                                                                      [ 77%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_int8 PASSED [0.0010s]                                                                                                                       [ 88%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_jagged_max_min_dtypes_cpu_uint8 PASSED [0.0011s]                                                                                                                       [100%]                     
```
",2025-09-05 16:35:14+00:00,2025-09-22T18:23:11Z,,False,5,0,2,156,6,2,5,,55,4769,False,True,False,False,False,False,2,4,865,174,162,12,1,2,4.0,4.0,2025-09-05T16:36:53Z,pytorch
162272,closed,Add support for NestedTensor share_memory_,adabeyta,"Fixes: https://github.com/pytorch/pytorch/issues/161915

### Summary

Implements share_memory_() support for NestedTensor! 

### Changes

- Added share_memory_() method to NestedTensor class.
  - Shares storage for all NestedTensor components: _values, _offsets, _lengths, and cached seqlen tensors.
  - Guard for CUDA Tensors.

### Testing

Before Fix:

`pytest -q test/test_nestedtensor.py -k ""test_share_memory"" -v`

Output:

```
Running 1 items in this shard

test/test_nestedtensor.py Fatal Python error: Segmentation fault
```

After Fix:

`pytest -q test/test_nestedtensor.py -k ""test_share_memory"" -v`

Output:

```
Running 1 items in this shard

test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_share_memory_cpu PASSED [0.0753s] 
```
",2025-09-05 16:22:58+00:00,2025-09-22T20:01:05Z,,False,7,6,5,48,0,2,13,2025-09-22 20:00:01+00:00,42,757,False,True,False,False,False,False,2,6,1579,106,77,29,1,5,4.0,9.0,2025-09-05T16:39:27Z,pytorch
162269,closed,Avoid crash with release_available_cached_blocks,morrison-turnansky,"updated release behavior for cached blocks
Fixes #159567


cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-05 15:48:50+00:00,2025-09-08T17:47:50Z,,False,7,0,1,1,1,1,7,2025-09-08 17:46:47+00:00,48,98,False,True,False,False,False,False,1,5,615,2,1,1,1,1,4.0,5.0,2025-09-05T15:56:10Z,pytorch
162268,closed,[CUDA 13][cuDNN] Bump CUDA 13 to cuDNN 9.13.0,eqy,"Fixes some `d_qk` != `d_v` cases on Hopper that are broken by cuDNN 9.11-9.12

cc @csarofeen @ptrblck @xwang233",2025-09-05 15:41:07+00:00,2025-09-06T05:29:29Z,,False,12,0,1,17,17,5,12,2025-09-06 01:59:07+00:00,45,111,False,True,False,False,False,False,5,11,1957,34,17,17,1,1,6.0,12.0,2025-09-05T15:59:44Z,pytorch
162267,open,Fix DCE eliminating in-place operations by improving Node.is_impure(),Eldalie,"Change is_impure to check in-place operations on Node to prevent eliminate_dead_code from eliminating in-place operations.


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-05 15:31:50+00:00,2025-09-18T07:26:50Z,,False,34,6,1,118,5,3,40,,69,181,False,True,False,False,False,False,3,28,7831,123,118,5,1,1,6.0,28.0,2025-09-06T02:11:51Z,pytorch
162266,closed,[CD][BE] Get rid of SETUPTOOLS and PYYAML extra pins,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162136
* #162265
* #162297
* __->__ #162266

As those weren't really a pins to begin with, and requirments.txt
already has those",2025-09-05 14:53:48+00:00,2025-09-05T21:33:59Z,,False,3,0,4,0,18,1,3,2025-09-05 21:32:55+00:00,52,207,False,False,False,False,False,False,1,2,945,202,95,107,1,4,5.0,3.0,2025-09-05T15:32:02Z,pytorch
162265,closed,[CD][BE] Delete Python-3.9 case,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162136
* __->__ #162265
* #162297

And raise error when building for an unsupported version",2025-09-05 14:53:44+00:00,2025-09-05T22:47:44Z,,False,6,0,5,2,8,1,6,2025-09-05 22:46:40+00:00,31,170,False,False,False,False,False,False,1,5,2798,15791,8346,7445,1,5,5.0,6.0,2025-09-05T15:31:36Z,pytorch
162264,closed,[EZ][CD] Update MacOS deployment platform to 11.0,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162136
* #162266
* #162265
* __->__ #162264

Fixes following warning
```
MACOSX_DEPLOYMENT_TARGET is set to a lower value (10.15) than the version on which the Python interpreter was compiled (11.0)
```
Update deployment platform in `README.MD` as well",2025-09-05 14:53:41+00:00,2025-09-05T19:59:12Z,,False,3,0,2,3,3,2,3,2025-09-05 19:58:07+00:00,49,331,False,True,False,False,False,False,2,2,907,8,4,4,1,2,5.0,3.0,2025-09-05T15:31:16Z,pytorch
162263,closed,[CD][BE] Remove unnecessary checks for XCode version,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162136
* #162266
* #162265
* #162264
* __->__ #162263

None of them have worked for a while, PyTorch for Mac is build with
XCode-15.4",2025-09-05 14:53:37+00:00,2025-09-05T17:03:44Z,,False,3,0,1,0,45,3,3,2025-09-05 17:02:39+00:00,52,212,False,False,False,False,False,False,3,2,821,45,0,45,1,1,5.0,3.0,2025-09-05T15:31:02Z,pytorch
162262,closed,"[inductor][WIP] Trie bucketing, runtime estimations etc.",IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162262



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 14:43:34+00:00,2025-09-24T14:19:11Z,,False,3,0,8,1401,185,11,3,2025-09-24 14:19:11+00:00,56,390,False,False,False,False,False,False,11,1,20,107662,67755,39907,1,8,1.0,1.0,2025-09-06T01:59:30Z,pytorch
162261,closed,codebase structure documentation to include torchgen ,Raman-RH,"📚 The doc update

adding description about torchgen folder in code structure guide

cc @albanD @malfet @svekars @sekyondaMeta @ezyang @Skylion007 @bobrenjc93 @BoyuanFeng",2025-09-05 13:59:39+00:00,2025-09-06T02:12:04Z,,False,3,0,2,1,0,1,3,2025-09-06 02:11:00+00:00,53,169,False,False,False,True,False,False,1,2,779,5,3,2,1,2,2.0,2.0,2025-09-06T02:09:00Z,pytorch
162260,open,[Draft] Update gloo commit,gaopengff,It's target to update gloo with https://github.com/pytorch/gloo/pull/458,2025-09-05 12:51:20+00:00,2025-09-05T13:08:08Z,,False,1,0,1,1,1,1,1,,26,72,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
162254,open,Fix: Remove incorrect non-negative validation for correction parameter in torch.var during export,parsshar-RH,"Fixes #161083

### Summary:
This PR fixes a bug where `torch.export.export `incorrectly validates that the `correction` parameter for `torch.var` should be non-negative, causing export to fail when using negative correction values that are perfectly valid and work correctly in eager PyTorch execution.

### Fix Impact:

- **Improved Consistency**: Aligns export behavior with eager PyTorch functionality
- **Enhanced Compatibility:** Allows more models to be exported successfully


cc @avikchaudhuri @gmagogsfm @zhxchen17 @tugsbayasgalan @angelayi @suo @ydwu4 @penguinwu",2025-09-05 09:32:33+00:00,2025-09-24T17:59:27Z,,False,6,0,1,3,2,2,6,,97,572,False,True,False,False,True,False,2,4,504,5,3,2,1,1,2.0,4.0,2025-09-09T15:17:24Z,pytorch
162253,open,[Build] CMake: a custom job pool for flash_attention,Aidyn-A,"The flash attention kernels are resource hungry on compilation, and because of that some users have to decrease `MAX_JOBS`, that causes other jobs be unnecessary slow. This PR introduces a job pool for for limiting number of processes for flash attention target only. The number of parallel jobs for flash attention is controlled via `FA_MAX_JOBS`, so one can do `MAX_JOBS=16 FA_MAX_JOBS=8 python3 setup.py ...` instead of the old way `MAX_JOBS=8 python3 setup.py ...`. This way, the build becomes significantly faster, allowing full parallelization for other targets. ",2025-09-05 09:28:50+00:00,2025-09-15T11:10:00Z,,False,1,1,2,12,0,1,2,,52,569,False,False,False,False,False,False,1,0,0,24,18,6,1,2,1.0,0.0,2025-09-07T17:11:20Z,pytorch
162252,closed,[aarch64] Fix libnvrtc.so unversioned symlink,tinglvv,"error reported by @xwang233 internally when testing upstream ARM 12.9 wheel
error from TestCompileKernel.test_compile_kernel:
```
root@e8faae565f19:/# python a.py
/usr/local/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)
  cpu = _conversion_method_template(device=torch.device(""cpu""))
Traceback (most recent call last):
  File ""//a.py"", line 17, in <module>
    add_kernel = _compile_kernel(kernel_source, ""add_tensors"")
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/torch/cuda/__init__.py"", line 1780, in _compile_kernel
    ptx = _nvrtc_compile(
          ^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/torch/cuda/_utils.py"", line 68, in _nvrtc_compile
    libnvrtc = _get_nvrtc_library()
               ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/torch/cuda/_utils.py"", line 38, in _get_nvrtc_library
    return ctypes.CDLL(""libnvrtc.so"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/ctypes/__init__.py"", line 379, in __init__
    self._handle = _dlopen(self._name, mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: libnvrtc.so: cannot open shared object file: No such file or directory
```

Need to create symlink from unversioned `libnvrtc.so` to the versioned `libnvrtc.so.12/13.`, as it is loaded dynamically at runtime through `ctypes.CDLL(""libnvrtc.so"")`",2025-09-05 08:16:51+00:00,2025-09-23T23:49:01Z,,False,3,3,2,16,1,1,6,2025-09-23 23:48:34+00:00,45,1571,False,True,False,False,False,False,1,2,369,39,27,12,1,2,4.0,2.0,2025-09-05T19:36:34Z,pytorch
162247,closed,[dynamo] Use relaxed CLOSURE_MATCH guard then ID_MATCH,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162247

I am unable to write a test that would fail here. The reason is that when we do _dynamo.disable(fn) in the compiled frame, the id of disabled function changes but currently we guard on the original function - `fn` whose id is not changing. This PR still guards on the `fn.__code__` just to be more precise.

Thanks to @thenumberouscode for pointing this out.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-05 05:14:34+00:00,2025-09-07T01:27:00Z,,False,9,2,1,1,1,1,11,2025-09-07 01:25:55+00:00,54,624,False,False,False,False,False,False,1,8,2612,2,1,1,1,1,5.0,9.0,2025-09-05T18:20:26Z,pytorch
162245,open,Improve device info with new flops and bandwidth formula based on hardware libraries,exclamaforte,"Previously, DeviceInfo provided theoretical hardware information based on a hardcoded list manually created from various datasheets. 

This update:
- Attempting to gather the information from a hardware library like `pynvml`, improving accuracy and expanding support to devices that don't have entries in the datasheet list.
- Adjusts flops and bw calculation based on these hardware values. For example, if the the memory or SMs are underclocked, it adjusts the theoretical max flops/bw accordingly.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 04:04:23+00:00,2025-09-20T00:34:58Z,,False,9,8,7,1075,24,5,17,,84,703,False,False,False,False,True,False,5,7,1816,1449,1250,199,1,7,6.0,9.0,2025-09-08T19:13:30Z,pytorch
162244,closed,[inductor][triton] more JITCallable._hash_lock support,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162278
* #162309
* __->__ #162244

Follow-up to #161768.

Context: ProcessPool pickles the outputs before sending them back to the main process. Triton kernels have some un-pickleable fields, so `prepare_for_pickle()` is used to strip out those fields. Previously, in the standard case (without triton_bundler.py), `prepare_for_pickle()` would strip out the un-pickleable fields and they would never be added back after unpickling, because the un-pickleable fields were not actually needed after compilation finished.

In #161768 updated `prepare_for_pickle` to also strip out the `fn._hash_lock` field, a newly added field in JITCallable instances which is a `threading.RLock()`, which is not pickleable.

It turns out that we do need to restore the `fn._hash_lock` field, even in the non-triton_bundler case - the MultiKernel case uses the hash lock.

To do this, we add `restore_after_unpickle()` which will restore fields (or if the old fields are not provided, initialize just the hash_lock)

Compile time benchmarks look good, maybe a very minor regression (see the comment below on the PR)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 03:35:41+00:00,2025-09-08T07:57:51Z,,False,5,0,3,20,9,3,5,2025-09-08 07:57:51+00:00,54,1378,False,False,False,False,False,False,3,4,912,6872,3494,3378,1,3,4.0,4.0,2025-09-05T04:45:51Z,pytorch
162243,open,[SymmMem] Tiled reduce,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162243
* #162320

Added op: `tile_reduce(Tensor input, Tensor(a!) out, int root, str group_name)`

For now supports only:
- NVSHMEM backed symmetric tensor;
- 2D tensor and tile;
- torch.float.

Testing on right-bottom quandrant:
```
rank 0:
tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 1., 1., 1.],
        [0., 0., 0., 0., 1., 1., 1., 1.],
        [0., 0., 0., 0., 1., 1., 1., 1.],
        [0., 0., 0., 0., 1., 1., 1., 1.]], device='cuda:0')
PASSED                                                                                             
```

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-05 03:34:08+00:00,2025-09-09T04:01:11Z,,False,4,6,9,152,0,4,10,,22,882,False,False,False,False,False,False,4,3,548,16080,9569,6511,1,9,3.0,4.0,2025-09-05T04:33:05Z,pytorch
162241,closed,[AOTI-FX] support registering custom FX backends,blaine-rister,"Differential Revision: D81747409

# Feature
Currently, `torch._inductor.compile_aot` always uses the `WrapperFxCodegen` class. In contrast, Python and C++ codegen allow users to register custom backends. This PR brings that feature to FX codegen.

# Test plan
Added a CI test registering a custom FX backend.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-05 03:20:38+00:00,2025-09-06T00:43:40Z,,False,3,0,5,56,4,3,3,2025-09-06 00:43:39+00:00,48,511,False,False,True,False,False,False,3,1,66,7277,2340,4937,1,5,1.0,1.0,2025-09-06T00:43:39Z,pytorch
162240,closed,Support vmap + custom autograd function/improve DTensor constructor inefficiency,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162240

This makes gemma3 exportable on transformers=4.55.4

In HF, there is a torch funciton mode called TransformGetItemToIndex which internally calls custom autograd function. When this custom autograd function is called under vmap, It triggers CustomFunctionHigherOrderOP which error-ed because there was no pre-dispatch proxy mode implementation. 

Since there are number of requests lately to add various operators in pre-dispatch IR, I introduce a decorator in export that works similar to `allow_in_graph`. Basically:
1) We intercept custom_autograd_function.apply at pre-dispatch mode when this decorator is applied 
2) We apply `flat_apply` HOP to hide the pytree spec for this autograd function. Note that this adds restriction that this custom autograd function needs to take in fx-able types.
3) subclass constructor decorator is implemented similarly, so we just refactor it to use similar implementation as this new decorator. eventually we should delete the subclass constructor decorator. 
4) Move some code in subclass constructor decorator to exit early in non-export environment which should shave off some inefficiency (around 1% according to @swolchok 's benchmark) 


Fixes: https://github.com/pytorch/pytorch/issues/161563#issuecomment-3246309758 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D82141316](https://our.internmc.facebook.com/intern/diff/D82141316)",2025-09-05 03:10:13+00:00,2025-09-11T18:22:41Z,,False,14,4,8,212,43,5,18,2025-09-11 17:42:44+00:00,80,1622,False,True,False,False,True,True,5,12,2467,7287,3728,3559,1,8,7.0,12.0,2025-09-05T13:32:03Z,pytorch
162239,closed,[inductor][choices] get_ktc utility method,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* __->__ #162239
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

\# why

- enable InductorChoices and subclasses to have a simple call
  to retrieve a generator will all the choices for an
  individual op/template combo

\# what

- refactor get_mm_configs to just be a loop over get_ktc

\# testing

ci",2025-09-05 03:01:59+00:00,2025-09-12T00:42:52Z,,False,2,0,2,46,36,1,2,2025-09-12 00:42:52+00:00,42,491,False,False,False,False,False,True,1,0,0,155,113,42,1,2,,,,pytorch
162238,closed,[inductor][template heuristics] don't take layout to generate choices,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* #161350
* #161351
* __->__ #162238
* #161349
* #161348
* #161347

# why

- unnecessary as we only ever need to know the dtype and maybe the
  device
- we already take in the kernel inputs which have the device
- enable us to specify the layout after finding all the configs
  but before generating the ChoiceCallers

# what

- replace all calls in template_heuristics that used to take Layout
  with now just taking out_dtype

# testing

ci

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81820115](https://our.internmc.facebook.com/intern/diff/D81820115)",2025-09-05 03:01:55+00:00,2025-09-12T00:16:57Z,,False,8,0,5,104,93,12,8,2025-09-09 17:17:09+00:00,69,840,False,False,False,False,False,False,12,7,1283,15924,8630,7294,1,5,3.0,7.0,2025-09-05T22:18:24Z,pytorch
162236,open,Load and compile and windows,yushangdi,"Fixes #ISSUE_NUMBER
",2025-09-05 02:12:15+00:00,2025-09-05T05:20:44Z,,False,2,0,1,2781,0,22,2,,28,20,False,True,False,False,False,False,22,0,0,2781,2781,0,1,1,,,,pytorch
162227,open,Put torchao (0.13.0) back to benchmark workflow,huydhn,"0.13.0 was released on Sep 3rd https://pypi.org/project/torchao/#history, which should have fixed the crashing issue on transformers now",2025-09-05 00:27:39+00:00,2025-09-12T22:06:20Z,,False,20,1,12,37,14,9,21,,47,136,False,True,False,False,False,False,9,19,4587,36312,24304,12008,1,12,5.0,19.0,2025-09-07T06:09:10Z,pytorch
162226,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-05 00:26:07+00:00,2025-09-05T04:33:13Z,,False,3,0,1,1,1,1,3,2025-09-05 04:32:10+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-05T00:26:08Z,pytorch
162225,closed,[ONNX] Hide draft export under a flag,justinchuby,"Use `TORCH_ONNX_ENABLE_DRAFT_EXPORT` to control whether draft_export should be used as a strategy in onnx export.

Follow up of https://github.com/pytorch/pytorch/pull/161454


cc @titaiwangms",2025-09-04 22:59:00+00:00,2025-09-05T20:50:08Z,,False,6,2,5,17,12,4,8,2025-09-05 19:54:54+00:00,37,192,False,False,False,False,False,False,4,5,1730,31,18,13,1,5,4.0,5.0,2025-09-05T16:26:38Z,pytorch
162224,closed,[fx] fix qualified name for methods of torch.Tensor,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162224

Fixes #160077, #154721

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 22:45:58+00:00,2025-09-06T05:17:25Z,,False,3,6,1,19,3,2,9,2025-09-06 05:16:23+00:00,51,342,False,True,False,False,False,False,2,2,603,22,19,3,1,1,7.0,3.0,2025-09-04T23:02:10Z,pytorch
162223,closed,Add -Wno-ctad-maybe-unsupported compiler flag,0xjeffro,"When running bazel build, we (Google) run into the following error.
The `-Wctad-maybe-unsupported` warning would be raised to an error and break the build in certain cases.
So, we propose to suppress the warning to make the build with bazel more smooth.

This is the error message we got:
```
c10/util/IntrusiveList.h:166:12: error: 'std::reverse_iterator' may not intend to support class template argument deduction [-Werror,-Wctad-maybe-unsupported]
  166 |     return std::reverse_iterator{end()};
      |            ^
c10/test/util/IntrusiveList_test.cpp:24:18: note: in instantiation of member function 'c10::IntrusiveList<(anonymous namespace)::ListItem>::rbegin' requested here
   24 |     auto it = c1.rbegin();
      |                  ^
c10/test/util/IntrusiveList_test.cpp:43:5: note: in instantiation of function template specialization '(anonymous namespace)::check_containers_equal<(anonymous namespace)::ListItem>' requested here
   43 |     check_containers_equal(l, v);
      |     ^
libcxx/include/__iterator/reverse_iterator.h:51:7: note: add a deduction guide to suppress this warning
   51 | class reverse_iterator
      |       ^
1 error generated.

```

@haifeng-jin 
",2025-09-04 22:43:42+00:00,2025-09-06T06:12:43Z,,False,14,0,1,1,1,1,14,2025-09-06 06:11:40+00:00,45,1191,False,False,False,False,False,False,1,12,2633,2,1,1,1,1,3.0,12.0,2025-09-06T02:35:20Z,pytorch
162222,closed,Update Kineto Submodule,sraikund16,"Summary: Update

Test Plan:
CI

Rollback Plan:

Differential Revision: D81727392
",2025-09-04 22:41:13+00:00,2025-09-23T06:10:05Z,,False,11,0,1,1,1,1,11,2025-09-23 06:08:59+00:00,23,81,False,False,False,False,False,False,1,3,826,2,1,1,1,1,3.0,3.0,2025-09-22T20:41:18Z,pytorch
162221,closed,[inductor] fix TemplateBuffer.extract_read_writes,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162126
* #162101
* #162311
* #162303
* __->__ #162221
* #162028


Make sure TemplateBuffer & ComputedBuffer have the same dependencies prefix.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 22:32:27+00:00,2025-09-06T20:38:33Z,,False,5,0,3,32,2,2,5,2025-09-06 20:38:31+00:00,49,445,False,True,False,False,False,False,2,4,965,130,88,42,1,3,4.0,4.0,2025-09-04T23:03:20Z,pytorch
162220,closed,Dynamo: set_eval_frame microoptimization,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162220
* #162219
* #161692
* #161634
* #161633
* #161595
* #161591

Optimize for common case and remove a pair of refcount operations (see new comments.)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 22:25:22+00:00,2025-09-09T01:11:19Z,,False,3,0,4,23,14,1,3,2025-09-09 01:10:13+00:00,40,411,False,False,False,False,False,False,1,2,493,30022,18912,11110,1,4,4.0,2.0,2025-09-04T23:02:13Z,pytorch
162219,closed,Overload _get_operation_for_overload_or_packet & friends to accept ArrayRef,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* __->__ #162219
* #161692
* #161634
* #161633
* #161595
* #161591

Avoids requiring vector allocation to call this.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-04 22:25:18+00:00,2025-09-09T01:10:12Z,,False,2,4,4,56,3,3,6,2025-09-09 01:10:12+00:00,75,251,False,False,False,False,False,False,3,1,48,30038,18942,11096,1,4,3.0,1.0,2025-09-05T16:07:50Z,pytorch
162218,closed,[easy] Don't force copy result of getAllOperatorsFor in init.cpp,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162508
* #161695
* #162337
* #162336
* #162298
* #161693
* #161596
* __->__ #162218
* #162220
* #162219
* #161692
* #161634
* #161633
* #161595
* #161591

It returns a const reference to a vector.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-04 22:25:15+00:00,2025-09-10T00:09:23Z,,False,3,0,4,2,2,1,3,2025-09-10 00:08:19+00:00,64,324,False,False,False,False,False,False,1,2,493,30075,18965,11110,1,4,3.0,2.0,2025-09-09T16:38:22Z,pytorch
162217,closed,"re-land triton runtime implementation""",dolpm,"Summary: original pr - https://github.com/pytorch/pytorch/pull/161798

Test Plan:
ci

Rollback Plan:

Differential Revision: D81724234


",2025-09-04 22:12:13+00:00,2025-09-06T00:53:36Z,,False,9,2,1,579,1,13,11,2025-09-06 00:52:33+00:00,38,137,False,False,False,False,False,False,13,2,498,580,579,1,1,1,4.0,2.0,2025-09-04T23:49:15Z,pytorch
162216,closed,support unbacked softmax / logsoftmax,ColinPeppler,"### DDE

```
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(3*u0, 0) (unhinted: Eq(3*u0, 0)).  (Size-like symbols: u0)

Caused by: (_decomp/decompositions.py:1185 in _softmax)
```

```
torch._dynamo.exc.UserError: Could not guard on data-dependent expression Eq(u0, 0) (unhinted: Eq(u0, 0)).  (Size-like symbols: u0)

Caused by: logsoft = torch.nn.functional.log_softmax(nz, dim=0)  # test/inductor/test_unbacked_symints.py:573 in fn (_decomp/decompositions.py:1212 in _log_softmax)
```

```
GuardOnDataDependentSymNode: Could not guard on data-dependent expression Ne(u0, 0) (unhinted: Ne(u0, 0)).  (Size-like symbols: u0)

Caused by: (_refs/__init__.py:2218 in _reduction)
```

### Cannot convert symbols to int
```
  File ""torch/_inductor/lowering.py"", line 7160, in prepare_softmax_online
    and V.graph.sizevars.size_hint(rnumel) >= config.unroll_reductions_threshold
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""orch/_inductor/sizevars.py"", line 591, in size_hint
    return int(out)
           ^^^^^^^^
  File ""sympy/core/expr.py"", line 342, in __int__
    raise TypeError(""Cannot convert symbols to int"")
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162216



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 22:05:02+00:00,2025-09-18T15:44:28Z,,False,6,4,7,32,10,4,10,2025-09-18 15:43:23+00:00,37,1452,False,False,False,False,False,False,4,5,1971,17170,10344,6826,1,7,5.0,5.0,2025-09-05T15:10:29Z,pytorch
162214,closed,[DCP][Quantization] Fix the issue when scale vector is in a different SafeTensors file,saumishr,"Summary: The current dequantization implementation assumes that the weight and scale tenors are in the same SafeTensors files. This diff fixes the issue to support the case when these could be in different files.

Test Plan:
buck test fbcode//caffe2/test/distributed/checkpoint\:test_quantized_hf_storage

Buck UI: https://www.internalfb.com/buck2/532bf151-bb40-41fd-b080-ff898675afe2
Test UI: https://www.internalfb.com/intern/testinfra/testrun/15199648851011082

Rollback Plan:

Differential Revision: D81718598




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 21:55:54+00:00,2025-09-05T22:45:06Z,,False,9,0,1,138,39,2,9,2025-09-05 22:44:02+00:00,86,613,False,True,False,False,False,False,2,2,501,177,138,39,1,1,3.0,3.0,2025-09-04T21:59:32Z,pytorch
162213,open,NCCL added as submodule,Jaiaid,"Using

- * cmake 3.27.2
- * CUDA 12.2
- * pytorch merged main on 2025.09.04 (2.9.0)

From `build` directory
`cmake .. -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc` was failing because `nccl` not found in `third_party` directory after `git submodule update --init --recursive`.

Can be solved in two ways 

1. Use `-DUSE_NCCL=no`
2. Perfom `git submodule add -f https://github.com/NVIDIA/nccl third_party/nccl` (`-f` needed as `third_party/nccl` was already in `.gitignore`). `third_party/nccl` is removed from staging later.

This pull request updated the `.gitmodule`

",2025-09-04 21:39:20+00:00,2025-09-18T14:25:24Z,,False,2,0,6,3,0,1,2,,23,572,False,False,False,False,False,False,1,0,0,3000885,1707899,1292986,2,6,,,,pytorch
162212,closed,remove gso from collapse_view_helper,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162212

",2025-09-04 21:28:21+00:00,2025-09-10T00:18:21Z,,False,6,22,9,68,36,3,28,2025-09-10 00:17:17+00:00,36,94,False,False,False,False,False,False,3,5,1380,38192,25686,12506,1,9,3.0,5.0,2025-09-05T22:11:35Z,pytorch
162211,closed,minimize graph capture output,avikchaudhuri,"Currently OutputGraphGuardsState is separated out as a serializable interface for OutputGraph, but some of the typing around it is incorrect in dynamo's guards.py and output_graph.py: more fields are used by code than claimed by OutputGraphGuardsState, and it works because either the full OutputGraph is passed in or the parts that use those fields are dead when OutputGraphGuardsState is passed in.
In this PR we try to further separate the necessary fields of OutputGraph that should be retained by a full graph capture mechanism, not just limited to dynamo (as it is currently) but also something like make_fx (in the future). Since these fields do not need to be serialized, the result is an intermediate ""common"" data structure that is between OutputGraphGuardsState and OutputGraph in the inheritance hierarchy.

Differential Revision: D81718791


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 21:24:41+00:00,2025-09-20T15:53:35Z,,False,25,14,1,212,63,8,39,2025-09-20 15:52:32+00:00,29,1025,False,False,False,False,False,False,8,2,493,275,212,63,1,1,4.0,2.0,2025-09-12T17:05:13Z,pytorch
162210,closed,Import SVE128 Changes into custom branch,Nicoshev,"Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 21:03:32+00:00,2025-09-04T21:04:00Z,2025-09-04T21:03:47Z,True,2,0,10,5344,430,48,2,2025-09-04 21:03:47+00:00,40,279,False,True,False,False,False,False,48,0,0,6426,5670,756,1,10,,,,pytorch
162209,closed,MXFP8 grouped GEMM support for torch._scaled_grouped_mm + submodule bump,danielvegamyhre,"## Summary
- We just landed 2d-2d support for mxfp8 grouped gemm in FBGEMM: https://github.com/pytorch/FBGEMM/pull/4816 
- This is needed for backward pass of mxfp8 MoE training with grouped gemms
- Changes:
    - Add dispatching + input validation for mxfp8 grouped gemm in `torch._scaled_grouped_mm`
    - Add meta registration input validation for mxfp8 grouped gemm, for composability with compile
    - Add unit tests exercising torch._scaled_grouped_mm with mxfp8 inputs
    - Bump FBGEMM third party submodule to include:
          - https://github.com/pytorch/FBGEMM/pull/4816 
          - https://github.com/pytorch/FBGEMM/pull/4820
          - https://github.com/pytorch/FBGEMM/pull/4821
          - https://github.com/pytorch/FBGEMM/pull/4823

#### How fbgemm dependency was bumped
Documenting this since I haven't found it documented elsewhere:
- `cd ~/pytorch/third_party/fbgemm`
- `git fetch`
- `git checkout <hash>`
- `cd ~/pytorch`
- `git add third_party/fbgemm`

## Test plan

#### Test build 
```
USE_FBGEMM_GENAI=1 python -m pip install --no-build-isolation -v -e .
...
Successfully installed torch-2.9.0a0+gitf5070f3
```
[full build log](https://www.internalfb.com/phabricator/paste/view/P1933787581)

#### Unit tests
```
pytest test/test_matmul_cuda.py -k test_mxfp8_scaled_grouped_mm_
...

test/test_matmul_cuda.py .........                                                                                                                        [100%]

============================================================== 9 passed, 1668 deselected in 5.34s ===============================================================
```
",2025-09-04 21:01:26+00:00,2025-09-16T22:31:08Z,,False,14,31,26,534,73,9,45,2025-09-06 15:25:34+00:00,72,1640,False,False,False,True,False,False,9,13,6816,2115,1288,827,1,26,8.0,15.0,2025-09-04T21:26:50Z,pytorch
162208,closed,[Dynamo] Don't guard data ptrs by default with mark_static_address,mlazos,"Fixes https://github.com/pytorch/pytorch/issues/156377

Since we now re-record cudagraphs, it's not necessary to guard by default anymore and induce a full recompile. 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-04 20:56:09+00:00,2025-09-12T07:16:18Z,,False,8,0,1,11,10,3,8,2025-09-12 07:15:14+00:00,66,384,False,True,False,False,False,False,3,6,1079,21,11,10,1,1,4.0,6.0,2025-09-08T19:58:23Z,pytorch
162207,closed,[Graph Partition] interface for custom cg wrapper,BoyuanFeng,"This PR adds an interface to allow users to specify custom cudagraph wrapper. User example: [vllm](https://github.com/vllm-project/vllm/pull/24281)


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-04 20:41:57+00:00,2025-09-19T18:44:35Z,,False,4,5,11,71,3,3,9,2025-09-06 03:13:07+00:00,49,359,False,False,False,False,False,False,3,3,957,13692,9805,3887,1,11,5.0,5.0,2025-09-05T13:29:21Z,pytorch
162206,closed,Use same NVSHMEM version across CUDA builds,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162206

#161321 bumped NVSHMEM version to 3.3.24 for CUDA 13, leaving CUDA 12 with 3.3.20.
This PR bumps the NVSHMEM version to 3.3.24 for CUDA 12 as well.",2025-09-04 20:31:33+00:00,2025-09-12T16:28:44Z,,False,27,0,3,31,31,4,27,2025-09-09 20:59:52+00:00,43,241,False,False,False,False,False,False,4,26,6396,25072,16568,8504,1,3,5.0,26.0,2025-09-04T20:35:02Z,pytorch
162205,open,Enable Vulkan backend support for more than Android,ericcurtin,"So we can build pytorch and Vulkan for platforms like non-Android Linux

",2025-09-04 20:30:09+00:00,2025-09-18T14:19:50Z,,False,2,0,1,49,1,6,2,,51,73,False,False,False,False,False,False,6,0,42,50,49,1,1,1,1.0,1.0,2025-09-18T14:19:39Z,pytorch
162204,closed,[dynamo] Add support for const prop on .item,angelayi,"Fixes some of the errors in https://fb.workplace.com/groups/1028545332188949/permalink/1303030824740397/

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 20:25:57+00:00,2025-09-05T00:29:55Z,,False,4,2,1,23,6,2,6,2025-09-05 00:28:52+00:00,44,276,False,True,False,False,False,False,2,2,493,29,23,6,1,1,3.0,2.0,2025-09-04T21:56:50Z,pytorch
162203,open,Update round size with 1 division behavior,morrison-turnansky,"have round size return nearest power of 2 greater than or equal to size with 1 division

Fixes #161139


cc @ptrblck @msaroufim @eqy @jerryzh168",2025-09-04 20:20:36+00:00,2025-09-22T15:20:47Z,,False,4,0,1,2,0,1,4,,42,144,False,True,False,False,False,False,1,3,175,2,2,0,1,1,1.0,3.0,2025-09-04T20:21:24Z,pytorch
162202,closed,[DCP][Quantization] Fix for FP8 multiplication during dequantization,saumishr,"Summary:
Weight vector needs to be upcasted since some FP8 formats (like Float8_e4m3fn) don't have CPU implementations in PyTorch. Reference: https://docs.pytorch.org/docs/stable/tensors.html#id13

We will use FP32 for the scale vector multiplication and convert to the target dtype. 

Upcasting helps with the following:

1.  **Full CPU support**: `float32` has complete CPU kernel implementations for all operations
2.  **Numerical stability**: `float32` provides more precision during intermediate calculations
3.  **Compatibility**: Works across all devices (CPU/GPU) and PyTorch versions

Test Plan:
UTs

Rollback Plan:

Differential Revision: D81711093




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 20:15:38+00:00,2025-09-05T16:07:29Z,,False,5,4,1,7,2,1,9,2025-09-05 16:06:25+00:00,68,758,False,True,False,True,False,False,1,2,494,9,7,2,1,1,4.0,2.0,2025-09-04T20:18:21Z,pytorch
162201,closed,Stash and restore tls state for AC,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162201



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 19:28:13+00:00,2025-09-04T19:39:55Z,,False,2,0,1,73,11,5,2,2025-09-04 19:39:55+00:00,34,297,False,False,False,False,False,False,5,0,0,84,73,11,1,1,,,,pytorch
162200,closed,[triton][export] serialization in internal path + unit tests,dolpm,"Summary: will package triton artifacts to be runnable in nativert if wrappers exist.

Test Plan:
unit tests

Rollback Plan:

Differential Revision: D81368559


",2025-09-04 19:16:27+00:00,2025-09-10T09:50:16Z,,False,9,0,1,61,19,2,9,2025-09-10 09:49:13+00:00,60,160,False,False,False,False,False,False,2,1,476,80,61,19,1,1,2.0,1.0,2025-09-04T22:36:13Z,pytorch
162195,closed,Upgrades dlpack to v1.1 to include fp8/fp4,syed-ahmed,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162195

",2025-09-04 18:29:03+00:00,2025-09-17T16:40:20Z,,False,7,6,15,176,22,3,13,2025-09-17 16:39:14+00:00,42,94,False,False,False,False,False,False,3,6,1538,45070,31939,13131,1,15,7.0,8.0,2025-09-07T17:21:03Z,pytorch
162194,closed,[dynamic shapes] DynamicInts prototype,pianpwk,"Initial prototype for dynamic int inputs, allows users to run with `torch.compile(f)(DynamicInt(4))`, compiling dynamically and using the underlying hint at runtime.

Current behavior:
- Also works in eager (mostly by subclassing int), as scalar input to torch functions, or numpy/math/etc. For example, `x = DynamicInt(3); torch.randn(x); torch.add(y, z, alpha=x); np.arange(x)` all act as if x = 3.
- Behavior for arithmetic ops is to return new DynamicInts rather than static ints; `DynamicInt(3) * 2 = DynamicInt(6)`. This is via SymNode magic methods, but coverage might not be 100% - for example, I had to explicitly override floordiv to avoid int casting. This is not necessarily the case for non-magic method ops (e.g. `math.cos(x)`). The alternative here is to int cast on all operations, but I opted for this for dynamism propagation in non-compiled regions.
- Doesn't ban fullgraph=False; DynamicInt objects might be leaked back to the user, but I guess this is fine, because they can be casted to ints when needed?
- Dynamo only allocates one symbol per DynamicInt; specifying the same DynamicInt for multiple inputs leads to input deduplication, and a guard installed.
- We don't raise on int specialization (in allowlist/maybe_mark_dynamic style) - but an easy change if needed.
- DynamicInts as nn.Module attributes are handled.
- We don't guard on the DynamicInt id, e.g. users can do the following without recompiling (maybe we should guard?)
```python
x = DynamicInt(4)
f(x)
f(1)
f(DynamicInt(3))  # same as f(3)
```


Follow-up work:
- Specifying shape constraints, either at the int-level, e.g.
```python
DynamicInt(64, name=""s0"", constraints=[""s0 % 32 == 0"", ""s0 <= 1024""]
```
or at the compilation level, e.g. something like
```python
s0 = DynamicInt(64, name=""s0"")
s1 = DynamicInt(128, name=""s1"")
with some_compiler_config.dynamic_int_constraints([""s1 == 2*s0"", ""s0 % 32 == 0""]):
    f(s0, s1)
```
This should subsume the need for specifying derived SymInts?
- SymFloat support - currently it seems backed floats are specialized by the tensorify float pass, and there's no handling in inductor.
- Propagating dynamism in tensor constructors, e.g. `x = DynamicInt(4); torch.randn(x)` could annotate `_dynamo_dynamic_indices`.


Differential Revision: D81698719

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 18:13:06+00:00,2025-09-18T23:27:36Z,,False,32,13,1,293,15,11,45,2025-09-18 23:26:32+00:00,38,2477,False,False,False,False,False,False,11,6,4218,308,293,15,1,1,5.0,6.0,2025-09-04T20:22:59Z,pytorch
162193,closed,[SymmMem] Feed tensor.data_ptr instead of handle.buffer_ptr into kernels,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162193

After MemPool support, `get_buffer_ptrs` points to base address of allocation segment.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 18:10:24+00:00,2025-09-04T21:27:11Z,,False,4,2,2,23,17,1,6,2025-09-04 21:26:07+00:00,72,277,False,False,False,False,False,False,1,3,595,56,31,25,1,2,4.0,4.0,2025-09-04T18:11:28Z,pytorch
162192,closed,Add contiguous subgraph transformation threshold,exclamaforte,cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos,2025-09-04 18:02:27+00:00,2025-09-06T03:04:29Z,,False,8,0,1,6,3,2,8,2025-09-06 02:48:03+00:00,48,209,False,False,False,False,False,False,2,5,1391,9,6,3,1,1,3.0,5.0,2025-09-05T23:09:13Z,pytorch
162191,open,Compile AOTI to windows target,yushangdi,"Mostly from https://github.com/pytorch/pytorch/pull/159794 


- use `reinterpret_cast<std::uintptr_t>` instead of `long` to be compatible with windows cross-compilation. On a 64-bit system, void* is typically 64 bits, but long int may be only 32 bits (especially on Windows, where long is 32 bits even on 64-bit systems).

Example CMakeLists.txt:
```

cmake_minimum_required(VERSION 3.27 FATAL_ERROR)
project(model LANGUAGES CXX)
set(CMAKE_CXX_STANDARD 17)

# Set a library target
add_library(model SHARED)


# Add macro definitions
target_compile_definitions(model PRIVATE NOMINMAX TORCH_INDUCTOR_CPP_WRAPPER STANDALONE_TORCH_HEADER  C10_USING_CUSTOM_GENERATED_MACROS  USE_CUDA)

# Add compile flags
target_compile_options(model PRIVATE /O2 /DLL /MD /std:c++20 /wd4819 /wd4251 /wd4244 /wd4267 /wd4275 /wd4018 /wd4190 /wd4624 /wd4067 /wd4068 /EHsc /Zc:__cplusplus /permissive- /openmp /openmp:experimental )

# Backend-specific flags
target_compile_options(model PRIVATE   -c)


enable_language(CUDA)
set(CMAKE_CUDA_STANDARD 17)
find_package(CUDAToolkit REQUIRED)

# Make output use .pyd instead of .dll
set_target_properties(model PROPERTIES SUFFIX "".pyd"" LINK_FLAGS ""/DEF:${CMAKE_CURRENT_SOURCE_DIR}/windows_symbol_exports.def"" )

set(KERNEL_TARGETS """")
set(KERNEL_OBJECT_FILES """")
# Function to compile ptx to cubin
function(embed_gpu_kernel KERNEL_NAME PTX_FILE)
    set(CUBIN_BASENAME ${KERNEL_NAME}.cubin)
    set(CUBIN_FILE ${CMAKE_CURRENT_BINARY_DIR}/${CUBIN_BASENAME})
    # --- PTX to FATBIN Command & Target ---
    add_custom_command(
        OUTPUT ${CUBIN_FILE}
        COMMAND ${CUDAToolkit_NVCC_EXECUTABLE} --cubin ${PTX_FILE}
                -o ${CUBIN_FILE} ${NVCC_GENCODE_FLAGS}
                -gencode arch=compute_80,code=sm_80
        DEPENDS ${PTX_FILE}
    )

    add_custom_target(build_kernel_object_${KERNEL_NAME} DEPENDS ${CUBIN_FILE})
    set(KERNEL_TARGETS ${KERNEL_TARGETS} build_kernel_object_${KERNEL_NAME} PARENT_SCOPE)

endfunction()
target_sources(model PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/model.wrapper.cpp)
target_sources(model PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/model_consts.weights.cpp)

embed_gpu_kernel(triton_mm ${CMAKE_CURRENT_SOURCE_DIR}/triton_mm.ptx)

embed_gpu_kernel(model_triton_tem_fused_addmm_relu_t_0 ${CMAKE_CURRENT_SOURCE_DIR}/model_triton_tem_fused_addmm_relu_t_0.ptx)

embed_gpu_kernel(model_triton_tem_fused_addmm_relu_sigmoid_t_1 ${CMAKE_CURRENT_SOURCE_DIR}/model_triton_tem_fused_addmm_relu_sigmoid_t_1.ptx)
add_dependencies(model ${KERNEL_TARGETS})
target_link_libraries(model PRIVATE ${KERNEL_OBJECT_FILES})

# Add linker flags
target_link_options(model PRIVATE )

# Add libraries
target_link_libraries(model PRIVATE CUDA::cudart cuda)
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 17:58:03+00:00,2025-09-17T03:48:09Z,,False,1,0,1,516,182,12,1,,30,2906,False,True,False,False,False,False,12,0,0,698,516,182,1,1,,,,pytorch
162190,closed,Fix persistent buffer bug,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162682
* #162559
* #162558
* #162557
* #162556
* #162487
* __->__ #162190

For non-persistent buffers, we should properly register them. 

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela
",2025-09-04 17:57:40+00:00,2025-09-11T14:57:33Z,,False,6,0,8,1,9,3,6,2025-09-11 14:56:29+00:00,25,412,False,True,False,False,False,False,3,5,1265,31392,21037,10355,1,6,3.0,5.0,2025-09-05T17:15:24Z,pytorch
162189,closed,"[refactor] add helper sizevars function, is_size_one, for size==1 checks",ColinPeppler,"## Summary
- document guard behavior in `SizeVarAllocator.is_size_one`
- use `is_size_one` for broadcast/expand checks.
- This diff is a no-op since we'd use `shape_env.evaluate_expr(... fallback_value=False)`

https://github.com/pytorch/pytorch/blob/a4f9132a178ce8c41b17ff6e89aa382ec1dbe701/torch/_inductor/sizevars.py#L450-L453

------
https://chatgpt.com/codex/tasks/task_e_68b8d0d1f2c48328b2d38c00e738bc8c

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 17:55:15+00:00,2025-09-08T22:49:25Z,,False,18,10,5,15,17,3,28,2025-09-08 22:48:20+00:00,72,612,False,False,False,True,False,True,3,17,3711,58,28,30,1,5,3.0,18.0,2025-09-04T19:28:52Z,pytorch
162187,closed,[Inductor][B200] Bump max-autotune test threshold for `bfloat16` on B200,eqy,"to unblock https://github.com/pytorch/pytorch/pull/159494

cuBLAS tests looked ok...


cc @ptrblck @msaroufim @jerryzh168",2025-09-04 17:45:19+00:00,2025-09-04T22:33:52Z,,False,2,3,1,1,1,1,5,2025-09-04 22:33:52+00:00,72,121,False,False,False,False,False,False,1,1,114,2,1,1,1,1,3.0,1.0,2025-09-04T18:02:47Z,pytorch
162186,open,[CUDA][CUDAGraph] Reduce capture overhead in CUDA Graph memory reuse,eee4017,"Previous work #158352 delivered CUDAGraph memory footprint reduction with no replay-time impact, but capture time regressed (up to 20× slower) due to repeated full-graph traversals. See previous benchmark results [here](https://github.com/pytorch/pytorch/pull/158352#issuecomment-3215947565)

This PR removes capture/reply overhead while preserving the memory savings:

1. **Terminals as free markers**
   We stop inserting empty nodes and instead record the current stream terminals as free markers. This avoids mutating the user’s graph and keeps semantics unchanged.

2. **Incremental, cached reachability**
   We add a **per-graph reuse context** that caches reverse-traversal state:

   * `graph_reuse_context[graph].visited[stream]` tracks nodes already seen from that stream’s terminal frontier.
   * On each allocation during capture, we resume traversal from the latest terminals and only visit unseen nodes.
   * A block is freed when all its recorded markers are in the visited set of its allocation stream—i.e., all markers are proven predecessors of future work.

See [the performance results here](https://docs.google.com/spreadsheets/d/e/2PACX-1vRPvdd9Xa8W87ixbiA0da_qvOhrUAjUpFz0G-_j-MsDnoeRyhEa4_ut_W3rqcg1VVZVFJ-gucwov-3b/pubhtml?gid=1468302443&single=true), we sweep synthetic multi-stream CUDA Graphs built by `capture_benchmark.py` (same as before, we generate random interleaving of alloc/free/join with given probabilities, see [gist here](https://gist.github.com/eee4017/e2092d215b1d4bd46534148939af39e3)), and we compare median capture/replay times and memory. On an NVIDIA H100 PCIe across 24 configs, the optimization preserves reserved memory reduction at ~24–98%, leaves allocated memory unchanged, and brings capture time back to baseline (range 0.96–1.04× vs. baseline) with replay time unchanged (range 0.97–1.11×).

cc @ptrblck @msaroufim @eqy @jerryzh168 @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng",2025-09-04 17:26:46+00:00,2025-09-25T16:15:59Z,,False,1,20,5,153,150,2,21,,68,1939,False,False,False,True,False,False,2,0,0,395,199,196,1,5,3.0,0.0,2025-09-09T16:01:18Z,pytorch
162185,closed,[B200][NVFP4] Fix argument passing in `test_blockwise_mxfp8_nvfp4_mxfp4_numerics_`,eqy,"to unblock https://github.com/pytorch/pytorch/pull/159494

cc @ptrblck @msaroufim @jerryzh168",2025-09-04 17:21:43+00:00,2025-09-05T01:26:05Z,,False,3,0,1,2,2,1,3,2025-09-05 01:25:03+00:00,82,93,False,True,False,False,False,False,1,2,493,4,2,2,1,1,4.0,2.0,2025-09-04T17:30:52Z,pytorch
162184,open,testing infra and some fixes,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162184
* #162183
* #162167



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 17:11:27+00:00,2025-09-04T21:32:44Z,,False,1,0,1,447,80,6,1,,28,309,False,True,False,False,False,False,6,0,0,527,447,80,1,1,,,,pytorch
162183,closed,testing infra and some fixes,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162183

This PR is quite large in that it covers most of rough edges in the new strict export flow:

1. Handle nn_module_stack correctly now that we are tracing wrapper module
2. module_call_spec needs to get queried from source directly because we are not running the bytecode anymore.
3. Correct input and output handling.

@diff-train-skip-merge

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela
",2025-09-04 17:11:22+00:00,2025-09-10T20:49:19Z,,False,30,1,10,549,86,12,31,2025-09-10 20:48:15+00:00,28,607,False,True,False,False,False,False,12,27,7790,32156,20457,11699,1,10,4.0,27.0,2025-09-05T17:15:19Z,pytorch
162181,closed,[BE]: Update cpp-httplib submodule to 0.26.0,Skylion007,"Update cpp-httplib with better error handling, bugfixes, and performance. Header only library update.",2025-09-04 17:07:06+00:00,2025-09-04T19:00:38Z,,False,3,0,1,1,1,1,3,2025-09-04 18:59:36+00:00,44,101,False,True,False,False,False,False,1,2,493,2,1,1,1,1,3.0,2.0,2025-09-04T17:18:20Z,pytorch
162180,closed,[B200][MXFP8] Fix regex in `test_blockwise_mxfp8_nvfp4_error_messages_recipe_mxfp8_cuda`,eqy,"to unblock https://github.com/pytorch/pytorch/pull/159494

cc @ptrblck @msaroufim @jerryzh168",2025-09-04 17:06:16+00:00,2025-09-04T23:32:59Z,,False,3,0,2,8,11,1,3,2025-09-04 23:29:15+00:00,88,93,False,True,False,False,False,False,1,2,493,21,9,12,1,2,4.0,2.0,2025-09-04T17:21:11Z,pytorch
162177,open,Fix comment on broadcasting example to clarify dimension mismatch,dsashidh,"Fixes #162116 

Updated the comment in the broadcasting example to clarify that tensors with mismatched dimension sizes (0 vs 2) are not broadcastable. Removed incorrect reference to missing dimensions.
",2025-09-04 16:44:06+00:00,2025-09-12T19:19:47Z,,False,11,0,1,2,2,1,11,,65,203,False,True,False,False,False,False,1,10,3416,4,2,2,1,1,3.0,10.0,2025-09-04T16:44:17Z,pytorch
162176,closed,AMD CPU CI - Add freezing + fix label trigger,amukho,"Added the following changes:

1. Added freezing by default for AMD CPU based CI (to follow pattern introduced by https://github.com/pytorch/pytorch/pull/152298 )
2. Fixed issue with label based CI triggers

Addresses code review comment in https://github.com/pytorch/pytorch/pull/161155
",2025-09-04 16:42:22+00:00,2025-09-15T19:30:43Z,,False,21,0,1,7,3,1,21,2025-09-15 19:29:39+00:00,45,287,False,True,False,False,False,False,1,20,5080,10,7,3,1,1,6.0,20.0,2025-09-04T16:45:07Z,pytorch
162173,closed,Remove extra load from already persistent inner reductions,PaulZhang12,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162173



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 15:52:43+00:00,2025-09-05T20:35:56Z,,False,1,0,1,27,19,2,1,2025-09-05 20:35:56+00:00,58,297,False,False,False,False,False,False,2,0,0,46,27,19,1,1,,,,pytorch
162172,closed,[export] Fix torch.export.load with storage offset,yiming0416,"Summary: As titled

Test Plan:
CI

Rollback Plan:

Differential Revision: D81687701


",2025-09-04 15:45:55+00:00,2025-09-04T22:51:39Z,,False,6,0,1,23,3,2,6,2025-09-04 22:50:35+00:00,50,86,False,True,False,False,False,False,2,1,524,26,23,3,1,1,2.0,2.0,2025-09-04T15:52:32Z,pytorch
162171,closed,[Precompile] [RFC] Implement aot_compile_module,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162527
* __->__ #162171

This PR adds a new interface _aot_compile to `OptimizedModule`, so that the following is possible: 


```
mod = SimpleLinearModule()
inputs = [
            ModelInput(
                args=(torch.randn(3, 3),),
                kwargs={},
                contexts=[torch.no_grad(), eval_mode(model)],
            ),
            ModelInput(
                args=(torch.randn(3, 3),), kwargs={}, contexts=[train_mode(model)]
            ),
        ]
        assert isinstance(model, torch._dynamo.eval_frame.OptimizedModule)
        model._aot_compile(
            inputs,
        )
```

After this PR, you can AOT precompile NanoGPT and use it to train directly. I'll share my fork of the repo to make this work. 

## ModelInput
The `ModelInput` API is a work in progress; for now it represents a set of inputs and contexts to instruct the compiler to compile. Most commonly, this is ""compile an eval mode with no grad, and  a training mode with grad"", but also contains things like autocasting contexts, etc. 

## Dispatch 
Dispatching is super simple here, we just iterate through all the precompiled fullgraphs and check guards for each one until there's one htat passes. I'm a bit worried that having this in python code is going to be too expensive. The guard checks are happening in C++ anyway, though, so the only python bottlenecked step here is just the for loop, so perhaps the overhead will not be high. I'll work on measuring this, though. 

## TODOs

This PR does not support `mod.compile()`, only `torch.compile(mod)`. In order to support `mod.compile()`, we'll need to update torch.nn.Module with an updated implementation — I can add that frontend later. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 15:01:40+00:00,2025-09-14T23:33:34Z,,False,23,4,8,271,39,5,27,2025-09-14 23:32:31+00:00,47,1945,False,False,False,False,False,False,5,22,7895,97650,59709,37941,1,8,4.0,23.0,2025-09-06T02:22:44Z,pytorch
162170,closed,Add BundledAOTAutogradSerializableCallable,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162171
* __->__ #162170
* #162169

This PR hooks up the python wrapper inductor backend to aot_compile. This is *not* the best way for us to grab the output of AOTAutograd; that involves a refactor to make AOTAutograd itself return a serializable callable. I'll do that refactor soon, but I want a basic interface to test with for now. 

In the medium term, we'll want aot_compile to call AOTAutograd directly, instead of using the TorchInductorWrapper's callback through compile_fx. 




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 15:01:34+00:00,2025-09-09T05:43:26Z,,False,20,0,5,95,1,2,20,2025-09-09 05:42:21+00:00,42,738,False,False,False,False,False,True,2,18,5178,71734,39358,32376,1,5,4.0,18.0,2025-09-04T15:56:12Z,pytorch
162169,closed,[easy] [precompile] Convert CompileArtifacts to callable,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162171
* #162170
* __->__ #162169

The goal of this PR stack is to be able to implement `aot_compile_module`, which AOT precompiles a torch.nn.Module. 
Step 1 is a simple refactor to make CompileArtifacts itself the callable, which makes it easier to use directly. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 15:01:30+00:00,2025-09-07T23:37:36Z,,False,6,4,5,48,32,2,10,2025-09-07 23:37:34+00:00,56,516,False,False,False,False,False,True,2,5,252,71678,39285,32393,1,5,4.0,5.0,2025-09-04T15:47:02Z,pytorch
162168,closed,Fix the regression issue caused by non-arrch64 platforms not hitting the MKLDNN path.,Yuxingwang-intel,"This issue was introduced by the commit in issue #161065. Added an extra check to provide a proper path for other platforms.
",2025-09-04 14:29:48+00:00,2025-09-12T00:25:24Z,,False,22,7,6,4,6,1,29,2025-09-12 00:17:12+00:00,85,125,False,True,False,False,False,False,1,19,5119,88,43,45,2,6,7.0,19.0,2025-09-08T03:21:13Z,pytorch
162167,closed,New export implementation with flat inp/out,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162190
* #162183
* __->__ #162167


This is my first attempt of building new export API. The main thing it addresses is correctly getting input and output relations. Subsequent diffs willl add functionality for dynamic shapes, nn_module_stack etc.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81793205](https://our.internmc.facebook.com/intern/diff/D81793205)",2025-09-04 14:23:28+00:00,2025-09-08T18:34:33Z,,False,9,0,3,387,87,8,9,2025-09-06 20:03:55+00:00,43,592,False,False,False,False,False,False,8,6,1132,500,400,100,1,3,4.0,6.0,2025-09-04T18:00:02Z,pytorch
162166,open,Fix retracing to always run the check input constraints,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162166
* #162089
* #162013
* #161977
* #161653



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 13:56:12+00:00,2025-09-04T17:20:39Z,,False,2,0,1,6,1,2,2,,55,306,False,True,False,False,False,False,2,0,0,7,6,1,1,1,,,,pytorch
162165,open,Add ReduceOp.unbox and fix use-after-free in ReduceOp pybind,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162165

Analogous to Work.unbox and ProcessGroup.unbox.

Fixes https://github.com/pytorch/pytorch/issues/162668

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @dcci",2025-09-04 13:49:41+00:00,2025-09-15T16:02:56Z,,False,13,2,5,48,3,2,15,,60,350,False,True,False,False,False,False,2,12,4990,84059,49731,34328,1,5,6.0,13.0,2025-09-04T16:12:46Z,pytorch
162164,closed,Don't require FakeStore to be passed into fake backend,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162164

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @msaroufim @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 13:41:46+00:00,2025-09-04T16:44:57Z,,False,3,2,1,14,27,6,5,2025-09-04 16:43:52+00:00,54,422,False,False,False,False,False,False,6,2,701,41,14,27,1,1,5.0,3.0,2025-09-04T13:48:51Z,pytorch
162163,closed,A basic CLAUDE.md based on bad things I see claude code doing,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162163

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-04 13:26:32+00:00,2025-09-05T14:53:44Z,,False,3,0,1,15,0,1,3,2025-09-05 14:52:39+00:00,61,150,False,False,False,False,False,False,1,2,801,15,15,0,1,1,4.0,2.0,2025-09-04T14:17:44Z,pytorch
162162,closed,Forward fix for user defined triton kernel grid calc,nandesuka,"Summary:

This change fixes the test: inductor:fxir_backend - test_custom_triton_autotune_dynamic which was broken by https://github.com/pytorch/pytorch/pull/160997

Test Plan:
inductor:fxir_backend - test_custom_triton_autotune_dynamic

Rollback Plan:

Differential Revision: D81679217




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 13:06:07+00:00,2025-09-04T22:51:26Z,,False,7,0,1,1,0,1,7,2025-09-04 22:51:26+00:00,52,492,False,True,False,False,False,False,1,3,698,1,1,0,1,1,4.0,4.0,2025-09-04T13:07:56Z,pytorch
162159,closed,clean up extra declaration in aten/src/ATen/native/Blas.cpp,vkuzo,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162159
* #162059
* #161717
* #161407

Summary:

Implementing @drisspg's suggestion from
https://github.com/pytorch/pytorch/pull/161717/files

Test Plan: CI

Reviewers:

Subscribers:

Tasks:

Tags:",2025-09-04 12:25:14+00:00,2025-09-18T20:10:55Z,,False,1,0,1,1,8,1,1,2025-09-18 20:10:55+00:00,59,282,False,False,False,False,False,False,1,0,0,9,1,8,1,1,1.0,0.0,2025-09-04T17:22:15Z,pytorch
162158,closed,Fixed comment to match logic in distributed_c10d.py,Codeboi007," inconsistent with the logic introduced in #162157  and modified in #142216.This update ensures the documentation matches the actual behavior of the code.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 12:09:49+00:00,2025-09-06T13:32:41Z,,False,4,2,2,3,3,1,6,2025-09-06 05:37:52+00:00,51,251,False,True,False,True,False,False,1,2,493,8,4,4,2,2,4.0,2.0,2025-09-04T16:18:09Z,pytorch
162155,closed,[WIP] Add AMP Integration guide for accelerators,zeshengzong,,2025-09-04 09:16:15+00:00,2025-09-08T08:31:59Z,,False,1,0,2,59,0,1,1,2025-09-08 08:31:59+00:00,48,0,False,False,False,False,False,False,1,0,0,75,67,8,1,2,,,,pytorch
162153,closed,Actually run aot_inductor_perf_smoketest on MPS,huydhn,A bug coming from https://github.com/pytorch/pytorch/pull/160741,2025-09-04 08:12:14+00:00,2025-09-10T01:00:58Z,,False,4,0,1,2,2,1,4,2025-09-10 01:00:58+00:00,47,64,False,True,False,False,False,False,1,3,830,4,2,2,1,1,2.0,3.0,2025-09-04T14:25:52Z,pytorch
162150,open,[SymmMem] Skip multicast init if any CUDA call fails,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162150

Sometimes [NV Fabric Manager can be corrupted](https://github.com/pytorch/pytorch/issues/150852#issuecomment-2795766064), causing cuMulticast* calls to error out with ""invalid arguments.""

This PR improves the skip of multicast binding when error occurs. Also added the skip path for `CUmemFabricHandle` case.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim

Differential Revision: [D81693280](https://our.internmc.facebook.com/intern/diff/D81693280)",2025-09-04 07:27:26+00:00,2025-09-04T17:10:39Z,,False,2,2,1,83,48,2,4,,52,593,False,False,False,False,True,False,2,1,310,131,83,48,1,1,3.0,2.0,2025-09-04T16:57:13Z,pytorch
162149,closed,[not for land] cache can-fuse,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162149



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 07:17:14+00:00,2025-09-19T23:34:06Z,,False,2,0,1,8,1,1,2,2025-09-19 23:34:06+00:00,29,297,False,False,False,False,False,False,1,0,0,9,8,1,1,1,,,,pytorch
162148,closed,Add api info for torch._C._nn.pyi,orangeH25,"Fix part of #148404 

APis involved are as followed:

- cross_entropy_loss
- hardsigmoid_
- hardswish
- hardswish_
- huber_loss",2025-09-04 07:16:56+00:00,2025-09-06T05:22:45Z,,False,7,0,3,53,0,1,7,2025-09-06 05:21:42+00:00,33,127,False,True,False,False,False,False,1,6,1140,770,670,100,1,3,3.0,6.0,2025-09-04T11:19:48Z,pytorch
162144,closed,[PT2]: Overriding Tensor device by SubmodNameToDevice,kqfu,"Summary:
A temporarily solution mainly for weights that are not moved to cuda in fake mode during publishing, but runs on cuda in serving.

This has some overlap with placement, but with 2 differences:
1. OverrideWeightsDevice only changes weights, not graph.
2. Placement only handles mapping between non-empty cuda indices, while here we override everything as submodNameToDevice is the ground truth.

Test Plan:
ICE replayer with custom package:
https://www.internalfb.com/intern/unidash/dashboard/ads_infra_cost_estimation/model_infra_cost_estimation/?e[select_ESTIMATION_RUN_ID]=ICE_kevinqfu_1756939411c164_replayeripnext_00

Rollback Plan:

Differential Revision: D81284723


",2025-09-04 06:35:22+00:00,2025-09-16T06:57:12Z,,False,14,0,1,40,0,3,14,2025-09-16 06:56:09+00:00,53,682,False,False,False,False,False,False,3,2,518,40,40,0,1,1,4.0,2.0,2025-09-11T18:20:57Z,pytorch
162142,closed,[SymmMEM] Allow to import _SymmetricMemory when NVSHMEM is not available,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162142

Summary:
As we have multiple backends, _SymmetricMemory should not be imported together with NVSHMEM related modules

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 06:24:26+00:00,2025-09-10T04:27:03Z,,False,8,2,3,9,2,1,10,2025-09-09 05:37:45+00:00,72,300,False,False,False,False,False,False,1,7,2163,29188,19072,10116,1,3,7.0,7.0,2025-09-04T06:53:13Z,pytorch
162138,closed,[Intel GPU][FlexAttention] Enable TMA path on Intel GPU,hoshibara,"The existing `can_use_tma` has some conditions that are unnecessary for Intel GPUs.
We have removed these useless conditions on the Intel GPU path.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 04:40:06+00:00,2025-09-05T16:55:56Z,,False,12,2,6,22,2,2,14,2025-09-05 16:54:53+00:00,55,350,False,False,False,False,False,False,2,8,1719,3851,3016,835,1,6,6.0,8.0,2025-09-04T04:40:34Z,pytorch
162137,closed,weakref const nested args,dolpm,"Summary:
this should get rid of the fetching-by-attributes

Test Plan:
 pytest test/dynamo/test_nested_const_weight_weakref.py

Should fix #141452


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-04 03:56:45+00:00,2025-09-04T19:51:34Z,,False,1,0,1,64,0,2,1,2025-09-04 19:51:34+00:00,25,319,False,True,False,False,False,False,2,0,0,64,64,0,1,1,,,,pytorch
162136,closed,[CD] Build Mac wheels using `setup-python` action,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162136


Biggest difference between both conda and homebrew CPython builds and one from python.org, is that later are universal binaries and they are always trying to build universal extension...

Workaround lots of universal binary build attempts by explicitly specifying both `_PYTHON_PLATFORM` and `--plat-name` as well as `ARCH_FLAGS`

Suppressed actionlint warning on use of `freethreaded` flag which is document in https://github.com/actions/setup-python/tree/v5

TODO: Remove lots of temporary workarounds when `3.14` is out in October 2025
",2025-09-04 03:52:35+00:00,2025-09-12T00:17:38Z,,False,4,12,21,94,344,5,16,2025-09-12 00:16:34+00:00,49,634,False,False,False,True,False,False,5,3,901,16446,8526,7920,1,21,5.0,4.0,2025-09-06T07:41:49Z,pytorch
162134,open,Add Module API shim and nativeRT implementation.,JacobSzwejbka,Introduce the initial draft of a shared module shim layer. The intention is for various inference engines like ExecuTorch and nativeRT to be able to share this and provide a unified api for inference. Combined with a unified AoT flow. The exact details of which runner are being used where can be hidden and allows the runtime stack more flexibility in using the best runtime for a given model without having to force a change in user code.,2025-09-04 03:45:44+00:00,2025-09-11T20:12:32Z,,False,2,14,4,199,0,3,16,,48,440,False,False,False,False,False,False,3,0,19,353,276,77,1,4,4.0,1.0,2025-09-04T16:00:21Z,pytorch
162133,closed,Add Module API shim and nativeRT implementation.,JacobSzwejbka,Introduce the initial draft of a shared module shim layer. The intention is for various inference engines like ExecuTorch and nativeRT to be able to share this and provide a unified api for inference. Combined with a unified AoT flow. The exact details of which runner are being used where can be hidden and allows the runtime stack more flexibility in using the best runtime for a given model without having to force a change in user code. ,2025-09-04 03:32:52+00:00,2025-09-04T03:35:40Z,,False,2,0,10,201,0,3,2,2025-09-04 03:35:40+00:00,48,441,False,False,False,False,False,False,3,0,0,67258,39696,27562,1,10,,,,pytorch
162132,closed,[Inductor][Intel GPU] Register triton template heuristic for addmm tma.,etaf,"Fixes #162048


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 03:02:38+00:00,2025-09-04T23:03:04Z,,False,3,0,1,22,15,1,3,2025-09-04 23:02:01+00:00,71,217,False,True,False,False,False,False,1,2,493,37,22,15,1,1,3.0,2.0,2025-09-04T17:16:48Z,pytorch
162130,open,Raise error when no record on `extra_files`,ILCSFNO,"Fixes #152178

Reopen to merge for PR #152664

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-09-04 02:47:55+00:00,2025-09-18T14:26:57Z,,False,1,0,7,10,7,2,1,,43,94,False,True,False,False,False,False,2,0,0,656117,427034,229083,1,7,,,,pytorch
162126,closed,[Inductor] don't call sympy_str when not needed,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162355
* __->__ #162126
* #162101

I see torch.compile spend 2% of time on sympy_str when compiling the bwd graph for MobileBertForQuestionAnswering.  Most time sympy_str is called when extracting read/write dependencies. But when we extracting read/writer deps, the result of sympy_str is just discarded (correct me if I'm wrong). To make things simple, I just remove those calls. But if people think it may be useful for debugging, I can add a flag to only call sympy_str when it's explicitly set.

<img width=""667"" height=""409"" alt=""Screenshot 2025-09-03 at 6 21 52 PM"" src=""https://github.com/user-attachments/assets/a5929473-873d-4540-8f1e-c29f92be7125"" />

(scuba link: https://fburl.com/scuba/pyperf_experimental/on_demand/3k2rduh9 )



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 01:21:45+00:00,2025-09-19T20:21:38Z,,False,2,0,8,7,11,1,2,2025-09-19 20:21:37+00:00,47,1044,False,True,False,False,False,False,1,1,48,80399,46848,33551,1,8,3.0,1.0,2025-09-04T17:16:12Z,pytorch
162119,open,ensure meta impls for nonfunctional collectives are registered on import torch; support nonfunctional collectives with FakeTensor,bdhirsh,"non-functional collectives (e.g. `torch.distributed.all_gather`) will work with FakeTensorMode automatically as long as each underlying dispatcher operator has a `Meta` implementation.

These ops already have `Meta` impls, but previously they would only get registered if you `import torch.distributed._tools.fake_collectives`.

This PR tries to ensure that they get registered at `import torch` time (if distributed is available), around the same place that we register the python meta impls of our aten ops.

local test:
```
import torch
import torch.distributed as dist
from torch.distributed.tensor import DeviceMesh
from torch.testing._internal.distributed.fake_pg import FakeStore
from torch.distributed.tensor import DeviceMesh, DTensor, Replicate

device_type = ""cuda""
world_size = 4

fake_store = FakeStore()
dist.init_process_group(
    ""fake"", store=fake_store, rank=0, world_size=world_size
)
default_pg = torch.distributed.distributed_c10d._get_default_group()

import torch.distributed._functional_collectives as funcol
from torch._subclasses import FakeTensorMode
with FakeTensorMode():
    inp = torch.randn(4, 4, device=device_type)

    dist.all_reduce(inp, group=default_pg)
```

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162119



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang",2025-09-04 00:56:53+00:00,2025-09-05T05:01:24Z,,False,15,1,3,75,52,4,16,,129,1379,False,False,False,False,False,False,4,14,8603,8057,5488,2569,1,3,4.0,14.0,2025-09-04T01:28:53Z,pytorch
162118,closed,Fix memory leak in AOTI when calling `aoti_torch_as_strided`,yushangdi,"Summary:
Fix memory leak in AOTI when calling `aoti_torch_as_strided`


If you have something like `AtenTensorHandle buf_handle`; and you allocated memory to it, you have to make it a `RAIIAtenTensorHandle` to release the ownership. Otherwise you have leaked the memory because even when the program ends, there's still a pointer pointing to the underlying storage of `buf_handle_restrided`, and the storage is never freed.

Test Plan:
```
buck run fbcode//mode/dev-nosan fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_pad_non_zero_memory_leak
```

Also verified by looking at `print(f""Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB"")`

Differential Revision: D81640339




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-04 00:39:17+00:00,2025-09-04T22:18:12Z,,False,6,0,1,31,0,2,6,2025-09-04 22:17:10+00:00,60,908,False,True,False,False,False,False,2,2,495,31,31,0,1,1,3.0,2.0,2025-09-04T16:53:21Z,pytorch
162117,closed,Making batching rule for F.embedding DTensor-aware,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162117

`vmap(F.embedding)(DTensor, DTensor)` was failing because F.embedding's
batching rule generates a new tensor via at::arange, at::arange
generates a regular tensor, and DTensor rightfully errors on mixed
DTensor-regular Tensor operations.

This PR fixes the problem by activating DTensor implicit replication on
just the at::arange and the subsequent add operation.

In order to accomplish this I move the DTensor implicit replication flag
to C++ (most batching rules are in C++).

Test Plan:
- new test

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-04 00:37:06+00:00,2025-09-05T21:41:21Z,,False,4,10,3,112,7,10,14,2025-09-05 21:40:17+00:00,50,693,False,True,False,False,False,False,10,3,572,169,137,32,1,3,4.0,4.0,2025-09-04T13:32:22Z,pytorch
162115,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-04 00:25:06+00:00,2025-09-04T04:27:39Z,,False,3,0,1,1,1,1,3,2025-09-04 04:26:37+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-04T00:25:07Z,pytorch
162114,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-04 00:25:01+00:00,2025-09-05T04:33:22Z,,False,6,0,1,1,1,1,6,2025-09-05 04:32:19+00:00,48,157,False,False,False,False,False,False,1,5,1335,2,1,1,1,1,2.0,5.0,2025-09-04T00:25:02Z,pytorch
162112,closed,[ROCm] fix miopen batchnorm changing output format,amdfaa,"It was found that the integration of miopen batchnorm was causing the output to always be in default contig memory format even when the input was channels last.  This also unskips a number of related unit tests.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-04 00:17:55+00:00,2025-09-11T21:32:14Z,,False,9,2,10,56,65,6,11,2025-09-11 19:37:52+00:00,50,330,False,True,False,False,False,False,6,3,1213,27016,17824,9192,3,10,4.0,3.0,2025-09-04T00:27:55Z,pytorch
162111,closed,[DLPACK] Optimize toDLPack Conversion Speed,tqchen,"Previously in gh-83069, the toDLPack converter introduces a normalization step that changes the strides to 1 when shape[i] == 1

This step, however, calls as_strided during toDLPack, and can slow down the toDLPack about 3x. This causes PyTorch's DLPack conversion to be around 0.6 us overhead per call from the < 0.2us.

This PR updates the logic by adding a need_normalize_strides check, to first confirm if the strides normalization is necessary. In most common cases, when the tensor is continguous, such normalization is not necessary.

We confirmed that having this additional step would recover the speed of toDLPack to below 0.2us and can help significantly speedup eager mode integration of DLPack with PyTorch.

If we detect that there is normalization needs, the older path will be invoked.

Fixes #162113 ",2025-09-04 00:15:19+00:00,2025-09-04T05:28:10Z,,False,6,0,1,35,8,1,6,2025-09-04 05:27:08+00:00,43,816,False,True,False,False,False,False,1,4,826,43,35,8,1,1,3.0,5.0,2025-09-04T00:20:05Z,pytorch
162109,closed,rewrite __maybe_broadcast should_expand check for unbacked,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162109
* #162099

",2025-09-03 23:31:54+00:00,2025-09-08T22:42:26Z,,False,8,12,8,37,11,2,20,2025-09-08 22:41:22+00:00,58,104,False,False,False,False,False,False,2,7,2455,79,54,25,1,8,4.0,9.0,2025-09-04T14:36:39Z,pytorch
162108,closed,[MPS] Add `native_dropout` and `native_dropout_backward`,kurtamohler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162108

Fixes #162002",2025-09-03 22:51:46+00:00,2025-09-09T01:45:13Z,,False,6,9,4,96,1,7,15,2025-09-09 01:44:09+00:00,56,107,False,True,False,False,False,False,7,3,664,28465,17513,10952,1,4,4.0,3.0,2025-09-04T17:08:34Z,pytorch
162107,open,Add GPU vs CPU performance benchmarking utilities,kay-hewett,"- Add comprehensive benchmarking tools for matrix operations and neural networks
- Include device detection for CUDA, MPS, and CPU
- Provide proper GPU synchronization and timing
- Add complete unit test suite with device-specific tests
- Include automated test runner script
- Add detailed documentation and contribution guide

Features:
- Cross-platform device support (CUDA/MPS/CPU)
- Modular, extensible design with type hints
- Comprehensive error handling and reporting
- Educational examples for proper GPU benchmarking
- No breaking changes to existing PyTorch functionality

Fixes #ISSUE_NUMBER
",2025-09-03 22:40:40+00:00,2025-09-22T22:22:42Z,,False,9,0,1,882,0,5,9,,49,604,False,True,True,True,False,False,5,6,554,882,882,0,1,1,2.0,6.0,2025-09-03T22:50:08Z,pytorch
162106,closed,Add torch.compile support for triton.constexpr_function,oulgen,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162106

Fixes #161868

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-03 21:57:18+00:00,2025-09-04T16:48:02Z,,False,3,0,3,48,0,2,3,2025-09-04 16:46:58+00:00,55,318,False,True,False,False,False,False,2,2,493,54,51,3,1,3,4.0,2.0,2025-09-03T22:01:06Z,pytorch
162105,closed,[dynamo] Make the MRO walk more narrow,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162105

I dont have a failing test case but just saw an extra guard somewhere.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 21:55:53+00:00,2025-09-04T17:55:41Z,,False,3,0,2,2,0,1,3,2025-09-04 17:54:36+00:00,38,336,False,False,False,False,False,False,1,2,493,14,8,6,1,2,5.0,2.0,2025-09-04T16:48:53Z,pytorch
162104,open,Stash and restore tls state for AC,soulitzer,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 21:46:18+00:00,2025-09-04T23:47:29Z,,False,4,0,1,73,11,5,4,,34,223,False,True,False,False,False,False,5,1,2673,84,73,11,1,1,1.0,1.0,2025-09-04T17:05:28Z,pytorch
162103,open,Modify ROCm MI2xx-based workflows to run on cron schedule,jithunnair-amd,"To mitigate queueing on MI2xx runners since Cirrascale runners are offline. Match cron schedule of periodic.yml


cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 21:36:52+00:00,2025-09-09T00:35:42Z,,False,7,0,3,17,3,2,7,,57,214,False,False,False,False,False,False,2,6,1337,28,21,7,1,3,4.0,6.0,2025-09-03T21:40:21Z,pytorch
162102,open,LOAF not for land hack,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162102
* #162030
* #162355
* #162126
* #162101



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-03 21:35:02+00:00,2025-09-22T17:36:49Z,,False,2,0,18,53,0,3,2,,22,350,False,False,False,False,False,False,3,0,0,80590,47015,33575,1,17,,,,pytorch
162101,closed,[inductor] avoid creating LoopBody twice,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162355
* #162126
* __->__ #162101

Previously in merge_loops, we have to construct LoopBody twice to make sure we can use the same symbol prefix as before. This PR change it to create LoopBody only once by allowing using the same symbol prefix for the new LoopBody.

In looks like it's ok to have duplicate symbols in sympy replacement:
```
>>> x, y = sympy.symbols(""x y"")
>>> (x + y).xreplace({x: 0, y: x + 1})
x + 1
>>> (x + y).xreplace({x: y * y, y: x + 1})
x + y**2 + 1
>>> (x + y + x * x).xreplace({x: 0, y: x})
x
```

UPDATE: add the same optimization for LoopBody.reorder_iter_loops


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 21:34:59+00:00,2025-09-19T20:21:37Z,,False,2,0,9,33,45,1,2,2025-09-19 20:21:36+00:00,40,892,False,True,False,False,False,False,1,1,48,80475,46906,33569,1,9,3.0,1.0,2025-09-03T22:03:03Z,pytorch
162100,closed,[CI] viable strict upgrade: Explicitly name which linux binary wheels should block,clee2000,"Reason:
rocm binary builds should not block viable strict upgrade.  It is queuing/canceled so viable strict is 1.2 days old

Tested by mangling the workflow file to get to the actual call of the python script `python ../test-infra/tools/scripts/fetch_latest_green_commit.py --required-checks '[""pull"", ""trunk"", ""lint"", ""^linux-binary-manywheel$"", ""^linux-binary-libtorch-release$"", ""linux-aarch64""]' --viable-strict-branch viable/strict --main-branch master`, which I then ran locally where I have credentials.  It returned d64718503728001a1e78168fd7f2d4ff23e57285 which is green.  Without this change, it returns 5e5870e858f60ff4bf87d03f3592097e934a9580, which is pretty old

The other solution would have been to mark it as unstable I think

Side note, why is it master and how is it working like that




cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 21:00:37+00:00,2025-09-03T22:39:37Z,,False,3,0,8,1,1,1,3,2025-09-03 22:38:36+00:00,82,924,False,False,False,False,False,False,1,2,829,26,13,13,1,8,3.0,3.0,2025-09-03T22:29:22Z,pytorch
162099,closed,make should_swap more dde friendly,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162354
* #162212
* __->__ #162099

unblock customers for common cases with DDE ,until @pianpwk  land the change to should_swap https://github.com/pytorch/pytorch/pull/160473. 

",2025-09-03 20:56:08+00:00,2025-09-08T22:41:22Z,,False,6,3,6,16,2,1,9,2025-09-08 22:41:21+00:00,34,256,False,False,False,False,False,False,1,5,1203,54,34,20,1,6,3.0,5.0,2025-09-03T21:26:45Z,pytorch
162098,open,WIP AOTPrecompile on modules,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162098



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 20:53:28+00:00,2025-09-03T23:21:10Z,,False,1,0,1,118,30,4,1,,28,266,False,False,False,False,False,False,4,0,0,148,118,30,1,1,,,,pytorch
162097,closed,[PGO] handle PGO profile merges,pianpwk,"Avoid merges from extra PGO key, if same source has different rank. Unlikely to happen (needs code hash match & source variable type to change), but being safe.

Differential Revision: D81299840


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 20:46:57+00:00,2025-09-05T04:59:21Z,,False,6,0,1,57,1,2,6,2025-09-05 04:58:19+00:00,31,367,False,False,False,False,False,False,2,1,506,58,57,1,1,1,2.0,2.0,2025-09-04T06:47:33Z,pytorch
162096,closed,Add NestedTensor dispatch for _is_any_true/_is_all_true,adabeyta,"Fixes: https://github.com/pytorch/pytorch/issues/161818

### Summary
Add NestedTensor support for `_is_any_true` and `_is_all_true`.

### Changes
- Register dispatch for `aten._is_any_true.default` and
  `aten._is_all_true.default`
- Add CPU tests:
  - `test_is_any_true_jagged`: dispatch_matches_values_buffer,
    all_false_returns_false, one_true_returns_true
  - `test_is_all_true_jagged`: dispatch_matches_values_buffer,
    all_true_returns_true, any_false_returns_false

### Testing

Before Fix:

`pytest -q test/test_nestedtensor.py -k ""test_is_any_true_jagged or test_is_all_true_jagged"" -v`

Output:
```
FAILED [0.0129s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_is_all_true_jagged_cpu - NotImplementedError: aten._is_all_true.default
FAILED [0.0007s] test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_is_any_true_jagged_cpu - NotImplementedError: aten._is_any_true.default
```

After Fix:

`pytest -q test/test_nestedtensor.py -k ""test_is_any_true_jagged or test_is_all_true_jagged"" -v`

Output:

```
Running 2 items in this shard

test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_is_all_true_jagged_cpu PASSED [0.0277s]                                                                                                                               [ 50%]
test/test_nestedtensor.py::TestNestedTensorDeviceTypeCPU::test_is_any_true_jagged_cpu PASSED [0.0013s]      
```
",2025-09-03 20:46:19+00:00,2025-09-22T20:23:51Z,,False,4,4,3,89,0,2,8,2025-09-22 20:22:47+00:00,55,1426,False,True,False,False,False,False,2,3,604,115,102,13,2,3,3.0,4.0,2025-09-03T20:49:12Z,pytorch
162095,open,[Torch FX] Run ensures on PassResult instead of the input graph_module.,abeakkas,"Summary: We need to run the ensures on res.graph_module in case a pass creates a new graph_module instead of modifying in place.

Test Plan:
CI

Rollback Plan:

Differential Revision: D81617603




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-03 20:44:42+00:00,2025-09-06T03:01:46Z,,False,5,0,1,2,1,1,5,,71,254,False,False,False,False,False,False,1,1,10,3,2,1,1,1,1.0,1.0,2025-09-06T03:01:46Z,pytorch
162094,open,[dynamo][guards] Skip guard on _compiled_call_impl,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162094



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 20:31:21+00:00,2025-09-04T18:01:36Z,,False,1,0,1,12,0,1,1,,50,266,False,False,False,False,False,False,1,0,0,12,12,0,1,1,,,,pytorch
162093,closed,Allow for using a dedicated binary for the torch subproc pool.,c00w,"Summary:
The binary torch is running inside of can be larger than needed and in certain
situations, this can cause a loss of memory.

Test Plan:
We've manually run tests via
```
TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 TORCHINDUCTOR_WORKER_SUPPRESS_LOGGING=0
make mc8-train-publish-cint-datafm-toy -C
minimal_viable_ai/models/ifr_mtml/main_v1/ 2>&1 | tee ~/run_out
```
and overriding the binary used to be the built fbpkg in /packages.

We've also kicked off manual runs at
```
fire-feid-20250903-1051-ae8c6827
```

Which do show the binary running -  https://fburl.com/scuba/procprint/e6lwv32m

Rollback Plan:
steps:
  - jk.update:
      jk: pytorch/compiler:subproc_worker_binary
      constant_bool: null
      consistent_pass_rate: null
      fractional_host_rollout: null
      sampling_rate: null
  - manual.note:
      content: ''

Differential Revision: D81616624


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 20:30:30+00:00,2025-09-05T01:44:52Z,,False,14,1,1,14,0,2,15,2025-09-05 01:43:49+00:00,62,1071,False,False,False,False,False,False,2,3,1369,14,14,0,1,1,2.0,3.0,2025-09-03T21:40:00Z,pytorch
162091,open,Fix unclear CUBLAS error in torch._scaled_mm with FP8 inputs missing out_dtype,dsashidh,"Fixes #160816 

When using FP8 inputs with torch._scaled_mm (for example DeepSeekV3 style FP8 blockwise GEMMs) without specifying out_dtype, users get an uninformative error: 

RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasLtMatmulAlgoGetHeuristic( ltHandle, computeDesc.descriptor(), Adesc.descriptor(), Bdesc.descriptor(), Cdesc.descriptor(), Ddesc.descriptor(), preference.descriptor(), 1, &heuristicResult, &returnedResult)

My change adds validation in aten/src/ATen/native/cuda/Blas.cpp that catches this case early and provides a helpful error message:

torch._scaled_mm: when using FP8 inputs please pass out_dtype (e.g., torch.float16 or torch.float32). Omitting it leads to an unsupported configuration in the CUDA backend.

I tested this by creating FP8 tensors with torch.float8_e4m3fn and calling torch._scaled_mm without the out_dtype parameter. The new error message now appears instead of the CUBLAS error.


",2025-09-03 19:50:28+00:00,2025-09-24T03:46:19Z,,False,6,5,2,25,0,2,11,,78,953,False,True,False,False,False,False,2,5,1463,27,26,1,1,2,5.0,6.0,2025-09-03T19:50:40Z,pytorch
162090,closed,[aot-precompile] default-filter global guards,dolpm,"if the user doesn't provide their own guard filter fn, we should by default filter global guards.

pytest test/dynamo/test_aot_compile.py

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 19:44:55+00:00,2025-09-05T22:45:59Z,,False,4,0,1,87,20,3,4,2025-09-05 22:44:58+00:00,45,309,False,False,False,False,False,False,3,3,529,107,87,20,1,1,3.0,3.0,2025-09-03T19:51:43Z,pytorch
162089,open,Fix persistent buffer bug,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162166
* __->__ #162089
* #162013
* #161977
* #161653



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81617464](https://our.internmc.facebook.com/intern/diff/D81617464)",2025-09-03 19:31:42+00:00,2025-09-04T13:56:15Z,,False,2,0,2,1,9,3,2,,25,422,False,True,False,False,False,False,3,1,160,45617,23870,21747,1,2,1.0,1.0,2025-09-03T20:11:18Z,pytorch
162088,closed,[BE][flex attention] compute RMSE in float64,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #162088

I saw a failure where the reference error was 0.0, and the compiled error was 0.035. Although the failure still occurs with or without this change, it was confusing to see RMSE of 0.0.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 19:10:07+00:00,2025-09-11T23:54:38Z,,False,9,0,2,2,0,1,9,2025-09-11 23:53:34+00:00,44,493,False,False,False,False,False,False,1,8,1939,83441,49466,33975,1,2,3.0,9.0,2025-09-11T13:12:37Z,pytorch
162086,closed,[CI][ROCM] Disable rocm build tests,jeanschmidt,"There is a queue time of 23 hours for `linux.rocm.gpu.mi250` and 12h for `linux.rocm.gpu.gfx942.1`

With https://github.com/pytorch/pytorch/pull/162044 more jobs will be running on `linux.rocm.gpu.gfx942.1` and this fleet is clearly not capable of handling the load.

So the goal here is to remove smoke tests for builds that uses ROCM in order to reduce the pressure in the fleet and reduce the lag between viable/strict.

Once the fleet is increased we can revert this PR

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 19:01:43+00:00,2025-09-04T06:23:45Z,,False,5,0,1,18,18,4,5,2025-09-04 06:23:44+00:00,35,591,False,False,False,False,False,False,4,4,3237,36,18,18,1,1,3.0,5.0,2025-09-03T19:10:56Z,pytorch
162085,closed,[inductor] fix test output path 2,xuhancn,"Fix test_output_path_2


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 18:47:13+00:00,2025-09-04T00:07:16Z,,False,3,0,3,13,7,1,3,2025-09-04 00:03:51+00:00,33,332,False,True,False,False,False,False,1,2,493,24,15,9,1,3,4.0,2.0,2025-09-03T19:26:11Z,pytorch
162084,closed,use sym_or instead of any to avoid dde in calc_conv_nd_return_shape,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162109
* #162099
* __->__ #162084

",2025-09-03 18:31:32+00:00,2025-09-04T01:21:29Z,,False,4,3,3,2,1,1,7,2025-09-04 01:20:25+00:00,67,114,False,False,False,False,False,False,1,3,512,7,4,3,1,3,3.0,3.0,2025-09-03T21:16:48Z,pytorch
162083,closed,[DO NOT LAND] DTensor Demo,SherlockNoMad,"Fixes #ISSUE_NUMBER
",2025-09-03 18:28:16+00:00,2025-09-13T03:42:26Z,,False,2,0,1,596,0,3,2,2025-09-13 03:42:26+00:00,26,20,False,True,False,False,False,False,3,0,0,596,596,0,1,1,,,,pytorch
162082,closed,Add const to stable amax,mikaylagawarecki,"Fixes https://github.com/pytorch/pytorch/issues/161826

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162082

",2025-09-03 18:18:25+00:00,2025-09-05T17:38:56Z,,False,3,0,2,2,2,1,3,2025-09-05 17:37:53+00:00,24,150,False,True,False,False,False,False,1,2,493,4,2,2,1,2,3.0,2.0,2025-09-05T14:46:28Z,pytorch
162081,open,wrapper_fxir store triton_meta `extra` field into node.meta,jazlyn5,"Summary:
In wrapper_fxir's `_generate_triton_call` function, allow an `extra` metadata field stored in JITFunction's `triton_meta` field to be added to `node.meta`.

use case is: if there are Triton-related configurations in triton_meta that the graph node should be aware of, it can be propagated to the node and accessed down the line. Eg. triton compiler optimization flags associated with this kernel.

Test Plan:
added tests in test_fxir_backend to confirm the lowered graph node has the `[""triton_meta""][""extra""]` field when populated.
```
caffe2/test/inductor:fxir_backend -- test_triton_meta_extra
```

Rollback Plan:

Differential Revision: D81604436




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 18:16:34+00:00,2025-09-03T23:20:20Z,,False,5,0,1,44,1,2,5,,59,865,False,False,False,False,False,False,2,0,0,45,44,1,1,1,,,,pytorch
162080,open,ci: Disable all jobs that require mi250,seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162080

We don't have nodes for these right now and this is causing us to queue
forever. If this is still the case after a week then I'm deleting all of
these.

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 18:15:07+00:00,2025-09-04T01:08:43Z,,False,5,0,2,36,18,4,5,,39,413,False,False,False,False,False,False,4,4,737,54,36,18,2,2,4.0,4.0,2025-09-03T18:30:44Z,pytorch
162079,open,Tune linalg_eigh_cusolver: better heuristic for syevj_batched selection on cuda,MauriceDHanisch,"Supersedes #151118. Same changes, reopened because the bot closed it due to inactivity.

This change is not tied to an open issue.

### Summary
This PR updates the heuristics in linalg_eigh_cusolver for batched matrix diagonalization. The current logic only applies syevj_batched for matrix sizes ≤ 32, which is too conservative. In the favorable regions, this heuristic improves performance by more than 10× compared to the previous default.

### Benchmark
benchmarked all three solver variants (`syevd`, `syevj`, `syevj_batched`) across a grid of matrix sizes and batch sizes, for both `float32` and `float64` on CUDA. The results are summarized below:
(See attached 2D heatmaps of absolute and relative timings)
→ Left column shows runtime of syevj_batched
→ Middle and right columns show speedup of syevj / syevd relative to syevj_batched

<img width=""2776"" height=""1380"" alt=""image"" src=""https://github.com/user-attachments/assets/0ca21bad-69c3-40ca-8cf2-37bbc23b7262"" />

All benchmarks were run on an NVIDIA RTX 4090 (CUDA 12.2).
The full benchmark setup (script, results, and plotting notebook) is available here:
📂 [benchmarks/batched_eigh](https://github.com/MauriceDHanisch/pytorch/tree/benchmark-cusolver-eigh-methods-c/benchmarks/batched_eigh)
🌿 Branch: [benchmark-cusolver-eigh-methods-c](https://github.com/MauriceDHanisch/pytorch/tree/benchmark-cusolver-eigh-methods-c)

It includes:
- A grid-based benchmark across matrix and batch sizes
- Results saved as .json
- A notebook that produces the 2D plots attached above

### Code Change
New logic
For `float32`:
- Use `syevj_batched` if:
    - `batch > 15 && matrix_size < 512`, or
    - `batch ≤ 15 && matrix_size < 100`
- Otherwise fall back to `syevd` for batched, or use existing `syevj` heuristic in unbatched case.

For `float64':
- Use `syevj_batched` if `batch > 15 && matrix_size < 256`
- Otherwise use syevd for batched
- Unbatched logic unchanged (default to `syevd`)

All uses of `syevj_batched` remain gated behind `use_cusolver_syevj_batched_`.

### Further Notes
As seen in the plots, the transition boundary is not linear and depends on batch size, matrix size, and dtype. For future flexibility, it may be worth exposing the solver choice to users via an explicit flag or context override.

cc @jianyuh @nikitaved @mruberry @walterddr @xwang233 @Lezcano",2025-09-03 18:05:33+00:00,2025-09-11T21:18:22Z,,False,9,0,2,37,9,1,9,,79,2335,False,False,False,False,True,False,1,5,280,68,48,20,1,2,1.0,5.0,2025-09-03T18:09:12Z,pytorch
162078,open,Comment out workflows due to reduced capacity,amdfaa,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 17:54:00+00:00,2025-09-12T12:56:47Z,,False,1,0,2,56,55,2,1,,45,138,False,True,False,False,False,False,2,0,0,111,56,55,1,2,,,,pytorch
162077,closed,Tune linalg_eigh_cusolver: better heuristic for syevj_batched selection on cuda,MauriceDHanisch,"Supersedes #151118. Same changes, reopened because the bot closed it due to inactivity.
",2025-09-03 17:49:34+00:00,2025-09-03T18:06:19Z,,False,8,0,2,37,9,1,8,2025-09-03 17:59:00+00:00,79,88,False,False,False,False,False,False,1,4,673,68,48,20,1,2,1.0,4.0,2025-09-03T17:53:10Z,pytorch
162076,closed,[pt] strip error messages in profile builds,rmaz,"Summary: Profile builds should match production builds, and error messages result in large static initializers running. Omit them for profile builds too.

Test Plan:
Before:
```
$ buck build //xplat/caffe2:aten_native_cpuApple -c user.sandcastle_build_mode=profile --show-output
$ llvm-nm buck-out/v2/gen/fbsource/31fc3668aa0b4012/xplat/caffe2/__aten_native_cpuApple__/libaten_native_cpuApple.pic.a | grep ZN3c106detail12_str_wrapperIJPKcRKiS3_RKxS3_RKS3_S3_EE4callES9_S5_S9_S7_S9_S9_S9
0000000000003234 T __ZN3c106detail12_str_wrapperIJPKcRKiS3_RKxS3_RKS3_S3_EE4callES9_S5_S9_S7_S9_S9_S9_
```

After:
```
$ buck build //xplat/caffe2:aten_native_cpuApple -c user.sandcastle_build_mode=profile --show-output
$ llvm-nm buck-out/v2/gen/fbsource/31fc3668aa0b4012/xplat/caffe2/__aten_native_cpuApple__/libaten_native_cpuApple.pic.a | grep ZN3c106detail12_str_wrapperIJPKcRKiS3_RKxS3_RKS3_S3_EE4callES9_S5_S9_S7_S9_S9_S9
```

Rollback Plan:

Reviewed By: yury-dymov, abashyam

Differential Revision: D81599582


",2025-09-03 17:35:17+00:00,2025-09-04T04:19:33Z,,False,6,0,1,2,2,1,6,2025-09-04 04:18:30+00:00,43,1006,False,False,False,False,False,False,1,2,602,4,2,2,1,1,3.0,3.0,2025-09-03T19:06:15Z,pytorch
162075,closed,[inductor][contigous mm] mild refactor,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* __->__ #162075

# why

- use the new heuristics logic better to handle kwargs

# what

- move all checks into the heuristics to yield a single choice or not
  choices if the decomposition should not be used
- fix `hip` device type, which should be `cuda`
- let heuristics handle the kwarg passing

# testing

in ci

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81706776](https://our.internmc.facebook.com/intern/diff/D81706776)",2025-09-03 17:19:13+00:00,2025-09-05T18:02:57Z,,False,6,0,11,33,14,4,6,2025-09-05 18:02:56+00:00,38,824,False,True,False,False,False,True,4,5,570,13825,9094,4731,1,11,4.0,5.0,2025-09-03T17:21:21Z,pytorch
162073,closed,"[cuDNN][SDPA] Enable cuDNN SDPA by default for SM 9.0, SM 10.0",eqy,"for 2.9
🙏

cc @csarofeen @ptrblck @xwang233 @msaroufim @jerryzh168",2025-09-03 17:16:20+00:00,2025-09-04T18:47:37Z,,False,3,0,4,5,4,2,3,2025-09-04 18:46:32+00:00,62,66,False,False,False,False,False,False,2,2,498,11,6,5,2,4,3.0,2.0,2025-09-03T17:57:34Z,pytorch
162072,closed,Relax fences for intrusive ptr's refcnt,mcfi,"Summary: Relax fences for intrusive ptr's refcnt dec op for performance testing.

lock needs acquire when the op succeeds and relaxed if the op is not. In addition, the expire call and the following refcnt reads were merged to remove one extra read.

incref does not need any fences because the caller should already have a valid reference. use_count follows the same reasoning.

decref only needs a release fence to make sure every write op prior to it has finished. When the refcnt goes to zero, there should be a acquire fence to make sure no read op reads stale data before the object is destructed. However, microbenchmark showed that the optimal fence for decref is not performing noticeably better than the current decref with acq-rel, so we keep decref as-is.

This change should have no material impact on x86, but for Arm64 (and other CPUs with weak memory models), it should boost performance.

",2025-09-03 17:10:14+00:00,2025-09-10T23:18:07Z,,False,19,17,1,25,16,1,36,2025-09-10 23:17:04+00:00,39,906,False,False,False,False,False,False,1,6,2396,41,25,16,1,1,7.0,8.0,2025-09-03T19:59:22Z,pytorch
162069,closed,[reland] Add inductor provenance mapping for cpp extern kernel (#161656),yushangdi,"Summary:

Add inductor provenance mapping for cpp extern kernel

Test Plan:
```
buck run fbcode//caffe2/test/inductor:provenance_tracing --  -r test_cpu_extern_kernel
```

Differential Revision: D81598857




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 16:50:53+00:00,2025-09-04T04:19:48Z,,False,9,0,1,62,17,3,9,2025-09-04 04:18:46+00:00,72,410,False,False,False,False,False,False,3,5,2125,79,62,17,1,1,3.0,5.0,2025-09-03T23:15:16Z,pytorch
162068,open,Update group_batch_fusion.py,pianpwk,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 16:25:39+00:00,2025-09-03T20:34:00Z,,False,2,0,1,3,1,1,2,,28,223,False,True,False,False,False,False,1,0,0,4,3,1,1,1,,,,pytorch
162067,closed,"[ROCm] TunableOp should use HIP version, not ROCm version",naromero77amd,"Fixes #160874



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",2025-09-03 16:04:56+00:00,2025-09-03T21:43:29Z,,False,3,0,2,9,11,1,3,2025-09-03 21:42:27+00:00,57,118,False,True,False,False,False,False,1,2,493,22,10,12,1,2,2.0,2.0,2025-09-03T16:11:28Z,pytorch
162066,open,[Reland] Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig,guangyey,"# Motivation
This is a regular PR to faciliate re-import for internal testing...

# Additional Context
This is a reland of the stack of https://github.com/pytorch/pytorch/pull/161856

# Additional Context
As mentioned in this [comment](https://github.com/pytorch/pytorch/pull/160666#issuecomment-3260221878), @joshuuuasu figured out that the regression is caused by `round_size` in `CUDACachingAllocator`.
To test the performance of `round_size`, I compared `round_size_old` (using `CUDAAllocatorConfig`) and `round_size_new` (using `AcceleratorAllocatorConfig`)
```cpp
size_t round_size_old(size_t size) {
  using c10::cuda::CUDACachingAllocator::Native::kMinBlockSize;
  if (size < kMinBlockSize) {
    return kMinBlockSize;
  } else {
    auto divisions = CUDACachingAllocator::CUDAAllocatorConfig::roundup_power2_divisions(size);
    if (divisions > 1 && size > (kMinBlockSize * divisions)) {
      return roundup_power2_next_division(size, divisions);
    } else {
      return kMinBlockSize * ((size + kMinBlockSize - 1) / kMinBlockSize);
    }
  }
}

size_t round_size_new(size_t size) {
  using c10::cuda::CUDACachingAllocator::Native::kMinBlockSize;
  if (size < kMinBlockSize) {
    return kMinBlockSize;
  } else {
    auto divisions = CachingAllocator::AcceleratorAllocatorConfig::roundup_power2_divisions(size);
    if (divisions > 1 && size > (kMinBlockSize * divisions)) {
      return roundup_power2_next_division(size, divisions);
    } else {
      return kMinBlockSize * ((size + kMinBlockSize - 1) / kMinBlockSize);
    }
  }
}
```
I bound them to `_cuda_round_size_old` and `_cuda_round_size_new`, each measuring the time for 100 calls.
```cpp
m.def(""_cuda_round_size_old"", [](size_t size) {
    size_t result;
    auto start = std::chrono::high_resolution_clock::now();
    for(auto i = 0; i < 100; i++) {
      result = c10::cuda::round_size_old(size + 512*i);
    }
    auto end = std::chrono::high_resolution_clock::now();
    std::cout << ""time cost = "" << std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count() << "" ns"" << std::endl;
    return result;
  });

  m.def(""_cuda_round_size_new"", [](size_t size) {
    size_t result;
    auto start = std::chrono::high_resolution_clock::now();
    for(auto i = 0; i < 100; i++) {
      result = c10::cuda::round_size_new(size + 512*i);
    }
    auto end = std::chrono::high_resolution_clock::now();
    std::cout << ""time cost = "" << std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count() << "" ns"" << std::endl;
    return result;
  });
```
With `PYTORCH_CUDA_ALLOC_CONF=""roundup_power2_divisions:[1024:4,2048:1, >:1]""`, the results running on the following test script were:
```python
import torch

for _ in range(10):
    # only run one function per test.
    torch._C._cuda_round_size_old(1000)
    # torch._C._cuda_round_size_new(1000)
```
`_cuda_round_size_old` first call `~9800 ns`, subsequent calls `~820 ns` (100 times):
```bash
time cost = 9800 ns
time cost = 910 ns
time cost = 820 ns
time cost = 830 ns
time cost = 820 ns
time cost = 820 ns
time cost = 830 ns
time cost = 830 ns
time cost = 820 ns
time cost = 820 ns
```
`_cuda_round_size_new` first call `~42470 ns`, subsequent calls `~770 ns` (100 times):
```bash
time cost = 42470 ns
time cost = 840 ns
time cost = 770 ns
time cost = 770 ns
time cost = 780 ns
time cost = 770 ns
time cost = 780 ns
time cost = 780 ns
time cost = 770 ns
time cost = 780 ns
```
So the steady-state performance is `~8 ns` per call for both, but the first invocation of `round_size_new` shows ~4x higher overhead. This is because `round_size_new` would like to initialize a static single `AcceleratorAllocatorConfig` and `CUDAAllocatorConfig`, however, `round_size_old` would only initialize a static single `CUDAAllocatorConfig`. Another reason is `AcceleratorAllocatorConfig` needs to initialize by parsing both `PYTORCH_ALLOC_CONF` and `PYTORCH_CUDA_ALLOC_CONF`, while `CUDAAllocatorConfig` only parses `PYTORCH_CUDA_ALLOC_CONF`. If I instead set `PYTORCH_ALLOC_CONF=""roundup_power2_divisions:[1024:4,2048:1, >:1]""`, the first-call overhead of `round_size_new` drops to ~23030 ns (roughly 2×). It sounds reasonable because `round_size_new` needs to initialize two static single instances than one static single instance initialized by `round_size_old`.
```bash
time cost = 23030 ns
time cost = 860 ns
time cost = 770 ns
time cost = 780 ns
time cost = 770 ns
time cost = 780 ns
time cost = 780 ns
time cost = 780 ns
time cost = 780 ns
time cost = 770 ns
```
Given this, I believe the observed regression in metrics `thrift.process_time.p99.60` on `sl_m897405705_slatest_amd_mi300xg1` is primarily due to the first-call initialization cost. Since subsequent calls take only ~8 ns, the steady-state performance should not be impacted. Based on this analysis, and considering @joshuuuasu's [precondition](https://github.com/pytorch/pytorch/pull/160666#issuecomment-3260221878), I suspect it is not reasonable to test metrics `thrift.process_time.p99.60` on `sl_m897405705_slatest_amd_mi300xg1`? 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 15:14:51+00:00,2025-09-25T07:01:59Z,,False,11,0,5,250,588,14,11,,73,5221,False,False,False,False,False,True,14,8,5603,852,257,595,1,4,3.0,8.0,2025-09-09T13:51:23Z,pytorch
162063,closed,[Dependabot] Update(deps): Bump transformers from 4.54.0 to 4.56.0 in /.ci/docker/ci_commit_pins,dependabot[bot],"Bumps [transformers](https://github.com/huggingface/transformers) from 4.54.0 to 4.56.0.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/huggingface/transformers/releases"">transformers's releases</a>.</em></p>
<blockquote>
<h1>Patch v4.55.4</h1>
<p>There was a mick mack on our side when cherry-picking the commit <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a> which led to a wrong commit in the patch!
Sorry everyone 😭</p>
<p>This patch is just the official fix for <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>!</p>
<h2>Patch release v4.55.3</h2>
<h1>Patch release 4.55.3</h1>
<p>Focused on stabilizing FlashAttention-2 on Ascend NPU, improving FSDP behavior for generic-task models, fixing MXFP4 integration for GPT-OSS</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>FlashAttention-2 / Ascend NPU – Fix “unavailable” runtime error (<a href=""https://redirect.github.com/huggingface/transformers/issues/40151"">#40151</a>) by <a href=""https://github.com/FightingZhen""><code>@​FightingZhen</code></a></li>
<li>FlashAttention kwargs – Revert FA kwargs preparation to resolve regression (<a href=""https://redirect.github.com/huggingface/transformers/issues/40161"">#40161</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>FSDP (generic-task models) – Fix sharding/runtime issues (<a href=""https://redirect.github.com/huggingface/transformers/issues/40191"">#40191</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>GPT-OSS / MXFP4 – Ensure swiglu_limit is correctly passed through (<a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>) by <a href=""https://github.com/danielhanchen""><code>@​danielhanchen</code></a></li>
<li>Mamba – Fix cache handling to prevent stale/incorrect state (<a href=""https://redirect.github.com/huggingface/transformers/issues/40203"">#40203</a>) by <a href=""https://github.com/manueldeprada""><code>@​manueldeprada</code></a></li>
<li>Misc – Minor follow-up fix addressing <a href=""https://redirect.github.com/huggingface/transformers/issues/40262"">#40262</a> by <a href=""https://github.com/ArthurZucker""><code>@​ArthurZucker</code></a></li>
</ul>
<h2>Patch release 4.55.2: for FA2 users!</h2>
<h1>Patch release 4.55.2!</h1>
<h2>only affects <code>FA2</code> generations!</h2>
<p>😢 Well sorry everyone, sometimes shit can happen...
4.55.1 was broken because of 🥁 git merge conflict.
I cherry-picked <a href=""https://redirect.github.com/huggingface/transformers/pull/40002"">huggingface/transformers#40002</a> without having <a href=""https://redirect.github.com/huggingface/transformers/pull/40029"">huggingface/transformers#40029</a> , thus <code>from ..modeling_flash_attention_utils import prepare_fa_kwargs_from_position_ids</code> is missing, and since this is a slow test, nothing caught it.</p>
<p>Will work to remediate and write the post-mortem when yanking the release.</p>
<h1>Patch release 4.55.1:</h1>
<p>Mostly focused around stabalizing the Mxfp4 for GPTOSS model!</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>Idefics2, Idefics3, SmolVLM – Fix tensor device issue (<a href=""https://redirect.github.com/huggingface/transformers/issues/39975"">#39975</a>) by <a href=""https://github.com/qgallouedec""><code>@​qgallouedec</code></a></li>
<li>Merge conflicts – Fix merge conflicts from previous changes by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>MXFP4 / CPU device_map – Default to dequantize when CPU is in device_map (<a href=""https://redirect.github.com/huggingface/transformers/issues/39993"">#39993</a>) by <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>GPT Big Code – Fix attention scaling (<a href=""https://redirect.github.com/huggingface/transformers/issues/40041"">#40041</a>) by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>Windows compatibility – Resolve Triton version check compatibility (<a href=""https://redirect.github.com/huggingface/transformers/issues/39986"">#39986</a>) by <a href=""https://github.com/Tsumugii24""><code>@​Tsumugii24</code></a> <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>Gemma3n model – Add missing None default values for get_placeholder_mask (<a href=""https://redirect.github.com/huggingface/transformers/issues/39991"">#39991</a>, <a href=""https://redirect.github.com/huggingface/transformers/issues/40024"">#40024</a>) by <a href=""https://github.com/Znerual""><code>@​Znerual</code></a></li>
<li>Fuyu model – Fix broken image inference (<a href=""https://redirect.github.com/huggingface/transformers/issues/39915"">#39915</a>) by <a href=""https://github.com/Isotr0py""><code>@​Isotr0py</code></a></li>
<li>PerceptionLM – Fix missing video inputs (<a href=""https://redirect.github.com/huggingface/transformers/issues/39971"">#39971</a>) by <a href=""https://github.com/shuminghu""><code>@​shuminghu</code></a></li>
<li>Idefics – Fix device mismatch (<a href=""https://redirect.github.com/huggingface/transformers/issues/39981"">#39981</a>) by <a href=""https://github.com/zucchini-nlp""><code>@​zucchini-nlp</code></a></li>
<li>Triton kernels – Remove triton_kernels dependency in favor of included kernels (<a href=""https://redirect.github.com/huggingface/transformers/issues/39926"">#39926</a>) by <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>GPT-OSS MXFP4 – Enable on older hardware (sm75+) (<a href=""https://redirect.github.com/huggingface/transformers/issues/39940"">#39940</a>) by <a href=""https://github.com/matthewdouglas""><code>@​matthewdouglas</code></a> <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>MXFP4 quantizer – Allow CPU inference with dequantize option (<a href=""https://redirect.github.com/huggingface/transformers/issues/39953"">#39953</a>) by <a href=""https://github.com/returnL""><code>@​returnL</code></a></li>
</ul>
<h2>CI &amp; Build</h2>
<ul>
<li>CI stability – Post-GPT-OSS fixes for green CI (<a href=""https://redirect.github.com/huggingface/transformers/issues/39929"">#39929</a>) by <a href=""https://github.com/gante""><code>@​gante</code></a> <a href=""https://github.com/LysandreJik""><code>@​LysandreJik</code></a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/huggingface/transformers/commit/e7d351cebad5f6dcdd169b0c034fdee0a000e6a9""><code>e7d351c</code></a> Release: v4.56.0</li>
<li><a href=""https://github.com/huggingface/transformers/commit/1067577ad204e649514ff3a5d3af0f7d52a63f14""><code>1067577</code></a> fix gpt-oss out shape (<a href=""https://redirect.github.com/huggingface/transformers/issues/40535"">#40535</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/7efb4c87ca3ed1a8d8c96f3f158f27f693f78b38""><code>7efb4c8</code></a> Flaky CI is annoying (<a href=""https://redirect.github.com/huggingface/transformers/issues/40543"">#40543</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/828a27fd326e49d4a1c4b08210bfd32c107facc9""><code>828a27f</code></a> Fix gpt-oss rope warning  (<a href=""https://redirect.github.com/huggingface/transformers/issues/40550"">#40550</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/74a24217f5a09cdd514e7a72af177bf61569cac6""><code>74a2421</code></a> Add bfloat16 support detection for MPS in is_torch_bf16_gpu_available() (<a href=""https://redirect.github.com/huggingface/transformers/issues/40458"">#40458</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/ffdd10fcedb1ab4f9217ac645b2e0dbe03623a53""><code>ffdd10f</code></a> Allow compression on meta device (<a href=""https://redirect.github.com/huggingface/transformers/issues/39039"">#39039</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/f0e778112fe6438f25142960fc4a3781c5e32566""><code>f0e7781</code></a> Clean-up kernel loading and dispatch (<a href=""https://redirect.github.com/huggingface/transformers/issues/40542"">#40542</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/f68eb5f135bc403a3e00ac3736c35c5e041e685a""><code>f68eb5f</code></a> Redundant code removal (<a href=""https://redirect.github.com/huggingface/transformers/issues/40534"">#40534</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/d888bd435d0c0eaabaabad5b33d52af518c7187c""><code>d888bd4</code></a> Fix typos (<a href=""https://redirect.github.com/huggingface/transformers/issues/40511"">#40511</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/11a6b95553d99d153710dfd1e64facb9e4f85219""><code>11a6b95</code></a> Oupsy  (<a href=""https://redirect.github.com/huggingface/transformers/issues/40544"">#40544</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/huggingface/transformers/compare/v4.54.0...v4.56.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=transformers&package-manager=pip&previous-version=4.54.0&new-version=4.56.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>

cc @seemethere @malfet @pytorch/pytorch-dev-infra @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-03 13:50:07+00:00,2025-09-19T09:50:44Z,2025-09-19T09:50:36Z,True,13,0,9,71,51,25,13,2025-09-19 09:50:36+00:00,96,11226,False,True,False,True,True,False,25,7,1337,105731,66901,38830,2,9,2.0,7.0,2025-09-03T19:45:58Z,pytorch
162062,closed,Update torch-xpu-ops commit pin,CuiYifeng,"Update the torch-xpu-ops commit to [intel/torch-xpu-ops@83c5a5](https://github.com/intel/torch-xpu-ops/commit/83c5a5a5516d498dde2ae131ca2d10a4abb94cfb), includes:

- Revert ""Disable xccl timer avoid drlm hang"" because XPU time event issue has been fixed
- Fallback lu_factor kernel to CPU for single batch
- Enable aten::linalg_inv and aten::linalg_inv_ex on XPU",2025-09-03 13:45:44+00:00,2025-09-04T17:06:40Z,,False,3,0,1,1,1,1,3,2025-09-04 17:05:36+00:00,31,362,False,True,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-04T14:20:11Z,pytorch
162059,closed,enable float32 and float16 in `torch._grouped_mm` fallback,vkuzo,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162159
* __->__ #162059
* #161717
* #161407

Summary:

Enables `torch.float32` and `torch.float16` options in
`torch._grouped_mm`. Note that the fast path is only enabled if `mat_a`,
`mat_b`, and `out_dtype` are `torch.bfloat16`.

Saving for future PRs:
1. enabling testing on more platforms
2. supporting out_dtype != mat_a.dtype
3. opinfo
4. better compile support

Test Plan:

```bash
// on A100 and H100
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm -x
// on H100
pytest test/test_matmul_cuda.py -s -k test_scaled_grouped_gemm -x
```

Reviewers:

Subscribers:

Tasks:

Tags:",2025-09-03 13:34:00+00:00,2025-09-04T17:50:05Z,,False,5,2,3,40,26,4,7,2025-09-04 17:48:57+00:00,58,667,False,False,False,False,False,False,4,4,1110,72431,43103,29328,1,3,4.0,4.0,2025-09-03T20:59:59Z,pytorch
162058,closed,[inductor] fix split_aot_inductor_output_path on Windows.,xuhancn,"fix split_aot_inductor_output_path on Windows.


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 13:27:54+00:00,2025-09-03T16:54:45Z,,False,3,0,2,7,1,1,3,2025-09-03 16:53:42+00:00,57,312,False,True,False,False,False,False,1,2,493,10,8,2,1,2,3.0,2.0,2025-09-03T15:57:55Z,pytorch
162056,open,Expand on autotune support for persistent reduction kernels,jataylo,"After the removal of want_no_x_dim for persistent reduction kernels, we can improve the autotuning setup for persistent reduction kernels.

Currently even with tuning enable, filtering will only try a single config in many cases. Avoid filtering with autotune mode, and override MAX_BLOCK limit. Also we always include tiny_config when autotuning is enabled.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 10:32:59+00:00,2025-09-23T17:27:48Z,,False,7,0,10,32,14,1,7,,59,561,False,False,False,False,True,False,1,3,473,100,59,41,1,10,2.0,3.0,2025-09-23T13:57:03Z,pytorch
162055,closed,fixed typo error,rohit-kumar-manav,"Fixes #162054


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-03 09:55:10+00:00,2025-09-05T00:17:18Z,,False,7,0,1,1,1,1,7,2025-09-04 00:07:01+00:00,16,72,False,True,False,False,False,False,1,5,1001,2,1,1,1,1,4.0,6.0,2025-09-03T10:56:58Z,pytorch
162053,open,[Inductor] Naive foreach autotune support,jataylo,"Initial autotuning support for foreach kernels, 4x improvement for some kernels in internal workload. More improvements can surely be made here in the future. Removing num_warps for definition to enable autotune support in generated wrapper code.

Before:
triton_for_fused_18.kd 🔍 | 4.986 ms | 4.986 ms | 2.493 ms | 2 |  
triton_for_fused_6.kd 🔍 | 0.098 ms | 0.098 ms | 0.049 ms | 2 |  
triton_for_fused_7.kd 🔍 | 0.036 ms | 0.036 ms | 0.018 ms | 2 |  

After:
triton_for_fused_18.kd 🔍 | 1.273 ms | 1.273 ms | 0.636 ms | 2 |  
triton_for_fused_6.kd 🔍 | 0.044 ms | 0.044 ms | 0.022 ms | 2 |  
triton_for_fused_7.kd 🔍 | 0.024 ms | 0.024 ms | 0.012 ms | 2 |  

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-03 09:48:19+00:00,2025-09-24T18:23:47Z,,False,11,2,4,14,3,2,13,,41,866,False,False,False,False,True,False,2,9,1688,25,18,7,1,4,4.0,9.0,2025-09-08T16:59:32Z,pytorch
162052,open,[ROCm] Add inductor codegen support for fast tanh path,jataylo,"Requires triton 2.9 bump to triton 3.5 first. Will improve tanh performance on ROCm.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 09:41:40+00:00,2025-09-23T17:27:27Z,,False,10,0,6,8,2,1,10,,54,393,False,False,False,False,True,False,1,8,2660,76724,57297,19427,1,6,3.0,8.0,2025-09-03T20:13:24Z,pytorch
162051,closed,Adding missing example of torch.full_like Issue#161899,vishalgoyal316,"Fixes #161899
",2025-09-03 08:50:05+00:00,2025-09-04T08:51:47Z,,False,4,0,1,18,0,1,4,2025-09-04 08:45:53+00:00,54,14,False,True,False,False,False,False,1,3,534,18,18,0,1,1,3.0,3.0,2025-09-03T09:54:17Z,pytorch
162050,open,[OpenReg] Add AMP Integration guide for accelerators,zeshengzong,"Fix part of #158917

Add AMP integration document and OpenReg code as example to explain steps of integration.",2025-09-03 08:45:11+00:00,2025-09-15T16:36:42Z,,False,3,14,12,146,0,6,17,,52,110,False,True,False,True,False,False,6,2,475,212,179,33,2,12,1.0,2.0,2025-09-12T07:02:48Z,pytorch
162047,closed,Add missing `tags` parameter to `custom_op` overload signatures,SigureMo,It appears to be an omission in #149782.,2025-09-03 07:32:04+00:00,2025-09-13T20:15:50Z,,False,13,2,2,2,0,1,15,2025-09-13 19:57:27+00:00,63,40,False,False,False,False,False,False,1,12,2745,6,4,2,2,2,4.0,12.0,2025-09-03T16:29:33Z,pytorch
162046,closed,[OpenReg] Update the docs about Accelerator Integration,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #161918
* #161917
* __->__ #162046

Fix the issue describled by this [comment](https://github.com/pytorch/pytorch/pull/161845#discussion_r2317299390)",2025-09-03 07:09:52+00:00,2025-09-10T08:01:18Z,,False,3,0,3,12,16,1,3,2025-09-10 07:45:10+00:00,55,237,False,True,False,True,False,False,1,2,499,12268,8509,3759,1,3,3.0,3.0,2025-09-09T20:36:30Z,pytorch
162045,open,[1/N] Port distributed shared test files for Intel GPU,libohao1201,"For https://github.com/pytorch/pytorch/issues/114850, we will port 1 distributed test to Intel GPU.

In this PR we will port 1 distributed shard test files.
We could enable Intel GPU with following methods and try the best to keep the original code styles:

- use ""torch.accelerator.current_accelerator()"" to determine the accelerator backend
- use ""requires_accelerator_dist_backend()"" to replace requires_nccl()
- use ""torch.accelerator.device_count()"" to get the number of accelerator 
- enabled XPU for some test path

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @gujinghui @EikanWang @fengyuan14 @guangyey",2025-09-03 07:07:28+00:00,2025-09-17T07:40:03Z,,False,2,10,1,106,82,1,12,,54,668,False,False,False,False,False,False,1,0,0,188,106,82,1,1,2.0,0.0,2025-09-16T10:17:53Z,pytorch
162044,closed,[ROCm] Use MI325 (gfx942) runners for binary smoke testing,jithunnair-amd,"### Motivation

* MI250 Cirrascale runners are currently having network timeout leading to huge queueing of binary smoke test jobs: 
<img width=""483"" height=""133"" alt=""image"" src=""https://github.com/user-attachments/assets/17293002-78ad-4fc9-954f-ddd518bf0a43"" />

* MI210 Hollywood runners (with runner names such as `pytorch-rocm-hw-*`) are not suitable for these jobs, because they seem to take much longer to download artifacts: https://github.com/pytorch/pytorch/pull/153287#issuecomment-2918420345 (this is why these jobs were specifically targeting Cirrascale runners). However, it doesn't seem like Cirrascale runners are necessarily doing much better either e.g. [this recent build](https://github.com/pytorch/pytorch/actions/runs/17332256791/job/49231006755).
* Moving to MI325 runners should address the stability part at least, while also reducing load on limited MI2xx runner capacity. 
* However, I'm not sure if the MI325 runners will do any better on the artifact download part (this may need to be investigated more) cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @amdfaa

* Also removing `ciflow/binaries` and `ciflow/binaries_wheel` label/tag triggers for `generated-linux-binary-manywheel-rocm-main.yml` because we already trigger ROCm binary build/test jobs via these labels/tags in `generated-linux-binary-manywheel-nightly.yml`. And for developers who want to trigger ROCm binary build/test jobs on their PRs, they can use the `ciflow/rocm-mi300` label/tag as per this PR.


### TODOs (cc @amdfaa):
* Check that the workflow runs successfully on the MI325 runners in this PR. Note how long the test jobs take esp. the ""Download Build Artifacts"" step
* Once this PR is merged, clear the queue of jobs targeting `linux.rocm.gpu.mi250`

cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-03 06:54:59+00:00,2025-09-05T21:41:04Z,,False,8,0,5,20,24,5,8,2025-09-03 18:34:09+00:00,58,1911,False,False,False,False,False,False,5,7,2019,44,20,24,2,5,4.0,7.0,2025-09-03T15:36:44Z,pytorch
162043,closed,[OpenReg] Update the docs about Accelerator Integration,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Fix the issue describled by this [comment](https://github.com/pytorch/pytorch/pull/161845#discussion_r2317299390)",2025-09-03 06:47:46+00:00,2025-09-04T09:19:32Z,,False,1,0,1,12,16,1,1,2025-09-04 09:19:32+00:00,55,207,False,True,False,True,False,False,1,0,0,28,12,16,1,1,,,,pytorch
162042,open,Update ctc loss docs float32 input required for CuDNN,redwrasse,"Discovered while working on https://github.com/pytorch/pytorch/pull/159106 the non-obvious requirement that inputs must be float32 to use CuDNN (https://github.com/pytorch/pytorch/pull/159106#issuecomment-3189981705), otherwise the native CUDA implementation is called.

Updates the docs.

",2025-09-03 06:40:23+00:00,2025-09-19T20:46:08Z,,False,10,1,2,3,2,1,11,,53,290,False,False,False,True,False,False,1,8,1911,7,4,3,1,2,2.0,9.0,2025-09-05T15:49:23Z,pytorch
162041,closed,[AsyncTP] Use assertEqual instead of allClose for bf16 tests,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162041
* #162040


The async tp result and regular MM result are very close. If we adjust the allclose threshold, the test succeeds. This seems to indicate that the error is from numerical error of low precision.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-03 05:52:56+00:00,2025-09-08T16:14:00Z,,False,3,3,5,1,1,1,6,2025-09-08 16:12:56+00:00,60,388,False,False,False,False,False,False,1,2,493,80383,46744,33639,1,5,4.0,2.0,2025-09-03T15:25:25Z,pytorch
162040,closed,[AsyncTP] Fixes AsyncMM,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162041
* __->__ #162040

The original implementation set beta to be 1, which cause the out (C) being added to the the output. Thus if the output is not initialized as zero beforehand, the output can be incorrect.

Removing the alpha and beta fixes the issue.

Thanks @ngimel to figure out the root cause.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-03 05:52:53+00:00,2025-09-08T10:55:08Z,,False,10,0,5,4,1,2,10,2025-09-08 10:54:03+00:00,23,473,False,True,False,False,False,False,2,9,1752,80375,46740,33635,1,5,4.0,9.0,2025-09-03T15:31:18Z,pytorch
162038,open,Fix formatting in error message for custom_op `mutates_args` validation,SigureMo,"Add the missing <code>`</code> and whitespace.
",2025-09-03 03:33:12+00:00,2025-09-03T20:33:39Z,,False,2,0,1,1,1,1,2,,71,47,False,True,False,False,False,False,1,0,9,2,1,1,1,1,1.0,1.0,2025-09-03T16:30:27Z,pytorch
162037,closed,Optimize AMP custom_backend_name error message,zeshengzong,"Print out amp target dtype and let custom backend easier find out expected dtype while integration.

## Test Result

### Before
```python
In [1]: import torch
   ...: import torch_openreg
   ...: 
   ...: a = torch.randn(3, 4)
   ...: b = torch.randn(4, 2)
   ...: with torch.autocast(""openreg"", dtype=torch.float16):
   ...:     torch.mm(a, b)
   ...: 
/home/coder/code/pytorch/torch/amp/autocast_mode.py:332: UserWarning: In openreg autocast, but the target dtype is not supported. Disabling autocast.
 openreg Autocast only supports dtypes of torch.float32 currently.
  warnings.warn(error_message
```

### After
```python
In [1]: import torch
   ...: import torch_openreg
   ...: 
   ...: a = torch.randn(3, 4)
   ...: b = torch.randn(4, 2)
   ...: with torch.autocast(""openreg"", dtype=torch.float16):
   ...:     torch.mm(a, b)
   ...: 

/home/coder/code/pytorch/torch/amp/autocast_mode.py:332: UserWarning: In openreg autocast, but the target dtype torch.float16 is not supported. Disabling autocast.
 openreg Autocast only supports dtypes of torch.float32 currently.
  warnings.warn(error_message)
```


cc @mcarilli @ptrblck @leslie-fang-intel @jgong5",2025-09-03 03:00:45+00:00,2025-09-04T08:29:05Z,,False,7,0,1,1,1,1,7,2025-09-04 08:28:01+00:00,46,1159,False,False,False,False,False,False,1,6,1422,2,1,1,1,1,3.0,6.0,2025-09-03T03:21:00Z,pytorch
162035,closed,[SymmMem][CI] Make sure group names are consistent,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162035

Unblocking #161741

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-03 01:43:46+00:00,2025-09-03T20:41:31Z,,False,5,0,1,15,8,1,5,2025-09-03 20:40:27+00:00,50,190,False,False,False,False,False,False,1,4,686,23,15,8,1,1,4.0,5.0,2025-09-03T01:46:17Z,pytorch
162034,closed,[Inductor UT][Intel GPU] Align the _has_sufficient_memory check with CUDA,hoshibara,"Fixes the issue caused by #143553.
Align the _has_sufficient_memory check with CUDA.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 01:40:49+00:00,2025-09-03T15:35:22Z,,False,5,0,2,9,3,2,5,2025-09-03 15:35:22+00:00,73,288,False,True,False,False,False,False,2,3,256,12,9,3,1,2,1.0,3.0,2025-09-03T01:52:58Z,pytorch
162033,open,[RFC] Switch all of c10d to use .h instead of .hpp,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #162033

It would have saved me some grief in our internal build system if the headers
were .h instead of .hpp, since the .hpp were missed by some .h only globs.  I
did this with claude code so no big deal if it is rejected.

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-03 01:36:45+00:00,2025-09-18T13:34:02Z,,False,12,0,1,11559,11351,185,12,,50,445,False,False,False,False,False,False,185,11,2125,22910,11559,11351,1,1,5.0,12.0,2025-09-03T04:09:33Z,pytorch
162031,open,Add Loads from fixed inputs,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162031
* #161118


## TODO
Check on multi indices
```Python

    @cute.jit
    def score_mod(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):
        in_ptr4 = buffers[0]
        tmp0 = tSrS_ssa
        tmp1 = b_idx
        tmp2 = h_idx
        tmp3 = cute.make_fragment(1, cutlass.Int32)
        tmp4 = tmp3.store(32*tmp1 + tmp2)
        tmp5 = cute.make_fragment(1, cutlass.BFloat16)
        tmp6 = tmp3[0]
        tmp7 = tmp5[0] = (in_ptr4[tmp6])
        tmp8 = (tmp5.load()).to(cutlass.Float32)
        tmp9 = (tmp0 + tmp8)
        tSrS_ssa = tmp9

        return tSrS_ssa
        
 ```
 
I dont think that 
```
        tmp4 = tmp3.store(32*tmp1 + tmp2)
        tmp5 = cute.make_fragment(1, cutlass.BFloat16)
        tmp6 = tmp3[0]
        tmp7 = tmp5[0] = (in_ptr4[tmp6]
        
```
      
 is right since this tmp6 value will be larger than the actual index dim int his case its B -> see if its possible to 1d index

",2025-09-03 01:14:49+00:00,2025-09-20T05:20:03Z,,False,1,0,15,280,15,4,1,,27,1010,False,True,False,False,False,False,4,0,0,79525,54743,24782,1,15,,,,pytorch
162030,closed,[inductor] turn on loaf (for oss) by default,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162030



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-03 00:57:07+00:00,2025-09-24T06:03:09Z,,False,12,0,20,13,4,3,12,2025-09-24 06:02:05+00:00,44,310,False,False,False,False,False,False,3,11,5508,120060,77298,42762,1,19,4.0,12.0,2025-09-09T23:57:28Z,pytorch
162029,closed,[inductor] Reorder input nodes in pattern matcher,angelayi,"Fixes https://github.com/pytorch/pytorch/issues/162019

Given 
```
def graph():
    input_1 = empty()
    output_1 = relu(input_1)
    output_use_1 = sqrt(output_1)
    input_2 = empty()
    mul = input_2 * 2
    output_2 = add(mul, output_1)
    final_node = mm(output_use_1, output_2)
```

```
def pattern(input_1, input_2)
    tmp1 = relu(input_1)
    tmp2 = input_2 + tmp1
    return tmp1, tmp2

def replacement(input_1, input_2)
    return fused_relu_add(input_1, input_2)
```

The existing pattern matcher will incorrectly place the `fused_relu_add` before `sqrt`, using input `mul` before it has been declared:
```
def graph_replaced_bad():
    input_1 = empty()
    output_1, output_2 = fused_relu_add(input_1, mul)  # mul used before it is declared
    output_use_1 = sqrt(output_1)
    input_2 = empty()
    mul = input_2 * 2
    output_2 = add(mul, output_1)
    final_node = mm(output_use_1, output_2)
```

so instead before doing the replacement we will reorder the graph so that all input nodes will be ordered before the output nodes:
```
def reordered_graph():
    input_1 = empty()
    input_2 = empty()  # moved
    mul = input_2 * 2  # moved
    output_1 = relu(input_1)
    output_use_1 = sqrt(output_1)
    output_2 = add(mul, output_1)
    final_node = mm(output_use_1, output_2)
```

and after replacing:
```
def graph_replaced_good():
    input_1 = empty()
    input_2 = empty()  # moved
    mul = input_2 * 2  # moved
    output_1, output2 = fused_relu_add(input1, mul)
    output_use_1 = sqrt(output_1)
    final_node = mm(output_use_1, output_2)
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 00:48:10+00:00,2025-09-09T20:48:50Z,,False,1,4,1,156,79,2,5,2025-09-09 20:48:50+00:00,49,1779,False,True,False,False,False,False,2,0,0,235,156,79,1,1,2.0,0.0,2025-09-03T13:32:32Z,pytorch
162028,closed,[ez][inductor] add a few outer dimension reduction cases for LOAF,shunting314,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162102
* #162030
* #162126
* #162101
* #162221
* __->__ #162028


For the not able to fuse issue reported here: https://github.com/pytorch/pytorch/issues/93718 , LOAF can fuse the outer dimension softmax into a single kernel and brings 1.87x speedup for the example shape mentioned in the issue.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-03 00:21:20+00:00,2025-09-05T09:31:21Z,,False,3,0,1,42,0,1,3,2025-09-05 09:30:16+00:00,65,578,False,False,False,False,False,False,1,2,493,42,42,0,1,1,4.0,2.0,2025-09-03T22:03:43Z,pytorch
162027,open,[dynamo] add error_on_graph_break kwarg to torch.compile,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162027
* #162023
* #161747
* #161739



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-03 00:02:22+00:00,2025-09-04T04:01:03Z,,False,2,0,4,67,17,4,2,,56,296,False,False,False,False,False,False,4,0,0,52454,28857,23597,1,4,,,,pytorch
162026,closed,[SymmMem] Increase signal pad size for NVL72,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162026

so that the signal calls do not step on each other's foot.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-02 23:54:20+00:00,2025-09-04T17:42:45Z,,False,3,2,1,11,1,1,5,2025-09-04 17:41:41+00:00,44,249,False,False,False,False,False,False,1,2,493,12,11,1,1,1,3.0,2.0,2025-09-04T17:16:48Z,pytorch
162025,open,[hop] refactor check input alias and mutation to be a graph pass,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162754
* #161732
* __->__ #162025
* #161808
* #161664
* #161557

",2025-09-02 23:24:45+00:00,2025-09-23T22:42:42Z,,False,1,2,6,76,195,8,3,,64,144,False,False,False,False,False,True,8,0,0,97559,61583,35976,1,6,2.0,0.0,2025-09-19T12:17:11Z,pytorch
162024,closed,don't use detect_fake_mode in materialize_as_subgraph,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161732
* #162025
* __->__ #162024
* #161808
* #161664
* #161557

",2025-09-02 23:24:42+00:00,2025-09-02T23:31:33Z,,False,2,0,1,6,2,1,2,2025-09-02 23:30:44+00:00,53,144,False,False,False,False,False,False,1,0,0,8,6,2,1,1,,,,pytorch
162023,open,[dynamo] add error_on_graph_break() to compiler namespace,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162027
* __->__ #162023
* #161747
* #161739



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-09-02 23:21:49+00:00,2025-09-04T20:01:22Z,,False,1,0,5,91,59,5,1,,57,304,False,False,False,False,False,False,5,0,0,52525,28882,23643,1,5,,,,pytorch
162022,closed,[FP8][cuBLAS][H100] only test fp32 outputs for rowwise `_scaled_mm` on H100,eqy,"only cuBLAS supports float32 output and cuBLAS only supports rowwise for SM 9.0

Intended to land after #161305

cc @csarofeen @ptrblck @xwang233",2025-09-02 23:06:40+00:00,2025-09-19T15:19:22Z,,False,3,2,1,25,13,1,5,2025-09-19 15:18:17+00:00,75,145,False,False,False,False,False,False,1,2,498,38,25,13,1,1,3.0,2.0,2025-09-18T14:09:28Z,pytorch
162021,closed,[FSDP][Collectives] skipping reduce_scatter when world size is 1,anshul-si,"**Summary:** In its current state, FSDP collectives uses cuda synchronizations and communication ops regardless of what the world size is. However, now that replicate will use FSDP, there will be instances where group size = 1 and these synchronizations and ops will be used needlessly. I have updated fsdp_collectives to skip reduce_scatter in the foreach_reduce API when world_size ‎ = 1. I have created edited a test that uses CommDebugMode to verify that the reduce_scatter has been removed. I also edited an affected test which used 1-way FSDP by verifying and changing its assert statements for CommDebugMode. I have also added a test command. 


**Test Cases**
1. pytest test/distributed/_composable/fsdp/test_fully_shard_training.py -k test_train_parity_single_worldsize1
2. pytest test/distributed/_composable/test_composability/test_2d_composability.py -k test_tp_with_fsdp_offloading

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162021



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-09-02 22:54:44+00:00,2025-09-16T17:19:15Z,,False,3,1,6,38,24,3,4,2025-09-16 17:18:10+00:00,64,1093,False,True,False,False,False,False,3,2,494,59306,40972,18334,1,6,4.0,2.0,2025-09-03T17:36:34Z,pytorch
162020,closed,[inductor] Fix int64 from MutationOutput Buffer,yushangdi,"Summary:
When we have a user defined triton kernel, it marks the mutated outputs as `MutationOutput` with a NoneLayout. This MutationOutput may later be used as input to another inductor-generated triton kernel.


When we determine whether to use int32 or int64 for the inductor generated triton kernel, we need to look at the number of elements for all buffers involved. If one of the buffer is a MutationOutput, we should still consider it's number of elements, instead of skipping it.

To get a hint on the MutationOutput size, we look at the buffers corresponding to `mutation_names` in MutationOutput.

Test Plan:
```
buck run mode/opt  fbcode//caffe2/test/inductor:test_aot_inductor -- -r test_autotune_int64_user_defined_triton_kernel
```

Differential Revision: D81530083




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 22:43:57+00:00,2025-09-04T09:49:04Z,,False,10,4,1,81,0,3,14,2025-09-04 09:48:01+00:00,47,985,False,True,False,False,False,False,3,4,2118,81,81,0,1,1,5.0,5.0,2025-09-03T00:51:51Z,pytorch
162018,open,[torch elastic] support pid in log prefix template,xunnanxu,"Summary:
## Before

pid was not supported and required logger to dump them individually
This led to challenge matching the pids as not all loggers would dump them.

## After

`${pid}` is supported as a placeholder in `TORCHELASTIC_LOG_LINE_PREFIX_TEMPLATE`.

To do that, this changeset:
- Pushed down template materialization from agent to `multiprocessing.PContext` as that's where it gets the `pid`.
- Shifted the docstrings as needed.
- Default `LaunchConfig.log_line_prefix_template` to following env var if persists to consolidate the logic

Test Plan:
CI

Run job

Differential Revision: D81177345




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 22:35:09+00:00,2025-09-09T03:40:57Z,,False,9,0,1,113,48,5,9,,50,684,False,True,False,True,False,False,5,0,0,161,113,48,1,1,,,,pytorch
162017,closed,[inductor] performance model interface,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162017
* #161534

# why

- enable the system to register performance model ranking functions
- those functions are then used to rank th choices before benchmarking
- enable more modular, faster, more expansive autotuning

# what

- PerformanceModelChoices, expansion of inductor choices
- mechanism
  - users can register functions (through decorators or registration
    function)
  - functions are hardware, template, op specific
  - when an input hits the performance model, if we have any functions
    registered for that template/op combo
    - we expand the config space for that template
    - we ask the function to rank all the choices
    - we forward filter down to topk
- control
  - topk: -1 to say ""same as default search space"" or > 1 for specific
  - topk: 0 turns off any performance modeling
  - discard unranked: controls what to do with choices that have not
    been estimated (discard, or keep)

You can check out the testing code to see examples of toy functions and
their integration into the system

This choice handler is not turned on by default

Even if the choice handler is turned on, it will be a noop if no
functions are registered, or if topk = 0

# testing

```
python -m pytest test/inductor/test_performance_model_interface_e2e.py
python -m pytest test/inductor/test_performance_model_interface.py
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @mlazos

Differential Revision: [D81534564](https://our.internmc.facebook.com/intern/diff/D81534564)",2025-09-02 22:29:20+00:00,2025-09-18T17:05:42Z,,False,9,0,22,1356,0,10,9,2025-09-18 17:05:42+00:00,38,1714,False,False,False,False,False,False,10,8,1264,107748,65447,42301,1,22,1.0,8.0,2025-09-02T22:34:39Z,pytorch
162016,open,Support custom callback functions in schedule,H-Huang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162016
* #161072

This is going to be used in https://github.com/pytorch/torchtitan/issues/1682

Add a `register_custom_function` to the `_PipelineScheduleRuntime` which allows users to implement any custom function to replace the runtime operation dynamically.

The signature of the callback should look like:

```python
class _CustomFunctionProtocol(Protocol):
    def __call__(self, action: _Action, ctx: _PipelineContext) -> None: ...
```

`_PipelineContext` contains a reference to the schedule which is executing the operations.

### Testing

Added a test which adds custom methods for `FORWARD` and `OVERLAP_F_B` which are just the same implementations as those used in the default schedule runtime. Check that the schedule can still run, numerics are correct, and the callbacks are executed the correct number of times.

cc @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-02 22:25:39+00:00,2025-09-08T21:20:41Z,,False,1,1,4,299,23,2,2,,45,1001,False,False,False,False,False,False,2,0,0,509,391,118,1,4,,,,pytorch
162015,closed,[c10d] Lessen density of barrier warning,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162015

Warnings are great, but too dense when there are many ranks.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 22:06:24+00:00,2025-09-03T02:21:59Z,,False,3,3,2,11,11,2,6,2025-09-03 02:20:56+00:00,40,232,False,False,False,False,False,False,2,2,532,24,12,12,1,2,4.0,4.0,2025-09-02T22:25:20Z,pytorch
162014,open,[inductor] pdl: enable launch and deduplicate waits,v0i0,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162014
* #160928



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-02 22:04:28+00:00,2025-09-25T01:07:06Z,,False,7,6,8,51,3,3,13,,51,315,False,False,False,False,False,False,3,6,1461,167430,114856,52574,1,8,3.0,9.0,2025-09-02T22:09:32Z,pytorch
162013,open,Fix dynamic shapes,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162166
* #162089
* __->__ #162013
* #161977
* #161653

Previously we were not setting the right config for new export. 

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81598900](https://our.internmc.facebook.com/intern/diff/D81598900)",2025-09-02 21:54:34+00:00,2025-09-04T13:56:14Z,,False,5,1,4,2,12,2,6,,18,486,False,True,False,False,False,False,2,4,492,45713,23918,21795,1,4,3.0,4.0,2025-09-02T23:03:06Z,pytorch
162012,closed,[pt2e] Avoid getting model device once per node,andrewor14,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162012

**Summary:** Previously, we call `assert_and_get_unqiue_device`
once per node in both prepare and convert. This is expensive and
unnecessary since the model device is the same across all nodes,
so we should just call this once in the beginning and reuse the
same model device across all the nodes.

**Test Plan:**
python test/test_quantization.py -k TestQuantizePT2E

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-09-02 21:35:26+00:00,2025-09-04T18:31:55Z,,False,2,0,1,87,13,5,2,2025-09-04 18:31:50+00:00,47,518,False,False,False,False,False,False,5,1,6,100,87,13,1,1,1.0,1.0,2025-09-04T18:31:51Z,pytorch
162011,open,[SymmMem] Replace unnecessary nvshmemx_collective_launch,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162011

The `nvshmemx_collective_launch` function must be used when the CUDA kernels use NVSHMEM synchronization or collective APIs (e.g., nvshmem_wait, nvshmem_barrier, nvshmem_barrier_all, or any other collective operation). CUDA kernels that do not use synchronizing NVSHMEM APIs are not required to be launched by this API. 

Also added `NVSHMEM_CHECK` or `CUDA_CHECK` around the launch calls.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 21:27:30+00:00,2025-09-12T19:33:48Z,,False,1,5,1,25,17,3,6,,56,561,False,False,False,False,False,False,3,0,0,42,25,17,1,1,2.0,0.0,2025-09-03T17:37:47Z,pytorch
162010,open,DO NOT MERGE: testing fallback commands,seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162010
* #162009

Adds a workflow that tests that our fallback commands work as expected.

I don't think we should merge this

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-09-02 21:25:19+00:00,2025-09-03T01:50:05Z,,False,1,0,5,53,0,1,1,,39,261,False,False,False,False,False,False,1,0,0,77,69,8,1,5,,,,pytorch
162009,closed,build: Add fallback commands to setup.py,seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162010
* __->__ #162009

Adds fallback commands for the following:
* python setup.py install
* python setup.py develop

Ideally these should just work and should provide backwards compat.

Thought process here is that multiple people rely on these commands and just because setuptools wants to drop support for this I don't think a lot of our downstream users who build from source are expecting these to be gone.

This should provide some room for developers to move away from these commands until we have a unified frontend for doing all of these commands that should abstract most of these away.

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-09-02 21:25:13+00:00,2025-09-03T02:57:16Z,,False,9,2,5,35,0,1,11,2025-09-03 02:56:12+00:00,40,727,False,False,False,False,False,False,1,8,2687,51,43,8,1,5,4.0,8.0,2025-09-02T21:33:48Z,pytorch
162008,open,[CPUBLas] Add specialization to maintain good perf,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162846
* __->__ #162008



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-09-02 21:06:39+00:00,2025-09-12T22:20:26Z,,False,1,0,2,55,0,1,1,,50,187,False,False,False,False,False,False,1,0,0,86888,54613,32275,1,2,1.0,0.0,2025-09-02T21:17:50Z,pytorch
162007,closed,[MPS] enable cat op for sparse,Isalia20,"Enable cat op for sparse on MPS

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen",2025-09-02 20:48:34+00:00,2025-09-12T19:08:47Z,,False,10,0,1,2,2,2,10,2025-09-12 19:07:43+00:00,30,90,False,False,False,False,False,False,2,8,1707,4,2,2,1,1,4.0,9.0,2025-09-02T21:07:19Z,pytorch
162006,closed,[SymmMem] Use non-blocking version of getmem,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162006

As titled, so that the `getmem` calls in the loop are non-blocking, so that we max out the issuance rate.
Also had a single `nvshmem_quiet()` at the end to make sure all the getmem calls complete.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 20:42:48+00:00,2025-09-02T23:56:27Z,,False,3,0,1,6,2,1,3,2025-09-02 23:55:24+00:00,44,368,False,False,False,False,False,False,1,2,493,8,6,2,1,1,3.0,2.0,2025-09-02T20:48:11Z,pytorch
162005,closed,[aot-compile] strip internal tracebacks for non-verbose graph breaks + include user file/lineno,dolpm,"pytest test/dynamo/test_aot_compile.py -k test_aot_compile_graph_break_error_fmt


before
```
Traceback (most recent call last):
  File ""/data/users/$USER/vllm-tests/graph-break.py"", line 15, in <module>
    aot_compiled_fn = compiled.aot_compile((example_inputs, {}))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/eval_frame.py"", line 717, in aot_compile
    return aot_compile_fullgraph(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/aot_compile.py"", line 132, in aot_compile_fullgraph
    capture_output = convert_frame.fullgraph_capture(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 947, in fullgraph_capture
    dynamo_output = compile_frame(
                    ^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 1020, in compile_frame
    bytecode, tracer_output = transform_code_object(code, transform)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/bytecode_transformation.py"", line 1592, in transform_code_object
    tracer_output = transformations(instructions, code_options)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 992, in transform
    tracer_output = trace_frame(
                    ^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 312, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 821, in trace_frame
    run_tracer()
  File ""/data/users/$USER/pytorch/torch/_dynamo/convert_frame.py"", line 803, in run_tracer
    tracer.run()
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 1472, in run
    while self.step():
          ^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 1342, in step
    self.dispatch_table[inst.opcode](self, inst)
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 902, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 3364, in CALL
    self._call(inst)
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 3358, in _call
    self.call_function(fn, args, kwargs)
  File ""/data/users/$USER/pytorch/torch/_dynamo/symbolic_convert.py"", line 1260, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/variables/lazy.py"", line 212, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/variables/functions.py"", line 1513, in call_function
    unimplemented_v2(
  File ""/data/users/$USER/pytorch/torch/_dynamo/exc.py"", line 596, in unimplemented_v2
    raise Unsupported(msg)
torch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`
  Explanation: User-inserted graph break. Message: None
  Hint: Remove the `torch._dynamo.graph_break()` call.

  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`

 For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html
```
after
```
Traceback (most recent call last):
  File ""/data/users/$USER/vllm-tests/graph-break.py"", line 15, in <module>
    aot_compiled_fn = compiled.aot_compile((example_inputs, {}))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/eval_frame.py"", line 737, in aot_compile
    raise e.with_traceback(None) from e.__cause__  # User compiler error
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`
  Explanation: User-inserted graph break. Message: None
  Hint: Remove the `torch._dynamo.graph_break()` call.

  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`

 For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html

from user code:
   File ""/data/users/$USER/vllm-tests/graph-break.py"", line 5, in foo
    torch._dynamo.graph_break()

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=""+dynamo""
```
consistent w/ std torch.compile
```
Traceback (most recent call last):
  File ""/data/users/$USER/vllm-tests/graph-break.py"", line 16, in <module>
    res = compiled(*example_inputs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data/users/$USER/pytorch/torch/_dynamo/eval_frame.py"", line 850, in compile_wrapper
    raise e.with_traceback(None) from e.__cause__  # User compiler error
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.Unsupported: Call to `torch._dynamo.graph_break()`
  Explanation: User-inserted graph break. Message: None
  Hint: Remove the `torch._dynamo.graph_break()` call.

  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`

 For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html

from user code:
   File ""/data/users/$USER/vllm-tests/graph-break.py"", line 5, in foo
    torch._dynamo.graph_break()

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=""+dynamo""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-02 20:36:27+00:00,2025-09-03T23:20:53Z,,False,8,2,1,51,10,2,10,2025-09-03 23:19:50+00:00,95,6088,False,True,False,False,False,False,2,6,1360,61,51,10,1,1,4.0,7.0,2025-09-02T21:14:43Z,pytorch
162003,closed,[SymmMem] Better tuning of A2AV based on accurate node boundary,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #162003

Use `world_within_direct_access()` to distinguish intra- vs inter- node. 
Previously we assumed a fixed node size of 8, which is not true for NVL72.

Also added env var `TORCH_SYMMMEM_NBLOCKS` for control.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-02 20:26:22+00:00,2025-09-09T04:21:16Z,,False,3,6,3,39,23,2,9,2025-09-09 04:18:19+00:00,63,396,False,True,False,False,False,False,2,2,493,33238,19988,13250,1,3,5.0,2.0,2025-09-03T17:42:12Z,pytorch
162001,closed,[BE] Cleanup stale comments/copy from `gemm` ,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162008
* __->__ #162001
* #161999

Followup after https://github.com/pytorch/pytorch/pull/154012

Since the introduction of `gemm_no_downcast_stub` it's no longer necessary to allocate temporary array and then manually implement the `beta` logic in the codebase",2025-09-02 19:58:08+00:00,2025-09-11T15:49:52Z,,False,11,0,1,2,32,1,11,2025-09-11 15:48:46+00:00,45,340,False,False,False,False,False,False,1,10,2863,34,2,32,1,1,4.0,10.0,2025-09-02T23:14:03Z,pytorch
162000,closed,Build vLLM nightly wheels,huydhn,"This uses the same approach as building triton wheel where we publish a nightly wheel for vLLM whenever its pinned commit is updated.  The key change is to use `pytorch/manylinux2_28-builder` as the base image to build vLLM, so there are a couple of changes on the vLLM Dockerfile used by lumen_cli

1. `pytorch/manylinux2_28-builder` is RedHat instead of Debian-based, so no apt-get
2. Fix a bug in `.github/actions/build-external-packages/action.yml` where `CUDA_VERSION` is not set correctly, preventing CUDA 12.9 build
3. Fix a bug in `.github/actions/build-external-packages/action.yml` where `TORCH_WHEELS_PATH` is not set correctly and always defaulted to `dist`
4. In vLLM Dockerfile, use the correct index for the selected CUDA version, i.e. https://download.pytorch.org/whl/nightly/cu12[89] for CUDA 12.[89]
5. Install torch, vision, audio in one command. Unlike the CI image `pytorch-linux-jammy-cuda12.8-cudnn9-py3.12-gcc11-vllm`, `pytorch/manylinux2_28-builder` doesn't have any torch dependencies preinstalled
6. Bump xformers version to 0.0.32.post2 now that PyTorch 2.8.0 has been landed on vLLM

We need to prepare 3 wheels for vLLM, xformers, and flashinfer-python. And I rename them in the same convention as PyTorch nightlies `MAJOR.MINOR.PATCH.devYYYYMMDD` so that vLLM nightlies will work with torch nightlies on the same date.

### Usage

* Install latest nightlies
```
pip install --pre torch torchvision torchaudio vllm xformers flashinfer_python \
  --index-url https://download.pytorch.org/whl/nightly/cu129
```

* Install a specific version
```
pip install --pre torch==2.9.0.dev20250903 torchvision torchaudio \
  vllm==1.0.0.dev20250903 \
  xformers=0.0.33.dev20250903 \
  flashinfer_python=0.2.14.dev20250903 \
  --index-url https://download.pytorch.org/whl/nightly/cu129
```",2025-09-02 19:54:41+00:00,2025-09-15T23:08:57Z,,False,7,13,39,348,105,4,20,2025-09-07 06:09:20+00:00,25,1806,False,True,False,True,False,False,4,6,1592,12226,8432,3794,1,30,4.0,6.0,2025-09-03T23:19:47Z,pytorch
161999,closed,[BLAS] Avoid downcasts for fp16fp16->fp32 BLAS,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162008
* #162001
* __->__ #161999

Followup after https://github.com/pytorch/pytorch/pull/154012

Fixes CPU part of https://github.com/pytorch/pytorch/issues/160841

cc @jianyuh @nikitaved @mruberry @walterddr @xwang233 @Lezcano",2025-09-02 19:54:18+00:00,2025-09-04T23:36:35Z,,False,7,0,1,5,5,1,7,2025-09-04 23:35:30+00:00,46,307,False,True,False,False,False,False,1,6,1242,10,5,5,1,1,4.0,6.0,2025-09-03T05:21:05Z,pytorch
161998,closed,Perf nitpicks on python_arg_parser's is_int_or_symint_list,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161998

This function has come up in DTensor perf work, and I had a nitpick on #160256 so here it is. I have neither compiled nor measured this, but am reasonably confident it's better nonetheless.",2025-09-02 19:46:27+00:00,2025-09-03T05:39:37Z,,False,3,0,1,4,4,1,3,2025-09-03 05:38:33+00:00,58,283,False,False,False,False,False,False,1,2,493,8,4,4,1,1,2.0,2.0,2025-09-03T03:04:01Z,pytorch
161996,closed,[Reland][Inductor] Prune configs that require more shared memory than the hardware limit. ,wychi,"Summary:
This is a re-land of [PR161040](https://github.com/pytorch/pytorch/pull/161040), which had previously caused test failures on AMD GPUs. The tests are now configured to target only NVIDIA GPUs.

This diff removes configurations that exceed the hardware shared memory limit, which causes the following compilation error:
```
No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 327680 Hardware limit:232448 Reducing block sizes or `num_stages` may help.
```

Test Plan:
```
pytest test/inductor/test_max_autotune.py
pytest test/inductor/test_triton_heuristics.py
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 19:09:15+00:00,2025-09-03T04:24:15Z,,False,3,0,4,126,17,5,3,2025-09-03 04:23:11+00:00,90,803,False,False,False,False,False,False,5,2,493,147,128,19,2,4,3.0,2.0,2025-09-02T19:54:44Z,pytorch
161994,open,"Reapply ""Make functionalization `ViewMeta` serializable with pickle. (#143712)""",bdhirsh,"Attempted rebase of https://github.com/pytorch/pytorch/pull/143712. First I'm sanity checking that OSS CI passes. I'll probably need to abandon this PR and make a diff to check internal CI

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161994

This reverts commit 6c713ccb5e0df227dd5b630057cbccd373cbe7d6.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81524507](https://our.internmc.facebook.com/intern/diff/D81524507)",2025-09-02 18:30:41+00:00,2025-09-23T23:42:02Z,,False,8,0,7,947,397,35,8,,79,610,False,False,False,False,False,False,35,7,1165,60261,25867,34394,1,7,5.0,8.0,2025-09-02T20:32:54Z,pytorch
161992,closed,[hipify] Replace cudaStreamCaptureStatusNone,YulunW,"Replacing additional cuda symbols to hip symbols

Differential Revision: D81420086


",2025-09-02 18:23:49+00:00,2025-09-03T20:24:38Z,,False,5,0,1,1,0,1,5,2025-09-03 20:23:35+00:00,44,85,False,False,False,False,False,False,1,3,535,1,1,0,1,1,4.0,3.0,2025-09-02T18:27:22Z,pytorch
161991,open,Add meta function for native_multi_head_attention,haowu14,"Test Plan:
lowering passes
```
TORCH_LOGS=""+dynamo"" TORCHDYNAMO_VERBOSE=1 RECURSIVE_PM=1 buck2 run @//mode/opt @//mode/inplace -c fbcode.enable_gpu_sections=true -c fbcode.platform=platform010 -c fbcode.nvcc_arch=a100,h100 -c fbcode.split-dwarf=true -c fbcode.dwp=true -c fbcode.enable_distributed_thinlto=true -c fbcode.use_link_groups=true inference_enablement/model_processing/infra/components/lowering/re:re_cinder -- -r '{""aot_inductor"": {""serialized_inference_model_input_path"":""ads_storage_fblearner/tree/user/facebook/fblearner/predictor/771029565/0/gpu_lowering/input.predictor.precompute.mix.gpu.merge"",""serialized_inference_model_output_path"":""ads_storage_fblearner/tree/user/facebook/fblearner/predictor/771029565/0/gpu_lowering/sm90_arm_output.predictor.precompute.mix.gpu.merge"",""submodule_names_to_lower"":[""merge""],""inductor_lowering_context"":{""aot_inductor_lowering_settings"":{""max_batch_size"":8192,""min_acc_module_size"":10,""workdir"":""/tmp/inductor_lowering_i_9kda5t"",""name"":""gmpp_gpu_inductor_lowering"",""dll_name"":""inductor_engine.so"",""use_scripting"":true,""preset_lowerer"":"""",""precision"":1,""output_precision"":1,""remote_cache_file_path_folder"":""ads_storage_fblearner/tree/user/facebook/fblearner/predictor/771029565/"",""save_remote_cache"":true,""aot_inductor_config"":""{'\''max_autotune'\'': True, '\''comprehensive_padding'\'': False}"",""disable_dynamic_shapes"":false,""remove_unexpected_type_cast"":true,""disable_constraint_solver"":false,""sample_input_tile_factor"":1,""disable_acc_tracer"":false,""generate_sample_inputs"":true,""tile_sample_input_by_dynamic_shape"":false,""node_replacement_dict"":"""",""auto_dynamic_shapes"":false,""auto_dynamic_shapes_min_size"":1,""auto_dynamic_shapes_max_size"":1048576,""max_acc_splits"":-1,""dynamic_size"":-1,""pre_dispatch_export"":true,""merge_split_optimization"":false}},""model_entity_id"":771029565,""model_snapshot_id"":0,""add_sample_inputs"":true,""hardware_type"":0,""platform_arch"":2,""lowering_lib_pkg"":""ien.lower.cg1:132""}}'
```
E2E test: f787959188

Rollback Plan:

Reviewed By: henryoier

Differential Revision: D80179945


",2025-09-02 18:08:19+00:00,2025-09-20T01:07:37Z,,False,8,0,1,69,1,2,8,,49,2060,False,False,False,False,False,False,2,2,436,70,69,1,1,1,2.0,3.0,2025-09-09T02:57:17Z,pytorch
161990,closed,[aot precompile] Handle closure variables.,zhxchen17,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161990

We previously assume aot precompile should only work on non closures. This is hard to enforce in practice because we will see a lot of cases with decorater (e.g. hugging face models)
```
def check_inputs(fn):
    def _fn(self, *args, **kwargs):
        for arg in args:
            assert arg.shape[0] > 1

        return fn(*args, **kwargs)
    return _fn


@check_inputs
def foo(x, y):
    a = x + x
    b = y + y
    c = a + b
    return c
```
It doesn't make sense to not support these cases since they are straightfowrad to do.

This PR adds the logic to handle closure and make sure they can be precompiled properly.

Differential Revision: [D81509535](https://our.internmc.facebook.com/intern/diff/D81509535/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-02 18:04:08+00:00,2025-09-02T23:17:44Z,,False,7,0,2,58,7,4,7,2025-09-02 22:26:07+00:00,42,982,False,False,False,False,False,False,4,3,522,67,59,8,1,2,3.0,3.0,2025-09-02T18:36:25Z,pytorch
161988,closed,Fix largeTensorTest malfunction on XPU,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161988

# Motivation
https://github.com/pytorch/pytorch/pull/143553/files#diff-6492991193449e118ff0c8d42ca544cc38a73604e505ff246a3c711aeab91748R1345 makes `largeTensorTest` malfunction on XPU. This PR aims to fix it.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 17:17:46+00:00,2025-09-04T16:48:27Z,,False,4,0,8,35,18,5,4,2025-09-04 16:10:06+00:00,38,507,False,True,False,False,False,False,5,3,588,75,46,29,1,8,4.0,4.0,2025-09-03T03:33:10Z,pytorch
161987,open,Check ASAN/UBSAN coverage in c10 the same way we do ATen & csrc,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161987

I'm not certain we have ASAN configured properly for c10. Let's make sure we do.",2025-09-02 17:14:29+00:00,2025-09-02T20:22:18Z,,False,2,0,3,62,1,4,2,,63,174,False,False,False,False,False,False,4,1,39,63,62,1,1,3,1.0,1.0,2025-09-02T19:15:38Z,pytorch
161986,closed,Argsort doc stable kwargs,cleonard530,"Fixes #129311

Updated torch.argsort documentation to reflect that the 'stable' parameter is a keyword argument and not a normal parameter.   

@albanD, @soulitzer  

",2025-09-02 17:01:48+00:00,2025-09-02T21:24:46Z,,False,8,0,2,3,1,1,8,2025-09-02 20:42:55+00:00,25,167,False,True,False,True,False,False,1,6,1567,216,5,211,1,2,2.0,7.0,2025-09-02T17:20:37Z,pytorch
161984,closed,[SymmMem] Add a helper API to distinguish intra- and inter- node,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162003
* __->__ #161984
* #161983

Added a helper API to tell if the world is entirely within a P2P domain or crosses network.
This is mainly for nblocks tuning purpose. (In later PRs)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-09-02 16:39:15+00:00,2025-09-04T17:39:07Z,,False,3,2,2,26,2,4,5,2025-09-04 17:38:02+00:00,64,360,False,False,False,False,False,False,4,2,493,56,43,13,1,2,3.0,2.0,2025-09-04T17:13:30Z,pytorch
161983,closed,[SymmMem] Increase minimum nthreads to cover sync needs in NVL72,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162003
* #161984
* __->__ #161983

`sync_remote_blocks` maps threads to peers. Previously min nthreads is warp size, which is too small to cover NVL72. Bumping it.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 16:39:11+00:00,2025-09-02T23:19:14Z,,False,3,2,2,19,10,1,5,2025-09-02 23:18:10+00:00,64,320,False,False,False,False,False,False,1,2,493,37,23,14,1,2,3.0,2.0,2025-09-02T17:05:57Z,pytorch
161982,closed,tmp local changs,laithsakka,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161982
* #160735
* #160655

",2025-09-02 16:22:16+00:00,2025-09-21T20:57:58Z,,False,3,0,1,49,27,1,3,2025-09-21 20:57:58+00:00,16,114,False,False,False,False,False,False,1,1,65,76,49,27,1,1,1.0,1.0,2025-09-21T20:57:58Z,pytorch
161981,closed,[ROCm/Windows] Fix build failures and support some BLAS calls,jammm,"* Support getrsBatched/geqrfBatched/gelsBatched on Windows ROCm (fixes https://github.com/ROCm/TheRock/issues/1367)
* Fix windows pytorch build with USE_DISTRIBUTED=ON by default

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-09-02 15:45:08+00:00,2025-09-03T20:27:19Z,,False,5,10,3,5,31,3,15,2025-09-03 20:26:16+00:00,61,370,False,True,False,False,False,False,3,3,507,36,5,31,2,3,6.0,3.0,2025-09-02T15:46:10Z,pytorch
161980,open,[TEST] Don't bother restoring requires_grad in flatten/unflatten,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #161980

Signed-off-by: Edward Yang <ezyang@meta.com>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 15:25:55+00:00,2025-09-12T17:35:54Z,,False,2,2,1,2,3,1,4,,64,228,False,False,False,False,False,False,1,0,0,5,2,3,1,1,2.0,0.0,2025-09-02T18:51:57Z,pytorch
161979,open,[RELAND v2] Close some sources of fake tensor leakage,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161979



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-02 14:59:40+00:00,2025-09-02T21:21:35Z,,False,1,0,1,239,30,4,1,,53,266,False,False,False,False,False,False,4,0,0,269,239,30,1,1,,,,pytorch
161978,closed,fusion of large accumulated reads only at ir level,xuanzhang816,"This is to revert some of the changes in https://github.com/pytorch/pytorch/pull/158667

In particular, we only disallow fusion of large accumulate read at IR level and not at scheduler level, as users can create their own custom fusion logics for the scheduler level. 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161978



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 14:45:59+00:00,2025-09-13T04:08:31Z,,False,10,0,2,33,22,4,10,2025-09-13 04:07:28+00:00,50,568,False,False,False,False,False,False,4,9,2344,98357,63726,34631,1,2,3.0,9.0,2025-09-04T01:54:11Z,pytorch
161977,open,Testing infra and some fixes,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162166
* #162089
* #162013
* __->__ #161977
* #161653

This PR is quite large in that it covers most of rough edges in the new strict export flow:
1. Handle nn_module_stack correctly now that we are tracing wrapper module 
2. module_call_spec needs to get queried from source directly because we are not running the bytecode anymore. 
3. Correct input and output handling. 



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81589677](https://our.internmc.facebook.com/intern/diff/D81589677)",2025-09-02 14:14:27+00:00,2025-09-04T13:56:13Z,,False,4,11,8,525,84,9,15,,28,742,False,True,False,False,False,False,9,3,480,157467,87049,70418,1,8,3.0,3.0,2025-09-02T19:03:27Z,pytorch
161976,closed,torch.zeros bound checks for symint,morrison-turnansky,"Fixes #161490

I added a bounds check for negative symints to create a better error message.

cc @albanD",2025-09-02 14:09:25+00:00,2025-09-06T05:38:48Z,,False,14,0,4,8,0,2,14,2025-09-06 05:37:46+00:00,35,104,False,True,False,False,False,False,2,12,3355,40,24,16,1,4,3.0,12.0,2025-09-02T14:14:59Z,pytorch
161975,closed,[WIP] Experiment with multiD Collectives Trie bucketing,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161975
* #161499
* #161406
* #161405



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 13:58:16+00:00,2025-09-24T14:19:06Z,,False,2,0,3,319,6,3,2,2025-09-24 14:19:06+00:00,55,327,False,False,False,False,False,False,3,0,0,668,480,188,1,3,,,,pytorch
161973,closed,Version pin fixup,zklaus,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161973
* #156713
* #156712
* #156711
* #156710

",2025-09-02 13:48:11+00:00,2025-09-02T13:49:30Z,,False,1,0,1,2,2,1,1,2025-09-02 13:49:30+00:00,17,134,False,True,False,False,False,False,1,0,0,4,2,2,1,1,,,,pytorch
161972,open,Better logging for triton gemm tuning compilation timeouts,AmdSampsa,"I've encountered a situtation where, during triton gemm tuning, a triton kernel (pre)compilation timed out.
Pain to debug, but with this PR slightly easier as it informs the user about the actual kernel where the compilation timed out.

When triton kernel compilation(s) time(s) out we get extra logging lines like this:
```bash
E0902 12:42:36.168925 1791674 /root/pytorch-me/torch/_inductor/select_algorithm.py:2816] [0/0] Timeout benchmark choice: TritonTemplateCaller(/tmp/torchinductor_root/rp/crpmahu73qmfbh7fhlwhat2rp2uuh6sfpssagpsrpgmgdg3wbijd.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=16, BLOCK_N=16, EVEN_K=False, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=8, num_stages=2, num_warps=1) 
E0902 12:42:36.169642 1791674 /root/pytorch-me/torch/_inductor/select_algorithm.py:2816] [0/0] Timeout benchmark choice: TritonTemplateCaller(/tmp/torchinductor_root/5a/c5arb7q5cveo4mwqy4mdgxzjmn6iemt73ue5ggxqrm4wfpzdm5wo.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=32, BLOCK_N=16, EVEN_K=False, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2) 
E0902 12:42:36.169867 1791674 /root/pytorch-me/torch/_inductor/select_algorithm.py:2816] [0/0] Timeout benchmark choice: TritonTemplateCaller(/tmp/torchinductor_root/tm/ctmk3xqhaatc6quyxhbnemyqz4gyxfz4k36nkc3so2sekgmso6s4.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4) 
E0902 12:42:36.169958 1791674 /root/pytorch-me/torch/_inductor/select_algorithm.py:2816] [0/0] Timeout benchmark choice: TritonTemplateCaller(/tmp/torchinductor_root/i5/ci5pon4lebravl2hzt44gzbjhfb6qwcqdbeuk26u2ryajsewj7h4.py, ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=False, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4) 
....
```
that's about it.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 12:57:34+00:00,2025-09-19T20:01:30Z,,False,4,3,3,54,36,1,7,,58,2272,False,True,False,False,False,False,1,3,1200,98,58,40,1,3,3.0,4.0,2025-09-02T13:20:35Z,pytorch
161971,open,adding parse_to to constant_fold_functions,Raman-RH,"Fixes #161207

### Summary: 
included torch._C._nn._parse_to in constant_fold_functions in torch/_dynamo/variables/torch.py.

Calls to [nn.Module.to](http://nn.module.to/)(...) parse via _parse_to will be constant-folded



Now, 
```
import torch

class Mod(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = torch.nn.Linear(1024, 1024)

    def forward(self, x):
        res = self.fc(x)
        self.to(""cpu"")
        return res

mod = Mod()
mod.compile(fullgraph=True)
x = torch.rand(1, 1024)
mod(x)
```

does not produce error 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-09-02 12:55:51+00:00,2025-09-16T12:59:15Z,,False,9,7,5,28,0,2,16,,42,747,False,True,False,False,False,False,2,8,1092,50,39,11,1,5,6.0,11.0,2025-09-02T12:59:33Z,pytorch
161968,closed,[ci] Increase shards for linux-jammy-py3.10-clang18-asan on pull.yml to 7,jeanschmidt,"Tests for `linux-jammy-py3.10-clang18-asan` in pull are intermittently failing due to time out.

EX:
* https://github.com/pytorch/pytorch/actions/runs/17393738817/job/49372960082
* https://github.com/pytorch/pytorch/actions/runs/17391190505/job/49366171245
* https://github.com/pytorch/pytorch/actions/runs/17390925896/job/49365066945
* https://github.com/pytorch/pytorch/actions/runs/17390344774/job/49363637395

Instead of increasing the time out, I believe it is more reasonable to increase the shards count for the tests, the total cost for CI might not be that significant as we pay per instances * hours, so the total additional should be only the startup+teardown of a single extra shard. 

The other option, increasing timeout, could solve the problem, but this approach if applied repeatedly would consistently increase the human hours cost of developers. Plus the extra CI waiting times and potential conflicts that those might bring.


cc @seemethere @malfet @pytorch/pytorch-dev-infra",2025-09-02 10:03:18+00:00,2025-09-02T12:44:35Z,2025-09-02T12:08:48Z,True,2,0,1,7,7,1,2,2025-09-02 12:08:48+00:00,73,996,False,False,False,False,False,False,1,1,92,14,7,7,1,1,2.0,1.0,2025-09-02T12:01:08Z,pytorch
161967,open,[STABLE ABI] Add accessor template method to torch::stable::Tensor,pearu,"Note: this PR relies on ATen/core/TensorAccessor.h but it does not use `ArrayRef.h`-dependent features. Is there a need to port a lightweight version of `TensorAccessor` to torch stable ABI?

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* __->__ #161967
* #161911
* #161894
* #161892
* #161891
* #161896

",2025-09-02 09:59:21+00:00,2025-09-22T15:33:49Z,,False,1,1,11,21,0,1,2,,66,346,False,False,True,False,False,False,1,0,0,123502,79140,44362,1,11,1.0,0.0,2025-09-03T02:40:26Z,pytorch
161966,closed,fix to segmentation fault when empty tensor is passed to choose_qpara…,arkadip-maitra,"…ms_optimized

Fixes #153326 

Minimal code to reproduce error:
```
import torch

tensor = torch.tensor([])

torch.choose_qparams_optimized(
    tensor,
    0,
    200,
    0.16,
    8
)
```

Previous Output:
`Segmentation fault`

Now Output:
```
Traceback (most recent call last):
  File ""/home/amaitra/work/tests/issue_153326.py"", line 5, in <module>
    torch.choose_qparams_optimized(
RuntimeError: input tensor is empty and has no data
```


Caused because `const float* input_row =input_tensor.const_data_ptr<float>();` becomes null",2025-09-02 09:31:49+00:00,2025-09-03T20:27:31Z,,False,3,3,3,6,1,2,6,2025-09-03 20:26:29+00:00,70,538,False,True,False,False,False,False,2,2,493,43710,21556,22154,2,3,3.0,2.0,2025-09-02T17:22:27Z,pytorch
161965,open,Support to show the map of truncated function name and full name in profiler table print log,NeoZhangJianyu,"[Issue]
In profiler function, the long function name will be truncated and shown in log.
Like `at::native::xpu::VectorizedElementwiseKernel<4, at::...` in below:

```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                        model_inference         1.16%     205.799us       100.00%      17.717ms      17.717ms       0.000us         0.00%      32.603us      32.603us             1  
                                         ...
at::native::xpu::VectorizedElementwiseKernel<4, at::...         0.00%       0.000us         0.00%       0.000us       0.000us       6.354us        19.49%       6.354us       6.354us             1  
``` 

The full name is: 
`at::native::xpu::VectorizedElementwiseKernel<4, at::native::xpu::ClampScalarFunctor<float>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int> >`

It will lead to two issues:
1. Developer can't find the real name by the truncated name.
2. Some long names would have same truncated name. That will block to find the right full name.

[Solution]
1. Add parameter: `show_full_name` in torch.profiler.table() to show the full name of the function whose display name is truncated.
  Add new column name as ""Full Name"" as last column, to display the full name.

2. If the truncated name of two full names are same, change the truncated name with index to identify them. Like
    ```
    xxx...
    xxx~01
    xxx~02
    xxx~99
    xxx~100
    ```
4. The new parameter `show_full_name` default value is False to keep legacy behavior as default.
5. Same for ""Overload Name"" item.

Here is the example of new feature.

The truncated display names are:
```
at::native::xpu::VectorizedElementwiseKernel<4, at::... 
at::native::xpu::VectorizedElementwiseKernel<4, at::~01
...
at::native::xpu::VectorizedElementwiseKernel<4, at::~0n
...
at::native::xpu::VectorizedElementwiseKernel<4, at::~nnnn
```

```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg      Self XPU    Self XPU %     XPU total  XPU time avg    # of Calls  Full Name  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------  
                                        model_inference         0.31%     202.323us       100.00%      66.128ms      66.128ms       0.000us         0.00%      53.380us      53.380us             1             
                                             my_xpu_ops         0.62%     408.551us        98.52%      65.150ms      65.150ms       0.000us         0.00%      53.380us      53.380us             1             
                                           aten::linear         0.03%      20.981us        17.55%      11.605ms       5.802ms       0.000us         0.00%      20.936us      10.468us             2             
                                            aten::addmm        17.00%      11.239ms        17.43%      11.529ms       5.764ms      20.936us        39.22%      20.936us      10.468us             2             
                                            gemm_kernel         0.00%       0.000us         0.00%       0.000us       0.000us      20.936us        39.22%      20.936us      10.468us             2             
                                              aten::add        67.61%      44.712ms        70.53%      46.638ms       5.830ms      20.206us        37.85%      20.206us       2.526us             8             
at::native::xpu::VectorizedElementwiseKernel<4, at::...         0.00%       0.000us         0.00%       0.000us       0.000us      20.206us        37.85%      20.206us       5.051us             4  at::native::xpu::VectorizedElementwiseKernel<4, at::native::xpu::BinaryFunctor<float, float, float, at::native::xpu::AddFunctor<float> >, at::detail::Array<char*, 3>, TrivialOffsetCalculator<2, unsigned int> >  
                                             aten::relu         0.02%      13.802us         9.79%       6.472ms       6.472ms       0.000us         0.00%       6.354us       6.354us             1             
                                        aten::clamp_min         9.33%       6.170ms         9.77%       6.458ms       6.458ms       6.354us        11.90%       6.354us       6.354us             1             
at::native::xpu::VectorizedElementwiseKernel<4, at::~01         0.00%       0.000us         0.00%       0.000us       0.000us       6.354us        11.90%       6.354us       6.354us             1  at::native::xpu::VectorizedElementwiseKernel<4, at::native::xpu::ClampScalarFunctor<float>, at::detail::Array<char*, 2>, TrivialOffsetCalculator<1, unsigned int> >  
                                               aten::to         0.02%      10.570us         1.18%     780.357us     111.480us       0.000us         0.00%       5.884us       0.841us             7             
                                         aten::_to_copy         0.02%      13.088us         1.16%     769.787us     384.894us       0.000us         0.00%       5.884us       2.942us             2             
                                            aten::copy_         0.28%     185.344us         1.11%     734.935us     367.468us       5.884us        11.02%       5.884us       2.942us             2             
                 Memcpy D2M (DEVICE -> MEMORY(Unknown))         0.00%       0.000us         0.00%       0.000us       0.000us       5.572us        10.44%       5.572us       5.572us             1             
                 Memcpy M2D (MEMORY(Unknown) -> DEVICE)         0.00%       0.000us         0.00%       0.000us       0.000us       0.312us         0.58%       0.312us       0.312us             1             
                                            aten::empty         0.03%      19.869us         0.03%      19.869us       3.312us       0.000us         0.00%       0.000us       0.000us             6             
                                       aten::lift_fresh         0.00%       1.487us         0.00%       1.487us       0.372us       0.000us         0.00%       0.000us       0.000us             4             
                                          aten::detach_         0.01%       3.404us         0.01%       6.887us       1.722us       0.000us         0.00%       0.000us       0.000us             4             
                                                detach_         0.01%       3.483us         0.01%       3.483us       0.871us       0.000us         0.00%       0.000us       0.000us             4             
                                  urEnqueueKernelLaunch         3.73%       2.468ms         3.73%       2.468ms     352.560us       0.000us         0.00%       0.000us       0.000us             7             
                                                aten::t         0.04%      28.061us         0.08%      54.679us      27.339us       0.000us         0.00%       0.000us       0.000us             2             
                                        aten::transpose         0.03%      17.900us         0.04%      26.618us      13.309us       0.000us         0.00%       0.000us       0.000us             2             
                                       aten::as_strided         0.02%      10.517us         0.02%      10.517us       2.629us       0.000us         0.00%       0.000us       0.000us             4             
                                          aten::resize_         0.03%      18.735us         0.03%      18.735us       9.368us       0.000us         0.00%       0.000us       0.000us             2             
                                           aten::expand         0.01%       5.959us         0.01%       7.758us       3.879us       0.000us         0.00%       0.000us       0.000us             2             
                                    aten::empty_strided         0.04%      23.985us         0.04%      23.985us       7.995us       0.000us         0.00%       0.000us       0.000us             3             
                                     urEnqueueUSMMemcpy         0.83%     549.591us         0.83%     549.591us     274.796us       0.000us         0.00%       0.000us       0.000us             2             
                                       aten::empty_like         0.00%       2.228us         0.01%       4.449us       4.449us       0.000us         0.00%       0.000us       0.000us             1             
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  --------- 
```
",2025-09-02 08:41:25+00:00,2025-09-16T08:49:31Z,,False,7,15,4,91,11,1,22,,92,9483,False,False,True,False,False,False,1,5,17076,212,146,66,1,4,3.0,5.0,2025-09-02T09:25:30Z,pytorch
161964,closed,[Quant][Inductor][CPU] add qconv int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161964
* #161963

Pull-Request: https://github.com/pytorch/pytorch/pull/161487

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 08:28:18+00:00,2025-09-02T08:28:39Z,,False,1,0,1,45,14,2,1,2025-09-02 08:28:39+00:00,57,356,False,False,False,False,False,False,2,0,0,59,45,14,1,1,,,,pytorch
161963,closed,[Quant][Inductor][CPU] add qlinear int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161964
* __->__ #161963

Pull-Request: https://github.com/pytorch/pytorch/pull/161486",2025-09-02 08:28:11+00:00,2025-09-02T08:29:00Z,,False,2,0,1,126,20,2,2,2025-09-02 08:29:00+00:00,59,164,False,False,False,False,False,False,2,0,0,146,126,20,1,1,,,,pytorch
161962,closed,[Quant][Inductor][CPU] add qconv int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161962
* #161961

Pull-Request: https://github.com/pytorch/pytorch/pull/161487

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 08:24:05+00:00,2025-09-02T08:28:49Z,,False,2,0,1,45,14,2,2,2025-09-02 08:28:49+00:00,57,356,False,False,False,False,False,False,2,0,0,59,45,14,1,1,,,,pytorch
161961,closed,[Quant][Inductor][CPU] add qlinear int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161962
* __->__ #161961

Pull-Request: https://github.com/pytorch/pytorch/pull/161486",2025-09-02 08:23:58+00:00,2025-09-02T08:29:20Z,,False,2,0,1,126,20,2,2,2025-09-02 08:29:20+00:00,59,164,False,False,False,False,False,False,2,0,0,146,126,20,1,1,,,,pytorch
161959,closed,fix torch.sparse.log_softmax on CPU,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161959

Fix https://github.com/pytorch/pytorch/issues/152293.

**Example:**
```
import torch
from torch.sparse import log_softmax as sparse_log_softmax


def test_bug():
    a = torch.rand(4, 3)
    b = a - 10000000.0
    b_sparse = b.to_sparse()

    cpu_out_sparse = sparse_log_softmax(b_sparse, dim=1).to_dense()
    print('cpu_out_sparse =', cpu_out_sparse)

    b_sparse_double = b.double().to_sparse()
    cpu_out_sparse_double = sparse_log_softmax(b_sparse_double, dim=1).to_dense()
    print('cpu_out_sparse_double =', cpu_out_sparse_double)


if __name__ == '__main__':
    test_bug()
```

**Output:**

- before
```
cpu_out_sparse = tensor([[-2., -1., -2.],
        [-1., -1., -1.],
        [-1., -2., -2.],
        [-1., -1., -2.]])
cpu_out_sparse_double = tensor([[-1.5514, -0.5514, -1.5514],
        [-1.0986, -1.0986, -1.0986],
        [-0.5514, -1.5514, -1.5514],
        [-0.8620, -0.8620, -1.8620]], dtype=torch.float64)
```

- after
```
cpu_out_sparse = tensor([[-0.8620, -1.8620, -0.8620],
        [-1.0986, -1.0986, -1.0986],
        [-1.8620, -0.8620, -0.8620],
        [-1.0986, -1.0986, -1.0986]])
cpu_out_sparse_double = tensor([[-0.8620, -1.8620, -0.8620],
        [-1.0986, -1.0986, -1.0986],
        [-1.8620, -0.8620, -0.8620],
        [-1.0986, -1.0986, -1.0986]], dtype=torch.float64)
```",2025-09-02 07:49:55+00:00,2025-09-11T07:53:11Z,,False,8,1,3,14,3,2,9,2025-09-11 07:52:08+00:00,35,1403,False,True,False,False,False,False,2,7,1618,77330,45250,32080,1,3,6.0,7.0,2025-09-05T16:24:15Z,pytorch
161958,closed,Add api info for torch._C._nn.pyi,orangeH25,"Fix part of #148404 


APis involved are as followed:

- max_pool2d_with_indices
- max_pool3d_with_indices
- elu
- glu
- max_unpool2d
- max_unpool3d",2025-09-02 07:14:51+00:00,2025-09-02T20:40:27Z,,False,5,0,1,59,0,1,5,2025-09-02 20:39:24+00:00,33,148,False,True,False,False,False,False,1,3,519,59,59,0,1,1,3.0,3.0,2025-09-02T07:49:01Z,pytorch
161957,closed,"Reland ""Fix conv exhaustive autotuning and expand Exhaustive test coverage""",exclamaforte,"reland https://github.com/pytorch/pytorch/pull/159387

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 07:00:41+00:00,2025-09-12T01:37:49Z,,False,33,0,1,62,32,3,33,2025-09-12 01:36:47+00:00,75,256,False,True,False,False,False,False,3,28,5478,94,62,32,1,1,3.0,28.0,2025-09-02T17:01:58Z,pytorch
161955,open,[debug] Guard logs,anijain2305,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-02 06:29:11+00:00,2025-09-04T01:09:59Z,,False,6,0,2,62,14,2,6,,18,192,False,True,False,False,False,False,2,0,0,76,62,14,1,2,,,,pytorch
161953,closed,[Inductor][Tritonparse] Get Inductor kernel params,NikhilAPatel,"Summary: Save the config args that Inductor burns into `inductor_metadata` so we can optionally pass them to any Jit Hooks that are set. This allows us to pass them to Tritonparse.

Reviewed By: davidberard98, FindHao

Differential Revision: D80994791




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 06:23:51+00:00,2025-09-03T14:12:34Z,,False,9,0,1,14,1,2,9,2025-09-03 14:11:31+00:00,50,457,False,False,False,False,False,False,2,4,1767,15,14,1,1,1,3.0,5.0,2025-09-02T14:27:42Z,pytorch
161952,open,add shape check for avg_pool2d,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161952


Fix https://github.com/pytorch/pytorch/issues/153312.

**Example:**
```python
import torch 

print(torch.__version__)

tensor = torch.tensor([[ -7.8130e-88, -2.2092e-138,  -1.8673e+03, -7.6272e-253,  3.9203e+110,
           1.8380e-51,  2.8762e+268,  2.9094e+286,  5.1816e-228, -4.4916e+191,
          -7.4057e+80,  -9.1955e-18,  5.6536e+225,  8.8364e-175,  1.5053e-226],
        [-3.0521e+239, -2.8307e+306,   1.3297e-03, -9.9969e-132,  2.8920e-286,
           2.3964e+58, -6.8138e-281,  2.0321e-305,  -3.5127e+74,  -4.7560e-92,
          -8.9403e-99, -1.9739e-187, -2.5124e-173,  2.0458e+295,   4.4992e+52],
        [  6.8752e+21,  1.9332e+189, -8.6940e-189,  -6.6743e-15,   1.4691e+41,
           1.0338e+63,  -2.0779e-28, -7.6642e+104,  1.3390e+284, -8.0859e+194,
          8.4600e+107,   4.9115e-44,  1.1665e+285,  5.1275e+203,  9.7580e+303]],
       dtype=torch.float64)


try:
    res = torch.nn.functional.lp_pool1d(
        tensor,
        norm_type=-1.38119e+150,
        kernel_size=7879455037536781369,
        ceil_mode=True,
    )
    print(""CPU result:"", res)
except RuntimeError as e:
    print(f""CPU error: {e}"")

tensor_gpu = tensor.to(""cuda:0"")
try:
    res = torch.nn.functional.lp_pool1d(
        tensor_gpu,
        norm_type=-1.38119e+150,
        kernel_size=7879455037536781369,
        ceil_mode=True,
    )
    print(""GPU result:"", res)
except RuntimeError as e:
    print(f""GPU error: {e}"")
```

**Output:**

- before
```
2.9.0a0+git8703deb
CPU result: tensor([[0.],
        [0.],
        [0.]], dtype=torch.float64)
GPU error: integer out of range
```

- after
```
2.9.0a0+git2e893df
CPU error: integer out of range
GPU error: integer out of range
```
",2025-09-02 06:16:23+00:00,2025-09-10T01:35:24Z,,False,4,0,4,17,6,2,4,,30,1776,False,True,False,False,False,False,2,3,534,27,19,8,1,4,3.0,4.0,2025-09-09T07:59:38Z,pytorch
161951,closed,Modified the docs to add example for torch.is_floating_point and torc…,mansiag05,"…h.is_complex.

The PR proposes adding a simple, self-explanatory example to the documentation page. The example demonstrates the function's output for tensors with various data types, showing both True and False return values.

Fixes #161859 

cc @malfet 
",2025-09-02 06:09:13+00:00,2025-09-04T18:51:26Z,,False,6,0,1,24,2,1,6,2025-09-04 18:50:22+00:00,70,257,False,True,False,True,False,False,1,5,1410,26,24,2,1,1,3.0,5.0,2025-09-03T16:32:08Z,pytorch
161950,closed,[DTensor] forbid view ops to redistribute when local split is impossible,tianyu-l,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161950

This PR is a followup to https://github.com/pytorch/pytorch/pull/149764.

In that PR, it only forbids illegal view due to `Flatten`; this PR also forbids illegal view caused by `Split`.

This PR also updates the error message to be less about internal implementation details, which users may find confusing.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-02 06:09:12+00:00,2025-09-03T07:18:52Z,,False,3,12,1,50,31,2,15,2025-09-03 04:40:14+00:00,72,480,False,False,False,False,False,False,2,2,493,81,50,31,1,1,6.0,2.0,2025-09-02T06:13:38Z,pytorch
161949,closed,"Reland ""[Fix XPU CI][Inductor UT] Fix test cases broken by community. (#161142)""",etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161949

This PR reland #161142 which is reverted to be able to revert other PR.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 05:56:42+00:00,2025-09-02T23:44:34Z,,False,3,0,1,19,0,5,3,2025-09-02 23:43:30+00:00,80,370,False,True,False,False,False,False,5,2,493,19,19,0,1,1,3.0,2.0,2025-09-02T18:33:16Z,pytorch
161947,closed,Using get_paths() to get correct installation path for PYTHONPATY,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #161918
* #161917
* __->__ #161947

As the title stated.",2025-09-02 04:57:23+00:00,2025-09-03T06:39:10Z,,False,6,0,3,12,13,1,6,2025-09-03 06:38:06+00:00,65,144,False,False,False,False,False,False,1,5,1273,339,217,122,1,3,4.0,6.0,2025-09-02T22:25:20Z,pytorch
161946,closed,[HOTFIX] Disable DISTRIBUTED_C10D_DIRECT_ACCESS for now,ezyang,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.11.0) (oldest at bottom):
* __->__ #161946

Signed-off-by: Edward Yang <ezyang@meta.com>",2025-09-02 04:49:39+00:00,2025-09-15T03:37:28Z,,False,8,0,1,0,23,1,8,2025-09-15 03:37:28+00:00,55,150,False,True,False,False,False,False,1,7,1348,23,0,23,1,1,4.0,7.0,2025-09-02T04:50:35Z,pytorch
161940,open,[Inductor XPU GEMM] Step 10/N: Switch XPU triton scheduling to combined,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161940
* #161939
* #161938
* #160729
* #160706
* #160688
* #160687
* #160686
* #160685
* #160174

This PR officially replaces the Inductor XPU backend scheduling from TritonScheduling to CombinedScheduling, enabling support for both CUTLASS and Triton backends. It also refactors test_cutlass_backend to be enabled on XPU. Currently, the CUTLASS XPU backend does not yet support epilogue fusion.",2025-09-02 03:10:08+00:00,2025-09-08T05:20:28Z,,False,2,0,7,3,3,3,2,,71,481,False,False,False,False,False,True,3,0,0,75509,42646,32863,1,7,,,,pytorch
161939,open,[Inductor XPU GEMM] Step 9/N: Support generating XPU cutlass gemm kernel,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161940
* __->__ #161939
* #161938
* #160729
* #160706
* #160688
* #160687
* #160686
* #160685
* #160174

This PR implements the CUTLASS XPU backend kernel generation as proposed in RFC #160175. It reuses most of the CUTLASS CUDA kernel generation code, with only minor adjustments made to handle XPU-specific code generation.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 03:10:05+00:00,2025-09-08T05:45:40Z,,False,1,0,7,107,42,9,1,,72,609,False,False,False,False,False,False,9,0,0,75653,42750,32903,1,7,,,,pytorch
161938,open, [Inductor XPU GEMM] Step 8/N: Add XPU code compilation and codecache.,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161940
* #161939
* __->__ #161938
* #160729
* #160706
* #160688
* #160687
* #160686
* #160685
* #160174


This PR is the 8th part of RFC #160175, It adds the support for XPU cutlass compilation and codecache.",2025-09-02 03:10:01+00:00,2025-09-10T07:24:41Z,,False,1,0,6,213,6,5,1,,70,287,False,False,False,False,False,False,5,0,0,75700,42848,32852,1,6,,,,pytorch
161936,open,Fix _scaled_dot_product_attention_math inconsistent dtypes,CaoE,"`_scaled_dot_product_attention_math` may throw: `RuntimeError: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16` when mask is a high precision tensor https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/attention.cpp#L925. The add operator will make `attn`  ​a float tensor, resulting in inconsistent data types error https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/attention.cpp#L906.",2025-09-02 02:51:41+00:00,2025-09-16T06:42:12Z,,False,8,8,4,68,4,3,16,,58,472,False,True,False,False,False,False,3,7,1470,130,97,33,1,4,3.0,7.0,2025-09-03T05:34:22Z,pytorch
161934,closed,[CD] Fix setup-xpu action issue,chuanqi129,"Fix XPU CD test failure, refer https://github.com/pytorch/pytorch/actions/runs/17370923627/job/49315624191 ",2025-09-02 02:37:22+00:00,2025-09-02T16:04:51Z,,False,3,0,1,8,8,2,3,2025-09-02 16:03:48+00:00,31,107,False,True,False,False,False,False,2,2,837,16,8,8,1,1,3.0,3.0,2025-09-02T13:45:03Z,pytorch
161932,closed,[Intel GPU] Upgrade OneDNN XPU Tag to v3.9.1,LuFinch,cc @gujinghui @PenghuiCheng @XiaobingSuper @jianyuh @jgong5 @mingfeima @sanchitintel @ashokei @jingxu10 @min-jean-cho @yanbing-j @Guobing-Chen @Xia-Weiwen @snadampal,2025-09-02 02:13:26+00:00,2025-09-04T11:06:16Z,,False,14,0,1,2,2,1,14,2025-09-04 11:05:13+00:00,44,165,False,False,False,False,False,False,1,12,1512,4,2,2,1,1,5.0,12.0,2025-09-02T05:12:37Z,pytorch
161930,closed,dropout RNG fix: add custom_philox_rand.py and update replace_random.py,luise1030,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-02 01:39:03+00:00,2025-09-08T06:50:03Z,,False,3,0,2,149,8,2,3,2025-09-08 06:50:03+00:00,71,223,False,True,False,False,False,False,2,0,94,669521,446908,222613,1,2,1.0,1.0,2025-09-03T18:41:11Z,pytorch
161929,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-02 00:26:39+00:00,2025-09-03T04:27:42Z,,False,6,0,1,1,1,1,6,2025-09-03 04:26:40+00:00,46,156,False,False,False,False,False,False,1,5,1407,2,1,1,1,1,2.0,5.0,2025-09-02T00:26:40Z,pytorch
161928,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-09-02 00:26:15+00:00,2025-09-03T04:23:59Z,,False,6,0,1,1,1,1,6,2025-09-03 04:22:57+00:00,48,157,False,False,False,False,False,False,1,5,1384,2,1,1,1,1,2.0,5.0,2025-09-02T00:26:16Z,pytorch
161927,closed,[MPS] Add `igamma/igammac` ops,kurtamohler,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161927

Fixes #161725

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 22:12:08+00:00,2025-09-02T20:53:08Z,,False,4,1,3,782,11,9,5,2025-09-02 20:52:05+00:00,30,310,False,True,False,False,False,False,9,2,793,2311,1808,503,1,3,3.0,2.0,2025-09-01T22:17:54Z,pytorch
161926,open,Selectify pytorch platform args,jaejunku,"Summary: Platform args was a buck1 concept that we decided to port over to buck2 in order to make the migration easier. However, platforms args existing in the repo blocks some buck modernization like modefile free efforts, so we're trying to get rid of the usage.

Test Plan:
CI

Rollback Plan:

Differential Revision: D81451267


",2025-09-01 20:58:32+00:00,2025-09-12T20:01:15Z,,False,6,0,1,15,1,1,6,,31,332,False,False,False,False,False,False,1,1,46,16,15,1,1,1,2.0,2.0,2025-09-01T21:01:20Z,pytorch
161925,closed,[dynamo] Graph break on torch.cuda.sychronize,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161925

Today, AOTDispatcher ignores cuda.synchornize. Even if we wrap it in
some  HOP, we need it to be a barrier op to prevent any inductor
reordering. So graph breaking.

Fixes https://github.com/pytorch/pytorch/issues/160751

cc @ezyang @gchanan @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-01 20:54:52+00:00,2025-09-02T19:01:27Z,,False,4,0,3,15,2,3,4,2025-09-02 19:00:24+00:00,45,503,False,True,False,False,False,False,3,3,571,6194,4557,1637,1,3,5.0,3.0,2025-09-02T17:14:05Z,pytorch
161924,closed,[inductor] Fix removal of constexpr args from the launcher signature,kundaMwiza,"Fixes the case described below which occurs when:
- A user `torch.compile`s a function that uses a triton kernel.
- `TORCHINDUCTOR_DUMP_LAUNCH_PARAMS=1` .

Problem:

If the user defined triton kernel is not autotuned:

```python
import os
os.environ[""TORCHINDUCTOR_DUMP_LAUNCH_PARAMS""] = ""1""
@triton.jit
def kernel(..., BLOCK_SIZE: tl.constexpr):
    ...
@torch.compile
def fn(..)
    kernel[..](..., 128)

fn(..)
```

Then In `triton_heuristics. _interpret_args_grid`, `filter_signature` function:

```python
        def filtered_signature() -> list[str]:
                # constexprs are not passed in as args
                return [
                    x
                    for x in self.triton_meta[""signature""].keys()
                    if x not in cfg.kwargs.keys()
                ]
```

because `triton.autotune` is not used on the the `triton.jit` function, `cfg` above will be empty, and so `BLOCK_SIZE` will not be removed from the signature even though it is constexpr, even though it is removed from the arguments that are passed in to `interpret_args_grid`. This results in a mismatch between the number of parameters in the signature and the number of arguments, which leads to the error `NameError: name '_grid_2' is not defined`. 

Fix:

Use the triton jit kernel `constexprs` for args to remove.  Not sure if this is a good fix so suggestions are welcome.

Test plan: 

Added a parameter to an existing triton kernel to test for this edge case


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 19:03:20+00:00,2025-09-12T13:59:17Z,,False,17,4,5,22,6,2,21,2025-09-12 13:58:13+00:00,68,1668,False,True,False,False,False,False,2,16,5337,102,59,43,1,5,3.0,17.0,2025-09-01T19:03:55Z,pytorch
161922,closed,[inductor] Follow integer overflow rules in TypedExpr,isuruf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161922

Fixes https://github.com/pytorch/pytorch/issues/161763

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-09-01 18:23:37+00:00,2025-09-03T18:34:26Z,,False,3,0,3,19,1,2,3,2025-09-03 18:33:22+00:00,53,359,False,True,False,False,False,False,2,2,493,22,20,2,1,3,3.0,2.0,2025-09-02T18:32:13Z,pytorch
161920,closed,S390x: build nightly binaries for new pythons,AlekseiNikiforovIBM,"Enable python 3.13t, 3.14 and 3.14t on s390x for nightly binaries

Fixes #161515
",2025-09-01 17:53:52+00:00,2025-09-03T17:39:47Z,,False,4,0,3,201,8,3,4,2025-09-03 17:38:43+00:00,45,81,False,True,False,False,False,False,3,3,949,209,201,8,1,3,3.0,3.0,2025-09-03T08:21:16Z,pytorch
161919,closed,Update torch-xpu-ops commit pin,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161919
# Motivation
1. Fallback some linalg functionality such as `linalg_eig`, `linalg_householder_product`, `linalg_solve_triangular` to CPU;
2. Fix codegen dependency bug.

# Additional Context
This PR aims to fix https://github.com/pytorch/pytorch/issues/161498
",2025-09-01 16:10:48+00:00,2025-09-02T17:10:16Z,,False,10,0,3,1,1,1,10,2025-09-02 17:09:11+00:00,31,352,False,True,False,False,False,False,1,9,3207,4069,782,3287,1,3,3.0,9.0,2025-09-02T05:24:26Z,pytorch
161918,closed,[OpenReg] Strengthen Openreg's execution limits to minimize the waste of computing resources,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162826
* #160101
* __->__ #161918
* #161917

Currently, OpenReg supports Linux, Windows, and OS X, ensuring stability and ease of integration with third-party devices across all three platforms. It also doesn't rely on any other accelerators (such as CUDA or MPS).

Therefore, to minimize computational resource usage, `test_openreg` can be added to certain BLOCKLISTS to prevent its execution, limiting OpenReg's execution to only necessary scenarios.",2025-09-01 16:05:35+00:00,2025-09-12T23:53:20Z,,False,2,0,17,3,10,2,2,2025-09-12 23:53:20+00:00,92,531,False,False,False,False,False,False,2,1,54,98639,60374,38265,1,17,2.0,2.0,2025-09-12T15:08:43Z,pytorch
161917,closed,[OpenReg] Migrate OpenReg Tests from tests/test_openreg.py into torch_openreg/tests,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162826
* #160101
* #161918
* __->__ #161917

**Background:**

Almost all the tests in `test/test_openreg.py` are designed for `torch_openreg`, so placing these testcases in the test directory is not a good idea. Instead, they should be moved to the `tests` directory under `torch_openreg`, coordinating these tests with their corresponding functional logic.

**How to do:**

So how do we verify the quality of the third-party device integration mechanism?
We will maintain a `test_openreg` entrypoint in `test/run_test.py`.

This entrypoint will install `torch_openreg` and run all the testcases located in `torch_openreg`. As long as all testcases pass, we can guarantee that the out-of-tree backend integration mechanism is available.

**Next:**

We will also improve `torch_openreg's` test coverage in the future.",2025-09-01 16:05:28+00:00,2025-09-12T23:53:20Z,,False,5,0,16,829,731,13,5,2025-09-12 23:53:19+00:00,83,895,False,False,False,False,True,False,13,4,718,100064,61179,38885,1,16,3.0,5.0,2025-09-12T15:08:12Z,pytorch
161916,closed,"[CD] Add cuda 13.0 libtorch builds, remove CUDA 12.9 builds",atalman,"Related to https://github.com/pytorch/pytorch/issues/159779

Adding CUDA 13.0 libtorch builds, followup after https://github.com/pytorch/pytorch/pull/160956
Removing CUDA 12.9 builds, See https://github.com/pytorch/pytorch/issues/159980


cc @albanD @ptrblck @nWEIdia @malfet ",2025-09-01 15:44:42+00:00,2025-09-05T07:49:02Z,,False,15,6,14,591,3512,11,21,2025-09-05 07:47:58+00:00,59,276,False,False,False,False,False,False,11,14,9395,4149,614,3535,3,14,7.0,14.0,2025-09-01T16:30:36Z,pytorch
161914,closed,[easy] Handle Autotuners in get_triton_source_codes_for_gm,jamesjwu,"Stack from [ghstack](https://github.com/ezyang/ghstack/tree/0.12.0) (oldest at bottom):
* __->__ #161914

Some triton kernels are autotuners, in that case, grab the function from the autotuner. 
",2025-09-01 14:08:15+00:00,2025-09-15T15:20:11Z,,False,12,2,2,9,1,1,14,2025-09-15 15:19:07+00:00,58,195,False,False,False,False,False,False,1,11,2795,133539,90173,43366,1,2,4.0,11.0,2025-09-02T17:13:33Z,pytorch
161913,open,Dispatch GELU to oneDNN in compile mode.,puneetmatharu,"> [!CAUTION]
> **DRAFT RELEASE!**

## Summary

This PR introduces a dedicated `mkldnn::_gelu` operator and ensures `GELU(approximate=""none"")` subgraphs are lowered to a oneDNN eltwise GELU on CPU.

## Motivation

- On CPU, GELU appears post-decomposition as `mul(x*0.5, add(erf(x*sqrt(0.5)), 1))`, which is later replaced with a codegen-ed `Vectorized` implementation, instead of using faster implementations from oneDNN/ACL when available.

## Changes

- New op `mkldnn::_gelu(Tensor X, str algorithm)` calling oneDNN eltwise GELU; returns dense for dense inputs.
- Adds a new `Gelu` lowering + Inductor IR and AOTI shim `aoti_torch_cpu__gelu(...)`.
- Defines a fusion pattern (in the post-grad step) that recognises the decomposed GELU subgraph and lowers it to `mkldnn::_gelu` on pass number 2 (0-indexed), to ensure it only occurs after higher-priority linear/conv fusions.

## Remarks
- Uses oneDNN `eltwise_gelu_erf`; only approximate=""none"" is supported by this path.
- Returns dense for dense inputs so downstream ATen pointwise ops don't see a `torch._mkldnn` layout unexpectedly.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @malfet @snadampal @milpuz01 @aditew01 @nikhil-arm @fadara01 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 13:41:01+00:00,2025-09-02T09:57:45Z,,False,3,0,1,91,1,7,3,,40,1409,False,False,False,False,False,False,7,0,0,92,91,1,1,1,,,,pytorch
161912,open,Re-enable fusions for AArch64.,puneetmatharu,"> [!CAUTION]
> **DRAFT RELEASE!**

## Summary

This PR re-enables fusions for AArch64 in compile mode.

## Motivation

- The fusion logic on AArch64 was disabled due to previous issues with fusions in oneDNN; this has been addressed in https://github.com/uxlfoundation/oneDNN/pull/3767, allowing arbitrary post-ops (activations) with the ACL `matmul` and `inner_product` primitives.

## Changes

- Re-enables fusions in compile mode on AArch64 by reverting the relevant change in https://github.com/pytorch/pytorch/commit/7faa67f6efd4453befc6822b829a3a79fc31f163.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 13:36:45+00:00,2025-09-02T07:31:34Z,,False,3,0,1,1,5,1,3,,30,766,False,False,False,False,False,False,1,1,50,6,1,5,1,1,1.0,1.0,2025-09-01T14:39:24Z,pytorch
161911,open,[STABLE ABI] Add cpu operation.,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* __->__ #161911
* #161894
* #161892
* #161891
* #161896

",2025-09-01 13:30:31+00:00,2025-09-22T16:21:17Z,,False,1,0,16,15,0,1,1,,31,154,False,False,False,False,False,False,1,0,0,130026,83650,46376,1,16,,,,pytorch
161910,open,Use posix_fallocate() to reserve disk space for shared memory,wenjianhn,"Shared memory is allocated by creating a file in /dev/shm (by default) that can run out of space. Pytorch reserves the file size by calling ftruncate() that creates a sparse file, so it succeeds even if sufficient disk space is not available.

This could lead to a situation when a shared memory region is successfully created but a subsequent access to a shared memory page results in SIGBUS due to the disk being full.

Using posix_fallocate() instead of ftruncate() eliminates this problem because the former syscall always allocates space and it returns an error if the disk is full.

Related to https://github.com/pytorch/pytorch/issues/5040",2025-09-01 13:16:49+00:00,2025-09-19T00:36:18Z,,False,3,2,2,32,0,3,5,,61,646,False,False,False,False,False,False,3,1,42,34,33,1,1,2,2.0,1.0,2025-09-01T13:20:58Z,pytorch
161907,closed,Keep default `CMAKE_PREFIX_PATH` in test_aot_inductor_package,Flamefire,"`CMAKE_PREFIX_PATH` is a list of paths used to find dependencies. The test overwrites that with a single path causing dependencies such as protobuf or Abseil not being found.

Instead prepend the path to the existing value.


This fixes a test failure:
> pytorch-v2.7.1/test/inductor/test_aot_inductor_package.py"", line 242, in test_compile_after_package
>    self.assertTrue(so_path.exists())
> AssertionError: False is not true

Caused by:
```
/software/binutils/2.42-GCCcore-13.3.0/bin/ld: cannot find -labsl::utility: No such file or directory
/software/binutils/2.42-GCCcore-13.3.0/bin/ld: cannot find -labsl::variant: No such file or directory
collect2: error: ld returned 1 exit status
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 12:20:13+00:00,2025-09-04T17:03:31Z,,False,3,0,1,8,2,1,3,2025-09-04 16:28:01+00:00,61,899,False,True,False,False,False,False,1,2,493,10,8,2,1,1,3.0,2.0,2025-09-04T16:17:16Z,pytorch
161903,closed,Using pip3 install instead of python setup.py develop/install,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #161918
* #161917
* #161947
* __->__ #161903
* #161845

As the title stated.",2025-09-01 11:32:44+00:00,2025-09-03T03:13:26Z,,False,7,0,4,21,4,1,7,2025-09-03 03:12:22+00:00,61,164,False,False,False,False,False,False,1,6,1300,6219,4365,1854,1,4,2.0,6.0,2025-09-02T18:00:44Z,pytorch
161901,open,constant_pad_nd: allow zero-sized dims with mixed positive/negative pads; added regression test,PluvioXO,"Fixes #161014

### Summary
`aten::constant_pad_nd` incorrectly errors when mixed positive/negative pads
produce a zero-sized dimension (e.g., on a [5, 3] tensor, pads [-1, -2, 1, 1]
should yield shape [7, 0]). Zero-sized dims are valid; only negative sizes should error.

### What changed
- Relax size check from `> 0` to `>= 0`:
  - Python decomposition: torch/_refs/__init__.py
  - Native implementation: aten/src/ATen/native/PadNd.cpp
- Added regression tests: test/test_constant_pad_nd.py

### Test plan
- `pytest -q test/test_constant_pad_nd.py` passes locally.
- Verified:
  - `constant_pad_nd(x, [-1, -2]) -> (5, 0)` still OK
  - `constant_pad_nd(x, [-1, -2, 1, 1]) -> (7, 0)` now OK
  - Truly negative result (e.g., [-2, -2]) still raises.

### Notes
- Behavior aligns eager and decomposition/meta paths.
",2025-09-01 10:54:56+00:00,2025-09-11T18:45:14Z,,False,5,0,3,47,2,3,5,,95,813,False,True,False,False,False,False,3,3,544,109,77,32,1,3,2.0,4.0,2025-09-01T11:04:40Z,pytorch
161900,closed,Update misleading torch.sparse_coo_tensor error check,parsshar-RH,"Fixes #160622

### Summary
Updated the misleading torch.sparse_coo_tensor error check to provide clear context. 
earlier:
`RuntimeError: number of dimensions must be sparse_dim (3) + dense_dim (0), but got 1`

Updated:
`RuntimeError: 'len(size) == sparse_dim + dense_dim' is not satisfied: len(size) = 1, sparse_dim = 3, dense_dim = 0`

**Impacts:**

- Comprehensive error message that will improve developer experience.
- module: sparse 

cc @alexsamardzic @nikitaved @pearu @cpuhrsch @amjames @bhosmer @jcaip",2025-09-01 10:25:02+00:00,2025-09-10T19:58:18Z,,False,6,0,1,17,17,2,6,2025-09-10 19:57:14+00:00,53,510,False,True,False,False,True,False,2,5,1478,34,17,17,1,1,4.0,7.0,2025-09-04T10:32:12Z,pytorch
161898,open,[doc] Add example for torch.is_storage,parsshar-RH,"Fixes #161858


### Summary:
Added comprehensive documentation examples for `torch.is_storage()` to help users understand how to check if an object is a PyTorch storage object.

### Impact:

- Enhances API Documentation
- Helps users distinguish between PyTorch storage objects and other types


cc @svekars @sekyondaMeta @AlannaBurke",2025-09-01 09:25:52+00:00,2025-09-18T18:49:33Z,,False,3,0,3,19,6,1,3,,38,334,False,True,False,True,False,False,1,2,175,102297,66416,35881,1,3,1.0,2.0,2025-09-10T15:18:18Z,pytorch
161897,open,[STABLE ABI] Add packed_accessor32 and generic_packed_accessor template methods to torch::stable::Tensor,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161897
* #161967
* #161911
* #161894
* #161892
* #161891
* #161896

",2025-09-01 09:23:55+00:00,2025-09-22T15:20:48Z,,False,1,0,14,39,0,1,1,,104,154,False,False,False,False,False,False,1,0,0,129846,83601,46245,1,14,,,,pytorch
161896,closed,[STABLE ABI] Add clone method to torch::stable::Tensor,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* #161911
* #161894
* #161892
* #161891
* __->__ #161896

",2025-09-01 09:23:51+00:00,2025-09-22T20:40:31Z,,False,3,8,14,49,4,4,11,2025-09-22 20:39:27+00:00,54,154,False,False,False,False,False,False,4,2,518,129951,83558,46393,1,14,4.0,4.0,2025-09-03T02:45:41Z,pytorch
161895,closed,[STABLE ABI] Add copy_ operation.,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* #161911
* #161894
* #161892
* #161891
* #161896
* __->__ #161895

",2025-09-01 09:23:46+00:00,2025-09-20T10:31:40Z,,False,6,3,11,50,1,4,9,2025-09-20 10:30:36+00:00,33,164,False,False,False,False,False,False,4,5,1392,116761,73981,42780,1,11,3.0,6.0,2025-09-18T21:36:54Z,pytorch
161894,open,[STABLE ABI] Add device_type and device_index optional arguments to new_empty.,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* #161911
* __->__ #161894
* #161892
* #161891
* #161896

",2025-09-01 09:23:42+00:00,2025-09-22T16:21:22Z,,False,1,1,15,30,10,1,2,,78,154,False,False,False,False,False,False,1,0,0,129875,83596,46279,1,15,1.0,0.0,2025-09-03T03:01:47Z,pytorch
161893,open,[STABLE ABI] Add accessor template method to torch::stable::Tensor,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161911
* #161897
* #161896
* #161895
* #161894
* __->__ #161893
* #161892
* #161891

",2025-09-01 09:23:38+00:00,2025-09-01T13:30:34Z,,False,1,0,1,15,0,1,1,,66,164,False,False,False,False,False,False,1,0,0,15,15,0,1,1,,,,pytorch
161892,open,[STABLE ABI] Add sizes() and strides() methods to torch::stable::Tensor,pearu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* #161911
* #161894
* __->__ #161892
* #161891
* #161896

",2025-09-01 09:23:33+00:00,2025-09-22T16:21:24Z,,False,2,0,14,13,0,1,2,,71,154,False,False,False,False,False,False,1,1,496,129857,83591,46266,1,14,2.0,2.0,2025-09-03T02:33:00Z,pytorch
161891,open,[STABLE ABI] Add mutable_data_ptr() and const_data_ptr() methods to torch::stable::Tensor.,pearu,"This ghstack is a prerequisite for porting torchaudio C++ extensions to use torch stable ABI, see https://github.com/pytorch/audio/issues/4074, https://github.com/pytorch/audio/issues/4075, https://github.com/pytorch/audio/issues/4076, https://github.com/pytorch/audio/issues/4077, https://github.com/pytorch/audio/issues/4078

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161897
* #161967
* #161911
* #161894
* #161892
* __->__ #161891
* #161896

",2025-09-01 09:23:29+00:00,2025-09-22T18:37:48Z,,False,3,3,13,89,0,6,6,,90,482,False,False,False,False,False,False,6,2,1189,129788,83558,46230,1,13,3.0,4.0,2025-09-03T02:28:58Z,pytorch
161890,open,add doc get_start_xpu.rst to XPU Merge Rules List,ZhaoqiongZ,add doc get_start_xpu.rst to XPU Merge Rules List ,2025-09-01 08:05:54+00:00,2025-09-02T05:11:49Z,,False,1,0,1,1,0,1,1,,49,50,False,False,False,True,False,False,1,0,0,1,1,0,1,1,1.0,0.0,2025-09-02T05:11:33Z,pytorch
161887,closed,[AOTI] Fix a bug from load_constants,hl475,"Summary:
we have 
```
std::vector<size_t> constants_internal_offset(
        num_constants - num_folded_constants);
```

but the for loop does not consider it
```
for (size_t i = 0; i < num_constants; i++) {
...
constants_internal_offset[i]
...
```
even in the for loop, it does
```
bool from_folded = this->constant_from_folded(i);
      if (from_folded) {
        continue;
      }
```
but `i` could still be wrong

Rollback Plan:

Differential Revision: D81425007


",2025-09-01 07:06:11+00:00,2025-09-03T07:46:22Z,,False,6,0,1,3,1,1,6,2025-09-03 07:45:20+00:00,36,469,False,True,False,False,False,False,1,1,476,4,3,1,1,1,2.0,1.0,2025-09-02T22:10:52Z,pytorch
161885,closed,[BUG]Fixed handle cannot be hit in the cache in the IPC ExpandableSegment,mengph,"Fixed the bug that handle cannot be hit in the ipcMemHandle_to_devptr cache in the IPC scenario of ExpandableSegment.

Fixes #161884
",2025-09-01 06:53:45+00:00,2025-09-17T11:46:53Z,,False,16,0,1,10,1,1,16,2025-09-12 01:09:20+00:00,73,133,False,True,False,False,False,False,1,15,2665,11,10,1,1,1,3.0,16.0,2025-09-01T11:52:43Z,pytorch
161883,closed,[dynamo] Prevent unnecessary recompile on disabled functions in the compiled frame,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161925
* __->__ #161883

Trying out a re-impl of https://github.com/pytorch/pytorch/pull/160934

The above PR led to OOM, most likely because of the cache holding to a nested function (which if not held in the cache would have been garbage collected), which holds on to cuda tensors in its closure.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-09-01 05:25:49+00:00,2025-09-02T01:14:54Z,,False,3,0,2,59,7,7,3,2025-09-02 01:13:51+00:00,82,550,False,False,False,False,False,False,7,2,493,66,59,7,1,2,3.0,2.0,2025-09-01T23:21:10Z,pytorch
161882,closed,[DTensor] select strategy with no redistribute when redistribute cost is 0,tianyu-l,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161882

Before this PR, the `_select_strategy` always selects the first strategy with minimum redistribute cost. This causes unexpected behavior when
- multiple strategies have 0 redistribute costs
- the first one with 0 redistribute cost may perform local chunking

E.g. in memory efficient SDPA, the default orders of candidate strategies have a `Shard(2)` one before the `Replicate()` one. https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/_ops/_matrix_ops.py#L500-L512
When the input is `Replicate()`, `_select_strategy` will pick the `Shard(2)` strategy and do local chunking first, before local computation. This is clearly unexpected to users.

In this PR, we improve `_select_strategy` so that when multiple strategies have 0 redistribute cost, we prioritize the one which keeps input unchanged.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-09-01 04:41:12+00:00,2025-09-02T05:57:03Z,,False,3,9,2,54,14,3,12,2025-09-02 05:41:58+00:00,74,987,False,False,False,False,True,False,3,2,512,68,54,14,1,2,3.0,3.0,2025-09-02T02:04:22Z,pytorch
161881,open,[wip] dynamic int,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161881



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81423828](https://our.internmc.facebook.com/intern/diff/D81423828)",2025-09-01 04:40:50+00:00,2025-09-01T08:38:22Z,,False,2,0,2,124,0,4,2,,17,359,False,False,False,False,False,False,4,1,156,162,125,37,1,2,1.0,1.0,2025-09-01T05:17:46Z,pytorch
161880,open,skip hint override test if roccm,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161881
* __->__ #161880
* #161879



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-09-01 04:40:47+00:00,2025-09-01T07:45:47Z,,False,1,0,1,1,0,1,1,,32,317,False,False,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
161879,open,add hint_override kwarg to mark_dynamic,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161881
* #161880
* __->__ #161879



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-09-01 04:40:44+00:00,2025-09-01T07:45:48Z,,False,1,0,1,36,1,4,1,,39,353,False,False,False,False,False,False,4,0,0,37,36,1,1,1,,,,pytorch
161876,closed,[Bug] Add more boundary check for FractionalMaxPool3d,can-gaa-hou,"This PR aims to fix the bug mentioned at [#161853](https://github.com/pytorch/pytorch/issues/161853#issuecomment-3240695121)

cc @albanD @jbschlosser @mikaylagawarecki",2025-09-01 03:04:08+00:00,2025-09-16T07:20:41Z,,False,10,0,3,28,3,2,10,2025-09-16 06:59:05+00:00,53,167,False,True,False,False,False,False,2,6,1729,31,28,3,1,3,3.0,7.0,2025-09-01T06:35:55Z,pytorch
161872,closed,Include binary jobs in GitHub Actions secrets linter,akintunero,"## Summary

This PR addresses a TODO comment in the GitHub Actions linter to include binary jobs in the `secrets: inherit` validation check.

## Changes Made

- **Enhanced job filtering**: Extended the linter to check binary jobs in addition to build and test jobs
- **Improved security**: Binary jobs often need access to secrets for publishing artifacts
- **Updated comments**: Replaced TODO comment with clear explanation of the expanded scope
- **Maintained flexibility**: Kept the existing filter logic while adding binary job support

## Technical Details

The fix modifies the job filtering logic in `tools/linter/adapters/gha_linter.py`:

```python
# Before: Excluded binary jobs
if (""build"" in uses or ""test"" in uses) and ""binary"" not in uses:

# After: Includes binary jobs  
if (""build"" in uses or ""test"" in uses or ""binary"" in uses):
```

## Why This Matters

- **Better CI/CD Security**: Ensures binary jobs have proper secrets configuration
- **Completes TODO**: Addresses the specific TODO comment in the codebase
- **Improved Workflow Validation**: More comprehensive linting coverage
- **Future-Proof**: Better support for binary publishing workflows

## Testing

The change maintains backward compatibility while expanding the scope of the linter. Binary jobs that need secrets for publishing will now be properly validated.

## Related Issue

Addresses TODO comment in `tools/linter/adapters/gha_linter.py` line 74.

@pytorchbot label ""topic: not user facing""",2025-09-01 01:15:56+00:00,2025-09-05T06:38:33Z,,False,2,4,4,67,6,4,6,2025-09-05 06:38:33+00:00,52,1478,False,True,False,False,True,False,4,1,97,73,67,6,1,4,2.0,2.0,2025-09-01T01:22:03Z,pytorch
161870,open,Add LazyGroupNorm,vsey,"Fixes #161869
",2025-09-01 00:35:56+00:00,2025-09-04T17:20:24Z,,False,2,0,31,253,2,3,2,,17,14,False,True,False,False,False,False,3,1,37,73901,44933,28968,1,30,1.0,1.0,2025-09-01T13:02:18Z,pytorch
161867,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-09-01 00:32:05+00:00,2025-09-01T04:38:18Z,,False,3,0,1,1,1,1,3,2025-09-01 04:37:16+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-09-01T00:32:06Z,pytorch
161866,open,[dynamo] Error when attempting to skip due to compilation under torch dispatch mode while fullgraph=True,priyank-p0,"Fixes #161790
fix implements error message in case when dynamo tries to skip on fullgraph = True. Added test to test_modes.py.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-31 23:56:03+00:00,2025-09-09T00:02:28Z,,False,3,1,2,55,4,2,4,,104,298,False,True,False,False,False,False,2,1,250,65,58,7,1,2,2.0,2.0,2025-09-08T20:00:35Z,pytorch
161862,closed,[CUDAGraph] add config to error on skipping cudagraph,BoyuanFeng,"Many users want a config to force all cuda ops captured by cudagraph. When not possible, pt2 should error.

This PR adds `torch._inductor.triton.cudagraph_or_error` for that (default as False). Also added an environment variable `TORCHINDUCTOR_CUDAGRAPH_OR_ERROR` to control.

cc @mcarilli @ezyang @eellison @penguinwu @voznesenskym @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-31 18:10:23+00:00,2025-09-04T15:53:45Z,,False,10,1,5,24,0,3,11,2025-09-04 15:52:43+00:00,53,514,False,False,False,False,False,False,3,8,1412,49875,26514,23361,1,5,5.0,8.0,2025-09-02T03:02:55Z,pytorch
161857,open,Fix `Tensor.item` within the compile region,xuantengh,"Fixes #156135 



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-08-31 10:59:45+00:00,2025-09-04T15:01:23Z,,False,4,0,2,12,1,2,4,,43,196,False,True,False,False,False,False,2,2,74,13,12,1,1,2,1.0,2.0,2025-08-31T13:26:52Z,pytorch
161856,closed,Improve Allocator Config warning/error message,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161856
* #161788
* #161787
* #161786
* #161594

",2025-08-31 09:04:36+00:00,2025-09-15T05:49:10Z,,False,5,2,2,8,4,3,7,2025-09-15 05:49:10+00:00,46,134,False,False,False,False,True,False,3,4,752,18,11,7,1,2,3.0,4.0,2025-09-01T17:14:52Z,pytorch
161855,open,Refactor: clean up SelectiveBuilder for readability and consistency,Vruddhi18,"- Simplified dict construction with comprehensions in from_yaml_dict and to_dict
- Replaced list concatenations with set unions for merging metadata
- Used .get() consistently instead of membership checks + indexing
- Clarified variable names (e.g. base_name in is_operator_selected) to avoid shadowing
- Simplified return conditions and removed unnecessary else branches
- Standardized set unions with | operator in combine_selective_builders
- Used f-strings for clearer exception messages
",2025-08-31 05:37:18+00:00,2025-09-04T15:03:34Z,,False,4,0,1,64,139,1,4,,67,492,False,False,False,False,False,True,1,2,309,203,64,139,1,1,2.0,2.0,2025-08-31T08:00:56Z,pytorch
161852,closed,[MPS] Move sparsemps testing from test_mps to test_sparse,Isalia20,"Moves Sparse MPS testing from test_mps to test_sparse. Lots of skips now but I expect to remove them iteratively once ops are implemented

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen",2025-08-30 22:19:17+00:00,2025-09-02T19:05:16Z,,False,5,0,4,634,220,7,5,2025-09-02 19:04:14+00:00,57,196,False,False,False,False,False,False,7,2,493,3513,2667,846,1,4,2.0,2.0,2025-08-31T20:25:41Z,pytorch
161851,closed,"[AOTI] fix ut, add extension file type for Windows.",xuhancn,"fix ut, add extension file type for Windows.


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-30 20:38:57+00:00,2025-08-31T01:47:34Z,,False,6,0,1,8,1,1,6,2025-08-31 01:13:33+00:00,51,354,False,True,False,False,False,False,1,5,976,9,8,1,1,1,3.0,5.0,2025-08-30T20:39:33Z,pytorch
161850,closed,"[AOTI] split too long string to smaller pieces when its length larger than 16000, fix msvc c2026.",xuhancn,"Split too long string to smaller pieces when its length larger than 16000, fix msvc c2026.

reproducer:
```cmd
pytest test\inductor\test_aot_inductor.py -v -k test_runtime_checks_large_cpu
```

Error message:
<img width=""1660"" height=""174"" alt=""image"" src=""https://github.com/user-attachments/assets/56fcd9be-24cb-484b-bfdc-f719ff2650b8"" />

For MSVC c2026:
https://learn.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2026?view=msvc-170

We can split too long string to smaller pieces, it can fix this issue.

Local validated:
<img width=""1122"" height=""232"" alt=""image"" src=""https://github.com/user-attachments/assets/cac54cc9-be51-4a5d-b408-06755a4debd5"" />


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-30 19:30:48+00:00,2025-09-02T00:10:07Z,,False,6,2,3,31,4,1,8,2025-09-02 00:09:05+00:00,97,995,False,True,False,False,False,False,1,5,964,125,76,49,1,3,3.0,5.0,2025-08-30T19:32:27Z,pytorch
161849,open,chore: use patchelf from PATH,kranurag7,"    some distrubtions have binaries inside /usr/bin, this commit
    looks for patchelf binary using `command -v` which is a builtin in
    both sh and bash. This way we'll look for patchelf binary inside PATH
    and we'll be able to build torch wheels on other distributions as well
    without patching the code.

    the diff is also compatible with https://www.shellcheck.net/wiki/SC2155

Signed-off-by: kranurag7 <81210977+kranurag7@users.noreply.github.com>
",2025-08-30 18:44:11+00:00,2025-09-04T15:00:30Z,,False,3,2,1,2,1,1,5,,29,465,False,False,False,False,False,False,1,1,42,3,2,1,1,1,2.0,1.0,2025-08-30T18:53:21Z,pytorch
161848,closed,[WOQ] Integrate CUDA support for concat linear int8pack_mm woq optimization pattern,bbeckca,"Summary:
What: Enables CUDA support for concat linear int8_mm woq optimization pattern by:

- Updating pattern validation to accept CUDA devices
- Adding test coverage for CUDA

Why: Extend WOQ to more device types

Test Plan:
```
buck2 run 'fbcode//mode/opt' //caffe2/test/inductor:cuda_select_algorithm
```

Rollback Plan:

Differential Revision: D80884518


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-30 18:16:49+00:00,2025-09-18T18:09:14Z,,False,19,5,1,74,3,2,24,2025-09-18 18:08:11+00:00,83,562,False,False,False,False,False,False,2,8,3054,77,74,3,1,1,3.0,8.0,2025-08-30T18:39:23Z,pytorch
161846,closed,[MPS] add bunch of unary funcs for sparse tensors,Isalia20,"adds bunch of unary functions for sparse tensors

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen",2025-08-30 18:02:24+00:00,2025-08-30T21:14:12Z,,False,5,0,1,81,112,3,5,2025-08-30 21:13:10+00:00,49,107,False,False,False,False,False,False,3,2,498,193,81,112,1,1,3.0,2.0,2025-08-30T18:04:35Z,pytorch
161845,closed,[OpenReg] Migrate Accelerator Document from source/notes into source/accelerator,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #161918
* #161917
* #161947
* #161903
* __->__ #161845

As the tile stated.

As the document grows, the content will become more and more, so in order to make it easier for users to read and easier for developers to maintain, we have split this file into several separate files and placed them in a dedicated directory called ""accelerator"".",2025-08-30 15:14:38+00:00,2025-09-11T20:07:22Z,,False,9,2,4,73,47,3,11,2025-09-03 03:12:21+00:00,80,428,False,False,False,True,False,False,3,8,2545,7011,5100,1911,1,4,3.0,9.0,2025-09-02T18:03:16Z,pytorch
161844,closed,Remove background thread UT on XPU to fix CI,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161844

# Motivation
Because we revert `torch._C._set_allocator_settings` in https://github.com/pytorch/pytorch/pull/161626, this UT becomes invalid.
Fix https://github.com/pytorch/pytorch/issues/161697
",2025-08-30 07:21:50+00:00,2025-09-01T03:46:32Z,,False,6,0,1,0,11,1,6,2025-09-01 03:45:29+00:00,44,289,False,True,False,False,False,False,1,5,1689,11,0,11,1,1,3.0,5.0,2025-09-01T03:25:53Z,pytorch
161843,closed,Make pattern matcher resilient to ddes,bobrenjc93,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161843

Motivated by the following discord support chat: https://discord.com/channels/1189498204333543425/1409578286186758195

```
import torch
@torch.compile(fullgraph=True, mode='reduce-overhead')
def get_mask(W: torch.Tensor, percentage_nonzeros: torch.Tensor):
    total_elements = W.numel()
    k = int(total_elements * percentage_nonzeros)
    top_k_indices = torch.topk(torch.abs(W).flatten(), k)[1]
    mask = torch.zeros(total_elements, dtype=torch.bool, device=W.device)
    mask.scatter_(0, top_k_indices, True)
    mask = mask.view(W.shape)
    return mask

x = torch.randn((128, 64), device='cuda')
p = torch.tensor(0.50, device='cuda')
get_mask(x, p)
```

Results in

```
InductorError: GuardOnDataDependentSymNode: Could not guard on data-dependent expression Eq(TruncToInt(zuf0), 1) (unhinted: Eq(TruncToInt(zuf0), 1)).  (Size-like symbols: none)
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-30 07:01:25+00:00,2025-09-02T05:17:20Z,,False,4,2,3,19,2,2,6,2025-09-02 05:16:16+00:00,38,1155,False,False,False,False,False,False,2,3,566,27,22,5,1,3,3.0,3.0,2025-09-02T02:00:26Z,pytorch
161840,open,[BE] Move git config setup to checkout action,malfet,"Not sure why it was duplicated between build and test in the first place

And let's use the same set of setting across all repos
",2025-08-30 01:26:11+00:00,2025-08-30T16:00:43Z,,False,1,1,1,11,25,3,2,,45,129,False,False,False,False,False,False,3,0,0,36,11,25,1,1,1.0,0.0,2025-08-30T13:27:55Z,pytorch
161839,open,"[dynamo, 3.14] Python dynamo changes to get basic programs working",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* #163110
* #163109
* #163009
* __->__ #161839
* #161555
* #161838



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-30 01:04:27+00:00,2025-09-25T00:36:00Z,,False,1,0,5,40,12,6,1,,66,356,False,False,False,False,False,False,6,0,0,125625,83175,42450,1,5,,,,pytorch
161838,open,[dynamo] format cpython_defs.c,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* #163110
* #163109
* #163009
* #161839
* #161555
* __->__ #161838



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-30 01:04:21+00:00,2025-09-25T00:35:52Z,,False,1,0,4,238,252,1,1,,30,356,False,False,False,False,False,False,1,0,0,126067,83375,42692,1,4,2.0,0.0,2025-09-01T17:13:47Z,pytorch
161837,closed,Run inductor perf smoke test on H100,huydhn,Testing,2025-08-30 00:25:55+00:00,2025-09-04T03:39:49Z,,False,1,0,1,1,1,1,1,2025-09-04 03:39:49+00:00,36,7,False,False,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
161835,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-08-30 00:25:29+00:00,2025-08-31T04:25:09Z,,False,6,0,1,1,1,1,6,2025-08-31 04:24:07+00:00,46,156,False,False,False,False,False,False,1,5,1332,2,1,1,1,1,2.0,5.0,2025-08-30T00:25:30Z,pytorch
161836,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-08-30 00:25:29+00:00,2025-08-30T04:24:08Z,,False,3,0,1,1,1,1,3,2025-08-30 04:23:06+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-08-30T00:25:30Z,pytorch
161834,closed,Pass shared_ptr by value,lakshayg,"The way AsyncAllreduceCUDADeviceWork is currently implemented,
using it will force a copy of `shared_ptr<gloo::Context>`
because `std::move` does nothing for a const ref.

This PR changes the param type to shared_ptr<> instead of the
const ref. This allows more efficient parameter passing.

Here's an example that demonstrates the issue:

```cpp
#include <memory>
#include <iostream>

struct Foo {};

void useFoo_ref(const std::shared_ptr<Foo>& f) {
    std::shared_ptr<Foo> internal = std::move(f);
    std::cout << ""use_count: "" << internal.use_count() << '\n';
}

void useFoo_val(std::shared_ptr<Foo> f) {
    std::shared_ptr<Foo> internal = std::move(f);
    std::cout << ""use_count: "" << internal.use_count() << '\n';
}

int main() {
    std::shared_ptr<Foo> f1 = std::make_shared<Foo>();
    useFoo_ref(std::move(f1)); // prints ""use_count: 2""

    std::shared_ptr<Foo> f2 = std::make_shared<Foo>();
    useFoo_val(std::move(f2)); // prints ""use_count: 1""
}
```

This also aligns well with [C++ Core Guidelines][1] for handling
smart pointers.

[1]: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines?utm_source=chatgpt.com#Rr-summary-smartptrs


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 23:20:33+00:00,2025-08-30T18:01:44Z,,False,6,0,1,1,1,1,6,2025-08-30 18:00:41+00:00,24,1240,False,False,False,False,False,False,1,5,1340,2,1,1,1,1,4.0,5.0,2025-08-29T23:21:04Z,pytorch
161833,closed,[CD][CUDA13][ARM] aarch64 binary seems to be missing Triton dependency,nWEIdia,"Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions 

Seems to be missing Triton. 

cc @tinglvv @ptrblck @atalman @malfet 
",2025-08-29 23:15:17+00:00,2025-09-04T17:31:34Z,,False,12,0,1,2,2,1,12,2025-09-02 19:31:18+00:00,70,154,False,False,False,False,False,False,1,11,8978,4,2,2,1,1,5.0,11.0,2025-08-29T23:20:13Z,pytorch
161832,closed,[RFC][pt2] Add support for supplying an external worker to compile Triton,masnesral,"Summary: If we use the current approach and start the worker using whatever binary started us, we accumulate the memory overhead of that (potentially fat) binary. This adds the inductor changes to allow us to specify an external worker, presumably built against the same rev used to build the torch library.

Test Plan:
New unit test

Rollback Plan:

Differential Revision: D81356325


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 23:01:31+00:00,2025-09-03T21:01:11Z,,False,3,0,1,16,2,2,3,2025-09-03 21:01:11+00:00,73,587,False,False,False,False,False,False,2,1,94,18,16,2,1,1,1.0,1.0,2025-09-03T20:24:35Z,pytorch
161829,closed,[Inductor] update exp codegen for better precision,BoyuanFeng,"Prior to this PR, we have:
```
[Default Behavior] uses `tl.math.exp({x})`:
eager diff: tensor(2.6935e-06, device='cuda:0', dtype=torch.float64)
compile diff: tensor(9.2757e-06, device='cuda:0', dtype=torch.float64)
eager_latency:0.0013996509159580942, compile_latency:0.0013981951951980592


TORCHINDUCTOR_USE_FAST_MATH=1 uses `tl.extra.libdevice.exp2(tmp0 * 1.4426950408889634)`:
eager diff: tensor(2.2315e-06, device='cuda:0', dtype=torch.float64)
compile diff: tensor(3.5329e-06, device='cuda:0', dtype=torch.float64)
eager_latency:0.0013982331859319662, compile_latency:0.0013824134564199367


Update inductor to use `tl.extra.libdevice.exp(tmp0)`:
eager diff: tensor(2.3421e-06, device='cuda:0', dtype=torch.float64)
compile diff: tensor(2.3421e-06, device='cuda:0', dtype=torch.float64)
eager_latency:0.0014109122834153282, compile_latency:0.0014062877025520593
```

Since `tl.extra.libdevice.exp` leads to both better precision and on-par latency, we use it by default now.

Note that `tl.extra.libdevice.exp` used to have a perf issue in [January 2025](https://github.com/triton-lang/triton/issues/5735) since it used due to `ex2.approx.f32` instead of `ex2.approx.ftz.f32`. So `tl.extra.libdevice.exp2(tmp0 * 1.4426950408889634)` was used as a workaround. I double checked that the issue is resolved and `tl.extra.libdevice.exp` also uses [ex2.approx.ftz.f32](https://github.com/triton-lang/triton/issues/5735#issuecomment-3238421293) today.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-29 22:31:44+00:00,2025-08-30T04:57:55Z,,False,3,4,3,5,5,3,7,2025-08-30 04:56:53+00:00,50,1663,False,False,False,False,False,False,3,2,493,24,12,12,1,3,3.0,2.0,2025-08-30T00:41:06Z,pytorch
161828,open,[WIP] try another symint wrapper,pianpwk,"Fixes #ISSUE_NUMBER


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 22:12:33+00:00,2025-09-03T21:32:37Z,,False,1,0,9,232,7,8,1,,32,215,False,True,False,False,False,False,8,0,0,357,291,66,1,9,,,,pytorch
161827,closed,Log Const Folded Node,kqfu,"Summary: Log folded nodes for easier debugging.

Test Plan:
sandcastle.

Rollback Plan:

Reviewed By: henryoier

Differential Revision: D81352098


",2025-08-29 21:57:44+00:00,2025-09-02T23:24:57Z,,False,5,0,1,1,0,1,5,2025-09-02 23:23:54+00:00,21,148,False,True,False,False,False,False,1,2,544,1,1,0,1,1,4.0,3.0,2025-08-29T22:07:13Z,pytorch
161825,open,C++ API handle optimizer defaults ,stmcgovern,"Fixes #141884 

This fixes the issue for all optimizers and parameter options. 
A member function `overwrite_from` is added to the optimizer base class. Each optimizer then implements this function for comparing their accepted parameters to defaults. A SFINAE approach to handle the different optimizer parameters generically (in optimizer.h only) was evaluated, but I think this is easier to review and maintain.

This mirrors the Python API up to one edge case. An example of the edge case is provided below. 

Python can distinguish between 1) Key not present in dict = ""not specified""  and 2) Key present in dict = ""explicitly set"". The C++ implementation cannot.
The issue hinges on whether or not to track if a particular parameter was set by the user explicitly or not (discrepancy in the case when the constructor default is explicitly passed in). 

To track this seems like it will take more intervention than would be worth it (modify TORCH_ARG to keep track, use std::optional for the parameter types, use bitset tracking) and was not pursued in the current PR. I'm happy to alter the design if appropriate. 

### Example of edge case hinging on CONSTRUCTOR DEFAULTS vs OPTIMIZER DEFAULTS 

1. CONSTRUCTOR DEFAULTS:
   These are the values you get when calling AdamOptions()
   AdamOptions().lr() = 0.001
   AdamOptions().weight_decay() = 0
   AdamOptions().eps() = 1e-08

2. OPTIMIZER DEFAULTS:
   These are the values the user chose when creating the optimizer
   User's optimizer defaults:
   optimizer.lr() = 0.005
   optimizer.weight_decay() = 0.1
   optimizer.eps() = 1e-07

3. THE PROBLEM SCENARIO:
   User wants to add a parameter group with explicit weight_decay=0.0
   User sets: weight_decay(0)

4. THE CONFUSION:
   Constructor default weight_decay: 0
   User's explicit weight_decay:     0
   Are they equal? YES

   Since they're equal, our overwrite_from() logic thinks:
   ""User didn't set weight_decay explicitly, use optimizer default""

5. CURRENT BEHAVIOR:
   Final weight_decay: 0.1
   User expected:      0
   Match? ❌ NO

=== KEY INSIGHT ===
Constructor defaults are built into the C++ class definition.
Optimizer defaults are chosen by the user at runtime. We want to respect the user intention.",2025-08-29 21:52:19+00:00,2025-09-25T16:18:03Z,,False,5,7,3,823,41,16,12,,34,2229,False,True,False,False,False,False,16,4,9002,1158,970,188,1,3,2.0,5.0,2025-09-03T19:21:10Z,pytorch
161824,closed,[MPS] fix empty input in posneg functions,Isalia20,"fix empty posneg function for mps:
```python
import torch

input_tensor = torch.empty(0, device=""mps"")
out_pos = torch.isposinf(input_tensor)
```

Gives:
```
RuntimeError: [srcBuf length] > 0 INTERNAL ASSERT FAILED at ""/Users/Irakli_Salia/Desktop/pytorch/aten/src/ATen/native/mps/OperationUtils.mm"":551, please report a bug to PyTorch. Placeholder tensor is empty!
```

on main branch

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen",2025-08-29 21:26:17+00:00,2025-08-29T23:13:09Z,,False,4,0,1,11,0,2,4,2025-08-29 23:12:07+00:00,41,443,False,True,False,False,False,False,2,2,788,11,11,0,1,1,2.0,2.0,2025-08-29T21:59:23Z,pytorch
161823,open,[CUDA][cuBLAS][TF32] Skip checking TF32 options and setting cuBLAS handle TF32 modes for other dtypes,eqy,"For #161822

Basically #125888 appears to introduce measurable CPU overhead due to TF32 precision setting checks. Unfortunately one of the checks is in `getCurrentCUDABlasHandle`, which is called preceding _every_ cuBLAS matmul (and in a few other places such as workspace setup). We don't need to do this for non-`float32` matmuls so this PR is to alleviate the performance hit where it hurts the most (smaller dtypes that have faster matmuls) at the cost of some copypasta.

Some microbenchmark runs with this PR (microseconds):
```
7.812199214640714 
8.07804053692962 
7.865882366786536
7.898942214978888
8.018492849259928
```
and without
```
8.222943563396257
8.129948014357069
8.184711361991504
8.28010104214627
8.266569921033806
```

script:
```
import torch
import time

warmup = 128
iters = 16384

a = torch.zeros(512, 512, device='cuda', dtype=torch.bfloat16)
for _ in range(warmup):
    torch.matmul(a, a)

torch.cuda.synchronize()
t0 = time.perf_counter()
for _ in range(iters):
    torch.matmul(a, a)
torch.cuda.synchronize()
t1 = time.perf_counter()
print(f""{1e6 * (t1 - t0)/iters}"")
```

Longer term we'd prefer making `float32Precision` faster (better data structures, less validation, etc.?)

cc @ptrblck @msaroufim @jerryzh168 @csarofeen @xwang233 @zasdfgbnm",2025-08-29 21:24:33+00:00,2025-09-03T16:36:26Z,,False,5,1,3,31,11,5,6,,101,1275,False,False,False,False,False,False,5,4,1397,250,135,115,1,3,4.0,5.0,2025-08-29T22:03:58Z,pytorch
161821,open,[inductor] Runtime estimations: add triton's do_bench code,ruisizhang123,"As titled, this PR adds profiling support for BaseScheduleNode and FusedScheduleNode using Triton's do_bench mode.

The changes include:
- Adding estimation for BaseScheduleNode and FusedScheduleNode
- Rename the config name to `runtime_estimations_comp_benchmark` to reflect that we are profiling not only matmuls but also other ops

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161821
* #161405



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 21:18:56+00:00,2025-09-03T01:07:33Z,,False,2,3,4,136,71,4,5,,58,716,False,False,False,False,False,False,4,0,0,243,154,89,1,4,2.0,0.0,2025-09-02T12:30:25Z,pytorch
161820,closed,[SymmMEM] Move AsyncTP tests to a seperate test class,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161820

We move AsyncTP tests to a seperate test suite because 1) Async TP ops are not the core symmetric memory APIs, they are more like applications, 2) MultiProcContinuousTest will skip all the following tests if a test fails (we should fix this too). We still want to get the test signals for the core
symmetric memory APIs when Async TP ops fail.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 21:07:52+00:00,2025-08-30T00:41:46Z,,False,3,0,1,115,98,1,3,2025-08-30 00:40:43+00:00,53,508,False,True,False,False,False,False,1,2,493,213,115,98,1,1,3.0,2.0,2025-08-29T21:48:38Z,pytorch
161819,open,test,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161819



cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 20:55:55+00:00,2025-09-10T22:19:41Z,,False,1,6,3,77,39,5,7,,4,353,False,False,False,False,False,False,5,0,0,54848,30998,23850,1,3,1.0,0.0,2025-09-10T22:19:41Z,pytorch
161817,closed,[MPS] Add slow version of `kthvalue`,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161817

Which heavily borrows implementation logic from `topk`
As this method is non-deterministic, modified the logic for cpu-ops indices comparison with just an equality statement, as by default random numbers picked for input tensor allow for quite a lot of overlaps ",2025-08-29 20:55:14+00:00,2025-08-30T00:45:35Z,,False,4,0,3,114,1,4,4,2025-08-30 00:44:32+00:00,36,356,False,False,False,False,False,False,4,2,788,159,136,23,1,3,3.0,2.0,2025-08-29T20:57:40Z,pytorch
161816,closed,[Reland][Inductor] Prune configs that require more shared memory than the hardware limit,wychi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161816

Summary:
This is a re-land of [PR161040](https://github.com/pytorch/pytorch/pull/161040), which had previously caused test failures on AMD GPUs. The tests are now configured to target only NVIDIA GPUs.

This diff removes configurations that exceed the hardware shared memory limit, which causes the following compilation error:
```
No valid triton configs. OutOfMemoryError: out of resource: triton_mm Required: 327680 Hardware limit:232448 Reducing block sizes or `num_stages` may help.
```

Test Plan:

```
pytest test/inductor/test_max_autotune.py
pytest test/inductor/test_triton_heuristics.py

```

Reviewers:

Subscribers:

Tasks:

Tags:

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81400958](https://our.internmc.facebook.com/intern/diff/D81400958)",2025-08-29 20:35:36+00:00,2025-09-02T18:50:44Z,2025-08-30T07:02:53Z,True,9,2,4,126,17,5,11,2025-08-30 07:02:53+00:00,88,1033,False,False,False,False,False,False,5,6,392,147,128,19,2,4,3.0,6.0,2025-08-29T21:11:09Z,pytorch
161814,closed,[CI] Fix decorator logic in common_mps,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161814

Always decorate for MPS device. This fixes regressions in sparse testing
that were introduced by https://github.com/pytorch/pytorch/pull/160839",2025-08-29 20:22:29+00:00,2025-08-30T00:38:45Z,,False,3,0,1,4,7,1,3,2025-08-30 00:38:44+00:00,38,237,False,True,False,False,False,False,1,1,76,11,4,7,1,1,2.0,1.0,2025-08-29T20:28:49Z,pytorch
161813,closed,[CI/CD] Windows set git config --global core.ignorecase false,atalman,"Make sure git on windows have core.ignorecase false
",2025-08-29 20:13:27+00:00,2025-08-29T23:05:48Z,,False,3,0,2,2,0,2,3,2025-08-29 23:04:46+00:00,61,52,False,False,False,False,False,False,2,2,608,2,2,0,1,2,3.0,3.0,2025-08-29T20:23:56Z,pytorch
161811,closed,Add batch option for send/recv_object_list,H-Huang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161811

Pull-Request: https://github.com/pytorch/pytorch/pull/160342

cc @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 19:32:27+00:00,2025-08-29T19:54:46Z,,False,1,0,1,58,4,2,1,2025-08-29 19:54:46+00:00,42,223,False,False,False,False,False,False,2,0,0,62,58,4,1,1,,,,pytorch
161810,closed,An improved heuristic for operator reordering for peak memory + debugging logs,xuanzhang816,"Revisiting the idea in https://github.com/pytorch/pytorch/pull/140195

For the lpmf algorithm in the memory reorder pass, in some cases, when all the nodes that can be scheduled are quite large, it is beneficial to switch the scheduling strategy. So instead of using size as the criterion, we choose a node that can unlock more nodes to become schedulable by analyzing their successor nodes. 

For an internal use case, we observe up to 20 GiB memory difference and here are the before and after memory snapshot. More information can be found in [D81270682](https://www.internalfb.com/diff/D81270682) (internal only).

<img width=""348"" height=""227"" alt=""image"" src=""https://github.com/user-attachments/assets/fb71e840-1508-44ed-bc9d-5eb4d364607d"" />

In addition, add the functionality to upload the graph to tlparse for offline debugging. The format of the json is in consistency with the simulator [here](https://fburl.com/code/3l3d3qi4) (internal only).


Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161810



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 19:30:25+00:00,2025-09-13T00:43:38Z,,False,7,0,2,168,17,3,7,2025-09-13 00:42:35+00:00,78,1256,False,True,False,False,True,False,3,6,2004,98487,63861,34626,1,2,3.0,6.0,2025-08-29T19:36:13Z,pytorch
161809,open,[scan] materialize forward graph to supprot torch.grad.grad in subgraph,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161732
* __->__ #161809
* #161808
* #161664
* #161557

",2025-08-29 19:19:38+00:00,2025-08-29T21:01:46Z,,False,2,0,1,10,1,1,2,,71,134,False,False,False,False,False,False,1,0,0,11,10,1,1,1,,,,pytorch
161808,open,[scan][autograd] clone outputs that's aliasing with inputs or outputs in bw,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162754
* #161732
* #162025
* __->__ #161808
* #161664
* #161557

",2025-08-29 19:19:35+00:00,2025-09-19T12:13:58Z,,False,1,0,5,130,9,2,1,,75,144,False,False,False,False,False,False,2,0,0,97449,61648,35801,1,5,1.0,0.0,2025-09-19T12:13:58Z,pytorch
161806,closed,Investigate moving from constrain_to_fx_strides to constrain_to_fake_tensors,benjaminglass1,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161806

Fixes #153489
Fixes #159155

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 18:51:13+00:00,2025-09-08T17:59:01Z,,False,1,3,1,20,14,5,4,2025-09-08 17:59:01+00:00,76,324,False,True,False,False,False,False,5,0,0,34,20,14,1,1,2.0,0.0,2025-09-02T15:30:01Z,pytorch
161805,closed,[AOTI] Add Windows-compatible implementation of the mmap-related funcs,xuhancn,"Add Windows-compatible implementation of the mmap-related functions.

These code was validated on the small developing project: https://github.com/xuhancn/cross_os_mmap?tab=readme-ov-file#cross_os_mmap

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",2025-08-29 18:32:17+00:00,2025-09-03T00:57:30Z,,False,10,4,3,260,3,1,14,2025-09-02 20:07:44+00:00,70,334,False,False,False,False,False,False,1,9,1644,343,300,43,1,3,3.0,9.0,2025-08-29T20:09:14Z,pytorch
161804,closed,Fix `range.__getitem__()`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161804
* #161803
* #161802
* #161801



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:32:12+00:00,2025-09-04T02:34:13Z,,False,8,0,11,89,15,11,8,2025-09-04 02:33:08+00:00,25,296,False,True,False,False,False,False,11,7,1829,72916,44525,28391,1,10,3.0,7.0,2025-09-02T23:47:31Z,pytorch
161803,closed,redirect `iter(range)` to `range.__iter__()`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161804
* __->__ #161803
* #161802
* #161801



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:32:05+00:00,2025-09-04T02:33:08Z,,False,4,0,8,2,1,2,4,2025-09-04 02:33:07+00:00,44,296,False,False,False,False,False,False,2,3,326,72807,44434,28373,1,8,2.0,3.0,2025-09-02T23:46:58Z,pytorch
161802,closed,Add `range_count` and `range.__contains__`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161804
* #161803
* __->__ #161802
* #161801



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:31:59+00:00,2025-09-04T02:33:07Z,,False,4,0,8,27,15,4,4,2025-09-04 02:33:07+00:00,42,296,False,False,False,False,False,False,4,3,326,72856,44459,28397,1,8,2.0,3.0,2025-09-02T23:46:45Z,pytorch
161801,closed,Add `range_equals`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161804
* #161803
* #161802
* __->__ #161801



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:31:52+00:00,2025-09-04T02:33:06Z,,False,4,2,8,51,34,3,6,2025-09-04 02:33:06+00:00,18,296,False,False,False,False,False,False,3,3,326,72910,44494,28416,1,8,3.0,3.0,2025-09-02T23:43:34Z,pytorch
161800,closed,Add `range_iterator`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161804
* #161803
* #161802
* #161801
* __->__ #161800
* #161799



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:31:46+00:00,2025-09-03T16:56:14Z,,False,3,2,6,188,43,5,5,2025-09-03 16:55:08+00:00,20,316,False,False,False,False,False,False,5,2,493,15560,12048,3512,1,6,3.0,2.0,2025-09-02T23:40:50Z,pytorch
161799,closed,Add CPython test `test_range`,guilhermeleobas,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161804
* #161803
* #161802
* #161801
* #161800
* __->__ #161799



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 18:31:39+00:00,2025-09-03T16:55:08Z,,False,2,0,2,809,0,23,2,2025-09-03 16:55:07+00:00,29,316,False,False,False,False,False,False,23,1,48,16134,12667,3467,1,2,2.0,1.0,2025-09-02T23:36:12Z,pytorch
161798,closed,[nativert] triton runtime implementation,dolpm,"Summary:
att 
Test Plan:
ci
Rollback Plan:

Reviewed By: minjang

Differential Revision: D80828148


",2025-08-29 18:23:25+00:00,2025-09-04T22:11:18Z,,False,29,0,1,578,1,12,29,2025-09-04 22:11:18+00:00,40,101,False,False,False,False,False,False,12,7,1292,579,578,1,1,1,5.0,7.0,2025-09-02T18:20:42Z,pytorch
161797,closed,Run vLLM tests on all trunk commits before 2.9 branch cut,huydhn,This makes it easier to bisect issue now given that we don't have lots of time.,2025-08-29 18:19:06+00:00,2025-09-09T06:02:09Z,,False,5,0,1,3,0,1,5,2025-09-09 05:56:45+00:00,57,79,False,False,False,False,False,False,1,4,1161,3,3,0,1,1,3.0,4.0,2025-08-29T18:27:35Z,pytorch
161795,closed,[inductor][decompose k] disable on everything other than cuda,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161795
* #161767

# why

- untested so far

# what

- add an empty config heuristic for all devices for decompose k
- the cuda heuristic, because it is more specific, will still be picked
  up
- add notes explaining how to enable on other devices

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v -k ""decompose_k""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-29 18:03:22+00:00,2025-08-29T22:42:34Z,,False,3,0,2,5,0,1,3,2025-08-29 22:41:31+00:00,61,618,False,False,False,False,False,False,1,2,493,19,17,2,1,2,3.0,2.0,2025-08-29T18:57:10Z,pytorch
161794,closed,kill allow_complex_guards_as_runtime_asserts,avikchaudhuri,"Summary:
[reland]
Since `allow_complex_guards_as_runtime_asserts` is now sync'd with `prefer_deferred_runtime_asserts_over_guards`, we can kill the former (especially since it was a export-only concept).

Test Plan:
updated tests

Rollback Plan:

Differential Revision: D81334984


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 17:47:12+00:00,2025-09-04T00:18:06Z,,False,6,0,1,48,68,10,6,2025-09-04 00:17:04+00:00,44,475,False,False,False,False,False,False,10,2,493,116,48,68,1,1,3.0,2.0,2025-08-29T18:09:32Z,pytorch
161793,closed,"ci: Update sphinx, disable google search by default",seemethere,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161793


Includes fixes from https://github.com/pytorch/pytorch_sphinx_theme/pull/207

Signed-off-by: Eli Uriegas <eliuriegas@meta.com>",2025-08-29 17:05:08+00:00,2025-09-01T07:44:46Z,,False,5,0,1,1,1,1,5,2025-09-01 07:43:42+00:00,51,221,False,True,False,False,False,False,1,4,742,2,1,1,1,1,4.0,6.0,2025-08-29T17:06:03Z,pytorch
161792,closed,Fix compiler errors in 3.14 stub definitions,alexmalyshev,"The functions here expect to return pointers, but currently aren't returning anything.  Make them return NULL.

The properties array wants an extra set of braces.  One pair for the array, another for the first item in the array.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 16:54:35+00:00,2025-09-03T00:59:46Z,,False,4,0,1,4,4,1,4,2025-09-03 00:58:43+00:00,44,400,False,True,False,False,False,False,1,3,535,8,4,4,1,1,3.0,3.0,2025-08-29T17:04:40Z,pytorch
161788,closed,[Reland] Generalize torch._C._set_allocator_settings to be generic,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161856
* __->__ #161788
* #161787
* #161786
* #161594

# Motivation
This is a reland of https://github.com/pytorch/pytorch/pull/156175

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 15:20:13+00:00,2025-09-15T05:49:20Z,,False,2,2,3,26,34,9,4,2025-09-15 05:49:20+00:00,66,385,False,False,False,False,False,False,9,1,150,80,35,45,1,3,2.0,2.0,2025-08-30T17:58:19Z,pytorch
161787,closed,"[Reland] Deprecate overlapped functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161856
* #161788
* __->__ #161787
* #161786
* #161594
# Motivation
This is a reland of https://github.com/pytorch/pytorch/pull/156165
",2025-08-29 15:17:41+00:00,2025-09-15T05:49:30Z,,False,1,2,3,43,36,5,3,2025-09-15 05:49:30+00:00,102,213,False,False,False,False,False,False,5,0,38,96,53,43,1,3,2.0,1.0,2025-08-30T17:57:10Z,pytorch
161786,closed,[Reland] Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161856
* #161788
* #161787
* __->__ #161786
* #161594
# Motivation
This is a reland of https://github.com/pytorch/pytorch/pull/150312
",2025-08-29 15:15:04+00:00,2025-09-15T05:49:41Z,,False,1,6,2,160,495,4,7,2025-09-15 05:49:41+00:00,73,213,False,False,False,False,False,True,4,0,0,670,169,501,1,2,2.0,0.0,2025-08-29T17:24:00Z,pytorch
161785,closed,Inductor: dispatch GELU to oneDNN in CPU compile path and re-enable AArch64 fusions,puneetmatharu,"> [!CAUTION]
> **DRAFT RELEASE!**

## Summary

This PR introduces a dedicated `mkldnn::_gelu` operator and ensures `GELU(approximate=""none"")` subgraphs are lowered to a oneDNN eltwise GELU on CPU. It also re-enables fusions on AArch64 in compile mode.

## Motivation

- On CPU, GELU appears post-decomposition as `mul(x*0.5, add(erf(x*sqrt(0.5)), 1))`, which is later replaced with a codegen-ed `Vectorized` implementation, instead of using faster implementations from oneDNN/ACL when available.
- The fusion logic on AArch64 was disabled due to previous issues with fusions in oneDNN; this is currently being addressed in https://github.com/uxlfoundation/oneDNN/pull/3767#issuecomment-3191330732, allowing arbitrary post-ops (activations) with the ACL `matmul` and `inner_product` primitives.

## Changes

- New op `mkldnn::_gelu(Tensor X, str algorithm)` calling oneDNN eltwise `GELU`; returns dense for dense inputs.
- Adds a new `Gelu` lowering + Inductor IR and AOTI shim `aoti_torch_cpu__gelu(...)`.
- Defines a fusion pattern (in the post-grad step) that recognises the decomposed `GELU` and lowers it to `mkldnn::_gelu` on pass number 2 (0-indexed), to ensure it only occurs after higher-priority linear/conv fusions.
- Re-enables fusions in compile mode on AArch64.

## Remarks
- Uses oneDNN `eltwise_gelu_erf`; only approximate=""none"" is supported by this path.
- Returns dense for dense inputs so downstream ATen pointwise ops don’t see a `torch._mkldnn` layout unexpectedly.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 14:25:58+00:00,2025-09-01T13:52:20Z,,False,4,0,1,90,6,7,4,2025-09-01 13:45:10+00:00,83,1745,False,False,False,False,False,False,7,1,31,96,90,6,1,1,1.0,1.0,2025-09-01T13:46:03Z,pytorch
161782,open,init: cuda preload: add fallback to Wheel CUDA when system CUDA bad,aowenson-imm,"Fixes #150399

Add argument `force_wheel` to preload function `_load_global_deps`, so if `from torch._C import *` fails on system CUDA install e.g.:
> libcusparseLt.so.0: cannot open shared object file: No such file or directory

... then re-load but forcing load of the wheel install.

I've tagged user-facing because this prints a message to user:
> WARNING: system CUDA not compatible, switching to Wheel CUDA. Reason: 'libcusparseLt.so.0: cannot open shared object file: No such file or directory'

I don't care if someone with an CLA takes this over, I just want to see Torch fix its CUDA preloading, it causes me pain as a sysadmin.",2025-08-29 13:01:43+00:00,2025-09-04T15:18:33Z,,False,4,0,1,25,5,1,4,,67,638,False,True,False,False,False,False,1,1,39,30,25,5,1,1,1.0,1.0,2025-08-29T13:03:44Z,pytorch
161781,closed,[AOTI] normalize_path_separator zip file path,xuhancn,"normalize_path_separator zip file path


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10",2025-08-29 11:55:23+00:00,2025-08-29T17:54:47Z,,False,9,0,1,2,1,1,9,2025-08-29 17:53:45+00:00,45,172,False,False,False,False,False,False,1,8,2225,3,2,1,1,1,3.0,8.0,2025-08-29T11:55:44Z,pytorch
161780,closed,Fix _scaled_grouped_mm not reported as unsupported on SM100.,alexsamardzic,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161780



cc @ptrblck @msaroufim @eqy @jerryzh168",2025-08-29 11:47:19+00:00,2025-08-30T12:34:56Z,,False,7,2,3,1,1,1,9,2025-08-30 12:33:54+00:00,60,135,False,True,False,False,False,False,1,6,1533,6140,4691,1449,1,3,6.0,6.0,2025-08-29T11:49:18Z,pytorch
161779,closed,[inductor] Fix SubgraphInfo round trip,kundaMwiza,"Currently `numels` is not specific to a created subgraph since it is not retrieved by `dataclasses.fields(SubgraphInfo)` due to it not being type annotated, see [ref](https://docs.python.org/3/library/dataclasses.html#module-dataclasses:~:text=The%20%40dataclass%20decorator%20examines%20the%20class%20to%20find%20fields.%20A%20field%20is%20defined%20as%20a%20class%20variable%20that%20has%20a%20type%20annotation.%20With%20two%20exceptions%20described%20below%2C%20nothing%20in%20%40dataclass%20examines%20the%20type%20specified%20in%20the%20variable%20annotation.). 

So for example the following would happen:

```
self.numels = {""x"": sympy.Integer(5)}
subgraph_name = ""<x>""
with self.create_subgraph_body(subgraph_name):
     self.numels = {""x"", sympy.Integer(7)}
# this would print that x has size 7, not the original value of 5
print(self.numels) 
# numels would be None because dataclasses.fields(SubgraphInfo) does not include numels 
# since it is not type annotated
print(self.subgraph_bodies[subgraph_name]) 
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 10:01:54+00:00,2025-08-29T16:28:33Z,,False,5,0,1,1,1,1,5,2025-08-29 16:27:31+00:00,38,1227,False,True,False,True,False,False,1,4,593,2,1,1,1,1,3.0,4.0,2025-08-29T10:09:18Z,pytorch
161777,closed,[Intel GPU] Update Intel triton commit pin to Triton 3.5.x,etaf,,2025-08-29 09:08:25+00:00,2025-09-05T16:56:54Z,,False,9,0,1,2,2,2,9,2025-09-05 16:55:50+00:00,58,0,False,False,False,False,False,False,2,8,1405,4,2,2,1,1,3.0,8.0,2025-08-30T11:14:46Z,pytorch
161775,open,[WIP] change DCP code to use the new util,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161775
* #161704
* #161770

**Test**
`pytest test/distributed/checkpoint/fsdp/test_fsdp_dsd.py -s -k fsdp2`

Before changing DCP local shape and global offset compute:
<img width=""2236"" height=""55"" alt=""image"" src=""https://github.com/user-attachments/assets/8fe28629-24f0-4eb2-9b7d-40af3739b3b6"" />
After:
<insert here>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 08:22:05+00:00,2025-08-29T17:27:12Z,,False,2,0,1,7,4,1,2,,41,483,False,False,False,False,False,False,1,1,528,11,7,4,1,1,1.0,1.0,2025-08-29T17:27:12Z,pytorch
161773,closed,[OpenReg] Add tests of device and memory for OpenReg,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #160100
* __->__ #161773
* #160099
* #161603

As the title stated.",2025-08-29 08:04:48+00:00,2025-08-30T13:21:33Z,,False,5,0,6,159,1,3,5,2025-08-30 13:21:32+00:00,52,154,False,False,False,False,False,False,3,4,1095,237,188,49,1,6,3.0,5.0,2025-08-29T20:54:05Z,pytorch
161772,closed,Add new parameter for gen_pyi.py to make it more configureable.,0xjeffro,"This is a reposting of PR #128519.
This change is important to how we maintain PyTorch at Google.

From the previous PR:
""
This will make the script more flexible for the directory where it is executed.
...
We plan to use the deprecated_yaml from a blaze genrule that invokes pyi.py. As the input to the pyi.py, genrule requires the input file to be explicitly listed out. When we feed the value of tools/autograd/deprecated.yaml to genrule, it failed to resolve since tools/autograd is a package from blaze perspective. Any file under a blaze package will a proper blaze target to be access.
""

cc @haifeng-jin
",2025-08-29 07:35:00+00:00,2025-09-05T00:49:21Z,,False,5,0,2,14,1,2,5,2025-09-05 00:48:18+00:00,63,612,False,False,False,False,False,False,2,3,541,17,15,2,2,2,4.0,4.0,2025-08-29T07:42:42Z,pytorch
161771,open,[1/N] Port 3 distributed/_shared test cases to Intel GPU ,libohao1201,"For https://github.com/pytorch/pytorch/issues/114850, we will port distributed tests to Intel GPU.
We could enable Intel GPU with following methods and try the best to keep the original code styles:


- use ""torch.accelerator.current_accelerator()"" to determine the accelerator backend
- use ""requires_accelerator_dist_backend"" to enable ""xccl""
- enabled XPU for some test path
- skip some test cases which Intel GPU does not support

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @gujinghui @EikanWang @fengyuan14 @guangyey
",2025-08-29 07:05:17+00:00,2025-09-18T08:08:38Z,,False,7,8,1,647,524,3,15,,57,581,False,False,False,False,False,False,3,5,1418,1171,647,524,1,1,4.0,6.0,2025-09-01T17:16:52Z,pytorch
161770,open,[do NOT land] replace _StridedShard w/ sharding order in FSDP2,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161775
* #161704
* __->__ #161770



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 06:45:15+00:00,2025-08-29T17:35:47Z,,False,3,0,2,42,10,3,3,,62,192,False,False,False,False,False,False,3,2,376,102,67,35,1,2,1.0,2.0,2025-08-29T17:26:54Z,pytorch
161768,closed,[inductor][triton] support JITCallable._hash_lock,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #159158
* __->__ #161768

Fixes #161618

Triton # 7974 introduces a threading.RLock() in JITCallable, which is not pickle-able. This PR adds this field to the list of un-pickleable fields that need to be handled specially.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-29 06:15:57+00:00,2025-08-29T21:21:07Z,,False,3,0,3,6,1,2,3,2025-08-29 21:20:05+00:00,49,511,False,True,False,False,False,False,2,2,887,38,22,16,1,3,3.0,3.0,2025-08-29T13:13:53Z,pytorch
161767,closed,"[inductor][heuristics registry] missing heuristic is not an error anymore, cross device heuristics",coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161795
* __->__ #161767

# why

- not having a heuristic is an error but should not crash, just provide 0 configs
- some heuristics are cross device type
- cleaner to be explicit about being cross device type than having to
  enumerate every possible device type

# what

- on registration, supply device_type=None (explicitly) to say this
  heuristic is cross device
- test to guard the heuristics hierarchies

# testing

```
python3 -bb -m pytest test/inductor/test_template_heuristics_registry.py
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-29 05:41:37+00:00,2025-08-29T22:41:30Z,,False,2,0,3,232,21,3,2,2025-08-29 22:41:30+00:00,98,771,False,False,False,False,False,False,3,1,48,1739,1375,364,1,3,2.0,1.0,2025-08-29T18:57:00Z,pytorch
161766,open,Use CUDA for test_dtensor_ops,azahed98,"Enables CUDA for test_dtensor_ops.py.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 05:08:19+00:00,2025-09-03T22:01:54Z,,False,1,0,5,104,360,1,1,,29,115,False,False,False,False,False,False,1,0,0,470,107,363,1,5,,,,pytorch
161762,closed,[inductor] Lift fw_compiler and bw_compiler as toplevel functions.,zhxchen17,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161762

This is a no-op refactor to compiler_fx which lifts the logic of fw_compiler and bw_compiler to toplevel, so that they can be reused in a different stack (e.g. precompile).

Differential Revision: [D81292968](https://our.internmc.facebook.com/intern/diff/D81292968/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 03:25:25+00:00,2025-08-29T21:48:01Z,,False,8,0,5,265,187,1,8,2025-08-29 21:46:57+00:00,66,563,False,False,False,False,False,True,1,1,476,6910,4395,2515,1,4,3.0,1.0,2025-08-29T17:28:58Z,pytorch
161761,open,[4/n] Quantization with min & max bounds support - float to 8 bit on X86-64,sampathvic,"Summary:
X-link: https://github.com/pytorch/FBGEMM/pull/4790

X-link: https://github.com/facebookresearch/FBGEMM/pull/1813

At present quantization is performed at whole tensor level where each row's minimum and maximum values are calculated. In order to scale the quantization when tensor's are col-wise sharded, we are introducing these changes.

These changes enable passing the minimum and maximum values of each row of the whole tensor and use it to quantize the col-wise sharded tensors individually.

In this specific change, the support is added for float to 8-bit quantization on X86-64 architecture. In the future changes we will expand the support for other source/target bit rates and architectures.

Test Plan:
## Unit Tests

```
buck test mode/opt deeplearning/fbgemm:QuantUtilsTest
```
https://www.internalfb.com/intern/testinfra/testrun/7881299637204767

```
buck test mode/opt caffe2/torch/fb/model_transform/splitting/tests:split_dispatcher_test
```
https://www.internalfb.com/intern/testinfra/testrun/14636698895328167

## Integration Testing

Please refer to D80905814's test plan.

Rollback Plan:

Reviewed By: excelle08

Differential Revision: D78181177




cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-08-29 03:20:49+00:00,2025-08-29T04:26:59Z,,False,2,0,1,55,5,3,2,,75,1261,False,False,False,False,False,False,3,0,0,60,55,5,1,1,,,,pytorch
161759,open,[ROCm] Add specific compile options for CK SDPA,alugorey,"Updates CK version and adds CK specific compilation options


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-29 02:35:03+00:00,2025-09-10T14:58:56Z,,False,8,2,6,94,10,4,10,,47,178,False,False,False,False,False,False,4,7,3982,170,127,43,1,6,5.0,7.0,2025-09-04T20:37:31Z,pytorch
161756,closed,use host+device_id to make sure devices are unique in rendezvous request,ngimel,"Per title, for NVL72 systems where devices with the same indices on multiple hosts are within the same nvlink domain


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-29 02:00:47+00:00,2025-08-29T09:10:49Z,,False,3,0,1,10,4,1,3,2025-08-29 09:09:47+00:00,72,195,False,False,False,False,False,False,1,2,498,14,10,4,1,1,3.0,3.0,2025-08-29T02:06:50Z,pytorch
161755,closed,make einsum produce contiguous inputs in more cases,ngimel,"Fixes #161729 
Written by codex
This won't produce contiguous inputs for all einsum applications, because we flatten all right-only and left-only dimensions, so if right and left operand dimensions are interleaved in output, we cannot (with current algo) produce contiguous output, however, for common cases like in the linked issue it works. Let's see what CI says
",2025-08-29 01:35:54+00:00,2025-08-29T18:51:51Z,,False,3,0,2,23,0,2,3,2025-08-29 18:50:49+00:00,51,366,False,True,False,False,False,False,2,2,499,23,23,0,1,2,4.0,3.0,2025-08-29T18:07:42Z,pytorch
161754,closed,[ROCm] Bump AOTriton to 0.11b,xinyazhang,"Notable new features/optimizations for SDPA operators on AMD systems from AOTriton 0.11b:

* Invoke AITER Assembly kernels on gfx942/gfx950 when inputs meet requirements
  - AITER ASM kernels deliver over 500TFLOPS training performance. See
    [AOTriton 0.11b Release Page](https://github.com/ROCm/aotriton/releases/tag/0.11b) for more
    details.
* Now returns natural based `logsumexp` tensor, matching CUDA's behavior
  - PR #156903 is reverted in this PR as well since it is not needed anymore. 
* Enables `CausalVariant.LOWER_RIGHT`

The build system changes drastically along with new packaging scheme of
AOTriton 0.11

* AOTriton 0.11 packs GPU images separately from AOTriton runtime
* `aotriton.cmake` now selectively downloads image packs according to
  `PYTORCH_ROCM_ARCH`
* `aotriton.cmake` now only use pre-compiled runtime library that exactly
  matches the ROCM in the build environment. For PyTorch builds with ROCm
  versions not listed in the file, the build process will build AOTriton
  runtime without GPU images from source
  - This avoids any further ABI breaks like ROCM 6.4 -> 7.0
  - recursive git clone is disabled since building AOTriton runtime does not
    require submodules.

Bug fixes:

* Fix a kernel bug introduced when implementing SWA

Known Problems:

* gfx1100 target (Radeon RX 7000 Series) is moved back to experimental status
  due to accuracy issues. Triton compiler fixes are needed to restore the
  support status.
* Enabling TF32 tests affects accuracy for later non-TF32 tests on ROCM 7.0.
  This issue is under investigation.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-29 00:51:19+00:00,2025-09-05T17:55:44Z,,False,14,10,36,484,169,12,24,2025-09-03 20:45:47+00:00,29,1966,False,True,True,False,False,False,12,4,1238,911,617,294,1,30,5.0,4.0,2025-09-02T20:52:29Z,pytorch
161753,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-08-29 00:26:29+00:00,2025-08-29T04:54:02Z,,False,3,0,1,1,1,1,3,2025-08-29 04:53:00+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-08-29T00:26:30Z,pytorch
161752,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-08-29 00:26:15+00:00,2025-08-29T04:55:35Z,,False,3,0,1,1,1,1,3,2025-08-29 04:54:34+00:00,46,156,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-08-29T00:26:16Z,pytorch
161751,closed,[PGO] log add_extra_remote PGO to tlparse,pianpwk,"Summary: log when additional PGO profile is merged in, from added read key

Test Plan:
test_pgo

Rollback Plan:

Differential Revision: D81284190




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-29 00:23:52+00:00,2025-09-04T22:48:11Z,,False,8,0,2,6,0,1,8,2025-09-04 22:47:07+00:00,41,320,False,False,False,False,False,False,1,1,476,8,7,1,1,2,2.0,1.0,2025-09-04T06:47:00Z,pytorch
161750,closed,Fix linalg.norm: ensure dtype is respected on CUDA,Navya1707,"Previously, torch.linalg.norm ignored the dtype argument on CUDA, always promoting
to float64. This PR aligns behavior with torch.norm by honoring dtype.
Added tests for float16/float32 on CPU and CUDA.
",2025-08-28 23:57:04+00:00,2025-08-30T14:54:33Z,,False,2,0,1,23,1,2,2,2025-08-30 14:54:28+00:00,50,203,False,True,False,False,False,False,2,0,281,24,23,1,1,1,1.0,1.0,2025-08-30T13:55:32Z,pytorch
161749,open,"[cuBLAS] update cuBLAS determinism docs, remove workspace requirement checks",eqy,"Since CUDA 11.x (need to update the docs for this, current PR is saying 12.2 which is incorrect) we've been allocating cuBLAS workspaces explicitly per handle/stream combination https://github.com/pytorch/pytorch/pull/85447

According to the cuBLAS documentation, this appears to be sufficient for determinism without any explicit workspace requirements to e.g., `:4096:8` or `:16:8` as was previously expressed in PyTorch docs https://docs.nvidia.com/cuda/cublas/#results-reproducibility

Planning to add an explicit determinism test as well...

cc @ptrblck @msaroufim @jerryzh168 @csarofeen @xwang233 @mruberry @kurtamohler",2025-08-28 23:49:08+00:00,2025-09-18T14:08:55Z,,False,6,1,4,11,164,7,7,,76,625,False,False,False,True,False,False,7,5,463,197,22,175,2,4,2.0,5.0,2025-09-04T18:37:51Z,pytorch
161748,closed,Cleanup stale submodule directories after checkout,malfet,"Fixes https://github.com/pytorch/pytorch/issues/161510

Test plan:
```
% cd third_party/kineto
% git checkout fe80f9319479265f7a208e615e16a363b993d50c; git submodule update --init --recursive
M	libkineto/third_party/dynolog
M	libkineto/third_party/fmt
M	libkineto/third_party/googletest
Previous HEAD position was 5e75018 Fix Local Time on Windows Builds (#1104)
HEAD is now at fe80f93 Fix MSVC Error (#1134)
Submodule path 'libkineto/third_party/dynolog': checked out 'd2ffe0a4e3acace628db49974246b66fc3e85fb1'
Submodule path 'libkineto/third_party/dynolog/third_party/googletest': checked out '52eb8108c5bdec04579160ae17225d66034bd723'
Submodule path 'libkineto/third_party/dynolog/third_party/prometheus-cpp': checked out 'b1234816facfdda29845c46696a02998a4af115a'
Submodule path 'libkineto/third_party/dynolog/third_party/prometheus-cpp/3rdparty/civetweb': checked out 'd7ba35bbb649209c66e582d5a0244ba988a15159'
Submodule path 'libkineto/third_party/dynolog/third_party/prometheus-cpp/3rdparty/googletest': checked out 'e2239ee6043f73722e7aa812a459f54a28552929'
Submodule path 'libkineto/third_party/fmt': checked out '40626af88bd7df9a5fb80be7b25ac85b122d6c21'
Submodule path 'libkineto/third_party/googletest': checked out '52eb8108c5bdec04579160ae17225d66034bd723'
% git checkout 5e75018; git submodule update --init --recursive                                 
M	libkineto/third_party/dynolog
M	libkineto/third_party/fmt
M	libkineto/third_party/googletest
Previous HEAD position was fe80f93 Fix MSVC Error (#1134)
HEAD is now at 5e75018 Fix Local Time on Windows Builds (#1104)
warning: unable to rmdir 'third_party/prometheus-cpp': Directory not empty
Submodule path 'libkineto/third_party/dynolog': checked out '7d04a0053a845370ae06ce317a22a48e9edcc74e'
Submodule path 'libkineto/third_party/dynolog/third_party/googletest': checked out '58d77fa8070e8cec2dc1ed015d66b454c8d78850'
Submodule path 'libkineto/third_party/fmt': checked out '0041a40c1350ba702d475b9c4ad62da77caea164'
Submodule path 'libkineto/third_party/googletest': checked out '7aca84427f224eeed3144123d5230d5871e93347'
% cd ../..
% git status
HEAD detached from 649e397c6de
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
  (commit or discard the untracked or modified content in submodules)
	modified:   third_party/kineto (untracked content)

% time git submodule foreach --recursive git clean -ffdx
...
git submodule foreach --recursive git clean -ffdx  0.47s user 0.96s system 88% cpu 1.625 total
% git status
HEAD detached from 649e397c6de
```",2025-08-28 23:36:44+00:00,2025-08-30T01:31:49Z,,False,15,2,8,15,0,1,17,2025-08-30 01:30:47+00:00,50,2639,False,True,False,False,False,False,1,13,3463,29,22,7,1,8,4.0,14.0,2025-08-28T23:46:22Z,pytorch
161747,closed,[dynamo] change error_on_graph_break/fullgraph semantics,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162027
* #162023
* __->__ #161747
* #161739

This PR implements the semantics change to `torch._dynamo.error_on_graph_break`:
- ~`torch.compile` now has a new `error_on_graph_break` kwarg that serves as a lower-priority toggle for erroring/continuing on graph breaks~
- `error_on_graph_break` is a new internal `torch.compile `setting that is lower-priority than `fullgraph`. It allows the user to toggle erroring/continuing on graph breaks.
- `error_on_graph_break` does nothing when `fullgraph=True`
- `error_on_graph_break` does NOT guarantee a single graph

Followup [DONE]: need to change the programming model docs to reflect the 3 graph break modes for compilation:
- `fullgraph=True`: enforce one graph, no graph breaks, cannot be toggled
- `fullgraph=False, error_on_graph_break=True`: errors on graph breaks, latter can be toggled during compile time
- `fullgraph=False, error_on_graph_break=False`: resumes tracing on graph breaks, latter can be toggled during compile time

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela @mlazos",2025-08-28 23:10:07+00:00,2025-09-04T17:11:24Z,,False,9,5,8,390,58,11,14,2025-09-04 17:10:20+00:00,56,1287,False,False,False,True,False,False,11,8,2015,148804,79389,69415,1,8,5.0,8.0,2025-08-29T05:44:25Z,pytorch
161746,open,[WIP] symtoken inputs,pianpwk,"Fixes #ISSUE_NUMBER


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 23:00:16+00:00,2025-09-02T22:33:53Z,,False,2,0,2,107,0,6,2,,21,215,False,True,False,False,False,False,6,1,12,111,109,2,1,2,1.0,1.0,2025-08-31T12:59:29Z,pytorch
161745,open,[DRAFT] Test Wind AMI,atalman,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 22:54:33+00:00,2025-08-30T23:00:41Z,,False,1,0,5,20,20,8,1,,21,192,False,True,False,False,False,False,8,0,0,40,20,20,1,5,,,,pytorch
161744,open,Rename propagate_tensor_meta to make private again,azahed98,"Rename the wrapper `propagate_tensor_meta` added in #161334 to make it clearly private, and rename the existing LRU function to accommodate.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-08-28 22:32:25+00:00,2025-09-15T23:21:02Z,,False,23,0,3,7,7,2,23,,50,243,False,False,False,False,False,False,2,22,5837,24,12,12,1,3,4.0,22.0,2025-08-29T15:59:57Z,pytorch
161742,open,Silence a warning by using the newer function,alanhdu,"Currently, this function will unconditionally log a warning with thanks to the code at https://github.com/pytorch/pytorch/blob/1190b7f73e9a94c9280d2baf196fddaa4c3a0374/torch/fx/_symbolic_trace.py#L49-L55

I just followed the instructions in the code and used `is_fx_symbolic_tracing` as the recommended guard here. This is technically a breaking change because it allows `torch.compile` for this function now, but maybe that's ok? I could change this to do an `or` of the two conditions if that is preferred.",2025-08-28 21:53:44+00:00,2025-08-29T00:35:58Z,,False,2,0,1,2,2,1,2,,45,508,False,False,False,False,False,False,1,0,0,4,2,2,1,1,,,,pytorch
161741,closed,[Symmetric memory] set handle type for ROCm,ngimel,"Fixes #161722 


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-28 21:45:55+00:00,2025-09-03T21:04:24Z,,False,14,0,1,1,0,1,14,2025-09-03 20:33:39+00:00,43,207,False,True,False,False,False,False,1,13,2841,1,1,0,1,1,5.0,13.0,2025-08-28T21:51:07Z,pytorch
161740,open,Add support for tensor stride load and save,nautsimon,"Summary:
We add a new path for MTIA tensors that will deserialize tensors with strides preserved. As MTIA Allocator is not exposed externally, we cannot simply let MTIA be handled by the typical CPU/CUDA path. To solve this, we add a variant of _rebuild_device_tensor_from_cpu_tensor, that tries to rebuild the tensor with as_strided to preserve strides from the original tensor.

Uses the tests written from the previous diff to verify strides can be saved and loaded on MTIA.

Reviewed By: egienvalue

Differential Revision: D78512965




cc @egienvalue",2025-08-28 21:41:35+00:00,2025-09-09T19:23:27Z,,False,2,2,1,24,2,2,4,,43,555,False,False,False,False,False,False,2,0,64,26,24,2,1,1,2.0,1.0,2025-09-08T21:10:24Z,pytorch
161739,closed,[dynamo] rename set_fullgraph to error_on_graph_break,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162027
* #162023
* #161747
* __->__ #161739

Renaming `set_fullgraph` to `error_on_graph_break` for now. There are no semantic differences yet. In a followup PR, we will introduce a new `torch.compile` option `error_on_graph_break` that has lower priority than `fullgraph` so that `fullgraph` really returns 1 graph.

I could keep `set_fullgraph` as a deprecated alias for `error_on_graph_break` for now, but I'm hoping that won't be necessary since it's still private API (there are no internal callsites yet, and there are no significant OSS callsites yet).

 cc @albanD @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos @guilhermeleobas @xmfan as primary users for `set_fullgraph`

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 21:24:21+00:00,2025-09-04T01:16:11Z,,False,7,2,4,1577,1571,57,9,2025-09-04 01:15:09+00:00,53,1060,False,False,False,False,False,False,57,6,2214,151368,80508,70860,1,4,6.0,6.0,2025-08-29T18:14:04Z,pytorch
161738,open,"[dynamo, wip] pre-register functions for set_fullgraph",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161738

Registering functions for Dynamo to do specific behaviors to them is tricky - I probably won't finish up this PR, but putting it up for reference.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 20:31:11+00:00,2025-08-28T23:46:27Z,,False,2,0,1,156,2,6,2,,54,412,False,False,False,False,False,False,6,0,0,158,156,2,1,1,,,,pytorch
161737,closed,fix tests caused by has_triton,dolpm,"Summary: this will only cause it in the event that we are serializing a triton hop. there are a few tests that do weird mocking stuff that this function doesn't like, so this will prevent it from being called there.

Test Plan:
att

Rollback Plan:

Differential Revision: D81261486


",2025-08-28 20:23:31+00:00,2025-08-29T02:26:42Z,,False,5,0,1,1,8,1,5,2025-08-29 02:25:39+00:00,30,284,False,True,False,False,False,False,1,1,476,9,1,8,1,1,2.0,1.0,2025-08-28T20:25:31Z,pytorch
161736,closed,[do not submit] ci checking for heuristics refactor,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161736



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @Lucaskabela",2025-08-28 20:20:29+00:00,2025-09-12T00:42:14Z,,False,4,0,4,5225,607,39,4,2025-09-12 00:42:14+00:00,51,410,False,False,False,False,False,True,39,3,4094,17541,7937,9604,1,4,2.0,3.0,2025-08-29T06:36:52Z,pytorch
161734,closed,[BC Breaking] Remove flex + njt code paths,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161734



cc @cpuhrsch @jbschlosser @bhosmer @soulitzer @davidberard98 @YuqingJ @Chillee @yanboliang @BoyuanFeng",2025-08-28 20:06:00+00:00,2025-09-16T00:15:01Z,,False,7,0,4,0,477,5,7,2025-09-16 00:13:59+00:00,42,198,False,False,False,False,False,False,5,6,2301,101899,65327,36572,1,4,3.0,7.0,2025-09-08T02:06:13Z,pytorch
161733,closed,[Flex] Fix float16 default config 128 headdim,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161734
* __->__ #161733



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 20:05:57+00:00,2025-08-28T20:12:00Z,,False,2,0,1,1,1,1,2,2025-08-28 20:12:00+00:00,45,307,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
161732,open,[scan] materialize combine_fn in forward add more autograd tests ,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162754
* __->__ #161732
* #162025
* #161808
* #161664
* #161557



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 20:02:28+00:00,2025-09-23T22:44:18Z,,False,1,2,11,136,72,5,3,,65,347,False,False,False,False,False,False,5,0,0,97972,61882,36090,1,11,2.0,0.0,2025-09-19T12:20:33Z,pytorch
161731,open,Update audio commit pin,shoumikhin,"Fixes https://github.com/pytorch/executorch/issues/13773
",2025-08-28 19:49:42+00:00,2025-08-29T00:01:48Z,,False,1,0,1,1,1,1,1,,23,57,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
161730,open,Resize to 0 if not going to be used,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

* __->__ #161730
*  #161667

```Py
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((2, 32, 1024), (32768, 1024, 1), torch.float32)
            buf1 = empty_strided_cuda((2, 32, 1024), (32768, 1024, 1), torch.float32)
            buf2 = empty_strided_cuda((2, 32, 1024, 64), (2097152, 65536, 64, 1), torch.float32)
            # Topologically Sorted Source Nodes: [flex_attention], Original ATen: []
            stream0 = get_raw_stream(0)
            triton_tem_fused_0.run(arg0_1, arg1_1, arg2_1, buf0, buf1, arg4_1, arg3_1, arg5_1, arg6_1, buf2, 8, 2, 32, stream=stream0)
            del arg0_1
            del arg1_1
            del arg2_1
            del arg3_1
            del arg4_1
            del arg5_1
            del arg6_1
            del buf0
            del buf1
        return (buf2, )
```

Vs

```Py
        with torch.cuda._DeviceGuard(0):
            torch.cuda.set_device(0)
            buf0 = empty_strided_cuda((2, 32, 1024), (32768, 1024, 1), torch.float32)
            buf1 = empty_strided_cuda((0, ), (1, ), torch.float32)
            buf2 = empty_strided_cuda((2, 32, 1024, 64), (2097152, 65536, 64, 1), torch.float32)
            # Topologically Sorted Source Nodes: [flex_attention], Original ATen: []
            stream0 = get_raw_stream(0)
            triton_tem_fused_0.run(arg0_1, arg1_1, arg2_1, buf0, buf1, arg4_1, arg3_1, arg5_1, arg6_1, buf2, 8, 2, 32, stream=stream0)
            del arg0_1
            del arg1_1
            del arg2_1
            del arg3_1
            del arg4_1
            del arg5_1
            del arg6_1
            del buf0
            del buf1
        return (buf2, )
```
<img width=""428"" height=""145"" alt=""Screenshot 2025-08-28 at 12 37 11 PM"" src=""https://github.com/user-attachments/assets/240a7bca-97e1-40c4-bf93-f075fdc1a40d"" />



cc @msaroufim @jerryzh168 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-08-28 19:35:25+00:00,2025-09-10T08:36:59Z,,False,9,6,17,89,17,5,15,,35,2202,False,False,False,False,False,False,5,8,2157,92715,55372,37343,1,17,6.0,8.0,2025-09-01T17:10:31Z,pytorch
161728,open,Improve benchmarks/dynamo:check_perf_csv output and failure summary,adabeyta,"Resolves https://github.com/pytorch/pytorch/issues/161290

## Summary

Expands `dynamo/check_perf_csv.py` output capabilities with latency, compile time and memory information:

- Display's measured speedup and display % from target
- Added clear messaging for all passing model tests when no regression is found
- Added error handling if csv file is missing

### Example (Failing Check)

```bash
python benchmarks/dynamo/check_perf_csv.py -f reports-dir/inductor_training_smoketest.csv -t 1.40
```

**Example Output:**
```
Checking inductor_training_smoketest.csv (speedup threshold >= 1.40x)
hf_Bert                            speedup=1.005x, latency=390.8 ms/iter, compile=1.526s, mem_ratio=1.02x (eager=360.6 GB, dynamo=369.3 GB)
Error 1 model(s) performance regressed
    hf_Bert
  - hf_Bert: 1.005x (< 1.40x; -28.2% from target)
```

### Example (Passing Check)

```bash
python benchmarks/dynamo/check_perf_csv.py -f reports-dir/inductor_training_smoketest.csv -t 1.40
```

**Example Output:**
```
Checking inductor_training_smoketest.csv (speedup threshold >= 1.00x)
hf_Bert                            speedup=1.005x, latency=390.8 ms/iter, compile=1.526s, mem_ratio=1.02x (eager=360.6 GB, dynamo=369.3 GB)
All 1 model(s) passed threshold check (>= 1.00x)
```


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 19:09:23+00:00,2025-09-18T14:34:18Z,,False,3,0,2,41,8,1,3,,67,1439,False,False,False,False,True,False,1,1,42,18743,12726,6017,1,2,1.0,1.0,2025-09-03T19:02:26Z,pytorch
161724,open,Fix cdist export compute mode validation,ahkush,"Fixes #161089. Added '0' as the acceptable value for compute mode in _meta_registrations.py. Also, added a test case in test_export.py file.
",2025-08-28 18:33:53+00:00,2025-09-19T19:11:56Z,,False,7,0,3,24,2,2,7,,40,141,False,True,False,False,False,False,2,6,1482,44,33,11,2,3,4.0,8.0,2025-09-04T15:12:47Z,pytorch
161723,closed,expose number of outputs in native runtime for unified runtime,JacobSzwejbka,"This is only user outputs which is what we want. Spoke to @zhxchen17 though and it seems like nativeRT might have some bugs on propogating updates to things like input mutation or buffer mutation though. Something to take a look at in a follow up. 

Also I have no idea where the nativeRT tests are. Any pointers @zhxchen17  @SherlockNoMad ",2025-08-28 18:24:39+00:00,2025-09-04T01:21:36Z,,False,9,0,4,7,0,2,9,2025-09-04 01:20:34+00:00,62,340,False,True,False,False,False,False,2,8,2382,59129,34443,24686,1,4,3.0,8.0,2025-09-03T14:52:00Z,pytorch
161721,closed,[fx] Add lru_cache to warning,angelayi,"Summary: Added lru_cache to the warning message to avoid flooding logs

Test Plan:
CI

Rollback Plan:

Differential Revision: D81245618




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-28 17:28:56+00:00,2025-08-29T02:27:32Z,,False,5,0,1,7,1,1,5,2025-08-29 02:25:47+00:00,29,196,False,False,False,False,False,False,1,1,476,8,7,1,1,1,2.0,1.0,2025-08-28T17:29:46Z,pytorch
161718,open,test win ami,atalman,"Fixes #ISSUE_NUMBER
",2025-08-28 16:09:51+00:00,2025-08-29T17:32:52Z,,False,1,0,1,1,0,1,1,,12,20,False,True,False,False,False,False,1,0,0,1,1,0,1,1,,,,pytorch
161717,closed,move `_grouped_mm` fallback to composite explicit autograd,vkuzo,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162159
* #162059
* __->__ #161717
* #161407

Summary:

Moves the `torch._grouped_mm` fallback from cuda-only code to a place
where it can be used by multiple backends. Specifically:
1. make the fallback path and util functions reusable and move them to
   `ATen/native/GroupedMMUtils.h`
2. register a backend-agnostic kernel to composite explicit autograd key
3. refactor the grouped_mm tests to their own test case and enable CPU

At the end of this PR, here is the support matrix:
* CUDA SM90+: fast path with test coverage (no change)
* CUDA SM80+: fallback with test coverage (no change)
* CPU: fallback works, but without test coverage (new in this PR)
* other SM versions and other backends: will probably already work, but
  let's leave this to future PRs
* float32/float16: will probably already work, but let's leave this to
  future PRs

Test Plan:

```bash
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm -x
```

Reviewers:

Subscribers:

Tasks:

Tags:",2025-08-28 15:04:03+00:00,2025-09-04T17:48:58Z,,False,3,17,6,189,138,5,20,2025-09-04 17:48:56+00:00,58,1050,False,False,False,False,False,True,5,1,48,73358,43585,29773,1,6,5.0,1.0,2025-08-28T15:06:51Z,pytorch
161716,closed,[Lowering] Fix the edge case of empty subgraph split due to dataclass node,yuhuishi-convect,"Summary: Fix the edge case by allowing `call_function` nodes with no deps as graph entry (starter_nodes) in the splitter.

Test Plan:
The test shall pass in the current diff (after fix), and fail in the parent diff (before fix)

```
buck test mode/opt //glow/fb/fx/lowering:split_tests -- test_dataclass_as_graph_entry
```

Rollback Plan:

Differential Revision: D81232435




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-28 14:29:59+00:00,2025-09-10T21:24:50Z,,False,27,0,1,84,0,2,27,2025-09-10 21:23:45+00:00,74,433,False,True,False,False,False,False,2,12,3224,84,84,0,1,1,4.0,14.0,2025-08-31T13:09:33Z,pytorch
161715,closed,[ROCm] Enabling several UTs,pragupta,"All these UTs are working as is, just removing the skip
- test_p2p_ipc
- test_repros.py: working, added fp8 support
- test_activation_checkpointing.py
- test_content_store.py
- test_cuda_multigpu.py
- test_compute_comm_reordering.py
- test_segment_reductions.py
- test_dataloader.py
- test_math_ops.py
- test_loop_ordering.py
- test_control_flow.py
- distributed_test.py
- test_mem_tracker.py
- test_fsdp_optim_state.py
- test_fully_shard_mixed_precision.py: skippped for < ROCm7.0
- test_aot_inductor_custom_ops.py
- test_c10d_ops_nccl.py
- test_eager_transforms.py
- test_sparse_csr.py
- test_inductor_collectives.py
- test_fake_tensor.py
- test_cupy_as_tensor.py
- test_cuda.py: enable UTs that are working
- test_matmul_cuda.py: enable UTs that are working

Fixes #ISSUE_NUMBER


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-28 14:24:00+00:00,2025-09-09T15:50:30Z,,False,17,0,5,27,80,25,17,2025-09-09 15:49:26+00:00,27,1201,False,True,False,False,False,False,25,14,3427,18932,11637,7295,2,5,5.0,14.0,2025-08-28T17:06:24Z,pytorch
161712,closed,[MPS] Migrate round unary op to Metal,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161712

And actually use the right function, as [`torch.round`](https://docs.pytorch.org/docs/stable/generated/torch.round.html) doesn't use `std::round`, but rather `std::rint`, which can be easily seen by running something like
```python
import torch
print(torch.arange(-3., 3., step=.5, device='mps').round())
print(torch.arange(-3., 3., step=.5, device='mps').cpu().round())
```

Before this change it printed
```
tensor([-3., -3., -2., -2., -1., -1.,  0.,  1.,  1.,  2.,  2.,  3.], device='mps:0')
tensor([-3., -2., -2., -2., -1., -0.,  0.,  0.,  1.,  2.,  2.,  2.])
```
But after this change results match

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 13:16:37+00:00,2025-08-28T16:46:14Z,,False,3,0,2,21,7,5,3,2025-08-28 16:45:09+00:00,37,900,False,False,False,True,False,False,5,2,788,32,23,9,1,2,3.0,2.0,2025-08-28T14:47:24Z,pytorch
161711,open,[OpenReg] Adding README.md for openreg runtime,can-gaa-hou,"cc @FFFrog 
",2025-08-28 12:45:06+00:00,2025-08-28T12:59:27Z,,False,1,0,1,39,0,1,1,,46,12,False,False,False,False,False,False,1,0,0,39,39,0,1,1,,,,pytorch
161710,open,Update the heuristic for AArch64 bgemm,karmeh01,"Updates heuristic for bgmm. Based on [these changes](https://github.com/pytorch/pytorch/compare/main...Mousius:pytorch:bgemm?expand=1)

```
import torch
import time

# Set below to True to use cases selected by only one of the hueristics.
USE_ONLY_DIVERGENT_TEST_CASES = True   
BATCH_SIZES  = [ 1, 8, 32 ]
M_DIMS       = [ 4, 8, 16, 32, 64, 256, 512, 1024, 2048 ]
N_DIMS       = [ 4, 8, 16, 32, 64, 256, 512, 1024, 2048 ]
K_DIMS       = [ 4, 8, 16, 32, 64, 256, 512, 1024, 2048 ]
ITERS        = 40
DTYPE        = torch.bfloat16

def old_heuristic(m, n, k):
    is_above_min_dims = m > 8 and n > 8 and k > 8
    is_above_min_size = m * n * k <= 16 * 16 * 16
    return is_above_min_dims and is_above_min_size

def new_heuristic(m, n, k):
    is_above_min_dims = m >= 8 and n >= 8 and k >= 8
    heuristic = m <= 8 or k <= 8 or n >= 1024
    return heuristic and is_above_min_dims

def generate_test_cases():
   test_cases = []
   for b in BATCH_SIZES:
       for m in M_DIMS:
           for n in N_DIMS:
                   for k in K_DIMS:
                       if USE_ONLY_DIVERGENT_TEST_CASES:
                           if old_heuristic(m, n, k) != new_heuristic(m, n, k):
                               test_cases.append([b, m, n, k])
                       else:
                           test_cases.append([b, m, n, k])
   return test_cases

def test(x, y):
   for _ in range(5):
       torch.bmm(x, y)
   perf = 0.0
   for _ in range(ITERS):
       start = time.time()
       torch.bmm(x, y)
       end = time.time()
       perf += (end - start) / ITERS
   return perf

def main():
    device = torch.device(""cpu"")

    print(f""{'b':<10}{'m':<10}{'n':<10}{'k':<10}{'time (s)':10}"")
    cumulative_mean_time = 0.0
    last_b = None

    for b, m, n, k in generate_test_cases():
        x = torch.randn((b, m, n), dtype=DTYPE, device=device)
        y = torch.randn((b, n, k), dtype=DTYPE, device=device)
        mean_time = test(x, y)
        cumulative_mean_time += mean_time
        print(f""{b:<10}{m:<10}{n:<10}{k:<10}{mean_time:10.3e}"")
    print(f""Cumulative mean time = {cumulative_mean_time:.4f} s"")


if __name__ == ""__main__"":
   main()
```

From the script we see that cumulative mean time from all test cases (at 16 threads) is:

2.0007 s for the old heuristic
0.7480 s for the new heuristic

@aditew01 

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168",2025-08-28 11:46:27+00:00,2025-09-22T15:21:10Z,,False,2,5,5,44,11,4,7,,38,2405,False,False,False,False,False,False,4,0,0,65,49,16,1,5,3.0,0.0,2025-09-12T10:29:11Z,pytorch
161709,open,Migrate 6 UT test suites for Intel GPU,shangerxin,"# Description
Fixes #114850, we will port dynamo, fsdp tests to Intel GPU
We could enable Intel GPU with following methods and try the best to keep the original code styles:

# Changes
1. Get device type with from accelerator and get_devtype helper method
2. Replace the requires cuda statement with requires_gpu.
3. Replace the cuda() with to(device_type).
4. Add is_xpu check into the test logic. 


# Notify
cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mingfeima @sanchitintel @ashokei @jingxu10 @jerryzh168 @ipiszy @muchulee8 @aakhundov @coconutruben @gujinghui @fengyuan14 @guangyey",2025-08-28 11:35:32+00:00,2025-09-25T08:36:31Z,,False,2,10,5,254,231,6,12,,38,813,False,True,False,False,False,False,6,0,0,521,272,249,1,5,3.0,0.0,2025-08-28T13:05:22Z,pytorch
161708,closed,[CI] Migrate XPU build and test to python 3.10,chuanqi129,Follow #161167,2025-08-28 10:21:58+00:00,2025-08-30T05:45:56Z,,False,11,0,1,14,14,2,11,2025-08-29 22:31:43+00:00,46,14,False,False,False,False,False,False,2,10,3067,28,14,14,1,1,3.0,10.0,2025-08-28T17:25:09Z,pytorch
161707,open,[indexing] Prevent integer overflow from large step values in C++,thenumberouscode,"Fixes https://github.com/pytorch/pytorch/issues/160868
hmmm, I found an existing fix PR after I've finished this one. For reference, the old PR was
https://github.com/pytorch/pytorch/pull/147433/files. 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos @leslie-fang-intel ",2025-08-28 10:17:22+00:00,2025-09-25T02:16:16Z,,False,21,0,1,37,1,2,21,,65,434,False,True,False,False,False,False,2,18,3424,38,37,1,1,1,6.0,18.0,2025-08-28T11:47:35Z,pytorch
161706,closed,Update optional tag for `interpolation` in `torch.quantile()`,ILCSFNO,"Fixes #146156

Refix the issue with the extra needed fix.
",2025-08-28 09:35:40+00:00,2025-09-05T09:53:14Z,,False,4,0,7,1,1,1,4,2025-08-29 16:21:17+00:00,61,58,False,True,False,False,False,False,1,3,506,776180,511340,264840,2,7,3.0,3.0,2025-08-29T13:29:06Z,pytorch
161704,open,[WIP] add util for computing local shape and global offset from DTensorSpec with extra tensor_sharding field,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161775
* __->__ #161704
* #161770

**Test**
`pytest test/distributed/checkpoint/fsdp/test_fsdp_dsd.py -s -k fsdp2`

seems problematic:
<img width=""2236"" height=""55"" alt=""image"" src=""https://github.com/user-attachments/assets/8fe28629-24f0-4eb2-9b7d-40af3739b3b6"" />


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-28 08:56:28+00:00,2025-09-04T17:13:24Z,,False,3,1,3,58,6,1,4,,108,423,False,False,False,False,False,False,1,1,165,3519,2645,874,1,3,1.0,1.0,2025-09-04T17:00:45Z,pytorch
161703,open,port some distributed tensor test files for Intel GPU,wincent8,"it's another pr to port distributed tensor test for Intel GPU, while the other pr is https://github.com/pytorch/pytorch/pull/161604
We could enable Intel GPU with following methods and try the best to keep the original code styles:

Use torch.accelerator for general gpu
Skip the case if running on xpu which has known issues


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @tianyu-l @XilunWu @SherlockNoMad",2025-08-28 08:47:35+00:00,2025-09-18T08:36:01Z,,False,23,13,24,36,31,6,36,,53,463,False,False,False,False,False,False,6,17,4692,295,150,145,1,24,6.0,20.0,2025-09-02T09:13:57Z,pytorch
161700,closed,[ROCm] OffsetCalc Unroll Optimization,amd-hhashemi,"Our compiler is generating inefficient code for the offsetCalc in certain situations.
The root-cause for this needs to be identified. For now specialized unrolling based on 'dims' notably helps perf.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-28 08:05:47+00:00,2025-09-09T16:12:54Z,,False,9,3,3,18,0,1,12,2025-09-09 16:11:51+00:00,37,318,False,False,False,False,False,False,1,8,1411,34,26,8,1,3,5.0,8.0,2025-08-28T16:51:57Z,pytorch
161699,closed,update supported OS for Intel client GPU,ZhaoqiongZ,"update supported OS for Intel client GPU
",2025-08-28 07:43:25+00:00,2025-09-01T05:46:14Z,,False,6,0,1,1,6,1,6,2025-09-01 05:45:11+00:00,40,41,False,False,False,False,False,False,1,5,693,7,1,6,1,1,4.0,5.0,2025-09-01T00:53:57Z,pytorch
161695,closed,Port OpSchema.__post_init__ and OpSchema._recompute_comparison_key to C++,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163030
* #162990
* #162968
* #162508
* __->__ #161695

I initially didn't see good results porting this, but it was apparently because of pybind11 function calling overhead. (pybind11's object-handling primitives seem fine enough.) I'm interested in setting up nanobind, but this demonstrates it's not blocking.

Differential Revision: [D81530102](https://our.internmc.facebook.com/intern/diff/D81530102)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-08-28 06:59:34+00:00,2025-09-19T04:08:38Z,,False,8,19,14,369,32,4,27,2025-09-19 04:07:33+00:00,73,586,False,False,False,False,False,False,4,7,1819,117999,75366,42633,1,14,3.0,9.0,2025-09-02T21:46:09Z,pytorch
161694,closed,Remove unnecessary asserts in _correct_storage_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* #162219
* #162218
* __->__ #161694
* #161693
* #161692
* #161634
* #161633
* #161596
* #161595
* #161591
* #161590
* #161586
* #161466

These don't seem necessary to get reasonable errors:
- func was already passed to get_alias_info earlier in the calling function
- all we require for args and outs is the ability to index into them, which will cleanly throw if it fails anyway

Differential Revision: [D81530108](https://our.internmc.facebook.com/intern/diff/D81530108)",2025-08-28 06:59:30+00:00,2025-09-05T19:46:31Z,,False,4,3,4,30,29,1,7,2025-09-05 19:46:31+00:00,55,559,False,False,False,False,False,False,1,2,245,52239,29599,22640,1,4,2.0,2.0,2025-08-29T19:58:58Z,pytorch
161693,closed,Add mark_subclass_constructor_exportable_experimental-specific cache for is_traceable_wrapper_subclass_type,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162508
* #161695
* #162337
* #162336
* #162298
* __->__ #161693
* #161596
* #162218
* #162220
* #162219
* #161692
* #161634
* #161633
* #161595
* #161591

See new code comment for explanation. In short, global cache seems wrong because of monkey patching but this use case is checking for a programming error IIUC.

Differential Revision: [D81530104](https://our.internmc.facebook.com/intern/diff/D81530104)",2025-08-28 06:59:27+00:00,2025-09-11T23:51:03Z,,False,3,0,6,18,1,1,3,2025-09-11 23:51:03+00:00,107,486,False,False,False,False,False,False,1,2,272,82215,48511,33704,1,6,1.0,2.0,2025-09-02T21:45:58Z,pytorch
161692,closed,"Call checkLong in is_int_or_symint, completing TODO",swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* #162219
* __->__ #161692
* #161634
* #161633
* #161595
* #161591

Calling this first minimizes overhead for plain old ints, making cheap things cheap.

Differential Revision: [D81530098](https://our.internmc.facebook.com/intern/diff/D81530098)",2025-08-28 06:59:23+00:00,2025-09-09T01:10:11Z,,False,4,0,6,5,1,1,4,2025-09-09 01:10:11+00:00,51,331,False,False,False,False,False,False,1,3,262,82071,48418,33653,1,6,4.0,3.0,2025-09-02T21:45:49Z,pytorch
161690,closed,[Intel GPU] Fix XPU SDPA default priority_order UT fail,LuFinch,"Fixes #161483

When the whole `test/test_transformers.py` file is run, the case `test_default_priority_order` can pass because other xpu cases would call SDPA so that the priority order is set by https://github.com/pytorch/pytorch/blob/eec876deb659fe667aac2d97a48d7451c3e88dee/aten/src/ATen/native/mkldnn/xpu/Attention.cpp#L98-L112

However, when the case `test_default_priority_order` is run separately, the priority order is unset so that this case would fail. This PR fix this case.",2025-08-28 06:23:15+00:00,2025-09-03T04:44:34Z,,False,5,0,1,6,0,1,5,2025-09-03 04:43:30+00:00,55,485,False,True,False,False,False,False,1,4,567,6,6,0,1,1,4.0,5.0,2025-08-28T06:24:48Z,pytorch
161689,open,[WIP] verify slice clamp correctness,pianpwk,"Differential Revision: D81204584




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 06:00:06+00:00,2025-08-28T10:21:28Z,,False,3,0,1,4,4,1,3,,36,238,False,False,False,False,False,False,1,0,0,8,4,4,1,1,,,,pytorch
161688,closed,export: add explicit decomposition for aten.expand_copy and unit test,albertw7711,"Fixes #161080 
torch.export.export fails with TypeError: expand() got an unexpected keyword argument 'implicit' when calling torch.expand_copy(..., implicit=True). This happened because expand_copy = _make_copy_from_view(aten.expand) register aten. expand as the decomposition path for aten.expand_copy, which doesn’t accept the implicit argument.

I have added an explicit a decomposition for aten.expand_copy in torch/_decomp/decompositions.py to ignore the implicit argument, and a simple unit test to demonstrate the bug being fixed.",2025-08-28 05:34:53+00:00,2025-09-04T21:45:18Z,,False,10,8,8,17,1,2,18,2025-09-04 18:17:00+00:00,69,537,False,True,False,False,False,False,2,8,1202,68142,39996,28146,1,8,4.0,9.0,2025-08-28T05:51:42Z,pytorch
161687,closed,[ROCm] revamp miopen integration,ethanwee1,"Update sources under ATen/miopen and ATen/native/miopen to align with best practices. Avoid reshape_ calls inside backward operations.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @albanD",2025-08-28 05:28:23+00:00,2025-09-03T22:29:14Z,,False,9,0,9,1163,769,8,9,2025-09-03 22:28:11+00:00,32,339,False,False,False,False,False,False,8,6,1581,29415,21745,7670,1,9,4.0,6.0,2025-08-28T15:50:20Z,pytorch
161686,closed,Suppress dllexport warning on XPU windows build,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161686
# Motivation
Refer to https://github.com/pytorch/pytorch/pull/153986, suppressing the dllexport warning on XPU Windows builds, which will reduce the number of lines in the build log.
```bash
2025-08-27T16:24:34.6542666Z C:\actions-runner\_work\pytorch\pytorch\pytorch\c10\util\Exception.h(475,8): warning: 'dllimport' attribute ignored on inline function [-Wignored-attributes]
2025-08-27T16:24:34.6544084Z   475 | inline C10_API const char* torchCheckMsgImpl(const char* msg) {
```
",2025-08-28 04:14:54+00:00,2025-08-28T04:56:45Z,,False,1,0,1,4,0,1,1,2025-08-28 04:56:45+00:00,47,576,False,False,False,False,False,False,1,0,0,4,4,0,1,1,,,,pytorch
161685,closed,removed duplicate imports,rohit-kumar-manav,"Fixes #161684 


cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-28 03:58:51+00:00,2025-09-03T09:46:26Z,,False,3,0,1,0,3,2,3,2025-08-31 16:21:52+00:00,25,73,False,True,False,False,False,False,2,2,493,3,0,3,1,1,3.0,2.0,2025-08-28T04:15:24Z,pytorch
161683,open,[APS] Implement __torch_function__ for KeyedTensor,jd7-tr,"Summary:
1.  There are a bunch of `torch.ops.aten` operations that can't handle `KeyedTensor`: The error was occurring because these ops expects a regular `Tensor` but was receiving a `KeyedTensor` object.

2.  Implement `__torch_function__` for `KeyedTensor`, so when these incompatible operations are called with a `KeyedTensor`, the `__torch_function__` method automatically delegates the op to the underlying values tensor from the `KeyedTensor` and returns a new `KeyedTensor` with updated values.

Test Plan:
```
buck2 run mode/opt fbcode//aps_models/ads/gmp:launcher_with_publish mode=mtml_mobile_cvr_model/managed/Y2025Q2/local_mode_mtml_mobile_cvr_model_733415799_v0_fork +training.ir_serializer=manifold
```

MAST job: https://fburl.com/mlhub/pp937uxf

Rollback Plan:

Differential Revision: D81047278


",2025-08-28 03:54:03+00:00,2025-08-28T21:20:06Z,,False,12,0,1,9,3,1,12,,50,814,False,False,False,False,False,False,1,1,42,12,9,3,1,1,2.0,1.0,2025-08-28T05:11:11Z,pytorch
161681,open,add workflow to dispatch,yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161681
* #161585
* #161565

",2025-08-28 02:17:57+00:00,2025-08-28T04:21:39Z,,False,1,0,2,71,37,8,1,,24,114,False,False,False,False,False,False,8,0,0,108,71,37,1,2,,,,pytorch
161680,closed,[WOQ] Integrate CUDA support for int8pack_mm woq optimization pattern,bbeckca,"Summary:
What: Enables CUDA support for int8_mm woq optimization pattern by:

- Fixing dtype conversion in weight_int8pack_mm_kernel to match CPU
- Updating pattern validation to accept CUDA devices
- Adding test coverage for CUDA

Why: Extend WOQ to more device types

Test Plan:
```
buck2 run 'fbcode//mode/opt' //caffe2/test/inductor:cuda_select_algorithm
```

Rollback Plan:

Reviewed By: jerryzh168

Differential Revision: D80882442




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 01:55:03+00:00,2025-09-17T10:25:23Z,,False,22,0,1,167,16,3,22,2025-09-17 10:24:18+00:00,69,643,False,True,False,False,False,False,3,6,1608,183,167,16,1,1,3.0,6.0,2025-08-29T20:42:37Z,pytorch
161679,closed,[PT2] Add fastResizeToZero to all static dispatch kernels,kqfu,"Summary:
Add fastResizeToZero whenever we are reusing output tensors. Otherwise it keeps throwing warning
```
Warning: An output with one or more elements was resized since it had shape [10], which does not match the required output shape [181]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (function _resize_output_check)
```

Test Plan:
Run local replayer.

```
MODEL_TYPE=ads_mtml_offsite_cvr_oba_optout_dedicated_model
MODEL_ENTITY_ID=786096203
SNAPSHOT_ID=11

HARDWARE_TYPE=1 ./sigrid/predictor/scripts/start_gpu_with_gif.sh ${MODEL_ENTITY_ID}_${SNAPSHOT_ID} /data/users/$USER/models/${MODEL_ENTITY_ID}/${SNAPSHOT_ID} 3443 2>&1 | tee ~/logs/${MODEL_TYPE}/predictor_${MODEL_ENTITY_ID}_${SNAPSHOT_ID}

sigrid/predictor/scripts/start_gpu_replayer_localhost_with_gif.sh ${MODEL_ENTITY_ID}_${SNAPSHOT_ID} 1000 ${MODEL_TYPE} /data/users/$USER/requests/filter_requests_ads_mtml_offsite_cvr_oba_optout_dedicated_model_100 localhost /data/users/$USER/models/${MODEL_ENTITY_ID}/${SNAPSHOT_ID} false 3443 false 2>&1 | tee ~/logs/${MODEL_TYPE}/replayer_${MODEL_ENTITY_ID}_${SNAPSHOT_ID}
```

Before: P1921177565

After: P1921178087

Rollback Plan:

Differential Revision: D81177596
",2025-08-28 01:50:20+00:00,2025-08-28T20:20:56Z,,False,5,0,1,4,0,1,5,2025-08-28 19:58:42+00:00,57,1353,False,False,False,False,False,False,1,2,518,4,4,0,1,1,3.0,2.0,2025-08-28T02:15:41Z,pytorch
161678,closed,Add ciflow/vllm to vLLM commit hash update PR(s),huydhn,"As it should be, otherwise, PR(s) like https://github.com/pytorch/pytorch/pull/161121 were merged without the signals it needed.",2025-08-28 01:05:12+00:00,2025-09-19T18:46:49Z,,False,9,0,1,3,0,1,9,2025-09-19 18:46:49+00:00,48,128,False,False,False,False,False,False,1,7,1448,3,3,0,1,1,4.0,7.0,2025-08-28T01:18:12Z,pytorch
161677,closed,[SymmMEM] Fix test_empty_strided_p2p_persistent,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161677
* #161676

test_empty_strided_p2p_persistent allocates persistent symm memory tensors. However, it uses the same alloc_id for different tests, which could cause troubles if these tests are ran under the same process. This PR fixes the issue by using a different alloc_id for different test.

https://github.com/pytorch/pytorch/pull/161668 should also fix the issue but we can land this PR for a safer test.

cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-28 01:03:35+00:00,2025-08-29T16:13:06Z,,False,3,1,3,5,3,1,4,2025-08-29 16:12:01+00:00,47,570,False,True,False,False,False,False,1,2,493,29197,19182,10015,1,3,3.0,2.0,2025-08-28T05:13:29Z,pytorch
161676,closed,[BE][SymmMEM] Change Optional to the shorthand expression for symmetric memory modules,fegin,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161677
* __->__ #161676



cc @H-Huang @awgu @wanchaol @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-28 01:03:30+00:00,2025-08-29T07:32:23Z,,False,3,0,3,41,40,1,3,2025-08-29 07:31:19+00:00,86,175,False,False,False,False,False,False,1,2,493,29255,19210,10045,1,3,3.0,2.0,2025-08-28T01:08:08Z,pytorch
161675,closed,[inductor] performance model interface,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161675
* #161674
* #161673
* #158091
* #157699
* #161469
* #161468
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- enable the system to register performance model ranking functions
- those functions are then used to rank th choices before benchmarking
- enable more modular, faster, more expansive autotuning

# what

- PerformanceModelChoices, expansion of the lookup table
- kicks in when the lookup table has a lookup miss
- mechanism
  - users can register functions (through decorators or registration
    function)
  - functions are hardware, template, op specific
  - when an input hits the performance model, if we have a function
    registered for that template/op combo
    - we expand the config space for that template
    - we ask the function to estimate the performance all the choices
    - we filter the choices down to topk before returning them
- control
  - topk: -1 to say ""same as default search space"" or > 1 for specific
  - topk: 0 turns off any performance modeling
  - discard unranked: controls what to do with choices that have not
    been estimated (discard, or keep)

You can check out the testing code to see examples of toy functions and
their integration into the system

This choice handler is not turned on by default

Even if the choice handler is turned on, it will be a noop if no
functions are registered, or if topk = 0

# testing

```
python -m pytest test/inductor/test_performance_model_interface_e2e.py
python -m pytest test/inductor/test_performance_model_interface.py
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-28 00:51:45+00:00,2025-09-18T17:06:11Z,,False,1,0,4,5,4,1,1,2025-09-18 17:06:11+00:00,38,1907,False,False,False,False,False,False,1,0,0,78599,48435,30164,1,4,,,,pytorch
161674,open,[inductor] add lookup table recorder,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161675
* __->__ #161674
* #161673
* #158091
* #157699
* #161469
* #161468
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- enable users to record a correct lookup table themselves, including
  hashes

# what

- lookup table recorder system with single entry emission and full table
  dumping
- default single entry emission into logs
- default full table emission into output directory if configured

# testing

```bash
~/scripts/validate_recorder.sh
cat /tmp/tmp.UGNPucU0BG/inductor_lut_20250827_002056_529.json
```
```
{
    ""NVIDIA H100+((torch.float16, [128, 256], [0, 1]), (torch.float16, [128, 64], [64, 1]), (torch.float16, [64, 256], [256, 1]))+alpha=1&beta=1+addmm+tf32=False"": [
        {
            ""template_id"": ""triton::mm"",
            ""EVEN_K"": true,
            ""ALLOW_TF32"": false,
            ""USE_FAST_ACCUM"": false,
            ""ACC_TYPE"": ""tl.float32"",
            ""num_stages"": 2,
            ""num_warps"": 4,
            ""BLOCK_M"": 32,
            ""BLOCK_N"": 32,
            ""BLOCK_K"": 64,
            ""hint_override"": null,
            ""GROUP_M"": 8,
            ""template_hash"": ""0717af5834e39dcca7ea817f896b8d85b4886422da7a3ab5f6911b4cfe568896""
        },
      ...
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-28 00:51:41+00:00,2025-09-18T17:09:39Z,,False,1,0,3,920,0,5,1,,36,1574,False,False,False,False,False,False,5,0,0,79301,49250,30051,1,3,,,,pytorch
161673,closed,[inductor][ez] ChoiceCaller keeps a reference to KernelTemplateChoice,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161675
* #161674
* __->__ #161673
* #158091
* #157699
* #161469
* #161468
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- enables part of the code that have lost the information on how a
  choicecaller was created to know it
- enables recording the components for a ChoiceCaller after benchmarking
  to know how to recreate the (best) choice

# what

- add set/get to ChoiceCaller to keep track the KernelTemplateChoice
  it came from
- record it when generating a ChoiceCaller through a
  KernelTemplateChoice

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-28 00:51:37+00:00,2025-09-12T00:42:13Z,,False,1,0,3,16,0,2,1,2025-09-12 00:42:13+00:00,69,970,False,False,False,False,False,False,2,0,0,77991,48143,29848,1,3,,,,pytorch
161672,closed,[inductor] don't append None to choices,yushangdi,"Summary: don't append None as a choice to choices in autotune

Test Plan: See internal Diff

Differential Revision: D81188644




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-28 00:47:39+00:00,2025-08-28T18:49:57Z,,False,3,0,1,3,1,1,3,2025-08-28 18:48:53+00:00,39,331,False,False,False,False,False,False,1,1,476,4,3,1,1,1,2.0,1.0,2025-08-28T00:48:33Z,pytorch
161670,closed,[dynamo] Graph break on on user-defined class in compiled region,rtimpe,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161670

Currently, user-defined classes inside of a compiled frame will cause the whole
frame to be skipped by dynamo.  This change defers the Unsupported exception
until the __build_class__ builtin is actually called, which allows a graph break
to be inserted.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-28 00:32:14+00:00,2025-09-10T04:40:25Z,,False,21,4,16,24,15,373,25,2025-09-10 04:39:22+00:00,64,519,False,False,False,False,False,False,373,20,4173,105140,65691,39449,1,16,6.0,20.0,2025-08-28T04:29:47Z,pytorch
161669,open,[WIP] user-specified symints,pianpwk,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 23:52:12+00:00,2025-08-28T05:02:10Z,,False,2,0,1,97,7,3,2,,28,192,False,True,False,False,False,False,3,0,0,104,97,7,1,1,,,,pytorch
161668,closed,[SymmMem] Isolate set_device tests to avoid hang,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161668

`test_symmetric_memory.py` hangs like this:
```
SymmetricMemoryTest::test_empty_strided_p2p_persistent_set_device_False PASSED [5.6364s]                                                                                                                                                 
SymmetricMemoryTest::test_empty_strided_p2p_persistent_set_device_True ...
```

This set of tests parameterizes whether user sets the device before calling `symm_mem.emtpy`.
However, such parametrization does not work well with `MultiProcContinuousTest` because the set device will ""contaminate"" the next test function.

Solution is to move the ""set device"" tests to a separate test suite using the traditional `MultiProcessTestCase`, which would respawn processes every time. 

Hang is gone now.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-27 23:16:01+00:00,2025-08-28T05:44:57Z,,False,3,0,1,119,83,1,3,2025-08-28 05:43:52+00:00,48,950,False,False,False,False,False,False,1,2,493,202,119,83,1,1,3.0,2.0,2025-08-28T00:57:40Z,pytorch
161667,closed,Add return-max-scores to flex-attention,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161730
* #162295
* __->__ #161667

# Summary


### Update

API

```Py
class AuxRequest(NamedTuple):
    """"""Request which auxiliary outputs to compute from flex_attention.

    Each field is a boolean indicating whether that auxiliary output should be computed.
    """"""

    lse: bool = False
    max_scores: bool = False


class AuxOutput(NamedTuple):
    """"""Auxiliary outputs from flex_attention operation.

    Fields will be None if not requested, or contain the tensor if requested.
    """"""

    lse: Optional[Tensor] = None
    max_scores: Optional[Tensor] = None
    

  out_only = flex_attention(query, key, value, score_mod)
  out_max, aux_max = flex_attention(
      query,
      key,
      value,
      score_mod,
      return_aux=FlexAttentionAuxRequest(max_scores=True),
  )
  out_both, aux_both = flex_attention(
      query,
      key,
      value,
      score_mod,
      return_aux=FlexAttentionAuxRequest(lse=True, max_scores=True),
        )
```


Returns the max post mod scores from flex attention. 

Not being able to break BC is kinda of annoying here since we end up with a combinatorial problem where if we need to add any more return vals we need to new kwargs that gate if they get returned by the function and need to support the 2**N additional args possible return groups.


Ideally there isn't much more we need to return, but we might want to think about how best to set this up for expansion in the future. I added kwarg only now

Maybe we make a `ExtraReturns` type kwarg that can grow and we don't need to keep adding new top level args.

We could also return a Struct that holds all the extra tensors and start deprecation cycle for logsumexp eventually returning just 1 `ExtraReturns` like struct with the tensors.


### Req Grad
I currently dont return a max_scores that supports backproping grads. I think this might be feasible  but since max is essentially 1 hot 	on the inputs and a reduction we would either need to save another `max_location` from the forward or find the max_score but also only apply to first occurence if there is multiple equivalent scores (need to check if thats we define for vanilla max op in torch). 

For now no grad, we can re-visit if needed. 

## Perf
I am going to disable for flex_decode. Since at least initially the motivation is for training. I also more hard than it should be to have ops return nuns or optional tensors, If return max is at the false, we should probably just create a tensor of size zero so that we don't slow down the hot path.  

```Shell
🔝 Top 5 TFlops Deltas (by absolute %):
shape: (5, 7)
┌────────────────┬────────────────┬───────────────────────┬───────────────┬──────────────┬───────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D) ┆ TFlops (base) ┆ TFlops (max) ┆ delta     ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                   ┆ ---           ┆ ---          ┆ ---       ┆ ---       │
│ str            ┆ str            ┆ str                   ┆ f64           ┆ f64          ┆ f64       ┆ f64       │
╞════════════════╪════════════════╪═══════════════════════╪═══════════════╪══════════════╪═══════════╪═══════════╡
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 2048, 16,     ┆ 249.514658    ┆ 243.078974   ┆ 6.435684  ┆ 2.647569  │
│                ┆                ┆ 2048, 64)             ┆               ┆              ┆           ┆           │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16,     ┆ 57.971274     ┆ 56.633641    ┆ 1.337633  ┆ 2.361905  │
│                ┆                ┆ 1024, 64)             ┆               ┆              ┆           ┆           │
│ noop           ┆ torch.bfloat16 ┆ (4, 16, 1024, 16,     ┆ 244.052884    ┆ 248.65129    ┆ -4.598406 ┆ -1.849339 │
│                ┆                ┆ 1024, 64)             ┆               ┆              ┆           ┆           │
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 16,     ┆ 280.71254     ┆ 275.686991   ┆ 5.025549  ┆ 1.822918  │
│                ┆                ┆ 1024, 128)            ┆               ┆              ┆           ┆           │
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 16384, 16,    ┆ 152.970031    ┆ 150.489109   ┆ 2.480923  ┆ 1.648573  │
│                ┆                ┆ 16384, 64)            ┆               ┆              ┆           ┆           │
└────────────────┴────────────────┴───────────────────────┴───────────────┴──────────────┴───────────┴───────────┘

🔺 Top 5 Positive TFlops Deltas (highest +%):
shape: (5, 7)
┌────────────────┬────────────────┬────────────────────────┬───────────────┬──────────────┬──────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D)  ┆ TFlops (base) ┆ TFlops (max) ┆ delta    ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                    ┆ ---           ┆ ---          ┆ ---      ┆ ---       │
│ str            ┆ str            ┆ str                    ┆ f64           ┆ f64          ┆ f64      ┆ f64       │
╞════════════════╪════════════════╪════════════════════════╪═══════════════╪══════════════╪══════════╪═══════════╡
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 2048, 16,      ┆ 249.514658    ┆ 243.078974   ┆ 6.435684 ┆ 2.647569  │
│                ┆                ┆ 2048, 64)              ┆               ┆              ┆          ┆           │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16,      ┆ 57.971274     ┆ 56.633641    ┆ 1.337633 ┆ 2.361905  │
│                ┆                ┆ 1024, 64)              ┆               ┆              ┆          ┆           │
│ noop           ┆ torch.bfloat16 ┆ (2, 16, 1024, 16,      ┆ 280.71254     ┆ 275.686991   ┆ 5.025549 ┆ 1.822918  │
│                ┆                ┆ 1024, 128)             ┆               ┆              ┆          ┆           │
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 16384, 16,     ┆ 152.970031    ┆ 150.489109   ┆ 2.480923 ┆ 1.648573  │
│                ┆                ┆ 16384, 64)             ┆               ┆              ┆          ┆           │
│ causal         ┆ torch.bfloat16 ┆ (4, 16, 1024, 16,      ┆ 161.031318    ┆ 158.597808   ┆ 2.43351  ┆ 1.534391  │
│                ┆                ┆ 1024, 64)              ┆               ┆              ┆          ┆           │
└────────────────┴────────────────┴────────────────────────┴───────────────┴──────────────┴──────────┴───────────┘

🔻 Top 5 Negative TFlops Deltas (lowest -%):
shape: (5, 7)
┌────────────────┬────────────────┬───────────────────────┬───────────────┬──────────────┬───────────┬───────────┐
│ attn_type      ┆ dtype          ┆ shape(B,Hq,M,Hkv,N,D) ┆ TFlops (base) ┆ TFlops (max) ┆ delta     ┆ pct_delta │
│ ---            ┆ ---            ┆ ---                   ┆ ---           ┆ ---          ┆ ---       ┆ ---       │
│ str            ┆ str            ┆ str                   ┆ f64           ┆ f64          ┆ f64       ┆ f64       │
╞════════════════╪════════════════╪═══════════════════════╪═══════════════╪══════════════╪═══════════╪═══════════╡
│ noop           ┆ torch.bfloat16 ┆ (4, 16, 1024, 16,     ┆ 244.052884    ┆ 248.65129    ┆ -4.598406 ┆ -1.849339 │
│                ┆                ┆ 1024, 64)             ┆               ┆              ┆           ┆           │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 4,      ┆ 175.546923    ┆ 177.81205    ┆ -2.265127 ┆ -1.273888 │
│                ┆                ┆ 1024, 128)            ┆               ┆              ┆           ┆           │
│ sliding_window ┆ torch.bfloat16 ┆ (4, 16, 16384, 4,     ┆ 156.282597    ┆ 158.209134   ┆ -1.926537 ┆ -1.217715 │
│                ┆                ┆ 16384, 64)            ┆               ┆              ┆           ┆           │
│ sliding_window ┆ torch.bfloat16 ┆ (2, 16, 2048, 16,     ┆ 232.542929    ┆ 235.140136   ┆ -2.597207 ┆ -1.104536 │
│                ┆                ┆ 2048, 128)            ┆               ┆              ┆           ┆           │
│ alibi          ┆ torch.bfloat16 ┆ (2, 16, 1024, 16,     ┆ 169.652791    ┆ 171.475986   ┆ -1.823195 ┆ -1.063236 │
│                ┆                ┆ 1024, 128)            ┆               ┆              ┆           ┆           │
└────────────────┴────────────────┴───────────────────────┴───────────────┴──────────────┴───────────┴───────────┘ 
```




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-08-27 22:54:21+00:00,2025-09-16T14:39:21Z,,False,12,14,15,460,80,9,26,2025-09-08 22:44:50+00:00,39,8549,False,False,False,False,False,False,9,11,5892,95391,56776,38615,1,15,7.0,12.0,2025-08-28T00:44:02Z,pytorch
161666,closed,[Flex] Fix float16 default config 128 headdim,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 22:54:18+00:00,2025-08-27T22:56:08Z,,False,2,0,1,1,1,1,2,2025-08-27 22:56:08+00:00,45,297,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
161665,open,Make get_printoptions and printoptions public,i-aki-y,"This PR proposes to make the following three related functions consistently public:

```
set_printoptions
get_printoptions
printoptions
```

Currently, only set_printoptions is exposed publicly, which may be confusing to users who likely expect all three to have the same visibility.
This PR aligns the visibility of `get_printoptions` and `printoptions` with `set_printoptions`, making all three publicly accessible.

A similar PR was previously submitted but later closed due to inactivity:

#116473

For this work, I referred to the following documentation: https://github.com/pytorch/pytorch/wiki/Public-API-definition-and-documentation

Note: `set_printoptions` remains unchanged in this PR. As a result, its `__module__` differs from the other two:

```
set_printoptions.__module__  # → ""torch._tensor_str""
get_printoptions.__module__  # → ""torch""
printoptions.__module__      # → ""torch""
```

Personally, I think it would be cleaner that they have the same `__module__` values. Should we consider updating `set_printoptions.__module__` as well?",2025-08-27 22:23:28+00:00,2025-09-12T16:42:37Z,,False,7,0,2,34,5,4,7,,45,1051,False,False,False,True,False,False,4,5,1609,39,34,5,1,2,3.0,6.0,2025-08-27T22:56:30Z,pytorch
161664,open,[scan][be] remove unnecessary tensor checks,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162754
* #161732
* #162025
* #161808
* __->__ #161664
* #161557

",2025-08-27 22:17:07+00:00,2025-09-19T12:13:19Z,,False,1,0,3,1,4,1,1,,43,144,False,False,False,False,False,False,1,0,0,97265,61494,35771,1,3,2.0,0.0,2025-08-28T17:02:27Z,pytorch
161663,closed,[CD] Add CUDA 13.0 Windows build,tinglvv,"https://github.com/pytorch/pytorch/issues/159779

CUDA 13.0.0 for Windows Build
CUDA 12.9 still needs the WAR for OOM https://github.com/pytorch/pytorch/issues/156181

cc @ptrblck @nWEIdia @atalman @malfet 
",2025-08-27 21:58:06+00:00,2025-09-02T06:17:36Z,,False,11,2,2,2665,503,5,13,2025-09-01 15:27:21+00:00,32,207,False,False,False,False,False,False,5,10,3534,3182,2672,510,1,2,4.0,10.0,2025-08-27T22:29:06Z,pytorch
161662,closed,[cuDNN][TF32] Account for TF32 in `test_super_resolution_cuda` ,eqy,"cuDNN seems to be dispatching to TF32 kernels on B200

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel @ptrblck @msaroufim @jerryzh168 @zasdfgbnm",2025-08-27 21:47:08+00:00,2025-08-28T08:43:40Z,,False,3,0,1,2,0,1,3,2025-08-28 08:42:37+00:00,63,145,False,False,False,False,False,False,1,2,498,2,2,0,1,1,3.0,2.0,2025-08-28T03:47:05Z,pytorch
161661,closed,Fix type checking for persistent loads in the weights-only unpickler,dfyz,"The error message here implies that we can only call `self.persistent_load(...)` for ints or tuples, but due to the second part of the type check being inverted, weights-only unpickler will throw an exception iff `pid` is an int.",2025-08-27 21:41:44+00:00,2025-09-01T19:58:26Z,,False,5,0,1,1,1,1,5,2025-09-01 19:57:21+00:00,68,229,False,True,False,False,False,False,1,3,535,2,1,1,1,1,3.0,3.0,2025-08-27T21:45:37Z,pytorch
161659,closed,Guard config copy for pickle errors,oulgen,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161659



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81168335](https://our.internmc.facebook.com/intern/diff/D81168335)",2025-08-27 21:16:34+00:00,2025-08-28T06:28:55Z,,False,5,0,1,8,3,1,5,2025-08-28 06:27:51+00:00,35,359,False,False,False,False,False,False,1,4,1337,11,8,3,1,1,3.0,5.0,2025-08-27T21:18:25Z,pytorch
161658,closed,Error when TORCH_STABLE_ONLY is defined in TensorBase.h,mikaylagawarecki,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161658


",2025-08-27 21:15:30+00:00,2025-08-28T04:37:37Z,,False,9,1,8,103,0,5,10,2025-08-28 04:36:34+00:00,55,95,False,False,False,False,False,False,5,8,2530,961,532,429,1,8,3.0,9.0,2025-08-27T21:23:54Z,pytorch
161657,closed,[WIP][nativert] Move AOTInductorModelImpl to OSS,yiming0416,"Summary: As titled. WIP

Test Plan:
CI

Rollback Plan:

Differential Revision: D81087598
",2025-08-27 20:37:33+00:00,2025-09-05T18:42:27Z,,False,2,0,1,879,0,3,2,2025-09-05 18:42:27+00:00,48,89,False,False,False,False,False,False,3,0,0,879,879,0,1,1,,,,pytorch
161656,closed,Add inductor provenance mapping for cpp extern kernel,yushangdi,"Summary: Add inductor provenance mapping for cpp extern kernel

Test Plan: 

```
buck run fbcode//caffe2/test/inductor:provenance_tracing --  -r test_cpu_extern_kernel
```

Differential Revision: D81161751




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 20:30:06+00:00,2025-09-03T14:23:38Z,,False,20,0,1,80,17,3,20,2025-09-03 14:23:38+00:00,53,411,False,False,False,False,False,False,3,11,4426,97,80,17,1,1,4.0,12.0,2025-08-28T16:31:58Z,pytorch
161654,closed,Select Algorithm clear feedback savers,exclamaforte,"Add `clear_feedback_savers` and tests for the feedback functionality.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 19:50:47+00:00,2025-08-28T06:57:11Z,,False,14,0,2,123,0,2,14,2025-08-28 06:56:07+00:00,38,272,False,False,False,False,False,False,2,11,2839,125,124,1,1,2,3.0,11.0,2025-08-27T21:48:45Z,pytorch
161653,closed,Prototype for building new export API using flat in/out tracing,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162089
* #162013
* #161977
* __->__ #161653

This is my first attempt of building new export API. The main thing it addresses is correctly getting input and output relations. Subsequent diffs willl add functionality for dynamic shapes, nn_module_stack etc.

Differential Revision: [D81041032](https://our.internmc.facebook.com/intern/diff/D81041032/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 19:48:36+00:00,2025-09-04T07:14:35Z,,False,13,1,8,387,87,8,14,2025-09-04 07:14:35+00:00,63,601,False,False,False,False,False,False,8,9,2558,167362,92174,75188,1,8,4.0,9.0,2025-08-28T19:40:52Z,pytorch
161652,open,Issue Fix: incorrect offset computation for double-sharded tensors in compute_local_shape_and_global_offset,davidizzle,"### Fix incorrect offset computation for double-sharded tensors in compute_local_shape_and_global_offset

**Fixes Issue #161368**

This PR fixes a bug in torch.distributed.tensor._utils.compute_local_shape_and_global_offset where offsets were computed incorrectly when a tensor was sharded multiple times along the same dimension (e.g., placements like [Shard(0), Shard(0)] on a 2D mesh).
Previously, the function only handled single-dimension sharding correctly. In the double-shard case, it produced bogus offsets (e.g., many ranks reporting identical offsets or non-contiguous slices). This broke correctness when redistributing tensors across uneven meshes.

I have slightly changed the logic so as to calculate a composite index in a way that is tolerant to this. 
In essence, we first need to:
1.  Coalesce sharding by grouping placements by tensor dimensions, in case they are repeated (as in the issue referenced)
2. Compute composite indices across mesh axes
3. Correctly assign local shapes and global offsets even with multiple shards on the same dimension

All the rest has remained the same. 
I have tentatively added a quick regression test which reproduces the issue, aiming to ensure that local shards align with the expected slices of the global tensor, preventing future regressions.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @tianyu-l @XilunWu @kwen2501 @jiayisuse @osalpekar @lw @angelayi  ",2025-08-27 19:43:03+00:00,2025-09-09T07:27:59Z,,False,10,2,1,95,24,2,12,,107,1455,False,True,False,False,False,False,2,6,341,119,95,24,1,1,3.0,6.0,2025-08-27T22:10:49Z,pytorch
161651,closed,[ONNX][submodule] Update onnx and protobuf,titaiwangms,"Fixes #ISSUE_NUMBER
",2025-08-27 19:41:11+00:00,2025-08-28T17:33:58Z,,False,2,0,2,29,110,8,2,2025-08-28 17:33:58+00:00,42,20,False,True,False,False,False,False,8,1,66,1754,1272,482,1,2,1.0,1.0,2025-08-28T17:33:58Z,pytorch
161649,closed,Use vectorized stores for all dtypes in cat,ngimel,"resurrecting #151818


cc @ptrblck @msaroufim @eqy @jerryzh168",2025-08-27 19:21:33+00:00,2025-09-09T01:40:06Z,,False,31,0,9,148,11,2,31,2025-09-09 01:40:06+00:00,43,62,False,False,False,False,False,False,2,26,11106,681932,452344,229588,1,9,3.0,26.0,2025-08-27T19:38:03Z,pytorch
161648,closed,Add option for TorchDispatchMode to ignore torch.compile internals,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161648

If TorchDispatchMode.ignore_compile_internals() is True, then we turn
off the TorchDispatchMode during the compilation process, instead
turning it back on during runtime of the compiled artifact.

Test Plan:
- new test

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 19:14:48+00:00,2025-08-28T02:42:39Z,,False,3,8,4,115,1,3,11,2025-08-28 02:41:35+00:00,66,484,False,False,False,False,False,False,3,2,530,258,186,72,1,4,3.0,3.0,2025-08-27T19:21:16Z,pytorch
161647,closed,[Flex] Fix float16 default config 128 headdim,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161647



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-08-27 19:13:20+00:00,2025-08-29T01:49:11Z,,False,3,3,3,2,2,1,6,2025-08-29 01:48:08+00:00,45,330,False,True,False,False,False,False,1,2,493,8029,5616,2413,1,3,3.0,2.0,2025-08-27T19:43:19Z,pytorch
161645,closed,NamedTuple: Allow side effects for dynamic attributes,morrison-turnansky,"I confirmed that the tracing was correct i.e. NamedTupleVariable had the correct dynamic attribute added to it.

The problem was that NamedTupleVariable was always marked as immutable. This does not reflect the behavior of namedtuple. 

Subclasses of namedtuple may be mutable, so when a NamedTupleVariable is derived from a subclass that is mutable, I made NamedTupleVariable mutable as well. Then side_effects correctly updates the returned object. 

Fixes #161610


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 18:59:16+00:00,2025-09-09T19:43:09Z,,False,10,25,16,85,6,4,35,2025-09-09 19:42:06+00:00,53,639,False,True,False,False,False,False,4,8,1417,251,165,86,1,16,4.0,11.0,2025-08-27T19:00:20Z,pytorch
161644,closed,Remove test since it ooms on CI,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161644



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @yanboliang @BoyuanFeng",2025-08-27 18:56:28+00:00,2025-08-28T16:27:40Z,,False,6,0,1,0,36,1,6,2025-08-28 16:27:40+00:00,31,330,False,False,False,False,False,False,1,5,1240,36,0,36,1,1,4.0,5.0,2025-08-27T19:07:35Z,pytorch
161643,closed,Apply Triton tensor descriptor for flex-decoding for performance,EikanWang,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161643



cc @voznesenskym @penguinwu @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 18:53:15+00:00,2025-09-04T20:11:48Z,,False,9,8,6,50,4,2,17,2025-09-04 20:10:44+00:00,64,286,False,False,False,False,False,False,2,8,1157,72001,43047,28954,1,6,5.0,9.0,2025-08-27T18:54:03Z,pytorch
161641,closed,Ensure that tensors are contiguous before using no-graph MPS impl,rebeccajae,"Fixes #161640

Check if tensors are contiguous before using the no-graph implementation. Using the script in the issue above with this change I get expected results.

```
MPS contiguous result sample: tensor([ 1.3600, -2.9516,  1.3207, -3.5132,  1.7061], device='mps:0')
MPS non-contig result sample: tensor([ 1.3600, -2.9516,  1.3207, -3.5132,  1.7061], device='mps:0')
CPU non-contig result sample: tensor([ 1.3600, -2.9516,  1.3207, -3.5132,  1.7061])
```",2025-08-27 18:40:56+00:00,2025-08-27T22:33:04Z,,False,8,1,3,15,1,2,9,2025-08-27 22:32:01+00:00,65,458,False,True,False,False,False,False,2,6,1844,26,20,6,2,3,3.0,7.0,2025-08-27T18:44:48Z,pytorch
161639,closed,Bugfix for doing negative padding,skpark-rh,"Fixes #161014 

This bug fix introduces a fix that is consistent with the exception handling. Outlined in issue #161014, there is an edge case where the negative padding does not make the tensor size negative but still triggers the exception that the size is negative. The fix is simply adding `new_dim >=0` to include the zero dim and letting the operator return an empty tensor.

In the PR I have added the edge case where the test will now check the negative padding where the dimension gets reduced to zero.  But the sample is only for the `constant` type of padding. I would like some feedback if it is necessary to put the same sample on the `reduce` type as well.

This is my first PR to contribute to PyTorch and any help/feedback will be welcome! Thank you!

@malfet @manuelcandales @janeyx99 @ezyang


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-27 18:38:47+00:00,2025-09-19T21:06:01Z,,False,18,2,16,4,4,5,20,2025-09-19 20:57:08+00:00,33,1125,False,True,False,False,False,False,5,12,1964,116021,76651,39370,1,15,6.0,12.0,2025-08-27T18:56:32Z,pytorch
161637,closed,[BE] Refactor trymerge for readability,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161560
* __->__ #161637

Two changes:
- Extract getting the last_commit's sha into it's own function
- Rename merge_changes to merge_changes_locally to better explain it's functionality",2025-08-27 18:18:48+00:00,2025-08-27T22:45:09Z,,False,6,3,2,20,12,1,9,2025-08-27 22:44:04+00:00,38,264,False,False,False,False,False,True,1,5,1689,42,25,17,1,2,4.0,5.0,2025-08-27T18:34:24Z,pytorch
161635,open,"PythonArgParser::symintlist{,Optional} should use SymDimVector",swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161635
* #162281

They're usually/always used for sizes & strides, which is what SymDimVector is for (saves heap allocation of the list itself). Had to patch OptionalArray because there's a bunch of generated code that wants to convert these to SymIntArrayRef and the generator isn't easy to patch to manually wrap. Clear but small improvement in perf on ""detach DTensor in a loop"" benchmark; we aren't heap-allocating symdimlists anymore, though there's still some cost with destroying these because SymInt needs cleanup.

Differential Revision: [D81530103](https://our.internmc.facebook.com/intern/diff/D81530103)",2025-08-27 17:56:09+00:00,2025-09-10T01:07:29Z,,False,4,0,8,20,16,2,4,,62,701,False,False,False,False,True,False,2,3,663,100984,60847,40137,1,8,3.0,3.0,2025-08-27T19:39:29Z,pytorch
161634,closed,fastpath type Tensor in THPVariable_NewWithVar,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* #162219
* #161692
* __->__ #161634
* #161633
* #161595
* #161591

It is cheap to do an exact check against Tensor and much faster when it works (PyType_IsSubtype does not have this fastpath, I checked [source](https://github.com/python/cpython/blob/9ee0214b5dd982ac9fbe18dcce0e8787456e29af/Objects/typeobject.c#L2889)). Spot-checked in perf on detach-DTensor-in-a-loop benchmark; small win but clear.

Differential Revision: [D81530101](https://our.internmc.facebook.com/intern/diff/D81530101)",2025-08-27 17:56:05+00:00,2025-09-09T01:10:10Z,,False,3,0,7,1,1,1,3,2025-09-09 01:10:10+00:00,46,581,False,False,False,False,False,False,1,2,206,99535,60136,39399,1,7,4.0,3.0,2025-08-28T17:23:56Z,pytorch
161633,closed,Avoid redundant PyTuple_GetSize call in _maybe_handle_torch_function,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* #162219
* #161692
* #161634
* __->__ #161633
* #161595
* #161591

py::args::size() calls PyTuple_GetSize. Compiler can't know the two calls will always return the same result, so we have to consolidate them ourselves.

Differential Revision: [D81530096](https://our.internmc.facebook.com/intern/diff/D81530096)

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-27 17:56:01+00:00,2025-09-09T01:10:09Z,,False,3,0,7,3,2,1,3,2025-09-09 01:10:09+00:00,68,447,False,False,False,False,False,False,1,2,202,99538,60138,39400,1,7,4.0,2.0,2025-08-27T19:53:09Z,pytorch
161631,closed,TEST change. Do not merge,ZainRizvi,"Fixes #ISSUE_NUMBER
",2025-08-27 17:37:01+00:00,2025-08-27T18:43:09Z,,False,3,2,3,4,1,1,5,2025-08-27 18:43:09+00:00,25,20,False,True,False,False,False,False,1,2,13,5,4,1,1,3,1.0,2.0,2025-08-27T17:56:42Z,pytorch
161630,open,[Optimus] Replace torch rms norm with quack rms norm kernel,mengluy0125,"Summary: We observe significant perf gain to replace torch rms norm with the quack rms norm kernel (D80317390). We do this inside compiler, thus it can be reused across models with at least B200 GPUs

Test Plan:
### unit test

```
buck2 test mode/opt  -c fbcode.nvcc_arch=b200a -c fbcode.enable_gpu_sections=true -c fbcode.platform010_cuda_version=12.8 -m ""ovr_config//third-party/pypi/nvidia-cutlass-dsl/constraints:4.1.0"" //caffe2/test/inductor:kernel_optimization -- test_replace_rms_norm_with_quack
```

Buck UI: https://www.internalfb.com/buck2/c29f2d25-b674-44c5-b504-a475b209384c
Test UI: https://www.internalfb.com/intern/testinfra/testrun/12666374058307987
Network: Up: 38KiB  Down: 12MiB  (reSessionID-0cbaccbb-7f2c-4651-9fc2-c13760f0e8f7)
Analyzing targets. Remaining     0/287
Executing actions. Remaining     0/21254                                                                      11.9s exec time total
Command: test.     Finished 5 local, 1 cache (17% hit)                                                        7.2s exec time cached (60%)
Time elapsed: 24.5s
Tests finished: Pass 1. Fail 0. Fatal 0. Skip 0. Build failure 0

### EMS

Enable the following flag
```
        torch._inductor.config.pre_grad_fusion_options = {
            ""use_custom_rmsnorm_kernel_pass"": {""quack"": True}
        }
```

```
buck2 run mode/opt -c fbcode.platform010_cuda_version=12.8 aps_models/ads/ecosystem/tooling/tools/efficient_module_suite/benchmark:omnifm_perf_benchmark -- benchmark-with-prod-model --prod_config mast_omnifm_v3_B200_1kgpu --prod_config_override prod_config_override_jointarch --batch_size 1152 --enable_pt2 True --mfu_profile_module uniarch_layers.0.seq_interaction_layer --enable_ac True
```


without quack kernel

```
| Metric                | Value        |
|:----------------------|:-------------|
| Batch size            | 1152         |
| GPU type              | B200         |
| Latency               | 74.57 ms     |
| Model size            | 1.44 GB      |
| Flops                 | 14431.98 G   |
| Flops/example         | 12.53 G      |
| TFLOPS/sec            | 193.53       |
| MFU                   | 16.54%       |
| Activation/example    | 12.27 MB     |
| CPU time total        | 224.09 ms    |
| GPU time total        | 262.45 ms    |
| Estimated avg BW      | 2534.11 GB/s |
| Estimated avg BW util | 40.87%       |
Trace link: https://our.intern.facebook.com/intern/perfdoctor/trace_view?filepath=tree/traces/efficient_module_suite/uniarch.uniarch_layers.0.seq_interaction_layer.Aug_07_22_26_33_trace.json.gz&bucket=pyper_traces
Perfetto link: https://interncache-all.fbcdn.net/manifold/perfetto-artifacts/tree/ui/index.html#!/?url=https://interncache-all.fbcdn.net/manifold/pyper_traces/tree/traces/efficient_module_suite/uniarch.uniarch_layers.0.seq_interaction_layer.Aug_07_22_26_33_trace.json.gz
Snapshot link: https://www.internalfb.com/pytorch_memory_visualizer/ai_efficiency/tree/gpu_snapshot/uniarch.uniarch_layers.0.seq_interaction_layer.Aug_07_22_26_33.snapshot.pickle
```

with quack kernel

```
| Metric             | Value      |
|:-------------------|:-----------|
| Batch size         | 1152       |
| GPU type           | B200       |
| Latency            | 71.72 ms   |
| Model size         | 1.44 GB    |
| Flops              | 14431.98 G |
| Flops/example      | 12.53 G    |
| TFLOPS/sec         | 201.22     |
| MFU                | 17.20%     |
| Activation/example | 12.28 MB   |
| CPU time total     | 197.22 ms  |
| GPU time total     | 237.44 ms  |
Trace link: https://our.intern.facebook.com/intern/perfdoctor/trace_view?filepath=tree/traces/efficient_module_suite/uniarch_layers.0.seq_interaction_layer.Aug_26_17_07_41_trace.json.gz&bucket=pyper_traces
Perfetto link: https://interncache-all.fbcdn.net/manifold/perfetto-artifacts/tree/ui/index.html#!/?url=https://interncache-all.fbcdn.net/manifold/pyper_traces/tree/traces/efficient_module_suite/uniarch_layers.0.seq_interaction_layer.Aug_26_17_07_41_trace.json.gz
Snapshot link: https://www.internalfb.com/pytorch_memory_visualizer/ai_efficiency/tree/gpu_snapshot/uniarch_layers.0.seq_interaction_layer.Aug_26_17_07_41.snapshot.pickle
```

Rollback Plan:

Differential Revision: D80492377




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-27 17:16:11+00:00,2025-09-03T01:08:19Z,,False,5,2,1,1978,0,8,7,,59,4427,False,False,False,True,False,False,8,1,403,1978,1978,0,1,1,3.0,1.0,2025-08-27T19:44:18Z,pytorch
161628,closed,"Revert ""Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161628
* #161627
* #161626
* #161625

This reverts commit ae1a706444d6c0a6019ffc936c8b36574335a5d5.",2025-08-27 17:12:55+00:00,2025-08-27T21:38:23Z,,False,3,0,2,495,160,4,3,2025-08-27 21:37:19+00:00,83,185,False,False,False,False,False,True,4,2,821,666,502,164,1,2,2.0,2.0,2025-08-27T20:23:52Z,pytorch
161627,closed,"Revert ""Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161628
* __->__ #161627
* #161626
* #161625

This reverts commit c1145852a5eac96f5551b5d1805109ce4dc5e1fa.",2025-08-27 17:12:48+00:00,2025-08-27T21:37:18Z,,False,2,0,2,36,43,5,2,2025-08-27 21:37:18+00:00,110,185,False,False,False,False,False,False,5,1,48,90,43,47,1,2,2.0,1.0,2025-08-27T20:26:47Z,pytorch
161626,closed,"Revert ""Generalize torch._C._set_allocator_settings to be generic (#156175)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161628
* #161627
* __->__ #161626
* #161625

This reverts commit 908c5cc4c0f22d141776bde47c296b5186691855.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 17:12:40+00:00,2025-08-27T21:37:17Z,,False,2,0,2,34,26,9,2,2025-08-27 21:37:17+00:00,76,357,False,False,False,False,False,False,9,1,48,71,41,30,1,2,2.0,1.0,2025-08-27T20:27:19Z,pytorch
161625,closed,"Revert ""Back out ""Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)"" (#160999)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161628
* #161627
* #161626
* __->__ #161625

This reverts commit a818fa77e3a72271f144514ef349c5a666313205.",2025-08-27 17:12:32+00:00,2025-08-27T21:35:19Z,,False,3,0,2,42,44,5,3,2025-08-27 21:34:15+00:00,131,185,False,False,False,False,False,False,5,2,821,97,49,48,1,2,2.0,2.0,2025-08-27T20:27:34Z,pytorch
161624,closed,Expose Kineto event metadata in PyTorch Profiler events,mihaipgc,"## Overview
This PR allows the profiler users to access `Kineto` and `TorchOp` metadata in JSON string format through a new `metadata_json` attribute in `FunctionEvent` objects, which is triggered through a new `expose_kineto_event_metadata` flag in `ExperimentalConfig`.

## Testing
A unit test was added to validate functionality.

## Documentation
Added/updated function doc strings where appropriate.

## Example output
```python
import torch
from torch.profiler import profile

with profile(experimental_config=torch._C._profiler._ExperimentalConfig(expose_kineto_event_metadata=True)) as prof:
    res = torch.mm(torch.rand(1024, 1024), torch.rand(1024, 1024))

for event in prof.events():
    print(f'name: {event.key}, metadata: {event.metadata_json}')
```

```
name: aten::rand, metadata: ""Ev Idx"": 0
name: aten::empty, metadata: ""Ev Idx"": 1
name: aten::uniform_, metadata: ""Ev Idx"": 2
name: aten::rand, metadata: ""Ev Idx"": 3
name: aten::empty, metadata: ""Ev Idx"": 4
name: aten::uniform_, metadata: ""Ev Idx"": 5
name: aten::mm, metadata: ""Ev Idx"": 6
name: aten::resolve_conj, metadata: ""Ev Idx"": 7
name: aten::resolve_conj, metadata: ""Ev Idx"": 8
name: aten::resolve_conj, metadata: ""Ev Idx"": 9
```
",2025-08-27 16:38:14+00:00,2025-09-25T14:59:38Z,,False,13,2,9,93,12,12,15,2025-09-25 14:58:34+00:00,55,1206,False,False,False,True,False,False,12,10,2268,155,118,37,1,9,3.0,11.0,2025-08-27T16:57:24Z,pytorch
161622,closed,Use specific Tuple/List APIs instead of PySequence in is_int_or_symint_list,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161695
* #161694
* #161693
* #161692
* #161635
* #161634
* #161633
* __->__ #161622
* #161596
* #161595
* #161591
* #161590
* #161586
* #161466
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301

These APIs are faster and, since we can use the container-specific unchecked indexing APIs  don't repeat bounds checks.",2025-08-27 16:12:04+00:00,2025-09-02T19:31:49Z,,False,5,0,3,11,4,1,5,2025-09-02 19:31:49+00:00,75,443,False,False,False,False,False,False,1,4,410,21238,14610,6628,1,3,3.0,4.0,2025-08-27T19:31:34Z,pytorch
161620,closed,Back out Generalize torch._C._set_allocator_settings to be generic,atalman,"This backs out:
https://github.com/pytorch/pytorch/pull/156175

Since We Previously backed out:
https://github.com/pytorch/pytorch/pull/156165
https://github.com/pytorch/pytorch/pull/150312

By these 2 manual reverts (which are not complete): 
https://github.com/pytorch/pytorch/pull/161002
and https://github.com/pytorch/pytorch/pull/160999


Depending on signal on this PR we may need to backout these two:
https://github.com/pytorch/pytorch/pull/152932 and https://github.com/pytorch/pytorch/pull/155200 (edited) 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 15:53:55+00:00,2025-08-27T22:32:12Z,,False,3,0,4,15,15,7,3,2025-08-27 22:32:12+00:00,66,689,False,False,False,False,False,False,7,1,53,30,15,15,1,4,3.0,2.0,2025-08-27T15:56:29Z,pytorch
161619,closed,Fix running the benchmark jobs twice,huydhn,"I made a mistake in https://github.com/pytorch/pytorch/pull/160935 removing this condition check.  This ran the benchmark job twice for schedule jobs, i.e. https://github.com/pytorch/pytorch/actions/runs/17266546494.  This was missed during testing because `pull_request` and `workflow_dispatch` were working ok.
",2025-08-27 15:40:05+00:00,2025-08-27T17:19:56Z,,False,3,0,1,3,0,1,3,2025-08-27 17:18:13+00:00,36,313,False,True,False,False,False,False,1,2,795,3,3,0,1,1,3.0,2.0,2025-08-27T16:47:17Z,pytorch
161617,closed,Fix Inductor Periodic,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161617

Models are now passing accuracy. # of graph breaks is larger because
these were not actually tested in CI (if the model fails accuracy we
do not assert on # of graph breaks).

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-27 13:48:08+00:00,2025-08-28T02:37:14Z,,False,4,0,1,4,4,2,4,2025-08-28 02:36:10+00:00,21,440,False,True,False,False,False,False,2,3,813,8,4,4,1,1,3.0,3.0,2025-08-27T14:52:46Z,pytorch
161615,open,Move call to output generated code in inductor,CWOA,"This PR moves the call to copy the generated code from `/tmp/...` so that it is still called if attempting to compile the generated code fails. In both cases now, the generated code will be copied across to `torch_compile_debug/run_.../torchinductor/output_code.py` which makes debugging bad generated code easier.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 13:39:32+00:00,2025-09-09T15:06:47Z,,False,15,0,1,3,2,1,15,,46,517,False,True,False,False,False,False,1,12,2723,5,3,2,1,1,3.0,12.0,2025-08-27T13:57:19Z,pytorch
161613,closed,Removed redundant dtype conversion in scaled_dot_product_attention docstring example,RajeshvShiyal,"Suggested changes done for Fixes #161611. 

Removed the line attn_bias.to(query.dtype) entirely

Fixes #161611",2025-08-27 13:09:07+00:00,2025-08-28T20:21:23Z,,False,7,0,1,0,1,1,7,2025-08-28 19:58:11+00:00,84,110,False,True,False,True,False,False,1,6,2099,1,0,1,1,1,3.0,7.0,2025-08-27T13:11:30Z,pytorch
161607,open,[BE] Fix bad indentation in ATen/native/cuda/Blas.cpp,rec,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161607

",2025-08-27 11:04:49+00:00,2025-08-27T13:26:01Z,,False,2,0,1,4,4,1,2,,53,94,False,True,False,False,False,False,1,1,212,8,4,4,1,1,1.0,1.0,2025-08-27T11:30:35Z,pytorch
161606,closed,nccl est debug,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161606
* #161499
* #161406
* #161405



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 10:06:23+00:00,2025-09-24T14:18:58Z,,False,2,0,1,20,6,1,2,2025-09-24 14:18:58+00:00,14,327,False,True,False,False,False,False,1,0,0,26,20,6,1,1,,,,pytorch
161605,closed,[MPS] Fix batch_normboardcast error on mps,can-gaa-hou,"Fixes #ISSUE_NUMBER
",2025-08-27 09:56:48+00:00,2025-08-30T03:16:31Z,,False,1,0,1,27,8,1,1,2025-08-30 03:16:31+00:00,42,20,False,True,False,False,False,False,1,0,0,35,27,8,1,1,,,,pytorch
161604,closed,port distributed tensor test files for Intel GPU,wincent8,"In this pr, we port test/distributed/tensor test filesfor Intel GPU
We could enable Intel GPU with following methods and try the best to keep the original code styles:

Use torch.accelerator for general gpu
Skip the case if running on xpu which has known issues


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @tianyu-l @XilunWu",2025-08-27 09:55:55+00:00,2025-09-04T07:50:33Z,,False,7,2,15,16,10,3,9,2025-09-04 07:49:29+00:00,48,359,False,False,False,False,False,False,3,5,1378,126,66,60,1,15,5.0,7.0,2025-09-02T03:03:47Z,pytorch
161603,closed,[OpenReg] Rename cpu_fallback_blacklist to cpu_fallback_blocklist,FFFrog,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160101
* #160100
* #161773
* #160099
* __->__ #161603

As the title stated.

Related Infos: https://github.com/pytorch/pytorch/pull/158644#discussion_r2301460839",2025-08-27 09:43:13+00:00,2025-08-30T13:21:32Z,,False,4,0,3,2,2,1,4,2025-08-30 13:21:31+00:00,65,240,False,False,False,False,False,False,1,3,150,10955,8208,2747,1,3,2.0,4.0,2025-08-29T20:51:07Z,pytorch
161602,closed,[inductor] check block options after broadcasting and singleton dims have been removed,kundaMwiza,"This will allow for some more cases to use tensor descriptors e.g. before the following block params would not match 
because the innermost dimension does not have stride 1
```python
block_params=BlockParameters(shape=[64, 4, 1, 1], block_shape=[((XBLOCK + 3)//4), Min(4, XBLOCK), 1, 1], strides=[0, 1, 0, 0], offsets=[(xoffset//4), ModularIndexing(xoffset, 1, 4), 0, 0])
```
After broadcasting dimensions and singleton dimensions are removed:
```python
block_params=BlockParameters(shape=[4], block_shape=[Min(4, XBLOCK)], strides=[1], offsets=[ModularIndexing(xoffset, 1, 4)])
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 09:38:57+00:00,2025-08-30T08:11:56Z,,False,5,0,1,17,14,2,5,2025-08-30 08:10:53+00:00,86,785,False,False,False,False,False,False,2,4,597,31,17,14,1,1,3.0,4.0,2025-08-27T11:11:32Z,pytorch
161601,closed,[3/N] Enable 6 fsdp test on Intel GPU,daisyden,"For https://github.com/pytorch/pytorch/issues/114850, we will port distributed tests to Intel GPU. This PR is created base on PR https://github.com/pytorch/pytorch/pull/158533 and https://github.com/pytorch/pytorch/pull/159473 and will work on some test files under test/distributed/fsdp. We could enable Intel GPU with following methods and try the best to keep the original code styles in this PR:

1. add allow_xpu=True in instantiate_device_type_tests() if needed.
2. use ""torch.accelerator.current_accelerator()"" to determine the accelerator backend

3. enabled XPU for some test path




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 08:45:21+00:00,2025-09-06T16:48:19Z,,False,6,4,32,63,49,6,10,2025-09-06 16:47:17+00:00,37,888,False,False,False,False,False,False,6,5,2896,227597,142465,85132,1,30,4.0,7.0,2025-08-27T09:11:28Z,pytorch
161600,closed,[Intel GPU] Enable tensor memory descriptor in triton template for XPU.,etaf,"As Intel Triton now supports tensor descriptor, this PR updates the pinned Intel Triton version and introduces support for Triton MM template with tensor descriptor on XPU.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 08:34:27+00:00,2025-08-28T12:59:30Z,,False,3,0,2,21,3,5,3,2025-08-28 12:40:02+00:00,71,375,False,False,False,False,False,False,5,2,493,40,29,11,1,2,4.0,2.0,2025-08-28T01:07:57Z,pytorch
161599,closed,[unflatten] Fix test by supporting both MappingKey anf GetAttrKey,malaybag,"Summary: As title

Test Plan:
Run internal tests

Rollback Plan:

Differential Revision: D81115712


",2025-08-27 08:25:55+00:00,2025-08-29T10:09:44Z,,False,15,0,1,1,1,1,15,2025-08-29 10:08:41+00:00,65,101,False,True,False,False,False,False,1,10,2993,2,1,1,1,1,3.0,10.0,2025-08-27T16:16:31Z,pytorch
161598,open,Update torch-xpu-ops commit pin,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161598

",2025-08-27 07:10:18+00:00,2025-08-29T06:39:12Z,,False,1,1,3,1,1,1,2,,31,94,False,False,False,False,False,False,1,0,0,4605,1887,2718,1,3,1.0,0.0,2025-08-28T18:34:39Z,pytorch
161597,closed,Add new_zeros dtype variant to the shim and as a stable op,janeyx99,In case we want this before 2.9,2025-08-27 06:18:04+00:00,2025-08-28T14:20:19Z,,False,10,0,1,74,1,6,10,2025-08-28 13:57:27+00:00,58,31,False,False,False,False,False,False,6,8,1430,75,74,1,1,1,2.0,8.0,2025-08-27T21:48:10Z,pytorch
161596,closed,Remove logger.debug statements in DTensor dispatch,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162337
* __->__ #161596
* #162218
* #162220
* #162219
* #161692
* #161634
* #161633
* #161595
* #161591

These seem to have been costing us 5-10 usec per detach (out of ~~95 usec total).  If they need to ship let's talk about requirements and how we can make this more efficient given that we would prefer if an entire DTensor op could finish in 10 usec.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci

Differential Revision: [D81530106](https://our.internmc.facebook.com/intern/diff/D81530106)",2025-08-27 05:51:51+00:00,2025-09-11T06:58:40Z,,False,10,3,8,0,35,2,13,2025-09-11 06:58:38+00:00,50,629,False,True,False,False,False,False,2,9,2617,99661,60218,39443,1,8,5.0,11.0,2025-08-27T20:01:04Z,pytorch
161595,closed,Add C++ function for torch.distributed.tensor._op_schema.is_view_op,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162337
* #162336
* #161695
* #162298
* #162220
* #162219
* #162218
* #161693
* #161692
* #161634
* #161633
* #161596
* __->__ #161595
* #161591
* #161590
* #161586
* #161466

This seems to have been an especially slow one because of the repeated pybind access (schema is a pybind, as is arguments, and then we hit each argument). It's still ~~1% of total benchmark runtime because of the repeated single pybind function call, but that's a lot better.

Differential Revision: [D81530095](https://our.internmc.facebook.com/intern/diff/D81530095)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-27 05:51:47+00:00,2025-09-08T16:29:20Z,,False,9,0,6,12,4,3,9,2025-09-08 16:28:13+00:00,67,764,False,False,False,False,False,False,3,8,2369,69649,41300,28349,1,6,4.0,9.0,2025-08-27T20:02:17Z,pytorch
161594,closed,Change back logic of roundup_power2_divisions,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161856
* #161788
* #161787
* #161786
* __->__ #161594
# Motivation
This restores a bug fix to verify if that bug fix introduces an internal CI performance regression.

Differential Revision: [D81140711](https://our.internmc.facebook.com/intern/diff/D81140711)",2025-08-27 04:27:24+00:00,2025-09-15T05:49:54Z,,False,2,0,3,9,6,2,2,2025-09-15 05:49:54+00:00,45,338,False,True,False,False,False,False,2,1,156,13753,10424,3329,1,3,1.0,1.0,2025-08-27T16:34:52Z,pytorch
161593,closed,"Revert ""Back out ""Deprecate overleap functions in CUDAAllocatorConfig, use AcceleratorAllocatorConfig instead (#156165)"" (#160999)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161594
* __->__ #161593
* #161592

This reverts commit a818fa77e3a72271f144514ef349c5a666313205.

Differential Revision: [D81140713](https://our.internmc.facebook.com/intern/diff/D81140713)",2025-08-27 04:27:17+00:00,2025-08-29T15:08:43Z,,False,3,0,2,42,44,5,3,2025-08-29 15:08:43+00:00,131,268,False,False,False,False,False,False,5,1,156,339,197,142,1,2,1.0,1.0,2025-08-27T16:34:46Z,pytorch
161592,closed,"Revert ""Back out ""Refactor CUDAAllocatorConfig to reuse AcceleratorAllocatorConfig (#150312)"" (#161002)""",guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161594
* #161593
* __->__ #161592

This reverts commit a03cc53e6f6e2fe67316cb8c74c25f5b953f445b.

Differential Revision: [D81140712](https://our.internmc.facebook.com/intern/diff/D81140712)",2025-08-27 04:27:09+00:00,2025-08-29T15:08:51Z,,False,3,0,2,160,495,4,3,2025-08-29 15:08:51+00:00,104,268,False,False,False,False,False,True,4,1,156,908,315,593,1,2,1.0,1.0,2025-08-27T16:34:40Z,pytorch
161591,closed,Don't call check_has_torch_dispatch in THPVariable_NewWithVar if we already know,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162337
* #162336
* #161695
* #162298
* #162220
* #162219
* #162218
* #161693
* #161692
* #161634
* #161633
* #161596
* #161595
* __->__ #161591
* #161590
* #161586
* #161466

We already know when we're called from make_wrapper_subclass or make_dtensor. The check isn't particularly cheap.

Differential Revision: [D81530099](https://our.internmc.facebook.com/intern/diff/D81530099)",2025-08-27 04:17:30+00:00,2025-09-08T16:28:13Z,,False,5,0,7,22,5,1,5,2025-09-08 16:28:12+00:00,80,460,False,False,False,False,False,False,1,4,274,69656,41307,28349,1,7,3.0,4.0,2025-09-02T21:44:42Z,pytorch
161590,closed,Add torch.Tensor._make_dtensor to accelerate DTensor.__new__ further,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162220
* #162219
* #162218
* #161694
* #161693
* #161692
* #161634
* #161633
* #161596
* #161595
* #161591
* __->__ #161590
* #161586
* #161466

This seems to be a (very very roughly) ~8% improvement on DTensor benchmark very similar to the benchmark from #160580 (120ish usec -> 110ish usec)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim

Differential Revision: [D81530105](https://our.internmc.facebook.com/intern/diff/D81530105)",2025-08-27 04:17:26+00:00,2025-09-05T18:44:50Z,,False,10,12,6,153,77,4,22,2025-09-05 18:43:44+00:00,68,561,False,False,False,False,True,False,4,9,2293,69739,41368,28371,1,6,7.0,10.0,2025-08-27T21:43:30Z,pytorch
161589,closed,[RELAND] Close some sources of fake tensor leakage,tugsbayasgalan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161589

Reland of https://github.com/pytorch/pytorch/pull/159923 

Couple of fixes:
1. When we run into an operation we didn't proxy, we end up emitting fake constants. We detect this and warn using the FQN of the lifted constant. We warn because some internal users complained it was regressing their exportability. 

2. Previous attribute mutation detection logic in non-strict didn't account for nested module structure. This fixes silent incorrectness issue of exporting esm and qwen in non-strict

3. We modify yolov3 to fix the previous silent incorrect behaviour 
4. We use strict export for levit_128 because it errors in non-strict due to more strict side effect checking 


When upgrading torchbench pin, opacus_cifar10 seems to not run on eager anymore. I verified this by pushing a temporary PR on master with new pin. So i added it to expect_fail list. 



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81326893](https://our.internmc.facebook.com/intern/diff/D81326893)",2025-08-27 03:27:22+00:00,2025-09-01T15:30:11Z,,False,18,11,5,240,31,5,29,2025-09-01 15:30:10+00:00,50,1219,False,True,False,False,False,False,5,15,3522,299,254,45,1,4,6.0,16.0,2025-08-27T03:30:15Z,pytorch
161588,closed,Add C++ function to accelerate DTensor.__new__,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161695
* #161694
* #161693
* #161692
* #161635
* #161634
* #161633
* #161622
* #161596
* #161595
* #161591
* #161590
* __->__ #161588
* #161586
* #161466
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286

As the new comment explains, pybinding DispatchKeySet is expensive; one native function avoids the number of DispatchKeySets we incur pybind overhead for to 1.

Reviewers please advise on a better location for the new torch._C function; I just threw it in Module.cpp because I didn't know where to put it.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-27 01:48:21+00:00,2025-09-04T22:55:41Z,,False,4,2,2,25,7,2,6,2025-09-04 22:55:41+00:00,46,737,False,False,False,False,False,False,2,3,784,17532,11761,5771,1,2,4.0,4.0,2025-08-27T02:03:15Z,pytorch
161587,closed,[Intel GPU] Update Intel triton for Flex Attention performance improvements.,etaf,,2025-08-27 01:29:10+00:00,2025-08-27T10:41:14Z,,False,1,0,1,1,1,1,1,2025-08-27 10:41:14+00:00,76,0,False,False,False,False,True,False,1,0,0,2,1,1,1,1,,,,pytorch
161586,closed,Add inline fast paths for SymInt operators,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161586
* #161466

If SymInt::maybe_as_int() returns non-empty, then we get an inline
fast path. The philosophy here (as with the previous PR) is to
preserve performance in the ""plain old ints"" case.

Observed time spent in SymInt functions in computeStorageNBytes to
drop (and not cost shift elsewhere in the function) after this change,
profiling detach() using code similar to the benchmark from #160580
and Linux perf.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81530107](https://our.internmc.facebook.com/intern/diff/D81530107)",2025-08-27 01:24:26+00:00,2025-09-03T06:55:54Z,,False,12,2,5,359,41,4,14,2025-09-03 06:54:50+00:00,42,803,False,False,False,False,False,False,4,11,4131,69800,41524,28276,1,5,3.0,12.0,2025-08-27T20:15:20Z,pytorch
161585,open,[2/2]Add summary report for vllm build and test ,yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161585

",2025-08-27 01:21:44+00:00,2025-08-29T21:01:36Z,,False,1,0,18,241,32,9,1,,48,94,False,False,False,False,False,False,9,0,0,11758,8502,3256,1,17,,,,pytorch
161583,open,Memory pool support for Caching Host Allocator.,galv,"Includes basic support for stream capture to a cuda graph of pinned host memory to the memory pool.

We need a concept of memory pools in order to support a few things:

1. Support of torch.Tensor.pin_memory() during cuda stream capture
2. Custom allocators, which have a few niche use cases described here: https://github.com/pytorch/pytorch/issues/159906#issuecomment-3190768836

Note that (1) is most important right now, and therefore I have not added support for (2) at this time.

https://github.com/pytorch/pytorch/pull/146924 was my original attempt at this several months ago, where I thought that the major problem was how to implement a graph search algorithm inside the cuda graph to detect whether a given call to record_event() could be killed. In hindsight, this is not important in almost all use cases of interest today. In addition, this implementation also prevented me from recycling memory across seaprate cuda graphs sharing a memory pool (oops). Finally, it seems that there is preliminary evidence from https://github.com/pytorch/pytorch/pull/158352#issuecomment-3215947565 that this graph search on the cuda graph can increase stream capture time substantially. I'm sure that is solvable, but I don't want to hold off on this. Instead, I use the current approach in CUDACachingAllocator.cpp: If a block is used on only one stream, and that stream is the stream that allocated it, then it can be reused. Otherwise, it can never be reused.

The initial use case of this is for offloading cached activations from the forward pass to the host, in order to reduce device memory pressure. Running these training workloads with cuda graphs allows them to run a few percentage points faster. Note that this use case does not ever recycle memory, since all allocations happen in the forward pass, and all frees happen in the backward pass.

@guangyey please give me your feedback on this. I had a choice between making breaking changes to CachingHostAllocatorImpl, which will break your XPU backend and potentially anyone else using your interface (is anyone?), or making a duplicate of it. I did the latter, since I needed to add a new field `was_allocated_during_stream_capture_ ` to the Block template argument. This field needs to be inspected during the allocate() routine, in a way that cannot. Furthermore, I think that keeping implementation separate is good for when I eventually add support for reusing blocks during stream capture (see https://github.com/pytorch/pytorch/pull/146924/files#diff-5d3beb56bf9b6f380f91d5f6f063480ce2e14ca15c415d59d153436018089223R396-R404 for details on the algorithm), which will require two things: (1) usage of cuda graph APIs that exist in cuda but not sycl and (2) eagerly creating events when record_event() is called (which does not fit into the current implementation of CachingHostAllocatorImpl and its virtual methods at all).",2025-08-27 01:04:48+00:00,2025-08-30T07:06:00Z,,False,11,0,2,1252,17,10,11,,47,2892,False,False,False,False,False,False,10,10,12826,121700,62293,59407,1,2,3.0,11.0,2025-08-27T02:14:31Z,pytorch
161582,closed,[AOTI-FX] Enhance launch grid FloorDiv replacement using sympy.together. ,blaine-rister,"# Feature
2d launch grids with dynamic shapes can contain sympy expressions like `floor(x / 128 + y / 128)`. This breaks the dynamic shapes tracer which only supports `FloorDiv`, and not `floor`.  To handle this case, call `sympy.together` prior to pattern matching to convert this to `floor((x + y) / 128)`. Then, we can recognize the pattern and map it to `FloorDiv(x + y, 128)`.

# Test plan
Added a custom Triton test exposing this. The test calls a 2d autotuned kernel with dynamic shapes.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-27 01:03:57+00:00,2025-08-27T21:32:33Z,,False,6,0,5,40,4,2,6,2025-08-27 21:31:31+00:00,73,697,False,False,True,False,False,False,2,5,1447,36925,26189,10736,1,5,3.0,5.0,2025-08-27T15:02:32Z,pytorch
161578,open,Add failing tests for supportible dtensor view ops,wconstab,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161578



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @d4l3k @pragupta",2025-08-27 01:00:40+00:00,2025-08-27T14:09:54Z,,False,1,3,1,35,0,1,4,,50,162,False,False,False,False,False,False,1,0,0,35,35,0,1,1,2.0,0.0,2025-08-27T04:48:13Z,pytorch
161577,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

Pull-Request: https://github.com/pytorch/pytorch/pull/161558",2025-08-27 00:59:50+00:00,2025-08-27T15:22:32Z,,False,1,0,1,313,36,4,1,2025-08-27 15:22:32+00:00,53,154,False,False,False,False,False,False,4,0,0,349,313,36,1,1,,,,pytorch
161576,closed,Allow parallel start NUMA binding,pdesupinski,"# Context
In #161183, we added NUMA-binding support for `Callable` entrypoints to `elastic_launch`.

However, we would raise an exception if the subprocesses would be spawned in parallel via `ThreadPoolExecutor`, which is an option configurable via the `TORCH_MP_PARALLEL_START` environment variable (see diff).

The logic here was that `os.sched_setaffinity`, which we used to set CPU affinities, is [per process](https://docs.python.org/3/library/os.html#os.sched_setaffinity), so there could be a race condition during a parallel start:

> Restrict the process with PID pid (or the current process if zero) to a set of CPUs. mask is an iterable of integers representing the set of CPUs to which the process should be restricted.

But on further reading, the Linux docs say [`sched_setaffinity` is per *thread*.](https://man7.org/linux/man-pages/man2/sched_setaffinity.2.html) As it turns out, the Python doc is a misnomer.

I [verified that `sched_setaffinity` only affects the calling thread, not the entire calling process.](https://gist.github.com/pdesupinski/7e2de3cbe5bb48d489f257b83ccddf07)

The upshot is that we actually *can* safely use the inheritance trick from #161183 even with parallel start, since the setting will be inherited from the calling thread, and `os.sched_setaffinity` only affects the calling thread.

# This PR
Remove restrictions against parallel start for NUMA binding.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-27 00:12:48+00:00,2025-08-28T01:17:05Z,,False,6,0,3,25,72,6,6,2025-08-28 01:16:02+00:00,33,1692,False,False,False,True,False,False,6,1,498,97,25,72,1,3,2.0,2.0,2025-08-27T01:02:55Z,pytorch
161575,closed,Add MTIA to floor_divide op,trirpi,"Summary: Missed file in op registration resulting in fallback during test

Reviewed By: andyanwang, srsuryadev

Differential Revision: D81085615




cc @egienvalue",2025-08-27 00:11:11+00:00,2025-08-29T20:40:35Z,,False,15,0,1,1,1,1,15,2025-08-29 20:39:33+00:00,27,163,False,False,False,False,False,False,1,7,1297,2,1,1,1,1,4.0,8.0,2025-08-27T00:16:32Z,pytorch
161574,closed,[DTensor] Wrap sharding prop error with contextual exception,wconstab,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161574

Mainly, this helps tell the user more info about the operator that
failed to run if it fails during sharding propagation.

Previously, only this exception would be raised:
```
RuntimeError: ('Attempted to flatten sharded dimension 1, ', 'but only the leftmost dim of a Flatten can be sharded.')
```

Now you get both the above exception as well as

```
The above exception was the direct cause of the following exception:
RuntimeError: Sharding propagation failed for Op(op=aten.view.default, args_schema=Spec((Replicate(), Shard(dim=0), Shard(dim=1), Shard(dim=2)) on (8, 8, 4)), [64, 4] @ mesh: (1, 2, 2, 2))
```

<stacktrace omitted>
<details><summary>detailed error</summary>

```
======================================================================
ERROR: test_linear (__main__.TestDTensor)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_distributed.py"", line 668, in wrapper
    self._join_processes(fn)
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_distributed.py"", line 932, in _join_processes
    self._check_return_codes(fn, elapsed_time)
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_distributed.py"", line 972, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 4 exited with error code 10 and exception:
Traceback (most recent call last):
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_dispatch.py"", line 150, in dispatch
    self.sharding_propagator.propagate(op_info)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 309, in propagate
    OutputSharding, self.propagate_op_sharding(op_info.schema)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 45, in __call__
    return self.cache(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_sharding_prop.py"", line 329, in propagate_op_sharding_non_cached
    op_strategy = self.op_strategy_funcs[op_schema.op](strategy_schema)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_ops/_view_ops.py"", line 673, in reshape_strategy
    input_tgt_placements, output_placements = propagate_shape_and_sharding(
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_ops/_view_ops.py"", line 601, in propagate_shape_and_sharding
    in_dim = get_in_dim_to_shard(cmd)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_ops/_view_ops.py"", line 537, in get_in_dim_to_shard
    raise RuntimeError(
RuntimeError: ('Attempted to flatten sharded dimension 1, ', 'but only the leftmost dim of a Flatten can be sharded.')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_distributed.py"", line 816, in run_test
    getattr(self, test_name)()
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_distributed.py"", line 670, in wrapper
    fn()
  File ""/data/users/whc/pytorch/torch/testing/_internal/common_utils.py"", line 3224, in wrapper
    method(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/testing/_internal/distributed/_tensor/common_dtensor.py"", line 490, in wrapper
    raise e
  File ""/data/users/whc/pytorch/torch/testing/_internal/distributed/_tensor/common_dtensor.py"", line 487, in wrapper
    func(self, *args, **kwargs)  # type: ignore[misc]
  File ""/data/users/whc/pytorch/test.py"", line 60, in test_linear
    print(""results: "", distributed_linear(distributed_input))
  File ""/data/users/whc/pytorch/torch/nn/modules/module.py"", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/nn/modules/module.py"", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/nn/modules/linear.py"", line 134, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/data/users/whc/pytorch/torch/_compile.py"", line 53, in inner
    return disable_fn(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/_dynamo/eval_frame.py"", line 1005, in _fn
    return fn(*args, **kwargs)
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_api.py"", line 358, in __torch_dispatch__
    return DTensor._op_dispatcher.dispatch(
  File ""/data/users/whc/pytorch/torch/distributed/tensor/_dispatch.py"", line 163, in dispatch
    raise RuntimeError(
RuntimeError: Sharding propagation failed for Op(op=aten.view.default, args_schema=Spec((Replicate(), Shard(dim=0), Shard(dim=1), Shard(dim=2)) on (8, 8, 4)), [64, 4] @ mesh: (1, 2, 2, 2))
```
</details>

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @d4l3k @pragupta",2025-08-26 23:56:29+00:00,2025-08-28T15:57:22Z,,False,10,0,4,8,10,3,10,2025-08-28 15:56:17+00:00,60,4808,False,False,False,False,False,False,3,9,2450,17466,11591,5875,1,4,4.0,11.0,2025-08-27T00:10:47Z,pytorch
161573,closed,[ez] Improve formatting in error messages for dynamic shapes,justinchuby,"Show the repr of `dim` to make the message more clear. Example: before `but got batch instead`, after `but got ""batch"" instead`

cc @angelayi @tugsbayasgalan 
",2025-08-26 23:47:32+00:00,2025-08-29T02:28:03Z,,False,6,0,1,2,2,1,6,2025-08-28 23:53:01+00:00,60,159,False,False,False,False,True,False,1,5,1720,4,2,2,1,1,3.0,5.0,2025-08-28T23:37:03Z,pytorch
161572,closed,Update Kineto submodule,sraikund16,"Differential Revision: D81087601
",2025-08-26 23:37:59+00:00,2025-09-04T22:35:06Z,,False,23,0,2,1,1,1,23,2025-09-04 22:35:06+00:00,23,33,False,False,False,False,False,False,1,16,3899,4,2,2,1,2,5.0,16.0,2025-08-27T00:37:10Z,pytorch
161571,closed,[MTIA] Add MTIA dispatch for kernel foreach_maximum(Add D80022242 back),DoubleBiao,"Summary: dispatch MTIA to function foreach_tensor_maximum_scalar_kernel_mtia_

Test Plan:
CI

Rollback Plan:

Differential Revision: D81086607




cc @egienvalue",2025-08-26 23:23:16+00:00,2025-09-19T05:58:17Z,,False,16,0,1,1,0,1,16,2025-09-19 05:57:12+00:00,71,161,False,False,False,False,False,False,1,5,808,1,1,0,1,1,4.0,6.0,2025-08-26T23:36:11Z,pytorch
161570,closed,[benchmarks] Skip mobilenetv3_large_100 in CI for accuracy,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161570

To keep the CI green - https://github.com/pytorch/pytorch/issues/161419

Its unclear if this is a real failure. And debugging it is non trivial.
Skipping for now to keep the CI greenst

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 23:13:13+00:00,2025-08-27T03:45:10Z,,False,6,0,1,2,0,1,6,2025-08-27 03:44:07+00:00,58,450,False,True,False,False,False,False,1,5,2269,2,2,0,1,1,4.0,5.0,2025-08-26T23:16:29Z,pytorch
161568,closed,[dynamo] Correctly track mutation class source for MutableMappingVariable,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161568

Fixes https://github.com/pytorch/pytorch/issues/161505

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 22:59:13+00:00,2025-08-27T21:48:24Z,,False,3,3,3,19,11,3,6,2025-08-27 21:47:20+00:00,73,320,False,True,False,False,False,False,3,2,518,38,23,15,1,3,5.0,3.0,2025-08-26T23:03:18Z,pytorch
161567,closed,"Revert ""[Dynamo] Allow inlining into AO quantization modules (#152934)""",mlazos,"This reverts commit 20e2ca3e29ce9eb33eef17db077696222c175764.

Fixes https://github.com/pytorch/pytorch/issues/157434



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 22:51:42+00:00,2025-08-27T03:34:09Z,,False,3,0,1,0,16,2,3,2025-08-27 03:33:07+00:00,71,291,False,True,False,False,False,False,2,2,493,16,0,16,1,1,3.0,2.0,2025-08-26T23:05:47Z,pytorch
161565,closed,[1/2]Add summary report for vllm build,yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161585
* __->__ #161565


Demo Run
https://github.com/pytorch/pytorch/actions/runs/17259533323?pr=161565

<img width=""1538"" height=""720"" alt=""image"" src=""https://github.com/user-attachments/assets/64f6d7b4-cac6-4c12-863c-b15514bb8810"" />
",2025-08-26 22:42:50+00:00,2025-08-28T05:27:02Z,,False,6,6,12,232,9,8,12,2025-08-28 05:25:58+00:00,38,317,False,False,False,False,False,False,8,5,1591,13053,9012,4041,1,12,3.0,5.0,2025-08-27T18:15:13Z,pytorch
161564,open,fold to mm when row dimentions is small,yushangdi,"Summary:
Fix https://github.com/pytorch/pytorch/issues/159346

When we have a matmul like [M, B, K].transpose(0, 1) x [K, N], we should fold when B is small, and M, K, N are large. This could happen when we use MultiheadAttention with batch_first=True.


Folding to mm can speed up`F.linear(x, weight, bias)` by a lot.

Some benchmark numbers (all without max-autotune). If we use max-autotune, I think it'll only give folding to mm more benefits, because bmm triton template is limited to int32 indexing, so we have to fall back to aten bmm when BSxMxNxK is large.

Float16:

| BS |  M   |   N   |   K   | time_fold | time_no_fold | speedup  |
|  4 | 512  | 8192  | 8192  |   1.274   |    41.216    | 32.3516  |
| 64 | 512  | 2048  | 2048  |   1.618   |    1.751     | 1.0822   |
|  4 | 1024 | 2048  | 2048  |   0.576   |    2.253     | 3.9115   |


Float32:

| BS |  M   |   N   |   K   | time_fold | time_no_fold | speedup  |
|  4 | 512  | 8192  | 2048  |   4.059   |    22.967    | 5.6583   |
| 64 | 512  | 2048  | 2048  |  15.763   |    29.486    | 1.8706   |
|  4 | 1024 | 2048  | 2048  |   2.272   |    9.124     | 4.0158   |

You can find more benchmarking results in https://docs.google.com/spreadsheets/d/1d79g3CCE9Tn59JaAbxiNVYfpRxNyVDbYeWGVurqOses/edit?gid=1040716098#gid=1040716098.

In this PR, I set the numbers to be quite conservative, i.e. we only fold to mm if we're sure we can get performance gain from folding. This is to prevent any perf regression. But I think we could be more aggressive in our folding decision based on the benchmarking result.

Another potential work is to enable folding when the batch size is a dynamic symbol. Currently, we only fold when all shape sizes are integers. This means it's hard to take advantage of this when shape size are symints.

```
        # shape_env = s.node.shape_env
        # if shape_env is None:
        #     return False
        # return shape_env.var_to_range[s.node.expr].issubset(ValueRanges(0, 32))
```

Finally, note that in this implementation, we avoid any copy by using transpose. However, this means that the stride of the output after decomposition could be different from the output of the original op.

Test Plan:
```
buck run mode/dev-nosan fbcode//caffe2/test/inductor:test_inductor_cuda -- -r  test_folding_bmm_to_mm
```

Rollback Plan:

Differential Revision: D80385541




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-26 22:41:56+00:00,2025-09-16T22:00:46Z,,False,8,0,1,99,37,3,8,,39,2573,False,True,False,True,False,False,3,5,1360,136,99,37,1,1,3.0,6.0,2025-08-27T00:31:00Z,pytorch
161561,closed,async_compile: Fix the wait method to actually wait,c00w,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161561
* #161452

This method never triggered. It's used in 2 tests and they pass, so no serious
concern.

Note that I did introduce and fix a latent bug, which is if we called
shutdown_compile_workers, jobs would crash with this change due to ready_future
being finished if we called wait.

However we only call wait in tests so that bug is fine.

The other behaviour, is that if you called shutdown, I believe we may
potentially block on your first triton compile after that, until the pool was
ready. This should correctly switch to direct mode, until the pool is ready on
later warmups.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 22:27:24+00:00,2025-08-27T21:36:40Z,,False,3,0,1,13,4,2,3,2025-08-27 21:35:36+00:00,51,879,False,True,False,False,False,False,2,2,493,17,13,4,1,1,3.0,2.0,2025-08-26T23:29:09Z,pytorch
161560,closed,Check commit order,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161560
* #161637

",2025-08-26 22:27:08+00:00,2025-08-29T16:24:07Z,,False,3,3,12,302,3,2,6,2025-08-29 16:23:01+00:00,18,104,False,False,False,False,False,False,2,2,781,710,509,201,1,12,3.0,2.0,2025-08-27T15:42:37Z,pytorch
161559,open,[dynamic shapes] remove _maybe_guard_rel warnings,pianpwk,"Fixes #161051

stops warnings arising from `torch._check(sym_or(...))` calls

The issue happens when:
- SDPA calls expand
- expand calls this check, signifying either expand or no-op semantics
https://github.com/pytorch/pytorch/blob/47ecd2042f2f99368cd2d60731d23f4437eecd1d/torch/_refs/__init__.py#L3051-L3052
- The `sym_or` is passed to `_maybe_guard_rel` to process any upper/lower bound implications for axioms we know are True
- `_maybe_guard_rel` was extended to handle `And` expressions as well as relations, so users could write `torch._check(sym_and(a >= 0, a <= N, ...))` easily. For other types, we raised this warning:
https://github.com/pytorch/pytorch/blob/47ecd2042f2f99368cd2d60731d23f4437eecd1d/torch/fx/experimental/symbolic_shapes.py#L6845-L6852

The change only applies `_maybe_guard_rel` to `And` and relation expressions, not calling it for the `sym_or` case

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-26 22:23:17+00:00,2025-08-27T20:10:19Z,,False,1,0,1,6,7,1,1,,49,937,False,True,False,False,False,False,1,0,72,13,6,7,1,1,1.0,1.0,2025-08-27T20:10:18Z,pytorch
161558,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161560
* #161637
* __->__ #161558

",2025-08-26 22:22:07+00:00,2025-08-27T19:54:35Z,,False,4,1,3,39,36,3,5,2025-08-27 19:53:30+00:00,53,114,False,False,False,False,False,False,3,3,829,77,40,37,1,3,4.0,3.0,2025-08-27T18:02:25Z,pytorch
161557,open,[test][scan] refactor inductor test and prepare for adding bw tests,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162754
* #161732
* #162025
* #161808
* #161664
* __->__ #161557



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 22:19:23+00:00,2025-09-19T12:12:57Z,,False,1,0,4,44,15,1,1,,67,347,False,False,False,False,False,True,1,0,0,105939,68206,37733,1,4,1.0,0.0,2025-09-19T12:12:57Z,pytorch
161556,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:17:30+00:00,2025-08-26T22:18:33Z,,False,1,0,1,15,13,2,1,2025-08-26 22:18:33+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161555,open,"[dynamo, 3.14] compile actual code in C dynamo",williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163818
* #163796
* #163292
* #163191
* #163110
* #163109
* #163009
* #161839
* __->__ #161555
* #161838

No 3.14 CI tests enabled yet, but this was enough to get Dynamo compiling locally and Python Dynamo is at least being called.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 22:15:59+00:00,2025-09-25T00:23:21Z,,False,1,0,6,201,35,4,1,,46,481,False,False,False,False,False,False,4,0,0,126293,83571,42722,1,6,,,,pytorch
161554,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:13:31+00:00,2025-08-26T22:18:41Z,,False,1,0,1,15,13,2,1,2025-08-26 22:18:41+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161553,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:11:43+00:00,2025-08-26T22:19:03Z,,False,1,0,1,15,13,2,1,2025-08-26 22:19:03+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161552,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:10:26+00:00,2025-08-26T22:18:59Z,,False,1,0,1,15,13,2,1,2025-08-26 22:18:59+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161551,open,Tianren/test,tianrengao,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 22:09:04+00:00,2025-08-27T02:28:25Z,,False,2,0,13,80,17,6,2,,12,223,False,True,False,False,False,False,6,0,0,423,243,180,1,13,,,,pytorch
161550,closed,[dynamo] Fix graph break registry loading in fbcode,williamwen42,"Summary: Add `torch/_dynamo/graph_break_registry.json` as an internal dependency. Minor related fixes.

Test Plan:
Test on OSS.

Rollback Plan:

Differential Revision: D81078973




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 22:06:27+00:00,2025-08-27T19:26:22Z,,False,6,0,1,33,20,2,6,2025-08-27 19:25:18+00:00,51,352,False,True,False,False,False,False,2,1,476,53,33,20,1,1,3.0,1.0,2025-08-27T14:00:45Z,pytorch
161549,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:06:03+00:00,2025-08-26T22:18:50Z,,False,1,0,1,15,13,2,1,2025-08-26 22:18:50+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161548,closed,Ensure the comment id is always passed in to trymerge,ZainRizvi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 22:03:50+00:00,2025-08-26T22:05:15Z,,False,1,0,1,15,13,2,1,2025-08-26 22:05:14+00:00,53,94,False,False,False,False,False,False,2,0,0,28,15,13,1,1,,,,pytorch
161547,open,[Flop Counter] Support tracing a decomposed custom triton kernel in torch op,Lucaskabela,"Add a new API for registering triton kernels, and provide an example of how to escape hatch triton op to leverage this API

Context: For some custom `torch.library.op`, the operator decomposes into one or more custom kernels.  To date, there has not been a good system for having these counted by FlopCounterMode, as the function will decompose to the wrapped kernels, and then check the registry.

To support this use case, we add a custom API, `register_flop_formula_for_triton_kernel` and demonstrate how it is used with `register_torch_dispatch`.  Currently, this is done manually, but as a followup, we will support inbuilt support for this.

### Unit Test
```
python test/test_flop_counter.py TestFlopCounter.test_flop_counter_custom_triton
```

```
.
----------------------------------------------------------------------
Ran 1 test in 0.005s
```
",2025-08-26 21:58:00+00:00,2025-09-08T21:20:03Z,,False,3,5,11,136,10,3,8,,76,854,False,False,False,False,False,False,3,1,167,652,389,263,1,11,2.0,2.0,2025-08-26T23:02:42Z,pytorch
161546,closed,[ONNX] Remove private members from torch.onnx,justinchuby,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161546
* #161449
* #161323

Remove import of two functions

- _run_symbolic_function
- _run_symbolic_method

to the `torch.onnx` namespace.

Signed-off-by: Justin Chu <justinchuby@users.noreply.github.com>


cc @titaiwangms",2025-08-26 21:42:47+00:00,2025-09-02T16:32:34Z,,False,6,2,8,5,6,2,8,2025-09-02 16:31:27+00:00,45,309,False,False,False,False,False,False,2,5,2378,15175,9395,5780,1,8,3.0,5.0,2025-08-28T22:35:17Z,pytorch
161545,closed,Add test coverage to tf32 in max autotune mm configs,exclamaforte,"Add a test to make sure that the configs are using the correct setting of tf32 to prevent regression.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 21:22:10+00:00,2025-09-01T20:46:54Z,,False,12,0,4,36,2,1,12,2025-09-01 20:46:54+00:00,52,305,False,False,False,False,False,False,1,11,3223,54,44,10,1,4,4.0,11.0,2025-08-26T21:51:30Z,pytorch
161544,closed,[dynamo] allow resume functions to have name in both freevars and varnames,williamwen42,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161544

fixes https://github.com/pytorch/pytorch/issues/161542

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela

Differential Revision: [D81073109](https://our.internmc.facebook.com/intern/diff/D81073109)",2025-08-26 21:20:41+00:00,2025-08-27T00:26:21Z,,False,4,0,1,19,5,2,4,2025-08-27 00:25:19+00:00,74,413,False,True,False,False,False,False,2,3,656,24,19,5,1,1,4.0,4.0,2025-08-26T21:22:22Z,pytorch
161543,open,Design: Auto-running code example outputs in PyTorch documentation (issue #6662)Design: Auto-running code example outputs in PyTorch documentation (issue #6662)Docs/gen example autotest,DHANUSHRAJA22,"## Summary

This PR introduces a comprehensive design proposal for automatically generating and validating code example outputs in PyTorch documentation, addressing issue #6662. The design outlines a system to ensure documentation examples produce expected outputs, eliminating discrepancies between code and results.

## Design Overview

**Motivation**: Automatically generated and validated code outputs provide significant benefits:
- **Accuracy**: Ensures documentation examples produce expected outputs
- **Freshness**: Automatically updates outputs when API changes  
- **Trust**: Users gain confidence in examples that demonstrably work
- **Maintenance**: Reduces manual effort in keeping examples synchronized
- **Quality**: Catches breaking changes early in the development cycle

**Proposed Solution**: A minimal prototype consisting of:
1. **Code Block Extractor**: Extracts executable code blocks from documentation files
2. **Example Runner**: Executes code blocks with timeout and error handling
3. **Output Verifier**: Compares expected vs actual outputs with tolerance

**Integration Options**:
- Option A: Sphinx Extension for build-time execution
- Option B: CI Job Integration for validation in pull requests
- Option C: Custom nbval-style runner for notebook-like execution

**Risk Assessment**: Addresses technical risks (build time increase, environment dependencies, flaky tests) and process risks (contributor adoption, maintenance overhead) with specific mitigation strategies.

**Implementation Plan**: Phased approach over 8 weeks:
- Phase 1: Prototype (2 weeks)
- Phase 2: Integration (4 weeks) 
- Phase 3: Deployment (2 weeks)

**Success Metrics**: 
- Technical: >80% coverage, <5% false positive rate, <20% build time increase
- Quality: Reduced output mismatches, fewer bug reports, improved maintainer efficiency

## Request for Feedback

We seek community input on:
- Architecture and implementation approach preferences
- Integration points with existing PyTorch documentation infrastructure
- Performance requirements and resource constraints
- Error handling and fallback strategies
- Contributor workflow implications
- Testing and validation strategies

## Next Steps

All development work will remain exclusively on the `docs/gen-example-autotest` branch until core reviewer approval. Immediate actions include implementing a core prototype, testing with sample examples, establishing performance baselines, and creating documentation.

**References issue #6662 for the original feature request.**Fixes #ISSUE_NUMBER
",2025-08-26 20:54:10+00:00,2025-09-04T15:22:46Z,,False,3,0,2,267,0,1,3,,185,2556,False,True,True,True,True,False,1,1,171,723,495,228,1,2,1.0,1.0,2025-09-04T15:22:35Z,pytorch
161541,closed,Support Triton kernels in SAC region,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160883
* #160888
* __->__ #161541

SAC interaction with triton kernel:
- In eager, triton ops are not dispatchable, and so it is always ignored by SAC,  i.e., always recomputed.
- In compile, although we wrap triton kernels into HOPs, allowing us to intercept them, we still recompute by default rather than save by default, so that compile maintains the invariant of using less memory than eager.
- If you want to do something else (e.g. save the output of your triton kernel) you should wrap it in a custom op.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @fegin ",2025-08-26 20:53:05+00:00,2025-08-28T21:16:32Z,,False,16,2,6,94,1,2,18,2025-08-28 21:15:28+00:00,36,771,False,False,False,False,False,False,2,14,3615,39467,28418,11049,1,6,7.0,14.0,2025-08-26T21:14:20Z,pytorch
161540,closed,ROCm: Enable overload tests from test_matmul_cuda,jagadish-amd,"This patch enables hipblaslt backend tests for test_mm_bmm_dtype_overload and test_addmm_baddmm_dtype_overload.
Tests were disabled as part of #150812
Rocblas backend tests are not enabled yet, WIP.

Test command
PYTORCH_TEST_WITH_ROCM=1 pytest test/test_matmul_cuda.py -k 'test_mm_bmm_dtype_overload' -v PYTORCH_TEST_WITH_ROCM=1 pytest test/test_matmul_cuda.py -k 'test_addmm_baddmm_dtype_overload' -v



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-26 20:48:59+00:00,2025-09-16T05:06:15Z,,False,6,0,1,6,16,2,6,2025-09-02 16:27:46+00:00,49,522,False,False,False,False,False,False,2,5,891,22,6,16,1,1,3.0,5.0,2025-08-26T20:49:41Z,pytorch
161539,closed,Fix sort doc error,cleonard530,"Fixes #129298. Updated torch.sort documentation so that the 'stable' parameter is a Keyword Argument. This is how it's implemented in PyTorch.  
@malfet",2025-08-26 20:35:35+00:00,2025-08-27T17:02:57Z,,False,3,0,2,3,3,1,3,2025-08-27 17:01:56+00:00,18,152,False,True,False,True,False,False,1,2,706,3314,1902,1412,1,2,2.0,3.0,2025-08-27T14:13:24Z,pytorch
161538,closed,[TorchScript] ProfilingExecutor - RemoveProfileNodesAndSpecializeTypes None handling,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161538

ProfilingGraphExecutor works like this:
1. do some unrelated JIT optimizations
2. Add profiling nodes to collect JIT information like tensor dtypes and shapes
3. Do some more unrelated JIT optimizations
4. Remove the profiling nodes and extract the tensor info, and then use the JIT tensor info to do optimizations.

This PR is intended to fix a bug in Step 4, where the profiling nodes were removed. It was previously assumed that all the things that were profiled were either Tensors or Optional[Tensor]s - otherwise, step 2 would not have introduced a profiling node.

However, we saw a case where step 3 would remove replace Optional[Tensor] inputs with `None` inputs (e.g. if a conditional that returned a Tensor or a None could be statically known to only follow the `None` branch).

To fix this, we essentially just modify the RemoveProfileNodesAndSpecializeTypes assert so that it accepts Tensors, Optional[Tensor]s, or None (the new part).

Note that this issue is probably somewhat uncommon (maybe why we didn't see it for the first 4 years that this code existed). I expect that, typically, any time that step 3 would convert `Optional[Tensor] -> None`, step 1 would have already done that. So it's difficult to reproduce in an end-to-end TorchScript workload.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel

Differential Revision: [D81068172](https://our.internmc.facebook.com/intern/diff/D81068172)",2025-08-26 20:29:45+00:00,2025-08-27T23:13:22Z,,False,4,0,1,56,5,3,4,2025-08-27 23:12:18+00:00,84,1507,False,True,False,False,False,False,3,2,640,61,56,5,1,1,3.0,3.0,2025-08-26T20:32:00Z,pytorch
161537,closed,[VLLM][FLASHINFER UPDATE],yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161585
* #161565
* __->__ #161537

VLLM build x torch fails due to flashinfer build fail, detected that vllm team recently changed the point to flashinfer
",2025-08-26 20:20:38+00:00,2025-08-27T17:42:36Z,,False,17,0,9,9,4,3,17,2025-08-27 17:41:29+00:00,25,234,False,False,False,False,False,False,3,16,5011,3441,1908,1533,1,9,3.0,16.0,2025-08-27T01:34:32Z,pytorch
161536,closed,[BE] Consolidate inductor benchmark Docker images and rename jobs,huydhn,"We have 4 different version of inductor benchmark Docker images used in CI at the moment:

1. `pytorch-linux-jammy-cuda12.8-cudnn9-py3-gcc9-inductor-benchmarks` is used by almost all inductor jobs including nightly benchmark
2. `pytorch-linux-jammy-cuda12.8-cudnn9-py3.12-gcc9-inductor-benchmarks` runs inductor unit tests with python 3.12
3. `pytorch-linux-jammy-cuda12.8-cudnn9-py3.13-gcc9-inductor-benchmarks` runs inductor unit tests with python 3.13
4. `pytorch-linux-jammy-py3-gcc11-inductor-benchmarks` runs inductor unit tests on CPU

My proposal here is to clean up (2) and (3) and to keep (1) under the same setup from https://ghcr.io/pytorch/torchbench.  Simplicity is the key here as inductor workflows are getting more and more complex:
1. Unit tests for Python variant like 3.12 and 3.13 were useful when they were first added to CI.  They are much less useful now.  [Flambeau](https://hud.pytorch.org/flambeau/s/3876ec7b-43f0-42c6-bfbf-899035e5bb77) shows a 0.97 correlation between them.  And we are also moving to 3.14 nowadays.  I want to choose 3.12 for (1), but will do this separately.  This is also what TorchBench and vLLM are using on CI.
1. We are gradually cleaning up 3.9 on CI https://github.com/pytorch/pytorch/issues/161167

Another BE change here is to rename the jobs various inductor workflows because I think names like `linux-jammy-cuda12_8-py3_10-gcc9-inductor-build` is too long and confusing to look at, better just use human-friendly names like `inductor-build`.  Other information is already spelled out in the build environment.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 19:54:48+00:00,2025-09-01T19:08:15Z,,False,12,0,17,146,278,13,12,2025-09-01 19:07:11+00:00,65,1743,False,False,False,True,False,False,13,11,3620,20386,14326,6060,1,17,3.0,11.0,2025-08-27T01:14:10Z,pytorch
161535,open,Fix dev container configuration,pbezglasny,"Fix the configuration of dev container:
- update version of CUDA to match with NVIDIA repo
- Set working python to venv, set owner of the venv folder to vscode user(now there is an issue that vscode user cannot install build packages to system interpreter)

Tested working with CPU and CUDA environments with a built dev container image from scratch.
",2025-08-26 19:53:17+00:00,2025-09-02T16:27:02Z,,False,4,0,1,7,5,2,4,,31,351,False,True,False,False,False,False,2,2,101,12,7,5,1,1,3.0,3.0,2025-08-26T19:55:59Z,pytorch
161534,open,[inductor][addmm] incorporate into new get_mm_configs properly,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161534

# why

- addmm aten running with an expanded version of bias vs the regular
  bias sometimes causes numerics differences
- to avoid this for now, we make addmm aten use inp vs inp_expanded
  depending on if we're in max-autotune or not, matching the previous
  logic

# what

- pass unexpanded bias (inp)
- let template (heuristics) that it to be expanded (ATen in not max-autotune, Triton always) expand it

# testing

```
python3 -bb -m pytest test/inductor/test_aot_inductor.py::AOTInductorTestABICompatibleCpu::test_aoti_debug_printer_codegen_cpu
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520581](https://our.internmc.facebook.com/intern/diff/D81520581)",2025-08-26 19:48:54+00:00,2025-09-18T17:13:23Z,,False,13,3,33,101,34,6,16,,62,930,False,True,False,False,False,False,6,12,2452,129177,81228,47949,1,29,3.0,14.0,2025-09-02T20:28:31Z,pytorch
161533,closed,[4/N][SymmMem] Add `get_remote_tensor` + move up `get_buffer` and `get_signal_pad`,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):

`get_remote_tensor `: return a symmetric tensor given a peer rank.

The difference between `get_buffer` API and `get_remote_tensor` API:
- the former accepts an offset, whereas the latter doesn't
- the latter returns a symmetric tensor at `hdl.offset` on `peer`.

As a refactorization, this PR also moves the implementation of `get_buffer` and `get_signal_pad` to the `SymmetricMemory` level as their code is common to all backends.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-26 19:33:26+00:00,2025-09-01T07:03:14Z,,False,12,3,7,171,239,9,15,2025-09-01 07:02:09+00:00,82,587,False,False,False,False,False,True,9,11,3611,637,303,334,1,7,4.0,12.0,2025-08-26T21:06:17Z,pytorch
161532,closed,[3/N][SymmMem] Expose offset field from handle,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161533
* __->__ #161532
* #161471

As titled, so that kernels relying on direct pointers can use base address and `hdl.offset` to access remote memory.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-26 19:33:22+00:00,2025-08-31T18:10:06Z,,False,13,0,3,32,0,4,13,2025-08-31 18:09:02+00:00,46,308,False,False,False,False,False,False,4,12,3091,95,82,13,1,3,4.0,12.0,2025-08-26T20:21:49Z,pytorch
161531,open,Fix silent ImportError handling in torch._dynamo.utils to preserve critical error messages,haochengxia,"## Problem
Previously, the exception handling in `torch/_dynamo/utils.py` used a bare `except ImportError: pass` which silently suppressed **all** import errors, including critical system-level issues. This made debugging extremely difficult when encountering real import problems.

**Example of hidden error that was previously silenced:**
```
ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.29' not found
```

This type of error indicates missing system dependencies (like incompatible glibc/libstdc++ versions) but was completely hidden from users, making it nearly impossible to diagnose environment issues.

## Root Cause
The original code treated all import failures the same way:
```python
except ImportError:
    pass  # This hides ALL import errors, including critical ones
```

This approach conflates two very different scenarios:
1. **Optional modules not available** (expected, should be handled gracefully)
2. **Critical import failures** (system issues, missing dependencies - should be reported)

## Solution
Implemented proper exception handling that distinguishes between different types of import failures:

```python
except ModuleNotFoundError as e:
    # Optional modules may not be available in all configurations
    logging.getLogger(__name__).debug(""Optional module not found: %s"", e)
except ImportError as e:
    # Re-raise critical import errors (e.g., missing dynamic libraries like cxxabi)
    logging.getLogger(__name__).error(""Critical import error: %s"", e)
    raise
```

## Key Improvements

1. **Preserves critical error information**: System-level import failures (missing .so files, incompatible library versions) are now properly reported with full error messages and tracebacks

2. **Follows PyTorch logging conventions**: 
   - Uses `logging.getLogger(__name__)` instead of `warnings.warn()`
   - Uses appropriate log levels (`debug` vs `error`)
   - Uses structured logging format with `%s` placeholders

3. **Better categorization**:
   - `ModuleNotFoundError`: Truly missing optional modules → logged at debug level
   - `ImportError`: Critical system issues → logged as error and re-raised

4. **Maintains backward compatibility**: Optional module handling still works as expected, but critical errors are no longer silenced

## Impact
- **Debugging**: Users will now see meaningful error messages for system-level import issues
- **Development**: Easier to diagnose environment setup problems
- **Production**: No unnecessary noise (optional module warnings only appear in debug mode)
- **Consistency**: Follows established PyTorch logging patterns used throughout the codebase

## Testing
The change maintains existing functionality for optional imports while ensuring critical import errors are properly propagated. This is a debugging/observability improvement that doesn't change the core logic.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 19:29:39+00:00,2025-09-04T15:20:09Z,,False,4,0,1,7,2,1,4,,90,3024,False,True,False,False,True,False,1,2,269,9,7,2,1,1,2.0,2.0,2025-08-26T19:35:19Z,pytorch
161530,closed,[PGO] skip allowlist logging for empty graphs,pianpwk,"Summary: reduces spurious logging

Test Plan:
test_pgo

Rollback Plan:

Differential Revision: D81060182




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-08-26 19:13:10+00:00,2025-08-28T00:13:20Z,,False,8,0,1,34,2,4,8,2025-08-28 00:12:17+00:00,45,287,False,False,False,False,False,False,4,4,927,36,34,2,1,1,4.0,4.0,2025-08-27T03:04:03Z,pytorch
161529,open,[ROCm] Removed skip annotation to enable test,dwiddows,"Fixes #160768

(This is a first attempt to see if the continuous build passes.) 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-26 19:09:26+00:00,2025-09-22T11:19:42Z,,False,17,0,1,0,1,1,17,,45,297,False,True,False,False,False,False,1,14,2680,1,0,1,1,1,6.0,14.0,2025-08-28T00:03:31Z,pytorch
161528,closed,Fix excess refcounting in ObjLoaderFunc,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161528

expectRef is preferred over expect because it doesn't copy a std::shared_ptr.

Differential Revision: [D81053710](https://our.internmc.facebook.com/intern/diff/D81053710/)

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-26 18:16:13+00:00,2025-09-15T16:06:58Z,,False,5,0,1,10,10,4,5,2025-09-15 16:05:53+00:00,39,314,False,True,False,False,False,False,4,3,518,20,10,10,1,1,3.0,3.0,2025-09-09T19:45:52Z,pytorch
161527,open,Fix Arm64 OSS pytorch build with FBGEMM,mcfi,"Summary:
X-link: https://github.com/pytorch/FBGEMM/pull/4775

Without this change, Arm64 OSS pytorch build with FBGEMM failed with the following error.
Undefined symbols for architecture arm64:
  ""fbgemm::FindMinMax(float const*, float*, float*, long long)"", referenced from:
      at::native::fbgemm_linear_int8_weight_fp32_activation(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, c10::Scalar const&, c10::Scalar const&, at::Tensor const&) in QuantizedLinear.cpp.o
      at::native::fbgemm_linear_quantize_weight(at::Tensor const&) in QuantizedLinear.cpp.o
      PackedConvWeight<2>::apply_dynamic(at::Tensor const&, bool) in qconv_dynamic.cpp.o
      PackedConvWeight<3>::apply_dynamic(at::Tensor const&, bool) in qconv_dynamic.cpp.o
      at::Tensor PackedLinearWeight::apply_dynamic_impl<false>(at::Tensor, bool) in qlinear_dynamic.cpp.o
      at::Tensor PackedLinearWeight::apply_dynamic_impl<true>(at::Tensor, bool) in qlinear_dynamic.cpp.o
ld: symbol(s) not found for architecture arm64

This change fixed the issue by moving FindMinMax's implementation from QuantUtilsAvx2.cc to QuantUtils.cc. FindMinMax is a platform-agnostic function with AVX2-specific optimizations so conceptually it can be put in QuantUtils.cc.

Test Plan:
With this change, Arm64 OSS pytorch built successfully with FBGEMM enabled.

Rollback Plan:

Reviewed By: q10

Differential Revision: D81052327


",2025-08-26 18:03:05+00:00,2025-09-05T06:56:01Z,,False,16,0,1,4,2,1,16,,39,1414,False,True,False,False,False,False,1,9,3217,6,4,2,1,1,4.0,9.0,2025-08-27T18:39:08Z,pytorch
161526,open,[PGO] skip global sources in allowlist,pianpwk,"Summary: found in aps-omnifm_v3_base-20ba2b4465 suggestions

Test Plan:
test_pgo

Rollback Plan:

Differential Revision: D81054193




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 18:02:48+00:00,2025-08-26T22:21:23Z,,False,3,0,1,2,0,1,3,,38,305,False,False,False,False,False,False,1,0,0,2,2,0,1,1,,,,pytorch
161522,closed,Revert D80715960,seemethere,"Summary:
This diff reverts D80715960
(The context such as a Sandcastle job, Task, SEV, etc. was not provided.)

Depends on D80715960

Test Plan:
NA

Rollback Plan:

Differential Revision: D81052652


",2025-08-26 17:43:59+00:00,2025-08-26T19:46:53Z,,False,5,0,1,49,109,8,5,2025-08-26 19:38:27+00:00,16,200,False,False,False,False,False,False,8,1,31,158,49,109,1,1,1.0,1.0,2025-08-26T19:38:27Z,pytorch
161521,closed,[AMD] [Reland] Fix AMD User Defined Kernel Autotune,oniononion36,"Summary: This is a reland of D80285441, fixed the unit test.

Test Plan:
```
buck2 run mode/opt-amd-gpu -m rocm641 -c fbcode.split-dwarf=true -c fbcode.use_link_groups=true -c fbcode.enable_gpu_sections=true //hpc/new/models/feed/benchmark:feed_lower_benchmark -- --load=manifold://ads_storage_fblearner/tree/user/facebook/fblearner/predictor/894698382/0/gpu_lowering/new_input8 --skip-eager --skip-flop-estimation --sync-mode=0 --lower-backend=AOT_INDUCTOR

```
will succeed after this diff.

Rollback Plan:

Differential Revision: D80971224




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 17:16:07+00:00,2025-09-04T08:42:26Z,,False,9,0,1,111,4,3,9,2025-09-04 08:41:23+00:00,51,748,False,True,False,False,False,False,3,1,481,115,111,4,1,1,2.0,2.0,2025-08-27T22:33:41Z,pytorch
161520,closed,[2/n][export] Refactor PT2 Archive weight saving and loading,yiming0416,"Summary:
The saving (serialization) part of PT2 archive weight refactoring.
The loading (deserialization part) has been landed in D80035490

Test Plan:
CI

Rollback Plan:

Differential Revision: D80970931


",2025-08-26 17:15:58+00:00,2025-09-03T20:16:13Z,,False,18,0,1,288,43,4,18,2025-09-03 20:12:53+00:00,60,207,False,False,False,False,False,True,4,11,3586,331,288,43,1,1,4.0,11.0,2025-08-27T16:47:52Z,pytorch
161519,closed,[ROCm][CI] restore test_flex_attention tests,ethanwee1,"Reverts #161450 and targets specific subtests to skip on MI200.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 17:02:21+00:00,2025-08-26T20:46:21Z,,False,3,0,1,5,1,2,3,2025-08-26 19:31:33+00:00,44,380,False,False,False,False,False,False,2,2,907,6,5,1,1,1,2.0,2.0,2025-08-26T17:04:01Z,pytorch
161517,closed,[export] Support complex constant in serde,yiming0416,"Summary:

Fixes #160749

For a model like
```
class M(torch.nn.Module):
    def forward(self, x):
        s = torch.sin(x)
        z = 1j * s
        return z
```
Its graph will be
```
graph():
    %x : [num_users=1] = placeholder[target=x]
    %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%x,), kwargs = {})
    %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%sin, 1j), kwargs = {})
    return (mul,)
```

`1j` will appear as a constant complex argument in the `aten.mul`

Test Plan:
buck2 run mode/dev-nosan caffe2/test:test_export -- -r test_complex_constant

Rollback Plan:

Differential Revision: D80672323


",2025-08-26 16:45:51+00:00,2025-08-29T08:14:27Z,,False,10,0,1,114,7,6,10,2025-08-29 08:13:25+00:00,42,681,False,True,False,False,False,False,6,1,476,121,114,7,1,1,2.0,1.0,2025-08-26T17:38:12Z,pytorch
161514,closed,[reland] [dynamo] Refactor convert_frame.compile_frame to be self contained function. [5/n],zhxchen17,"Summary:
convert_frame.compile_frame used to take a callback transform function which will capture the frame object it has, but the frame information is not passed directly into compile_frame function.

This PR changes the signature of compile_frame so that frame information is directly passed in the function without taking a callback. This makes it easier to build fullgraph capture API on top of compile_frame.

Test Plan:
CI

Rollback Plan:

Differential Revision: D81041296




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 15:08:36+00:00,2025-08-26T19:17:11Z,,False,4,0,1,236,165,5,4,2025-08-26 19:16:08+00:00,91,654,False,False,False,False,False,True,5,1,478,401,236,165,1,1,2.0,1.0,2025-08-26T15:12:10Z,pytorch
161512,open,[ZENDNN] Integrate ZenDNN unary-binary fusions,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* __->__ #161512
* #161495
* #161158
* #161157
* #161156
* #161155

----

- Add zendnn unary-binary fusions support.
- Add a pass in optimize for the unary-binary ops.
- Update meta registration and shim files.

Co-authored-by: Priyansh Jain <Priyansh.Jain2@amd.com>
Co-authored-by: Dinesh Mareedu <Dinesh.Mareedu@amd.com>
Change-Id: Ie5b0e471e5fe777be35f4abd041bcfa47ab3f1ef

[RFC](https://github.com/pytorch/pytorch/issues/150296)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 14:54:47+00:00,2025-09-18T20:03:43Z,,False,8,0,5,432,26,11,8,,46,721,False,False,False,False,False,False,11,5,1230,117004,77837,39167,1,5,2.0,5.0,2025-08-26T16:04:58Z,pytorch
161511,open,Fix index_add for int64 input + zerodim index,manuelcandales,"Fixes #161446 

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161511

",2025-08-26 14:42:06+00:00,2025-08-27T19:01:57Z,,False,6,3,5,18,4,2,9,,45,110,False,True,False,False,False,False,2,5,1487,28,21,7,1,5,4.0,6.0,2025-08-26T16:25:30Z,pytorch
161508,closed,[Dynamo] Fix weakref.proxy error when `torch.compile`,can-gaa-hou,"Fixes #159258

The error occurs when we attempt to create a weak reference from a weak reference proxy.
https://github.com/pytorch/pytorch/blob/e9d42b3880dcdbd823bbdc9370c8b0b3af0ba2e3/torch/_dynamo/guards.py#L2910-L2915

In fact, we shouldn't create a weak reference from another reference or proxy, as it would check in CPython.
https://github.com/python/cpython/blob/f60f8225ed146a8f9b5fbf1eeed3474782127ea8/Objects/weakrefobject.c#L410-L418

However, `__weakrefoffset__` is not equal to **0** when the `guarded_object` is in `weakref.ProxyTypes`, and it will wrongly create a weak reference for the `weakref.ProxyTypes`. I think this could be a bug from CPython, but we can prevent it by adding more weakref type checks (`weakref.ProxyTypes` contains `weakref.ProxyType` and `weakref.CallableProxyType`) here.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 13:48:46+00:00,2025-08-28T22:35:23Z,,False,17,0,2,24,1,2,17,2025-08-28 22:34:21+00:00,53,986,False,True,False,False,False,False,2,15,5104,25,24,1,1,2,5.0,18.0,2025-08-26T18:26:52Z,pytorch
161504,closed,Run WoArm64 CI every 4 hours,iremyux,"Since WoArm64 isn’t part of CI yet, this PR schedules the workflow to increase visibility and insights. It will execute every 4 hours and still support manual runs via the `ciflow/win-arm64` tag.",2025-08-26 13:10:58+00:00,2025-08-27T15:47:41Z,,False,6,0,1,3,0,1,6,2025-08-27 15:46:38+00:00,28,195,False,False,False,False,False,False,1,5,1357,3,3,0,1,1,4.0,5.0,2025-08-26T15:21:23Z,pytorch
161501,closed,Typo correction in variable name inital_grad of Class TestFullyShardG…,vishalgoyal316,"Typo correction in variable name inital_grad of Class TestFullyShardGradientScaler implementation.

Fixes #161480


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-26 12:22:52+00:00,2025-08-26T17:19:06Z,,False,3,0,1,2,2,1,3,2025-08-26 17:16:45+00:00,70,192,False,True,False,False,False,False,1,2,493,4,2,2,1,1,2.0,2.0,2025-08-26T13:47:55Z,pytorch
161499,closed,[bucketing] custom_ops mode to hide inductor copies overhead,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161499

Adding ""_custom_ops"" bucketing to temporary fallback to eager execution of for_each,
to workaround too many generated kernels on inductor side.

This PR also reverts parts of bucketing changes for cycles detection that resulted in accuracy problems.


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81152293](https://our.internmc.facebook.com/intern/diff/D81152293)",2025-08-26 10:25:46+00:00,2025-09-08T20:04:15Z,,False,21,1,17,197,30,4,22,2025-09-08 20:03:11+00:00,60,733,False,False,False,False,False,False,4,20,6375,110826,66602,44224,1,16,4.0,20.0,2025-08-27T18:24:36Z,pytorch
161497,closed,[Inductor][WIndows] Fix Windows test case failure.,etaf,"Fixes windows test case failures:
- TritonCodeGenTests.test_inductor_sequence_nr
- TritonCodeGenTests.test_indirect_device_assert
- CompiledOptimizerTests.test_static_address_finalizer


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 09:57:24+00:00,2025-08-28T12:59:33Z,,False,6,0,2,10,3,3,6,2025-08-28 12:40:45+00:00,50,388,False,True,False,False,False,False,3,5,1600,25,16,9,1,2,3.0,5.0,2025-08-27T01:41:29Z,pytorch
161496,closed,Test https://github.com/pytorch/test-infra/pull/7054,huydhn,"No need to review https://github.com/pytorch/pytorch/actions/runs/17234329430

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-26 09:43:21+00:00,2025-08-27T10:38:17Z,,False,1,0,1,3,3,3,1,2025-08-27 10:38:17+00:00,52,195,False,False,False,False,False,False,3,0,0,6,3,3,1,1,,,,pytorch
161495,open,[ZENDNN] Integrate ZenDNN unary fusions,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* #161512
* __->__ #161495
* #161158
* #161157
* #161156
* #161155

----

- Add zendnn unary fusions support.
- Add a pass in optimize for the unary ops.
- Update meta registration and shim files.

Co-authored-by: Priyansh Jain <Priyansh.Jain2@amd.com>
Co-authored-by: Dinesh Mareedu <Dinesh.Mareedu@amd.com>
Change-Id: I7f34265a671bd1a3d9c98034e3f65ede2613d35a

[RFC](https://github.com/pytorch/pytorch/issues/150296)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 09:40:32+00:00,2025-09-18T20:03:32Z,,False,5,4,7,261,3,4,9,,39,707,False,False,False,False,False,False,4,2,169,117536,77941,39595,1,7,3.0,2.0,2025-09-09T05:47:43Z,pytorch
161493,closed,Unify TypeAlias definitions in optimizer.py,parsshar-RH,"Fixes #160834

This issue unifies TypeAlias definitions in [optimizer.py](https://github.com/pytorch/pytorch/blob/main/torch/optim/optimizer.py)

This ensures the following:

- Consistency and Standardization
- Enhanced IDE support
- Prevents runtime confusion
",2025-08-26 09:12:51+00:00,2025-09-03T09:03:18Z,,False,4,0,1,4,3,1,4,2025-08-30 00:35:05+00:00,43,261,False,True,False,False,False,False,1,3,538,7,4,3,1,1,3.0,3.0,2025-08-29T21:54:34Z,pytorch
161491,closed,Improve Scheduler init duration,ngocson2vn,"Early exit merge_loops() if config.loop_ordering_after_fusion is false.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 08:49:51+00:00,2025-08-28T00:28:58Z,,False,6,0,1,3,3,1,6,2025-08-28 00:27:56+00:00,31,274,False,False,False,False,True,False,1,3,535,6,3,3,1,1,3.0,3.0,2025-08-26T09:02:42Z,pytorch
161489,closed,Fix inconsistent types in _scaled_dot_product_attention_math,CaoE,"`_scaled_dot_product_attention_math` may throw: `RuntimeError: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16` when mask is a high precision tensor https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/attention.cpp#L925. The add operator will make `attn`  ​a float tensor, resulting in inconsistent data types error https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/transformers/attention.cpp#L906.",2025-08-26 08:34:05+00:00,2025-09-02T02:50:29Z,,False,1,0,1,47,4,3,1,2025-09-02 02:50:29+00:00,60,472,False,True,False,False,False,False,3,0,0,51,47,4,1,1,,,,pytorch
161488,closed,Fix `LBFGS` wolfe max iteration,zeshengzong,"Fixes #91581 , based on #135026

## Test Result

```bash
pytest test/test_optim.py

.........
========================== 1473 passed, 242 skipped in 2412.49s (0:40:12) ===========================
```",2025-08-26 08:26:12+00:00,2025-09-16T12:08:55Z,,False,6,0,1,36,1,2,6,2025-09-16 12:07:54+00:00,31,199,False,True,False,False,False,False,2,5,954,37,36,1,1,1,3.0,6.0,2025-09-15T14:17:29Z,pytorch
161487,closed,[Quant][Inductor][CPU] add qconv int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161487
* #161486


Summary:
Expand the patterns supported by qconv weight prepack, Specifically, expand the conv patterns of int8-mixed-bf16 datatype to support the following two cases:
Case 1:
the `out_dtype `of `dequantize_per_tensor  `is `torch.float32`

```
    dq_per_tensor  dq_per_channel
         |               |        
    to_bf16           to_bf16   
            \          /           
             Conv2d  
```    

Case 2:
the `out_dtype `of `dequantize_per_tensor  `is `torch.bfloat16`

```
    dq_per_tensor  dq_per_channel
         \               |        
                      to_bf16   
                       /           
             Conv2d  
``` 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-26 08:16:27+00:00,2025-09-04T02:02:42Z,,False,3,2,4,45,14,2,5,2025-09-04 02:01:37+00:00,57,959,False,False,False,False,False,False,2,2,506,43140,30338,12802,1,4,5.0,3.0,2025-09-02T07:18:04Z,pytorch
161486,closed,[Quant][Inductor][CPU] add qlinear int8-mixed-bf16 patterns,jiayisunx,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161487
* __->__ #161486

Summary:
Expand the patterns supported by qlinear weight prepack, Specifically, expand the linear patterns of int8-mixed-bf16 datatype to support the following two cases:
Case 1: 
the `out_dtype` of `dequantize_per_tensor ` is `torch.float32`

    dq_per_tensor  dq_per_channel
         |               |        
    to_bf16           to_bf16 
         |               |        
     OPT(reshape)     permute      
            \          /           
             addmm/mm            
                    |                
           OPT(reshape)          
   
or

    dq_per_tensor  dq_per_channel
         |               |        
    to_bf16           to_bf16
         |               |        
       expand         permute      
          \              |        
                      expand       
                       /            
               bmm               
                |                
            OPT(add) 

Case 2: 
the `out_dtype` of `dequantize_per_tensor ` is `torch.bfloat16`

    dq_per_tensor  dq_per_channel
         |               |        
                       to_bf16 
                         |        
     OPT(reshape)   permute      
            \          /           
             addmm/mm            
                    |                
           OPT(reshape)          
   
or

    dq_per_tensor  dq_per_channel
         |                |        
                        to_bf16
                          |        
       expand          permute      
          \               |        
                        expand       
                        /            
               bmm               
                |                
            OPT(add) 


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-26 08:16:19+00:00,2025-09-04T01:54:08Z,,False,3,5,4,127,19,2,8,2025-09-04 01:53:04+00:00,59,2009,False,False,False,False,False,False,2,2,506,43211,30407,12804,1,4,4.0,3.0,2025-09-02T07:16:21Z,pytorch
161485,open,[Intel GPU PT2E] Infer runtime out dtype based on dequant node in pat…,ZhiweiYan-96,"# Motivation

Current `qlinear_weight_prepack` fx pass at x86Quantizer and XPUQuantizer cannot guarantee the dtype consistency between matched pattern and the replaced pattern.  For example,
```
          x                           w
          |                           |
    deuqnt(fp16)                 dequant(fp16)
          |                            |
          |                        permute
           \                       /
                 addmm(fp16)
```
Would be replaced by
```
             x                        w
              \                      /   
                 qlinear(...,fp32)
```
The original output node data type info is lost during graph rewriting. 

# Solution

As graph talks itself, the output dtype can be inferred from dequant node directly. The PR uses the data type as qlinear output dtype.

# Verification

```bash
    python test/inductor/test_mkldnn_pattern_matcher.py -v -k test_qlinear_fp16_xpu
   python test/inductor/test_mkldnn_pattern_matcher.py -v -k test_qlinear_fp16_cpu
```

```bash
onednn_verbose,v1,primitive,exec,gpu:0,matmul,jit:gemm:any,undef,src:s8::blocked:ab::f0 wei:s8::blocked:ab::f0 bia:f16::blocked:ab::f0_mask2 dst:f16::blocked:ab::f0,attr-scratchpad:user attr-scales:src0:0:f32+dst:0:f32+wei:2:f32,,1x16384:16384x10,0.167969
onednn_verbose,v1,primitive,exec,gpu:0,matmul,ocl:ref:any,undef,src:f16::blocked:ab::f0 wei:s8::blocked:ab::f0 bia:f16::blocked:ab::f0_mask2 dst:f16::blocked:ab::f0,attr-scratchpad:user attr-scales:src0:0:f32+dst:0:f32+wei:2:f32,,1x10:10x20,0.0810547
```
Fixes #ISSUE_NUMBER


cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 08:05:05+00:00,2025-09-01T15:01:01Z,,False,2,2,4,193,27,8,4,,70,1838,False,True,False,False,False,False,8,1,58,228,197,31,2,4,1.0,1.0,2025-09-01T01:48:02Z,pytorch
161479,closed,[export] Support AC HOP in pre-dispatch,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161458
* __->__ #161479
* #161353

Adds the pre-dispatch handling for the AC hop. This lets the HOP pre-dispatch export without actually pre-dispatch tracing into it,. However, this is not sufficient to support AC in export:
- because the HOP body will still be in torch IR, so it will fail export verifiers
- the exported module also can't be ran in eager because the AC HOP relies on partitioner to embed RNG state saving/restoring

So it must be lowered by AOT Autograd into post-dispatch first before being executed, It suffices for my purposes though.

If users had checkpoint API use in their exported model, the behavior goes from silently incorrect to now be validation error.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 06:52:42+00:00,2025-08-28T01:47:34Z,,False,7,5,3,148,73,2,12,2025-08-28 01:46:29+00:00,39,936,False,False,False,False,False,False,2,6,3653,357,216,141,1,3,5.0,6.0,2025-08-26T20:36:06Z,pytorch
161478,closed,[export] Support AC HOP in pre-dispatch,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)

",2025-08-26 06:50:32+00:00,2025-08-26T16:26:00Z,,False,2,0,1,139,38,1,2,2025-08-26 16:26:00+00:00,39,94,False,False,False,False,False,False,1,0,0,177,139,38,1,1,,,,pytorch
161477,closed,[Inductor UT] Re-enable test_torchinductor_opinfo.py on XPU.,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161477

The PR #160222 replaced @skipCUDAIf with @requires_cuda_and_triton in test_torchinductor_opinfo.py, which caused the CI jobs for other devices to skip this large test suite. We attempted to revert #160222 but ran into conflicts. I then opened #160936 to revert the changes from #160222, but that resulted in CPU CI job timeouts. I also filed issue #161132 for assistance, but haven’t received a response yet.

To minimize the impact, this PR re-enables the test suite on XPU first. I will continue to seek help on re-enabling it for CPU afterwards.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 06:37:58+00:00,2025-08-28T05:14:31Z,,False,3,0,1,10,3,2,3,2025-08-28 03:29:24+00:00,60,847,False,False,False,False,False,False,2,2,493,13,10,3,1,1,3.0,2.0,2025-08-27T01:41:46Z,pytorch
161476,open,[1/N] Port 3 fsdp distributed test cases to Intel GPU,libohao1201,"For https://github.com/pytorch/pytorch/issues/114850, we will port 3 distributed tests to Intel GPU.
We could enable Intel GPU with the following methods and try the best to keep the original code styles:


- use ""torch.accelerator.current_accelerator()"" to determine the accelerator backend
- use ""requires_accelerator_dist_backend"" to enable ""xccl""
- enabled XPU for some test path
- skip some test cases that Intel GPU does not support

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @gujinghui @EikanWang @fengyuan14 @guangyey",2025-08-26 06:30:29+00:00,2025-09-16T09:46:02Z,,False,6,4,14,10,13,4,10,,53,585,False,False,False,False,False,False,4,2,101,151298,104502,46796,5,13,2.0,3.0,2025-08-26T06:48:11Z,pytorch
161475,open,fix(docs): Replace unstable torch.compiler_custom_backends link with stable version,Quantum-Kayak,"This PR resolves [#119272](https://github.com/pytorch/pytorch/issues/119272) by updating the broken or unstable link in the `torch.compile` docstring.

### 🛠️ What Changed
- Replaced the old reference to the main docs URL:
  `https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends`
- With the stable version:
  `https://pytorch.org/docs/stable/torch.compiler_custom_backends.html#registering-custom-backends`

This ensures the docstring link remains consistent and functional across all versions, including stable releases.

### 📌 Context
The original link broke or rendered incorrectly in some cases, especially on the stable docs site. A number of users reported this in #119272. This PR updates the reference to use the stable domain to prevent similar issues in the future.

Let me know if you'd like this rebased or squashed.

---

✅ CLA signed  
✅ All CI passing  
✅ No functional code changes — docstring-only  


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 06:14:56+00:00,2025-08-26T14:02:22Z,,False,1,0,4,83,1,4,1,,83,1161,False,True,False,True,False,False,4,0,62,84,83,1,2,4,1.0,1.0,2025-08-26T14:02:13Z,pytorch
161474,closed,[Inductor-FX] Support custom triton kernels,blaine-rister,"# Feature
Add support for custom Triton kernels to the FX backend. This turned out not to require any new features, except for a minor change to handle `tl.constexpr` arguments which are not part of the autotuning config.

# Caveat

This may not cover every possible case. For example, we might need more features for autotuning custom Triton code. This PR entirely skips the [custom codegen ](https://github.com/pytorch/pytorch/blob/main/torch/_higher_order_ops/triton_kernel_wrap.py#L1034-L1039) for user-defined grid functions, but there may be edge cases requiring this logic. However, this PR seems to do a reasonable job as many of the grids end up being written into Inductor/Triton metadata and don't require special codegen.

As a follow up, I'm planning to test this against all of AOTI's custom Triton kernel tests.

# Test plan
Added a CI test using a custom Triton kernel.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 06:12:48+00:00,2025-08-27T00:16:23Z,,False,6,0,2,69,1,2,6,2025-08-27 00:15:22+00:00,43,1088,False,False,True,False,False,False,2,5,1543,28022,19719,8303,1,2,3.0,5.0,2025-08-26T16:55:31Z,pytorch
161472,open,Fix to segmentation fault issue for torch.repeat_interleave,arkadip-maitra,"Fixes #157097  

**Minimal code to trigger error:**
```
import torch
ts1 = torch.rand((7, 2, 6, 4, 8, 3))
torch.repeat_interleave(ts1,torch.tensor([6773413839565225984]))
```
**Previous Output:** 
`Segmentation fault`

**Now Output:**
```
  File ""/home/amaitra/work/tests/issue_157097.py"", line 8, in <module>
    torch.repeat_interleave(ts1,torch.tensor([6773413839565225984]))
RuntimeError: cumulative sum values overflowed. check the repeats tensor values
```

Handled by checking that torch.cumsum(0) on repeats is not negative. Since repeats cannot be negative this check fixes the overflow issue caused on cumsum tensor values.",2025-08-26 05:05:37+00:00,2025-09-12T05:00:46Z,,False,11,2,7,14,0,4,13,,59,633,False,True,False,False,False,False,4,9,8117,103671,68182,35489,2,7,2.0,10.0,2025-08-26T05:06:21Z,pytorch
161471,closed,[2/N][SymmMem] Add MemPool allocator and tests,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161533
* #161532
* __->__ #161471

(Porting most of #161008)

Hooking SymmetricMemory Allocator to MemPool so that user can create symmetric tensors with regular `torch.zeros`, `torch.arange` etc factories. Also so that our ops can have functional variants that create `out` tensors on symmetric memory.

To end users, this PR supports a python UI as follows:
```
allocator = symm_mem.get_mempool_allocator(device)
mempool = torch.cuda.MemPool(allocator)
with torch.cuda.use_mem_pool(mempool):
    tensor = torch.arange(numel, dtype=dtype, device=device)
```

Added tests for both use cases above.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)",2025-08-26 04:41:00+00:00,2025-08-31T18:13:31Z,,False,18,0,4,171,0,10,18,2025-08-31 18:09:00+00:00,46,829,False,False,False,False,False,False,10,17,3488,199,185,14,1,4,4.0,18.0,2025-08-26T20:19:53Z,pytorch
161470,closed,"[1/N][SymmMem] Add offset to handle, cache on base address",kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161533
* #161532
* #161471
* __->__ #161470

For the kernels that need peer pointers directly, the rendezvous handle should allow user to get the offset of tensor wrt to base allocation address. Thus the need to add an `offset` field to SymmMem handle.

But we don't want to cache all the handles just bc they have different offsets, hence the search and cache logic below:

(i) At rendezvous, the search key is still `x.storage().data_ptr()`, like now, but it should do search in 2 parts - one is just dictionary lookup, like today, if that failed, it needs to search `allocations_` to see if the storage ptr falls in one of the segments. This is possible as we have all segments recorded during alloc.
(ii) If this segment hasn't been rendezvoused, we rendezvous it, cache it in the `symm_mem_` map with its base address as key.
(iii) We still need to return a handle for the current tensor, with a corresponding offset. This handle will be a shallow copy of the base handle, with the offset adjusted.

Some impl details:
(i.1) If we find a matching allocation, we can immediately use the allocation base address to do a re-search in `symm_mem_`.

(iii.1) To make the handle copy shallow, we move the common information -- base ptrs, base signal pad, etc -- to a structure referenced by both handles. The structure is called `NVSHMEMPeerAllocInfo`. A copy of handle just adds one more `intrusive_ptr` to it. The handle copy constructor accepts an `offset` argument.

Test:
Existing tests should not fail.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta

Differential Revision: [D81396679](https://our.internmc.facebook.com/intern/diff/D81396679)",2025-08-26 04:40:56+00:00,2025-08-31T01:51:39Z,,False,7,0,1,111,41,1,7,2025-08-27 00:49:09+00:00,58,1756,False,False,False,False,False,False,1,6,1121,152,111,41,1,1,3.0,6.0,2025-08-26T16:18:01Z,pytorch
161469,closed,[inductor][ez] kernel inputs reports a key,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161675
* #161674
* #161673
* #158091
* #157699
* __->__ #161469
* #161468
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- make eventually caching based on kernel input key simpler

# what

- a unique key for the kernel input based on input nodes, scalars, and
  device name

# testing

n/a, not in use yet

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-26 04:31:52+00:00,2025-09-12T00:42:13Z,,False,1,0,12,32,0,1,1,2025-09-12 00:42:13+00:00,42,685,False,False,False,False,False,False,1,0,0,87818,53929,33889,1,12,,,,pytorch
161468,closed,[inductor][ez] add src_hash property for Templates,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* __->__ #161468
* #162293
* #161350
* #161351

# why

enable caching/overriding/filtering based on src hash later

# what

- KernelTemplate has a src_hash that is None by default
- sha256 on TritonTemplate of the template src code
- None on ExternKernelChoice to have same API

# testing

n/a (not in use in this change)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [](https://our.internmc.facebook.com/intern/diff/)

Differential Revision: [D81821149](https://our.internmc.facebook.com/intern/diff/D81821149)",2025-08-26 04:31:49+00:00,2025-09-12T21:11:56Z,,False,9,2,22,20,3,3,11,2025-09-12 21:10:50+00:00,50,774,False,False,False,False,False,False,3,8,1432,131618,81431,50187,1,22,4.0,8.0,2025-09-05T22:21:45Z,pytorch
161467,open,clarifying docstring for the gather_list argument for gather(),EIFY,"We have the same requirement that it must be `None` on non-dst ranks yet this wasn't spelled out. This part of the docstring now reads the same as the gather_object() counterpart.

In response to https://github.com/KellerJordan/Muon/issues/46#issuecomment-3221359598


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim",2025-08-26 03:27:14+00:00,2025-09-05T02:51:16Z,,False,5,0,1,4,3,1,5,,62,364,False,False,False,True,False,False,1,3,814,7,4,3,1,1,2.0,3.0,2025-09-04T23:46:56Z,pytorch
161466,closed,Outline SymInt::maybe_as_int_slow_path,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161586
* __->__ #161466

Keeps SymInt::maybe_as_int small enough to inline.

Differential Revision: [D81530097](https://our.internmc.facebook.com/intern/diff/D81530097)",2025-08-26 02:50:08+00:00,2025-09-03T06:54:50Z,,False,6,0,4,11,5,2,6,2025-09-03 06:54:49+00:00,38,247,False,False,False,False,False,False,2,5,563,69410,41173,28237,1,4,3.0,5.0,2025-08-27T20:24:42Z,pytorch
161465,closed,forward fix #161102,ethanwee1,"PR #161102 caused tf32 to be the default precision for flex attention.  This PR forward-fixes the broken logic and restores ROCm MI200 CI flex attention test.


cc @eqy  @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 02:39:11+00:00,2025-08-26T15:13:02Z,,False,7,0,4,5,3,1,7,2025-08-26 15:11:58+00:00,19,368,False,True,False,False,False,False,1,6,1903,14,8,6,1,4,3.0,6.0,2025-08-26T02:42:31Z,pytorch
161464,closed,[Dependabot] Update(deps): Bump transformers from 4.54.0 to 4.55.4 in /.ci/docker/ci_commit_pins,dependabot[bot],"[//]: # (dependabot-start)
⚠️  **Dependabot is rebasing this PR** ⚠️ 

Rebasing might not happen immediately, so don't worry if this takes some time.

Note: if you make any changes to this PR yourself, they will take precedence over the rebase.

---

[//]: # (dependabot-end)

Bumps [transformers](https://github.com/huggingface/transformers) from 4.54.0 to 4.55.4.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/huggingface/transformers/releases"">transformers's releases</a>.</em></p>
<blockquote>
<h1>Patch v4.55.4</h1>
<p>There was a mick mack on our side when cherry-picking the commit <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a> which led to a wrong commit in the patch!
Sorry everyone 😭</p>
<p>This patch is just the official fix for <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>!</p>
<h2>Patch release v4.55.3</h2>
<h1>Patch release 4.55.3</h1>
<p>Focused on stabilizing FlashAttention-2 on Ascend NPU, improving FSDP behavior for generic-task models, fixing MXFP4 integration for GPT-OSS</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>FlashAttention-2 / Ascend NPU – Fix “unavailable” runtime error (<a href=""https://redirect.github.com/huggingface/transformers/issues/40151"">#40151</a>) by <a href=""https://github.com/FightingZhen""><code>@​FightingZhen</code></a></li>
<li>FlashAttention kwargs – Revert FA kwargs preparation to resolve regression (<a href=""https://redirect.github.com/huggingface/transformers/issues/40161"">#40161</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>FSDP (generic-task models) – Fix sharding/runtime issues (<a href=""https://redirect.github.com/huggingface/transformers/issues/40191"">#40191</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>GPT-OSS / MXFP4 – Ensure swiglu_limit is correctly passed through (<a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>) by <a href=""https://github.com/danielhanchen""><code>@​danielhanchen</code></a></li>
<li>Mamba – Fix cache handling to prevent stale/incorrect state (<a href=""https://redirect.github.com/huggingface/transformers/issues/40203"">#40203</a>) by <a href=""https://github.com/manueldeprada""><code>@​manueldeprada</code></a></li>
<li>Misc – Minor follow-up fix addressing <a href=""https://redirect.github.com/huggingface/transformers/issues/40262"">#40262</a> by <a href=""https://github.com/ArthurZucker""><code>@​ArthurZucker</code></a></li>
</ul>
<h2>Patch release 4.55.2: for FA2 users!</h2>
<h1>Patch release 4.55.2!</h1>
<h2>only affects <code>FA2</code> generations!</h2>
<p>😢 Well sorry everyone, sometimes shit can happen...
4.55.1 was broken because of 🥁 git merge conflict.
I cherry-picked <a href=""https://redirect.github.com/huggingface/transformers/pull/40002"">huggingface/transformers#40002</a> without having <a href=""https://redirect.github.com/huggingface/transformers/pull/40029"">huggingface/transformers#40029</a> , thus <code>from ..modeling_flash_attention_utils import prepare_fa_kwargs_from_position_ids</code> is missing, and since this is a slow test, nothing caught it.</p>
<p>Will work to remediate and write the post-mortem when yanking the release.</p>
<h1>Patch release 4.55.1:</h1>
<p>Mostly focused around stabalizing the Mxfp4 for GPTOSS model!</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>Idefics2, Idefics3, SmolVLM – Fix tensor device issue (<a href=""https://redirect.github.com/huggingface/transformers/issues/39975"">#39975</a>) by <a href=""https://github.com/qgallouedec""><code>@​qgallouedec</code></a></li>
<li>Merge conflicts – Fix merge conflicts from previous changes by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>MXFP4 / CPU device_map – Default to dequantize when CPU is in device_map (<a href=""https://redirect.github.com/huggingface/transformers/issues/39993"">#39993</a>) by <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>GPT Big Code – Fix attention scaling (<a href=""https://redirect.github.com/huggingface/transformers/issues/40041"">#40041</a>) by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>Windows compatibility – Resolve Triton version check compatibility (<a href=""https://redirect.github.com/huggingface/transformers/issues/39986"">#39986</a>) by <a href=""https://github.com/Tsumugii24""><code>@​Tsumugii24</code></a> <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>Gemma3n model – Add missing None default values for get_placeholder_mask (<a href=""https://redirect.github.com/huggingface/transformers/issues/39991"">#39991</a>, <a href=""https://redirect.github.com/huggingface/transformers/issues/40024"">#40024</a>) by <a href=""https://github.com/Znerual""><code>@​Znerual</code></a></li>
<li>Fuyu model – Fix broken image inference (<a href=""https://redirect.github.com/huggingface/transformers/issues/39915"">#39915</a>) by <a href=""https://github.com/Isotr0py""><code>@​Isotr0py</code></a></li>
<li>PerceptionLM – Fix missing video inputs (<a href=""https://redirect.github.com/huggingface/transformers/issues/39971"">#39971</a>) by <a href=""https://github.com/shuminghu""><code>@​shuminghu</code></a></li>
<li>Idefics – Fix device mismatch (<a href=""https://redirect.github.com/huggingface/transformers/issues/39981"">#39981</a>) by <a href=""https://github.com/zucchini-nlp""><code>@​zucchini-nlp</code></a></li>
<li>Triton kernels – Remove triton_kernels dependency in favor of included kernels (<a href=""https://redirect.github.com/huggingface/transformers/issues/39926"">#39926</a>) by <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>GPT-OSS MXFP4 – Enable on older hardware (sm75+) (<a href=""https://redirect.github.com/huggingface/transformers/issues/39940"">#39940</a>) by <a href=""https://github.com/matthewdouglas""><code>@​matthewdouglas</code></a> <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>MXFP4 quantizer – Allow CPU inference with dequantize option (<a href=""https://redirect.github.com/huggingface/transformers/issues/39953"">#39953</a>) by <a href=""https://github.com/returnL""><code>@​returnL</code></a></li>
</ul>
<h2>CI &amp; Build</h2>
<ul>
<li>CI stability – Post-GPT-OSS fixes for green CI (<a href=""https://redirect.github.com/huggingface/transformers/issues/39929"">#39929</a>) by <a href=""https://github.com/gante""><code>@​gante</code></a> <a href=""https://github.com/LysandreJik""><code>@​LysandreJik</code></a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/huggingface/transformers/commit/d79b2d981f28b2730d402244ac3c2e9a8c054eee""><code>d79b2d9</code></a> v4.55.4</li>
<li><a href=""https://github.com/huggingface/transformers/commit/90792b730ab418b51fdc96141b5a1d8c1a39b9c0""><code>90792b7</code></a> Revert &quot;Fix GPT-OSS swiglu_limit not passed in for MXFP4 <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>&quot;</li>
<li><a href=""https://github.com/huggingface/transformers/commit/a03df6acd44afe6bb8aced417b8a283eb0258978""><code>a03df6a</code></a> Fix GPT-OSS <code>swiglu_limit</code> not passed in for MXFP4 (<a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/170b2708cb1977690a87753bbe55280974388513""><code>170b270</code></a> Fixes <a href=""https://redirect.github.com/huggingface/transformers/issues/40262"">#40262</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/7dbc054e2a0c3cafd3ea22db0566db700b3a8cbf""><code>7dbc054</code></a> v4.55.3</li>
<li><a href=""https://github.com/huggingface/transformers/commit/c097a43898550846beeb42ddaba2fffa9513929e""><code>c097a43</code></a> [bugfix] fix flash-attention2 unavailable error for Ascend NPU (<a href=""https://redirect.github.com/huggingface/transformers/issues/40151"">#40151</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/663cbb0d046e4a22919e7c822478b1f4f090c626""><code>663cbb0</code></a> [FA2] Fix it finally - revert fa kwargs preparation (<a href=""https://redirect.github.com/huggingface/transformers/issues/40161"">#40161</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/c7bd5350f090a3d064ba892d492080cce4d03326""><code>c7bd535</code></a> Fix fsdp for generic-task models <a href=""https://redirect.github.com/huggingface/transformers/issues/40191"">#40191</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/e75d67ec39b4f8bc03dbeea9016fff16c2375c5a""><code>e75d67e</code></a> Fix GPT-OSS swiglu_limit not passed in for MXFP4 <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/d7f67d2006aedd69999809f71657acb32b7d7e14""><code>d7f67d2</code></a> Fix mamba caches (<a href=""https://redirect.github.com/huggingface/transformers/issues/40203"">#40203</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/huggingface/transformers/compare/v4.54.0...v4.55.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=transformers&package-manager=pip&previous-version=4.54.0&new-version=4.55.4)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>

cc @seemethere @malfet @pytorch/pytorch-dev-infra @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-26 02:19:17+00:00,2025-09-19T17:58:53Z,,False,4,0,1,1,1,1,4,2025-09-19 17:58:43+00:00,96,11491,False,True,False,True,True,False,1,1,66,2,1,1,1,1,1.0,1.0,2025-09-19T17:58:43Z,pytorch
161462,open,[MPS]Fix torch.argmax/torch.argmin fail for non-contiguous input,can-gaa-hou,"Fixes #160740

The error occurs when the parameter `dim` is not set while calling `torch.argmax` or `torch.argmin`. On MPS, the input tensor will be flattened when the `dim` has no value. 
https://github.com/pytorch/pytorch/blob/4651aaac47ff855e08a74e2fdbfa605bc53afba8/aten/src/ATen/native/mps/operations/ReduceOps.mm#L903-L915

The stride of the flattened tensor will be computed here and raise the error.
https://github.com/pytorch/pytorch/blob/1eccfb157ab9855b3f81872a23502fb15f455e0a/aten/src/ATen/native/TensorShape.cpp#L4028-L4037


In the CPU version of `torch.argmax` and `torch.argmin`, it will **reshape** the input tensor if `dim` has no value. Thus, in this PR, I fix it by reshaping the input tensor when `dim` has no value to compute the stride correctly. Also, I am adding a testcase for it.

https://github.com/pytorch/pytorch/blob/8c442e4fd3310e15f57770944f883ac1d73e77e2/aten/src/ATen/native/ReduceOps.cpp#L1763-L1777",2025-08-26 01:42:27+00:00,2025-09-03T16:36:16Z,,False,2,0,1,20,1,2,2,,64,936,False,True,False,False,False,False,2,1,129,21,20,1,1,1,1.0,1.0,2025-09-03T14:11:00Z,pytorch
161461,closed,Bump TorchBench version,huydhn,"To include the latest fixes from TorchBench.  I'll setup a nightly commit hash update for this next

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-26 01:35:24+00:00,2025-08-29T19:22:13Z,,False,5,0,11,1,1,1,5,2025-08-29 19:21:10+00:00,23,271,False,True,False,False,False,False,1,4,1396,36408,23943,12465,1,11,3.0,4.0,2025-08-28T23:41:08Z,pytorch
161458,closed,[dynamo][hop] Introduce Local Map HOP,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163133
* #162702
* __->__ #161458

Can't actually deploy it because of: https://github.com/pytorch/pytorch/issues/161456

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-26 01:07:29+00:00,2025-09-17T09:32:42Z,,False,12,9,22,701,3,9,21,2025-09-17 09:32:41+00:00,37,514,False,False,False,False,False,False,9,11,2521,124476,80109,44367,1,22,5.0,12.0,2025-08-27T15:57:35Z,pytorch
161455,closed,PythonArgs::toBool: order cheap mutually exclusive checks first,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301

symbools are not identical with Py_True or PyFalse, so we can do those cheap checks first and at least get plain old bools to go fast.",2025-08-26 00:33:53+00:00,2025-08-31T21:36:54Z,,False,3,1,4,9,2,1,4,2025-08-31 21:35:52+00:00,63,318,False,False,False,False,False,False,1,2,493,21020,14500,6520,1,4,3.0,2.0,2025-08-30T22:12:38Z,pytorch
161454,closed,[ONNX] Drop draft_export in exporter API,titaiwangms,"If onnx exporter fallbacks to draft_export with big models, this is taking forever for users, and possibly spam the printout, which keeps users from their stack trace with strict=False.

We could consider make another API for draft_export as debugging tool, or combine it with report=True when ""model is small""?


cc @justinchuby",2025-08-26 00:24:00+00:00,2025-08-26T22:23:14Z,,False,6,2,3,0,3,2,8,2025-08-26 22:13:47+00:00,40,329,False,True,False,False,False,False,2,5,1455,5829,2841,2988,1,3,3.0,5.0,2025-08-26T00:32:50Z,pytorch
161453,closed,[APS IR] Minfor fix - use GetAttrKey in get_keystr to match with flat args path in unflatten,malaybag,"Summary: While passing path info to [_check_input_constraints_for_graph](https://www.internalfb.com/code/fbsource/[6b5b2dc35902a26ce265e3c0ae5189a3faba1d38]/fbcode/caffe2/torch/export/unflatten.py?lines=594), GetAttrKey is used to specify path str. To match with that get_keystr should also use GetAttrKey.

Test Plan:
Existing tests

```
buck run mode/opt caffe2/test:test_export -- -r unflatten
```

```
Ran 413 tests in 204.533s

OK (skipped=1, expected failures=13)
```

Rollback Plan:

Differential Revision: D80984083


",2025-08-25 23:44:44+00:00,2025-08-27T00:06:27Z,,False,7,0,1,6,1,2,7,2025-08-27 00:05:24+00:00,92,526,False,True,False,False,False,False,2,1,476,7,6,1,1,1,2.0,1.0,2025-08-26T16:25:30Z,pytorch
161452,closed,inductor: Log the specific triton kernel that fails,c00w,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161561
* __->__ #161452

Added a optional name argument to SubprocPool.submit.

We record this in a dictionary, and when raising exceptions, add the name.
We manage the lifecycle the same as the pending futures.

Added a specific testcase to make sure this logs correctly.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 23:20:59+00:00,2025-08-27T21:35:35Z,,False,6,10,11,34,4,3,16,2025-08-27 21:35:34+00:00,51,554,False,False,False,False,False,False,3,5,1151,122,76,46,1,9,3.0,7.0,2025-08-26T17:58:27Z,pytorch
161451,closed,Set USE_NVSHMEM only if USE_DISTRIBUTED is set,lakshayg,,2025-08-25 23:17:48+00:00,2025-08-27T17:12:26Z,,False,12,0,1,1,1,1,12,2025-08-27 17:11:23+00:00,46,0,False,False,False,False,False,False,1,11,2305,2,1,1,1,1,3.0,11.0,2025-08-26T00:15:45Z,pytorch
161450,closed,Disable inductor/test_flex_attention.py,amdfaa,Currently inductor/test_flex_attention.py is causing rocm pytorch mi250 shard 1 to go over the timeout limit. This PR is for disabling that test.,2025-08-25 23:15:52+00:00,2025-08-26T01:29:58Z,,False,3,0,2,1,0,1,3,2025-08-26 01:28:54+00:00,39,145,False,False,False,False,False,False,1,2,829,3,2,1,2,2,2.0,2.0,2025-08-25T23:25:22Z,pytorch
161449,closed,[ONNX] Remove unused logic from internal verification module,justinchuby,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161546
* __->__ #161449
* #161323

Signed-off-by: Justin Chu <justinchuby@users.noreply.github.com>

cc @titaiwangms",2025-08-25 22:34:42+00:00,2025-09-02T16:23:59Z,,False,8,1,15,4,1655,2,9,2025-09-02 16:22:52+00:00,60,195,False,False,False,False,False,False,2,7,2799,17248,9668,7580,1,15,4.0,8.0,2025-08-25T22:35:31Z,pytorch
161448,closed,[WIP][Dynamo][vLLM] Support construction of TensorSchema,bbeckca,"Summary:
What: Add support for TensorSchema in user-defined objects for compatibility with Dynamo's tracing system

Why: Observed graph breaks similar to issue with TypedDict - https://github.com/pytorch/pytorch/issues/132629

```
    def test_tensor_schema_llava_pixel_inputs(self):
        class LlavaImagePixelInputs(TensorSchema):
            type: Literal[""pixel_values""] = ""pixel_values""
            pixel_values: Annotated[
                torch.Tensor,
                TensorShape(""bn"", 3, ""h"", ""w"")
            ]

        def fn(x, y):
            obj = LlavaImagePixelInputs(pixel_values=y)
            return x * obj.pixel_values.sum()

        x, y = torch.randn(4), torch.randn(2, 3, 8, 8)
        ref = fn(x, y)

        opt_fn = torch.compile(fn, backend=""eager"", fullgraph=True)
        res = opt_fn(x, y)

        self.assertEqual(ref, res)
```

```
ERROR: test_tensor_schema_llava_pixel_inputs (caffe2.test.dynamo.test_repros.ReproTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/data/users/benjibeck/fbsource/buck-out/v2/gen/fbcode/95f6e7be6bb24603/caffe2/test/dynamo/__test_repros__/test_repros#link-tree/caffe2/test/dynamo/test_repros.py"", line 6544, in test_tensor_schema_llava_pixel_inputs
    res = opt_fn(x, y)
  File ""/data/users/benjibeck/fbsource/buck-out/v2/gen/fbcode/95f6e7be6bb24603/caffe2/test/dynamo/__test_repros__/test_repros#link-tree/torch/_dynamo/eval_frame.py"", line 813, in compile_wrapper
    raise e.with_traceback(None) from e.__cause__  # User compiler error
torch._dynamo.exc.Unsupported: Observed exception
  Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.
  Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.
  Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.

  Developer debug context: raised exception TypeError([ConstantVariable(str: ""unhashable type: <class 'torch._dynamo.variables.misc.GetAttrVariable'>"")])

 For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html

from user code:
   File ""/data/users/benjibeck/fbsource/buck-out/v2/gen/fbcode/95f6e7be6bb24603/caffe2/test/dynamo/__test_repros__/test_repros#link-tree/caffe2/test/dynamo/test_repros.py"", line 6537, in fn
    obj = LlavaImagePixelInputs(pixel_values=y)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=""+dynamo""


----------------------------------------------------------------------
Ran 1 test in 1.662s
```
https://www.internalfb.com/intern/testinfra/testrun/4503599929393926

Test Plan:
```
buck2 test caffe2/test/dynamo:test_repros
```

```
buck2 run fbcode//caffe2/test/dynamo:test_tensor_schema_llava_pixel_inputs_isolated 2>&1 | tee tensor_schema_dynamo.log
```

Full log: P1910537335

Rollback Plan:

Differential Revision: D80549504




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-25 22:20:17+00:00,2025-08-27T20:39:55Z,,False,5,0,1,31,1,2,5,2025-08-27 20:39:54+00:00,56,3508,False,True,False,False,False,False,2,1,78,32,31,1,1,1,1.0,1.0,2025-08-27T20:39:54Z,pytorch
161447,closed,[Inductor] Fix cross-device scalar lowering - cpu scalar with cuda tensor fails in torch.compile,karthickai,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161447

This PR fixes bug in TorchInductor where cross-device scalar indexing fails during compilation, causing discrepancies from eager mode behavior.

Fixes: #140457 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-25 21:56:40+00:00,2025-09-09T01:08:08Z,,False,7,8,8,140,0,2,15,2025-09-09 01:07:05+00:00,96,465,False,True,False,False,False,False,2,6,7583,94353,58848,35505,1,8,3.0,6.0,2025-08-25T22:13:45Z,pytorch
161445,closed,[easy][test] Add repeat_interleave opinfo that exercises binary search fusion,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161445

This adds a configuration that would have caught the need for https://github.com/pytorch/pytorch/pull/159961 when https://github.com/pytorch/pytorch/pull/158462 was landed.

Notably:
* the test has output_size kwarg specified
* the input is 1D plus a size-1 dimension (otherwise, if there are non-size-1 dimensions, then the fusion won't occur)

Differential Revision: [D80981715](https://our.internmc.facebook.com/intern/diff/D80981715)",2025-08-25 21:52:23+00:00,2025-08-26T12:33:32Z,,False,9,0,3,7,3,2,9,2025-08-26 12:32:27+00:00,77,531,False,False,False,False,False,False,2,8,1834,2975,558,2417,1,3,4.0,8.0,2025-08-25T21:59:17Z,pytorch
161444,closed,Increase timeout value when pushing to ghcr.io,huydhn,"Seeing this timing out a lots in trunk now https://github.com/pytorch/pytorch/actions/runs/17165552358/job/48705069047.  The benchmark image is the largest one we have on CI, so it's probably over the 30 minutes limit.",2025-08-25 21:38:48+00:00,2025-08-26T01:52:21Z,,False,8,0,5,3,3,2,8,2025-08-26 01:51:19+00:00,46,218,False,False,False,False,False,False,2,7,2046,20853,12802,8051,1,5,3.0,7.0,2025-08-25T21:48:49Z,pytorch
161443,closed,[Inductor][Tritonparse] Call `jit_post_compile_hook` within Inductor Triton Kernel compile path,NikhilAPatel,"Summary: Since Inductor skips JIT compilation for Triton kernels, we need to manually invoke `knobs.runtime.jit_post_compile_hook` if one exists. Here, we do this to enable Tritonparse to extract launch metadata from Inductor launched kernels. We can control whether or not Inductor will run the hook with a new `TORCHINDUCTOR_RUN_JIT_POST_COMPILE_HOOK=1 ` config variable.

Reviewed By: davidberard98

Differential Revision: D80624932




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 21:37:54+00:00,2025-08-26T06:25:49Z,,False,8,0,1,24,0,2,8,2025-08-26 06:24:46+00:00,95,641,False,False,False,False,False,False,2,1,520,24,24,0,1,1,2.0,2.0,2025-08-25T21:50:14Z,pytorch
161442,open,[Inductor][FP8] Validate exhaustive autotuning for FP8 Inductor templates,jananisriram,"Summary: Validate exhaustive autotuning for FP8 Inductor templates: scaled MM templates require `block_k >= 32`. Before, exhaustive autotuning defaulted to a limited set of autotuning configs, as limitations for exhaustively autotuning on FP8 shapes had not been tested.

Test Plan:
```
CUDA_VISIBLE_DEVICES=0 TRITON_PRINT_AUTOTUNING=1 TRITON_ALWAYS_COMPILE=1 TORCH_LOGS=+inductor TORCHINDUCTOR_FORCE_DISABLE_CACHES=1 ENABLE_PERSISTENT_TMA_MATMUL=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM=1 TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_SEARCH_SPACE=DEFAULT buck2 run mode/{opt,inplace} pytorch/t
ritonbench:run -- --op fp8_gemm --only torch_fp8_gemm,pt2_fp8_gemm --metrics tflops,accuracy --input-loader=/home/jananisriram/personal/exhaustive_autotune_rowwise_persistent_tma/json_fi
les/rowwise_ptma_0.json --output=""/home/jananisriram/personal/exhaustive_autotune_rowwise_persistent_tma/autotune/gpu0_bench.csv"" --atol=1e-2 --rtol=0.5 2>&1 | tee ~/personal/exhaustive_
autotune_rowwise_persistent_tma/autotune/gpu0.log
```
autotunes on the maximum configs available, rather than the defaults, and skips configs not compatible with TMA.

Rollback Plan:

Differential Revision: D80958642




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 21:30:51+00:00,2025-09-02T21:21:13Z,,False,10,4,1,8,12,1,14,,73,1371,False,False,False,False,False,False,1,1,182,20,8,12,1,1,2.0,2.0,2025-08-27T00:41:57Z,pytorch
161441,closed,[BE] fix compute_global_tensor_shape test,XilunWu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161441

Fixes #161154 

**Test**
`pytest  test/distributed/tensor/test_utils.py -s -k test_compute_global_tensor_shape_1D`

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @mruberry",2025-08-25 21:26:02+00:00,2025-08-26T03:23:37Z,,False,5,0,2,3,2,2,5,2025-08-26 03:22:33+00:00,41,296,False,True,False,False,False,False,2,4,772,5,3,2,1,2,4.0,4.0,2025-08-25T21:47:31Z,pytorch
161440,closed,Add inductor provenance tracking artifacts to cache,yushangdi,"Summary: 

- Add inductor provenance tracking artifacts to cache
- Update the tlparse version pin to `0.4.0`. The old tlparse version errors out on the new tlparse output. The lowest tlparse version that works is `0.3.42`. 

tlparse error:
```
thread 'main' panicked at src/parsers.rs:671:71:
called `Result::unwrap()` on an `Err` value: Error(""EOF while parsing a value"", line: 1, column: 0)
stack backtrace:
   0:     0x55e4ff1c7f00 - <std::sys::backtrace::BacktraceLock::print::DisplayBacktrace as core::fmt::Display>::fmt::h6d42cc84fc840290
   1:     0x55e4ff1ee503 - core::fmt::write::h5af61a909e3ec64d
   2:     0x55e4ff1c4c33 - std::io::Write::write_fmt::h5a7b54aa6e4a315d
   3:     0x55e4ff1c7d52 - std::sys::backtrace::BacktraceLock::print::h555579e7396c26ac
   4:     0x55e4ff1c8caf - std::panicking::default_hook::{{closure}}::h9128866118196224
   5:     0x55e4ff1c8b1a - std::panicking::default_hook::h52e9e7314e0255f6
   6:     0x55e4ff1c9652 - std::panicking::rust_panic_with_hook::h541791bcc774ef34
   7:     0x55e4ff1c93fa - std::panicking::begin_panic_handler::{{closure}}::h6479a2f0137c7d19
   8:     0x55e4ff1c8419 - std::sys::backtrace::__rust_end_short_backtrace::ha04e7c0fc61ded91
   9:     0x55e4ff1c908d - rust_begin_unwind
  10:     0x55e4fef7a030 - core::panicking::panic_fmt::h5764ee7030b7a73d
  11:     0x55e4fef7a406 - core::result::unwrap_failed::h3ff7104a9ace307a
  12:     0x55e4fefb3c56 - <tlparse::parsers::ArtifactParser as tlparse::parsers::StructuredLogParser>::parse::h20bc51a17ffc494a
  13:     0x55e4fef9669a - tlparse::run_parser::h20c7729f151eec62
  14:     0x55e4fef99a1b - tlparse::parse_path::he4892147f47fbade
  15:     0x55e4fef7c760 - tlparse::main::hdc05613b32f4f53b
  16:     0x55e4fef89263 - std::sys::backtrace::__rust_begin_short_backtrace::h15f188f3edf42596
  17:     0x55e4fef8827d - std::rt::lang_start::{{closure}}::he2c21e32a442538e
  18:     0x55e4ff1be0f0 - std::rt::lang_start_internal::h15895544e2012228
  19:     0x55e4fef83975 - main
  20:     0x7f0b3662a610 - __libc_start_call_main
  21:     0x7f0b3662a6c0 - __libc_start_main_alias_2
  22:     0x55e4fef7a610 - <unknown>
  23:                0x0 - <unknown>
```

Test Plan:
```
buck run mode/dev-nosan fbcode//caffe2/test/inductor:provenance_tracing -- -r  test_kernel_information_generation
python test/dynamo/test_structured_trace.py -k test_chromium_event
```

Differential Revision: D80976585




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-25 21:15:48+00:00,2025-08-28T01:17:07Z,,False,9,0,1,75,43,8,9,2025-08-28 01:16:05+00:00,51,2632,False,False,False,False,False,False,8,1,476,118,75,43,1,1,2.0,1.0,2025-08-26T22:51:39Z,pytorch
161439,closed,[Inductor] Fix cross-device scalar lowering (cpu scalar with cuda tensor fails in torch.compile),karthickai,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161439



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-25 21:12:47+00:00,2025-09-25T02:11:08Z,,False,3,0,2,19,1,2,3,2025-08-25 21:40:04+00:00,96,305,False,True,False,False,False,False,2,1,64,22,20,2,2,2,1.0,1.0,2025-08-25T21:40:02Z,pytorch
161438,closed,Remove early torch::is_symint call in is_int_or_symint,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161635
* #161634
* #161633
* #161622
* #161596
* #161595
* #161591
* #161590
* #161588
* #161586
* #161466
* #161455
* __->__ #161438
* #161433
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286

If I am reading [THPUtils_checkIndex](https://github.com/pytorch/pytorch/blob/cf94cadbeee31a4d1d46a57f11bce7c9fd1cebc0/torch/csrc/utils.cpp#L28) correctly, it already checks torch::is_symint before hitting __index__ or __int__.",2025-08-25 21:11:25+00:00,2025-08-27T19:55:33Z,,False,2,0,1,0,8,1,2,2025-08-27 19:55:32+00:00,54,561,False,False,False,False,False,False,1,1,79,8,0,8,1,1,1.0,1.0,2025-08-27T19:55:33Z,pytorch
161437,closed,Skip const folding with symbolic expression,arui-meta,"Summary: When performing constant folding, we must skip over operators that have symbolic `fill_value`.

Test Plan:
CI

Rollback Plan:

Reviewed By: kalpit-meta-1

Differential Revision: D80965936




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-25 20:20:39+00:00,2025-08-27T22:11:05Z,,False,11,0,1,7,0,1,11,2025-08-27 22:10:02+00:00,43,257,False,False,False,False,False,False,1,1,476,7,7,0,1,1,2.0,1.0,2025-08-25T20:26:11Z,pytorch
161436,closed,Always run OIDC auth on B200 to be able to upload artifacts to S3,huydhn,"Reported by @drisspg , in its current form, the OIDC auth step wasn't run when the previous test step failed.  We need this to always run to be able to upload artifacts to S3.",2025-08-25 20:16:53+00:00,2025-09-25T02:11:06Z,,False,3,0,1,1,1,1,3,2025-08-25 21:05:23+00:00,65,175,False,False,False,False,False,False,1,2,818,2,1,1,1,1,4.0,2.0,2025-08-25T20:18:57Z,pytorch
161435,closed,"Back out ""[dynamic shapes] use prims_common contiguity in create_example_tensors""",pianpwk,"Summary:
Original commit changeset: 3ce7bf9df7b7

Original Phabricator Diff: D80503451

Test Plan:
-

Rollback Plan:

Differential Revision: D80970483




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 20:16:04+00:00,2025-08-28T17:28:25Z,,False,7,0,1,7,4,1,7,2025-08-28 17:28:25+00:00,81,356,False,False,False,False,False,False,1,3,181,11,7,4,1,1,2.0,3.0,2025-08-25T22:23:54Z,pytorch
161434,closed,[cuDNN][SDPA][Nested Tensor] add forward/backward caching support for cuDNN SDPA Nested tensor/varlen ,eqy,"Don't recompile every time

cc @csarofeen @ptrblck @xwang233 @msaroufim @jerryzh168 @cpuhrsch @jbschlosser @bhosmer @drisspg @soulitzer @davidberard98 @YuqingJ",2025-08-25 20:03:33+00:00,2025-09-08T17:52:18Z,,False,4,0,6,99,48,2,4,2025-09-08 17:51:16+00:00,102,159,False,False,False,False,False,False,2,3,610,303,177,126,2,6,3.0,3.0,2025-09-07T21:07:25Z,pytorch
161433,closed,Check for Index before Tensors in is_int_or_symint,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161635
* #161634
* #161633
* #161622
* #161596
* #161595
* #161591
* #161590
* #161588
* #161586
* #161466
* #161455
* #161438
* __->__ #161433
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286

Cheaper (and cheap in an absolute sense) check should come first.",2025-08-25 20:02:44+00:00,2025-08-27T19:55:48Z,,False,3,0,1,4,4,1,3,2025-08-27 19:55:48+00:00,50,399,False,False,False,False,False,False,1,2,123,8,4,4,1,1,1.0,2.0,2025-08-27T18:19:42Z,pytorch
161432,closed,Stop trying to intern arguments in PyObject_FastGetAttrString,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* __->__ #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301

If we want them interned, we should intern at callsites. (The numpy reference has bit rotted; see https://github.com/numpy/numpy/commit/b222eb66c79b8eccba39f46f020ed8303614a87f#diff-6bdb6105198083838f51c57b55b3a49472ed23043bb40018f1ea41138e687163)

Profiling a simple torchdispatch benchmark with perf before/after seems to show that time spent copying std::strings and interning Python strings is gone, though there is some noise and the improvement is very small.",2025-08-25 20:02:38+00:00,2025-08-30T06:56:58Z,,False,7,0,4,1,1,1,7,2025-08-30 06:55:53+00:00,61,649,False,False,False,False,True,False,1,6,1554,21001,14483,6518,1,4,3.0,6.0,2025-08-27T20:25:17Z,pytorch
161431,closed,[inductor] comm reordering: fix memory recalculation,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161431

_size_free_delta_update is filled only for nodes that change memory_size, but change applied for all nodes in the group.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 19:52:35+00:00,2025-09-24T14:21:21Z,,False,1,0,1,1,1,1,1,2025-09-24 14:21:21+00:00,52,417,False,True,False,False,False,False,1,0,0,2,1,1,1,1,,,,pytorch
161430,closed,Update Microsoft C++ Redistributable to the latest version,saman-amd,"Update Microsoft C++ Redistributable link to the latest version as one of the libraries used by AMD currently has a dependency on that.


",2025-08-25 19:25:32+00:00,2025-09-18T15:46:02Z,,False,5,0,1,1,1,1,5,2025-09-18 14:22:07+00:00,58,138,False,False,False,False,False,False,1,3,840,2,1,1,1,1,3.0,3.0,2025-08-25T19:34:02Z,pytorch
161429,closed,[ROCm] fix numpy version detection and adjust fudge_factors for MI355,dnikolaev-amd,"This PR fixes:

- Numpy >= 2.1 version detection (instead of python 3.13 version detection) to skip some tests (numpy 2.1 can be installed for older python versions)
```
test_quantization.py::TestDynamicQuantizedOps::test_qlinear
test_quantization.py::TestDynamicQuantizedOps::test_qlinear_legacy
test_quantization.py::TestQuantizedLinear::test_qlinear
test_quantization.py::TestQuantizedLinear::test_qlinear_leaky_relu
test_quantization.py::TestQuantizedLinear::test_qlinear_relu
test_quantization.py::TestQuantizedLinear::test_qlinear_tanh
test_quantization.py::TestQuantizedLinear::test_qlinear_with_input_q_dq_qweight_dq_output_fp32
``` 
- A couple of SDPA tests on MI355 by adjusting fudge_factors:

``` 
test_transformers.py::TestSDPACudaOnlyCUDA::test_mem_efficient_attention_attn_mask_vs_math_ref_grads_batch_size_1_seq_len_q_2048_seq_len_k_8_head_dim_8_is_causal_False_dropout_p_0_0_float32_scale_l1_cuda_float32
test_transformers.py::TestSDPACudaOnlyCUDA::test_mem_efficient_attention_vs_math_ref_grads_batch_size_8_seq_len_q_2048_seq_len_k_8_head_dim_128_is_causal_True_dropout_p_0_0_float32_scale0_cuda_float32 
```


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-25 19:25:21+00:00,2025-08-28T19:33:15Z,,False,6,0,1,4,2,2,6,2025-08-28 19:32:12+00:00,69,1246,False,True,False,False,False,False,2,5,1066,6,4,2,1,1,2.0,5.0,2025-08-26T19:01:35Z,pytorch
161427,closed,Remove Python 3.9 nightly builds,atalman,"Please see https://github.com/pytorch/pytorch/issues/161167


cc @albanD",2025-08-25 19:02:38+00:00,2025-08-25T22:06:46Z,,False,3,0,1,1,2164,6,3,2025-08-25 22:05:43+00:00,32,72,False,False,False,False,False,False,6,2,793,2165,1,2164,1,1,3.0,3.0,2025-08-25T19:33:21Z,pytorch
161426,closed,Fixes broken memory_viz link in CUDA memory docs,dsashidh,"Fixes #161375 


The  ""Using the visualizer"" section in torch_cuda_memory.md had a link to  https://pytorch.org/memory_viz written in inline Markdown link form. Strangely the same syntax worked earlier on the page as the issuer mentioned, but in this spot it's rendered sa a broken link.  

I wasn't able to pinpoint why the second occurrence was treated differently, but switching it to the Markdown autolink form fixes the problem consistently. I tested this by rebuilding the docs locally with make html and serving the HTML with a local http.server. With the autolink, the link resolves correctly. ",2025-08-25 18:58:47+00:00,2025-09-02T02:08:00Z,,False,8,0,1,1,1,1,8,2025-09-02 02:06:58+00:00,48,602,False,True,False,True,False,False,1,7,1811,2,1,1,1,1,5.0,8.0,2025-08-25T18:59:17Z,pytorch
161424,closed,DOC: Clarify documentation for torch.matmul and fix a typo,rec,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161424

",2025-08-25 18:16:56+00:00,2025-08-26T18:32:04Z,,False,4,0,2,10,10,1,4,2025-08-26 18:31:00+00:00,58,94,False,True,False,True,False,False,1,3,756,8333,4227,4106,1,2,3.0,4.0,2025-08-25T21:31:58Z,pytorch
161423,closed,[Pattern Matcher] improve error msg,mlazos,"Updates pattern matcher error message

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 18:14:11+00:00,2025-09-25T02:10:56Z,,False,3,0,1,3,1,1,3,2025-08-25 21:48:56+00:00,35,240,False,False,False,False,True,False,1,2,493,4,3,1,1,1,4.0,2.0,2025-08-25T18:38:28Z,pytorch
161421,closed,Add pg argument to consolidate_safetensors_files_on_every_rank,ankitageorge,"Summary: Based on feedback on https://github.com/pytorch/torchtitan/pull/1625, adding a pg argument to consolidate_safetensors_files_on_every_rank so that we don't infer the pg and users can supply one if needed.

Test Plan:
ensure existing tests pass

Rollback Plan:

Differential Revision: D80954339




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-25 18:01:12+00:00,2025-08-29T13:32:20Z,,False,4,0,1,15,22,1,4,2025-08-29 13:31:17+00:00,62,382,False,False,False,False,False,False,1,2,498,37,15,22,1,1,3.0,2.0,2025-08-29T05:18:03Z,pytorch
161420,closed,[Inductor] Improve RoPE,BoyuanFeng,"This PR fuses ROPE from 2 kernels into 1 kernel.

Shape:
```
q: [B, Hq, S, D]
k: [B, Hkv, S, D]
```

`Hq=32, Hkv=8, D=128` following Llama3 setting.

<img width=""980"" height=""624"" alt=""image"" src=""https://github.com/user-attachments/assets/652a8227-6f1d-465c-97fd-2b0af41f8ed9"" />








cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 17:59:37+00:00,2025-09-25T01:43:41Z,,False,9,8,24,241,0,6,17,2025-09-05 20:55:24+00:00,23,490,False,False,False,False,True,False,6,8,2140,82443,48455,33988,1,24,6.0,9.0,2025-08-28T19:32:58Z,pytorch
161417,closed,"[ROCm][CI] fbgemm_gpu depends on tbb, install it",amdfaa,"Fixes #ISSUE_NUMBER


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-25 17:12:54+00:00,2025-08-25T21:03:09Z,,False,2,0,1,1,0,1,2,2025-08-25 20:37:39+00:00,48,138,False,True,False,False,False,False,1,1,69,1,1,0,1,1,1.0,1.0,2025-08-25T20:37:39Z,pytorch
161416,closed,[hop] move insert_deferred_runtime_asserts under subtracer,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #160483
* #160467
* #160670
* #160669
* __->__ #161416
* #160548



cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-25 17:06:10+00:00,2025-08-27T17:44:10Z,,False,3,0,5,16,5,2,3,2025-08-27 17:43:05+00:00,58,202,False,False,False,False,False,False,2,2,493,10867,5276,5591,1,5,3.0,2.0,2025-08-26T22:03:32Z,pytorch
161414,open,[dynamic shapes] unbacked-safe slicing,pianpwk,"Summary:
Generates new unbacked symbols for slice output size & storage offset, when appropriate semantics are unclear. Teaches inductor to codegen the slice with flexible semantics.


Test Plan:
contbuild & OSS CI, see https://hud.pytorch.org/commit/pytorch/pytorch/56218d85e2da09d9ede3809718ec989c2151632c

Rollback Plan:

Differential Revision: D80948073




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 16:43:43+00:00,2025-09-23T17:01:05Z,,False,6,7,1,580,34,11,13,,38,563,False,False,False,False,False,False,11,0,54,614,580,34,1,1,2.0,1.0,2025-09-08T22:57:25Z,pytorch
161413,closed,[ez][CI] GIve the linux check job a name that isn't linux-job,clee2000,"Reason:
The default name is linux-job, which gets put in the linux category on HUD, but this isn't really a linux related job.  Renaming it like this will make it go into the ""other"" category on HUD

Other options:
Change the grouping code in test-infra",2025-08-25 16:43:00+00:00,2025-08-26T15:19:42Z,,False,3,0,1,2,0,1,3,2025-08-26 15:18:39+00:00,61,253,False,False,False,False,False,False,1,2,796,2,2,0,1,1,4.0,3.0,2025-08-25T22:01:07Z,pytorch
161412,closed,NLLLoss: validate target is 0D when input is 1D,mansiag05,"Add a shape check in nll_loss_forward to error out when both input and target are 1D. Added a unit test to cover the incompatible 1D/1D case.

Fixes #157420

cc @albanD @ngimel @peterbell10 @cyyever @kurtamohler ",2025-08-25 16:32:28+00:00,2025-09-09T07:05:18Z,,False,16,3,3,13,3,2,19,2025-09-06 20:58:45+00:00,47,212,False,True,False,False,False,False,2,14,5470,38,24,14,1,3,5.0,15.0,2025-08-25T18:24:44Z,pytorch
161411,closed,[pytorch] Simplify codes using `std::all_of()` for `_check_tensors_share_device_and_dtype()`,tsunghsienlee,"Summary: These two nested loops of checks could be simplified with `std::all_of()` to make it more compact.

Test Plan:
OSS CI & tests

Rollback Plan:

Differential Revision: D80946082


",2025-08-25 16:17:44+00:00,2025-08-26T08:57:30Z,,False,18,0,1,7,9,1,18,2025-08-26 08:56:27+00:00,92,187,False,False,False,False,False,False,1,10,2064,16,7,9,1,1,4.0,10.0,2025-08-25T17:38:55Z,pytorch
161410,closed,Fix issue with FloorDiv handling,nandesuka,"Summary:
The launch grid calculation is generated as string and evaluated to get sympy expressions. 

The default ""python"" mode generates expressions using negative integer division behaviour in python. This creates problems when we try generate generic code for representing the launch grid calculation at the FX graph level.

In this diff we:

1. Add a new mode ""python_slow"" which achieves the same ceildiv behaviour that is generally applicable to integer division
2. Move and generalize the logic for replacing sympy.floor to CeilDiv

Test Plan:
CI

Rollback Plan:

Differential Revision: D80744150




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 15:55:58+00:00,2025-09-04T16:55:48Z,,False,10,2,2,313,53,6,12,2025-09-04 16:55:48+00:00,32,809,False,True,False,False,False,False,6,0,5,368,314,54,1,2,2.0,1.0,2025-08-25T16:58:41Z,pytorch
161409,closed,[BE] [Inductor] Add Kernel name to all coor-desc tuning,njriasan,"Summary: When running coordinate descent tuning the logging is difficult to parse if the results are parallelized at all. This includes the kernel name in each step so post-processing can unify the results, even if run in parallel.

Test Plan:
NFC. Just a logging change.

Rollback Plan:

Differential Revision: D80942794




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 15:14:25+00:00,2025-09-05T02:54:20Z,,False,17,0,1,8,3,1,17,2025-09-05 02:53:17+00:00,55,527,False,False,False,False,False,False,1,7,1497,11,8,3,1,1,3.0,8.0,2025-08-25T15:15:35Z,pytorch
161408,closed,Forward fix periodic vision build,atalman,Trying to forward fix: https://github.com/pytorch/pytorch/issues/161358 use SM 80 architecture by default,2025-08-25 14:59:16+00:00,2025-08-25T23:29:26Z,,False,6,0,4,15,9,2,6,2025-08-25 23:28:25+00:00,33,105,False,True,False,False,False,False,2,5,1848,26,16,10,2,4,3.0,6.0,2025-08-25T15:24:55Z,pytorch
161407,closed,create torch._grouped_mm fallback path with for loops / bmm,vkuzo,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162159
* #162059
* #161717
* __->__ #161407

Summary:

Creates a fallback path for `torch._grouped_mm`, using the naive for
loop implementation (or bmm).

For the sake of keeping the PR small, this PR only enables SM80+ (CUDA
capability 8.0 and up), since I am testing this on an A100 machine. In
future PRs, we can increase the coverage of the fallback to:
1. float32 and float16, which will extend the GPU coverage
2. cpu

Test Plan:

```bash
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm_2d_3d -x
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm_3d_2d -x
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm_2d_2d -x
pytest test/test_matmul_cuda.py -s -k test_grouped_gemm_3d_3d -x
```

Reviewers:

Subscribers:

Tasks:

Tags:",2025-08-25 14:45:35+00:00,2025-09-04T17:48:57Z,,False,2,2,3,70,7,2,4,2025-09-04 17:48:55+00:00,59,828,False,False,False,False,False,False,2,1,48,72438,43131,29307,1,3,5.0,1.0,2025-08-25T21:32:08Z,pytorch
161406,closed,[inductor] Use runtime estimations in comm reordering/sink,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161975
* #161499
* __->__ #161406
* #161405



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81152295](https://our.internmc.facebook.com/intern/diff/D81152295)",2025-08-25 14:36:12+00:00,2025-09-24T14:45:38Z,,False,4,1,11,451,74,4,5,2025-09-24 14:45:38+00:00,58,494,False,False,False,False,False,False,4,3,474,1399,891,508,1,10,1.0,3.0,2025-08-27T18:24:29Z,pytorch
161405,closed,[inductor] Runtime estimations: use nccl estimator; mm only benchmark mode,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161405


During comms reordering , sink wait iterative observed previous runtime estimations pretty off for collectives and mms.

Adding optional usage of:
- c10d.time_estimator for collectives, which is based on NCCL estimator

Benchmark mode only for matmuls, as they are highly dependent on mm backend

- The logic mostly copied from Ruisi's PRs for inductor simple_fsdp https://github.com/pytorch/pytorch/pull/157572

This estimations corrections are in default `BaseSchedulerNode.estimate_runtime()`




cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben

Differential Revision: [D81152294](https://our.internmc.facebook.com/intern/diff/D81152294)",2025-08-25 14:36:06+00:00,2025-09-08T14:34:26Z,,False,17,30,16,324,32,8,47,2025-09-08 14:33:22+00:00,74,982,False,False,False,False,False,False,8,16,4475,107402,65042,42360,1,16,5.0,19.0,2025-08-27T18:24:24Z,pytorch
161404,open,Invert unary read and write for fusion,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161404


For [this repro](https://gist.github.com/eellison/75a99616a0fcca0436316bbfd8987fae) enables fusion of `to_blocked` with the prior `to_mx` calculation, so that there is only a single kernel per tensor, resulting in a 10% speedup of the non conversion code (need to update my local devserver to 12.9 to time the matmul as well).

The `to_mx` kernel has a contiguous write:

```Py
op6_op7: FusedSchedulerNode(SchedulerNode,SchedulerNode)
op6_op7.writes = [MemoryDep('buf6', c0, {c0: 2097152}), MemoryDep('buf7', c0, {c0: 67108864})]
op6_op7.unmet_dependencies = []
op6_op7.met_dependencies = [MemoryDep('arg1_1', c0, {c0: 67108864})]
op6_op7.outputs = [
    buf6: ComputedBuffer
    buf6.layout = FixedLayout('cuda:0', torch.float32, size=[8192, 256], stride=[256, 1])
    buf6.users = [
        NodeUser(node=SchedulerNode(name='op7'), can_inplace=False, is_weak=False),
        NodeUser(node=SchedulerNode(name='op9'), can_inplace=False, is_weak=False),
    ]
    buf7: ComputedBuffer
    buf7.layout = FixedLayout('cuda:0', torch.float8_e4m3fn, size=[8192, 256, 32], stride=[8192, 32, 1])
    buf7.users = [NodeUser(node=ExternKernelSchedulerNode(name='op10'), can_inplace=False, is_weak=False)]
]
```

While the `to_blocked` has a single discontiguous read and a single contiguous write.

```Py
op9: SchedulerNode(ComputedBuffer)
op9.writes = [MemoryDep('buf9', c0, {c0: 2097152})]
op9.unmet_dependencies = [   MemoryDep('buf6', 32768*((c0//32768)) + 8192*(((ModularIndexing(c0, 1, 16))//4)) + 256*(ModularIndexing(c0, 16, 32)) + 4*(ModularIndexing(c0, 512, 64)) + (ModularIndexing(ModularIndexing(c0, 1, 16), 1, 4)), {c0: 2097152})]
op9.met_dependencies = []
op9.outputs = [
    buf9: ComputedBuffer
    buf9.layout = FixedLayout('cuda:0', torch.float8_e8m0fnu, size=[2097152], stride=[1])
    buf9.users = [NodeUser(node=ExternKernelSchedulerNode(name='op10'), can_inplace=False, is_weak=False)]
]
```

To enable fusion, we invert the read, giving op9 and contiguous read and discontiguous write. More explanation here: https://gist.github.com/eellison/6f9f4a7ec10a860150b15b719f9285a9

[Tlparse with this optimization](https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/eellison/custom/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=10000). 

[Tlparse without this optimization](https://manifold.edge.x2p.facebook.net/v0/read/tree/logs/eellison/custom/index.html?bucketName=tlparse_reports&apiKey=tlparse_reports-key&withPayload=1&timeoutMsec=10000).

cc @msaroufim @jerryzh168 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-25 14:32:39+00:00,2025-09-25T18:31:43Z,,False,7,10,7,791,1,6,17,,38,2833,False,True,False,False,False,False,6,6,1698,256191,154943,101248,1,7,4.0,6.0,2025-08-25T20:40:38Z,pytorch
161401,closed,Fix torch.argmax/torch.argmin fail for non-contiguous input on MPS,can-gaa-hou,"The error occurs when the parameter `dim` is not set while calling `torch.argmax` or `torch.argmin`. On MPS, the input tensor will be flattened when the `dim` has no value. 
https://github.com/pytorch/pytorch/blob/4651aaac47ff855e08a74e2fdbfa605bc53afba8/aten/src/ATen/native/mps/operations/ReduceOps.mm#L903-L915

The error arises from here.
https://github.com/pytorch/pytorch/blob/4651aaac47ff855e08a74e2fdbfa605bc53afba8/aten/src/ATen/native/mps/operations/ReduceOps.mm#L961

The `Placeholder` will call the `view_impl` function, which fails to compute the stride and causes the error.
https://github.com/pytorch/pytorch/blob/1eccfb157ab9855b3f81872a23502fb15f455e0a/aten/src/ATen/native/TensorShape.cpp#L4028-L4037

In this PR, I fix it by copying the input tensor and reshaping it into a continuous 1-D tensor when `dim` has no value to compute the stride correctly. Also, I am adding a testcase for it.
",2025-08-25 12:54:30+00:00,2025-08-26T01:42:41Z,,False,2,0,1,21,1,2,2,2025-08-25 14:48:23+00:00,66,909,False,True,False,False,False,False,2,0,0,22,21,1,1,1,,,,pytorch
161400,open,fix _clone_meta stride computation for torch.preserve_format,morrison-turnansky,"Fixes #161010

fixed stride issue for cloning meta tensor",2025-08-25 12:34:12+00:00,2025-09-16T16:16:19Z,,False,7,7,4,16,1,2,14,,60,57,False,True,False,False,False,False,2,6,1079,31,23,8,1,4,3.0,7.0,2025-08-25T15:54:51Z,pytorch
161399,closed,[ATen][CUDA] Add family conditional for CUTLASS matmuls,Aidyn-A,"This PR adds a custom CMake command to compile CUTLASS kernels with family conditionals. The necessity of custom command arises because NVCC does not allow compiling with both `sm_100` and `sm_100f` flags enabled at the same time. Unfortunately, `sm_100` is stored in a global variable `CMAKE_CUDA_FLAGS` which cannot be adjusted for particular targets.
Th flags `sm_100f` and `sm_120f` are important to make those matmuls work on B300/GB300 and DGX Spark.

cc @ptrblck @msaroufim @eqy @jerryzh168 @manuelcandales @SherlockNoMad @angelayi",2025-08-25 12:11:48+00:00,2025-09-18T15:39:24Z,,False,3,10,5,77,14,3,13,2025-09-18 15:39:24+00:00,55,538,False,False,False,False,False,False,3,2,504,103,83,20,1,5,5.0,2.0,2025-08-25T12:40:02Z,pytorch
161398,closed,[Cutlass] Fix regression from  f7ad69f,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161335
* __->__ #161398



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-25 08:57:36+00:00,2025-09-25T02:10:58Z,,False,5,0,2,23,21,1,5,2025-08-25 22:08:33+00:00,38,307,False,True,False,False,False,False,1,4,176,31529,24373,7156,1,2,2.0,4.0,2025-08-25T17:39:14Z,pytorch
161397,open,[WIP][3/N] Enable TestCompositeCompliance/TestMathBits/TestFakeTensor on Intel GPU,daisyden,"Following https://github.com/pytorch/pytorch/pull/161246, this PR enabled 3 test classes on Intel GPU in test_ops.py.


cc @gujinghui @EikanWang @fengyuan14 @guangyey",2025-08-25 08:18:19+00:00,2025-09-19T11:20:21Z,,False,1,0,9,354,40,6,1,,82,166,False,False,False,False,False,False,6,0,0,496,405,91,1,9,,,,pytorch
161396,closed,[xla hash update] update the pinned xla hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned xla hash.",2025-08-25 07:42:41+00:00,2025-09-01T11:44:09Z,,False,6,0,1,1,1,1,6,2025-09-01 11:43:06+00:00,44,155,False,False,False,False,False,False,1,5,1406,2,1,1,1,1,2.0,5.0,2025-08-25T07:42:42Z,pytorch
161395,closed,Update slow tests,pytorchupdatebot,"This PR is auto-generated weekly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/weekly.yml).
Update the list of slow tests.",2025-08-25 07:41:07+00:00,2025-09-08T13:34:39Z,,False,9,0,1,242,242,1,9,2025-09-08 13:33:35+00:00,17,156,False,False,False,False,False,False,1,8,2411,484,242,242,1,1,2.0,8.0,2025-08-25T07:41:14Z,pytorch
161394,closed,"Update the operator benchmarking, to benchmark using torch.compile",jainapurva,"This pull request enhances the PyTorch operator benchmarking suite by introducing support for benchmarking with `torch.compile` mode, in addition to existing Eager and JIT. It also adds peak memory measurement (fwd/bwd pass); improves the output format in JSON to be used by dashboard for reporting; and introduce some more CLI options. The new CLI flags introduced are:

- Added `--use-compile` CLI argument and corresponding logic to run benchmarks using `torch.compile`, including mutual exclusivity with `--use-jit`
- Added `--benchmark-name` argument for customizing the benchmark name in output 
- Updated default value for `--output-json-for-dashboard` to `benchmark-results.json` for more predictable output file name

Sample command to run a single operator: 
`python -m pt.mm_test --use-compile`",2025-08-25 07:19:19+00:00,2025-09-09T19:10:21Z,,False,6,11,13,155,30,3,17,2025-09-09 18:17:40+00:00,66,805,False,False,False,False,True,False,3,5,1446,105414,65947,39467,1,13,4.0,7.0,2025-09-03T20:24:17Z,pytorch
161392,closed,Add uuid to XPU device properties,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161392

# Motivation
Fix https://github.com/intel/torch-xpu-ops/issues/1955
Refer to https://github.com/intel/llvm/blob/sycl/sycl/doc/extensions/supported/sycl_ext_intel_device_info.md#device-uuid, `ext::intel::info::device::uuid` returns `std::array<unsigned char, 16>` as the UUID.",2025-08-25 07:07:14+00:00,2025-09-02T06:42:40Z,,False,20,3,15,73,46,9,23,2025-09-02 06:41:35+00:00,33,369,False,True,False,True,False,False,9,19,4999,89984,58712,31272,1,15,5.0,21.0,2025-08-25T07:36:35Z,pytorch
161390,closed,Fix import error in failures_histogram.py script,Copilot,"The `failures_histogram.py` script in `scripts/compile_tests/` was failing to run due to an incorrect import statement. The script was trying to import `download_reports` from the `common` module, but this function is actually defined in the `download_reports` module.

## Problem
Running the script would fail with:
```
ImportError: cannot import name 'download_reports' from 'common'
```

This made the script completely unusable for analyzing Dynamo test failures and generating GitHub issues from test results.

## Solution
Fixed the import statement by separating the imports correctly:

**Before:**
```python
from common import download_reports, get_testcases, key, open_test_results, skipped_test
```

**After:**
```python
from common import get_testcases, key, open_test_results, skipped_test
from download_reports import download_reports
```

## Testing
- Verified the script now runs without import errors: `python failures_histogram.py --help` works correctly
- Confirmed all required functions are accessible and callable
- Created comprehensive tests to ensure the fix is working properly
- Verified argument parsing and core functionality remain intact

This is a minimal, surgical fix that resolves the import issue without affecting any other functionality of the script.

<!-- START COPILOT CODING AGENT TIPS -->
---

✨ Let Copilot coding agent [set things up for you](https://github.com/pytorch/pytorch/issues/new?title=✨+Set+up+Copilot+instructions&body=Configure%20instructions%20for%20this%20repository%20as%20documented%20in%20%5BBest%20practices%20for%20Copilot%20coding%20agent%20in%20your%20repository%5D%28https://gh.io/copilot-coding-agent-tips%29%2E%0A%0A%3COnboard%20this%20repo%3E&assignees=copilot) — coding agent works faster and does higher quality work when set up for your repo.
",2025-08-25 06:12:35+00:00,2025-08-25T06:22:41Z,,False,2,0,2,2,1,1,2,2025-08-25 06:19:57+00:00,48,1814,False,True,False,True,False,False,1,0,0,3,2,1,1,1,,,,pytorch
161389,closed,[CI] Fix XPU ci test permission issue,chuanqi129,"Due to new test runners, refer https://github.com/pytorch/pytorch/actions/runs/17161094208/job/48694776064#step:2:124",2025-08-25 06:04:13+00:00,2025-08-30T00:05:06Z,,False,3,0,1,1,1,1,3,2025-08-30 00:04:03+00:00,37,117,False,True,False,False,False,False,1,2,654,2,1,1,1,1,2.0,2.0,2025-08-29T23:55:24Z,pytorch
161388,closed,[pytorch] Leverage `unordered_map.try_emplace()` to simplify code,tsunghsienlee,"Summary: Because [`unordered_map.try_emplace()`](https://en.cppreference.com/w/cpp/container/unordered_map/try_emplace.html) does not invoke value's constructor if key is already existed, this matches with the previous the behavior on checking the key's existence first, and then instantiate the value.

Test Plan:
OSS CI & tests

Rollback Plan:

Differential Revision: D80916349


",2025-08-25 04:54:11+00:00,2025-08-25T23:35:05Z,,False,8,2,1,28,30,1,10,2025-08-25 23:34:02+00:00,65,382,False,False,False,False,False,False,1,2,511,58,28,30,1,1,4.0,3.0,2025-08-25T17:09:28Z,pytorch
161387,closed,[pytorch] Merge two nested if statement checks into one,tsunghsienlee,"Summary: This reduces the code indentation level by one.

Test Plan:
OSS CI & tests

Rollback Plan:

Differential Revision: D80915357


",2025-08-25 03:56:42+00:00,2025-08-26T08:46:40Z,,False,20,0,1,3,5,1,20,2025-08-26 08:45:39+00:00,55,136,False,False,False,False,False,False,1,12,2769,8,3,5,1,1,3.0,12.0,2025-08-25T17:28:01Z,pytorch
161386,closed,fix: support SymInt[] in custom op schema to prevent symbolic tracing error (#122129),Quantum-Kayak,"Fixes #122129

This PR updates the schema of `ezyang::split_with_sizes_and_clone` to use `SymInt[]` instead of `int[]`. This resolves an error when tracing with `torch.compile`, where symbolic shapes (SymInt) are passed to an op that only accepts `List[int]`.

Changes:
- Schema updated to `SymInt[]`
- Replaces deprecated `impl_abstract` with `register_fake`
- Adds a minimal repro script for validation

This allows custom ops to be used in `torch.compile` contexts with dynamic shapes without raising casting errors.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @ezyang
",2025-08-25 03:41:22+00:00,2025-08-26T13:57:33Z,,False,3,0,4,115,0,4,3,2025-08-26 13:57:16+00:00,85,731,False,True,False,False,False,False,4,2,264,115,115,0,2,4,2.0,2.0,2025-08-26T05:49:02Z,pytorch
161385,closed,[fx] Add is_fx_symbolic_tracing flag,angelayi,"Fixes https://github.com/pytorch/pytorch/issues/135276

cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-25 03:40:20+00:00,2025-08-26T22:27:36Z,,False,6,0,1,50,6,6,6,2025-08-26 22:26:31+00:00,36,249,False,True,False,False,False,False,6,5,1380,56,50,6,1,1,3.0,5.0,2025-08-26T16:52:29Z,pytorch
161383,closed,[rfc] aot precompile with custom backend api,zhxchen17,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161383

Adding a new feature to torch.compile(fullgraph=True) which ""aot_compile"" a function with given example inputs.

On user side it should look like:

```
def foo(x, y):
    return x + y

compiled_fn = torch.compile(fullgraph=True).aot_compile(((torch.randn(3, 4), torch.randn(3, 4)), {}))
```

This is different from the traditional `torch.compile` workflow where compiled object will be a drop-in replacement for the original eager model:
```
tensor input -> torch.compile() -> tensor output (and populates the cache entry)
```
`aot_compile` will instead return a compiled function as result, and it's purely functional and doesn't populate the compile cache entry in dynamo:
```
tensor input -> aot_compile() -> compiled function
```
The aot compiled function will be savable and loadable on disk as well:
```
torch.compile(fullgraph=True).aot_compile(...).save_compiled_function('my/path')
compiled_fn = torch.compiler.load_compiled_function(""my/path"")
```

Right now we treat compiler backend as a blackbox and it needs to implement the following interface to make compile artifacts serialzable:
```
class SerializableCallable:
    def save_compile_artifacts(): ....
    def load_compile_artifacts(): ....
```
We haven't implemented this for inductor yet, but this shouldn't be an issue since we gate this feature through `torch._dynamo.config.aot_compile` (which defaults to False), and this will be left as follow up PR to the current PR.

Differential Revision: [D80914270](https://our.internmc.facebook.com/intern/diff/D80914270/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-25 02:54:51+00:00,2025-08-27T21:27:32Z,,False,21,0,7,345,1,6,21,2025-08-27 21:26:27+00:00,44,1802,False,False,True,False,False,False,6,14,3088,20628,10954,9674,1,6,3.0,14.0,2025-08-25T19:31:42Z,pytorch
161382,closed,[poc] aot precompile with custom backend api,zhxchen17,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161382
* #160900

Adding a new feature to torch.compile(fullgraph=True) which ""aot_compile"" a function with given example inputs.

On user side it should look like:

```
def foo(x, y):
    return x + y

compiled_fn = torch.compile(fullgraph=True).aot_compile(((torch.randn(3, 4), torch.randn(3, 4)), {}))
```

This is different from the traditional `torch.compile` workflow where compiled object will be a drop-in replacement for the original eager model:
```
tensor input -> torch.compile() -> tensor output
```
`aot_compile` will instead return a compiled function as result:
```
tensor input -> aot_compile() -> compiled function
```
The aot compiled function will be savable and loadable on disk as well:
```
torch.compile(fullgraph=True).aot_compile(...).save_compiled_function('my/path')
compiled_fn = torch.compiler.load_compiled_function(""my/path"")
```

Right now we treat compiler backend as a blackbox and it needs to implement the following interface to make compile artifacts serialzable:
```
class SerializableCallable:
    def save_compile_artifacts(): ....
    def load_compile_artifacts(): ....
```
We haven't implemented this for inductor yet, but this shouldn't be an issue since we gate this feature through `torch._dynamo.config.aot_compile` (which defaults to False), and this will be left as follow up PR to the current PR.

Differential Revision: [D80914270](https://our.internmc.facebook.com/intern/diff/D80914270/)

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-25 02:52:37+00:00,2025-08-28T19:27:39Z,,False,6,0,1,314,1,6,6,2025-08-28 19:27:39+00:00,44,1697,False,False,True,False,False,False,6,1,56,315,314,1,1,1,1.0,1.0,2025-08-28T19:27:39Z,pytorch
161380,closed,CUDA 13 -- sm_120 -- Nvidia 5090 -- ptxas warning : Value of threads …,DrStone71,"bug fix:

i have opened a issue ( https://github.com/pytorch/pytorch/issues/161376 ) and i suggest this bug fix.

In this metod compile fine.

Fixes #161376




cc @ptrblck @msaroufim @eqy @jerryzh168",2025-08-24 23:18:41+00:00,2025-09-03T09:57:28Z,,False,17,1,2,2,1,1,18,2025-09-02 13:28:01+00:00,70,200,False,True,False,False,False,False,1,14,2583,5,3,2,1,2,5.0,15.0,2025-08-25T19:23:16Z,pytorch
161379,closed,[small][muon] Use addmm for Newton–Schulz orthogonalization,chuanhaozhuge,"A performance optimization. Using `torch.addmm`, which fuses `matrix multiply + scale + add` into one op.

**Benchmark**
In a QWEN-like 0.5B model training we observed average `optimizer.step()` latency speedup: matmul ~44.5 ms -> addmm ~27.4 ms: a **1.62×** speedup.

matmul
<img width=""1403"" height=""600"" alt=""Screenshot 2025-08-24 at 3 15 37 PM"" src=""https://github.com/user-attachments/assets/a77a68d4-da3c-473a-97f0-e6ef0a3b46d9"" />

addmm
<img width=""1426"" height=""602"" alt=""Screenshot 2025-08-24 at 3 13 42 PM"" src=""https://github.com/user-attachments/assets/e493af36-44d3-4026-9f7c-fd0f9cdbc7e5"" />


**Testing**
End-to-end training: 
We used a training script that pre-trains a QWEN-like model on `openwebtext-100k` dataset. We trained for one epoch and the resulting loss curves show consistency between normal matmul and addmm.
<img width=""1035"" height=""434"" alt=""Screenshot 2025-08-24 at 2 56 21 PM"" src=""https://github.com/user-attachments/assets/b96b13e3-0a01-4908-853c-d917b41f3d75"" />

Unit test:

```python
    # dummy model and data
    model0 = Linear(10, 10, bias=False)
    model1 = copy.deepcopy(model0)
    inputs = torch.randn(8, 10)
    targets = torch.randn(8, 10)
    loss = MSELoss()

    lr = 1e-3
    wd = 0.1
    momentum = 0.95

    opt_ref_muon = Muon(
        params=model0.parameters(),
        lr=lr,
        weight_decay=wd,
        momentum=momentum,
        nesterov=nesterov,
        adjust_lr_fn=""original"",
    )

    opt_exp_muon = Muon(
        params=model1.parameters(),
        lr=lr,
        weight_decay=wd,
        momentum=momentum,
        nesterov=nesterov,
        adjust_lr_fn=""original"",
        use_addmm=True,
    )

    out_ref = model0(inputs)
    loss_ref = loss(out_ref, targets)
    opt_ref_muon.zero_grad()
    loss_ref.backward()
    opt_ref_muon.step()

    out_exp = model1(inputs)
    loss_exp = loss(out_exp, targets)
    opt_exp_muon.zero_grad()
    loss_exp.backward()
    opt_exp_muon.step()

    for p_ref, p_exp in zip(model0.parameters(), model1.parameters()):
        torch.testing.assert_close(p_ref, p_exp)
```

shows numeric difference, but this is expected on bf16 precision:
```
Mismatched elements: 96 / 100 (96.0%)
Greatest absolute difference: 8.985400199890137e-05 at index (1, 9) (up to 1e-06 allowed)
Greatest relative difference: 0.007370449136942625 at index (0, 6) (up to 1e-05 allowed)
```

~~Introduced a flag that allows users to opt in, as there are numerical differences relative to the original implementation.~~
Update: since `addmm` fuses the math ops, there are fewer intermediate roundings and is therefore more numerically accurate compared to the original form. Based on this, we opt to make `addmm` the default and only option.
",2025-08-24 22:42:46+00:00,2025-08-26T09:18:33Z,,False,8,0,5,10,16,2,8,2025-08-26 09:17:31+00:00,59,2731,False,False,False,False,False,False,2,7,1860,100,47,53,1,5,4.0,9.0,2025-08-25T20:15:23Z,pytorch
161378,closed,[NVIDIA] Generate aarch64 binaries for blackwell family,johnnynunez,"Include Thor, Spark and GB300. When Orin be SBSA, we can include it
Fixes #161377

@Skylion007 @tinglvv @ptrblck @nWEIdia ",2025-08-24 20:39:43+00:00,2025-08-25T14:41:51Z,,False,4,3,3,3,0,1,7,2025-08-25 14:37:31+00:00,55,122,False,True,False,False,False,False,1,3,416,7,5,2,1,3,3.0,3.0,2025-08-24T20:42:53Z,pytorch
161374,open,Use `itertools.batched` if available within `new_subgroups`,evanwporter,"Uses the new `itertools.batched` (`cp3.12`) functionality if its available instead of list comprehension. Something worth considering is if we should move the try except outside of the function so it doesn't have to check every time `new_subgroups` is called.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-24 18:22:53+00:00,2025-09-05T18:22:04Z,,False,4,5,4,12,4,1,9,,59,337,False,False,False,False,False,False,1,1,118,38,23,15,1,4,2.0,2.0,2025-08-24T18:37:44Z,pytorch
161373,closed,Update README.md,thehottestfemboy,More precise,2025-08-24 17:55:28+00:00,2025-09-18T14:22:40Z,,False,3,0,1,1,1,1,3,2025-09-18 14:22:29+00:00,16,12,False,False,False,False,False,False,1,1,118,2,1,1,1,1,1.0,1.0,2025-09-18T14:22:29Z,pytorch
161370,open,[Submodule] Bump kineto to latest,cyyever,Simplify the CUPTI check because kineto has used `CUDA::cupti`.,2025-08-24 07:51:23+00:00,2025-09-19T06:01:05Z,,False,14,0,1,3,34,2,14,,33,63,False,False,False,False,False,False,2,13,2246,37,3,34,1,1,4.0,13.0,2025-08-24T18:39:27Z,pytorch
161369,open,Enable `torch.Generator` to support pytorch/xla generator implementation ,iwknow,"Currently, the implementation of `torch.Generator` only support ""cpu"" and ""cuda"" device type.  https://github.com/pytorch/pytorch/blob/main/torch/csrc/Generator.cpp#L55-L61

This change enables `torch.Generator` to support more device type by allowing any device backend to register their own generator factory through a Generator Registry. This is similar to what ""DeviceGuardImpl registry"" does today.


# Key Changes:

## New registry API:

* Added GeneratorRegistry.h and GeneratorRegistry.cpp in c10/core/impl.
* API supports registerGenerator(DeviceType, GeneratorFactory), unregisterGenerator(DeviceType), and getGeneratorFactory(DeviceType).
* Uses c10::DeviceType as the key and stores a factory function returning c10::intrusive_ptr<c10::GeneratorImpl>.

## Python/C++ integration:

* The registry is consulted in the torch.Generator constructor path for non-CPU/CUDA devices.
* If a factory is registered for the requested device, it constructs the appropriate generator; otherwise, raises an error.

## Backend extensibility:

* Out-of-tree backends (e.g., torch_xla, torch-directml, torch_npu) can now register their custom generator implementation at module load via a static registrar object.
Example usage:
```
C++
namespace {
  struct Registrar {
    Registrar() {
      at::detail::registerGenerator(c10::DeviceType::XLA, &CreateXlaGenerator);
    }
  } registrar_instance;
}
```

This allows torch.Generator(device='xla') to return an XlaGeneratorImpl when the torch_xla extension is imported.",2025-08-24 05:10:41+00:00,2025-09-16T16:17:02Z,,False,15,3,5,106,1,3,18,,73,1512,False,False,False,False,False,False,3,13,5749,26812,18417,8395,1,5,4.0,16.0,2025-08-24T22:57:54Z,pytorch
161367,closed,[ignore][codex-test] Add typing to simple library registry,bobrenjc93,"## Summary
- add type annotations for simple library registry and dispatch rule holder
- remove allow-untyped-defs directive

## Testing
- `python -m mypy torch/_library/simple_registry.py` *(fails: repo expects mypy==1.16.0)*
- `lintrunner -a torch/_library/simple_registry.py` *(fails: attr-defined error in torchgen/gen_schema_utils.py)*
- `python test/test_torch.py TestTorch.test_dir` *(fails: ModuleNotFoundError: No module named 'torch')*

------
https://chatgpt.com/codex/tasks/task_e_68aa3cc210488326befdd992c79115a0",2025-08-24 03:34:48+00:00,2025-09-23T02:10:01Z,,False,21,0,2,14,13,1,21,2025-09-23 02:08:58+00:00,58,525,False,False,False,False,False,False,1,20,6286,35,18,17,1,2,3.0,21.0,2025-09-04T17:42:07Z,pytorch
161364,closed,[dynamo] Pass requires_grad to nn.Parameter construction,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161362
* __->__ #161364

Fixes https://github.com/pytorch/pytorch/issues/161191

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-24 01:17:00+00:00,2025-09-25T02:10:47Z,,False,3,0,4,15,1,2,3,2025-08-25 16:49:32+00:00,56,330,False,True,False,False,False,False,2,2,493,2622,1792,830,1,4,4.0,2.0,2025-08-24T17:55:23Z,pytorch
161363,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-08-24 00:29:11+00:00,2025-08-28T04:15:24Z,,False,15,0,1,1,1,1,15,2025-08-28 04:14:21+00:00,46,156,False,False,False,False,False,False,1,14,4487,2,1,1,1,1,2.0,14.0,2025-08-24T00:29:12Z,pytorch
161362,closed,[dynamo][vllm] Support typing.get_type_hints,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161362



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-23 23:33:45+00:00,2025-08-27T09:56:38Z,,False,13,2,13,125,1,11,15,2025-08-27 09:55:34+00:00,44,266,False,False,False,False,False,False,11,12,5054,17782,10017,7765,1,13,5.0,12.0,2025-08-23T23:36:14Z,pytorch
161359,open,"Add gfx1150,gfx1151 in hipBLASLt supported list",jammm,"Add gfx1150,gfx1151 in hipBLASLt supported list",2025-08-23 15:31:42+00:00,2025-08-25T07:46:07Z,,False,8,0,1,1,1,1,8,,47,47,False,False,False,False,False,False,1,3,200,2,1,1,1,1,3.0,3.0,2025-08-23T15:33:05Z,pytorch
161355,closed,[dynamo][hops] Remove const outputs from the speculated subgraph,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161355



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-23 06:24:50+00:00,2025-09-04T18:53:09Z,,False,5,7,5,167,73,6,12,2025-09-04 18:52:04+00:00,64,266,False,False,False,False,False,False,6,4,2494,22503,15155,7348,1,5,4.0,7.0,2025-08-27T12:30:55Z,pytorch
161354,closed,[dynamo][higher order ops] Refactor for out spec,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161355
* __->__ #161354

Preparing for the next PR to add more info in the output spec.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-23 06:24:45+00:00,2025-08-27T14:42:25Z,,False,3,0,1,45,31,1,3,2025-08-27 14:41:21+00:00,48,338,False,False,False,False,False,True,1,2,493,76,45,31,1,1,3.0,2.0,2025-08-27T12:46:57Z,pytorch
161353,closed,[dtensor] support local_map as a decorator,xmfan,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161458
* #161479
* __->__ #161353

And extract it out as a convenience function for dynamo to wrap

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-23 05:33:53+00:00,2025-08-28T01:51:28Z,,False,5,0,4,137,109,2,5,2025-08-28 01:46:28+00:00,42,255,False,False,False,False,False,False,2,4,1074,1364,696,668,1,4,3.0,5.0,2025-08-26T21:41:18Z,pytorch
161352,closed,space added between type and checking for typechecking,RajeshvShiyal,"space added between type and checking for ""typechecking""

Fixes #161282


cc @svekars @sekyondaMeta @AlannaBurke",2025-08-23 05:25:59+00:00,2025-08-26T02:08:41Z,,False,4,0,1,1,1,1,4,2025-08-26 02:07:37+00:00,54,112,False,True,False,False,False,False,1,3,827,2,1,1,1,1,3.0,4.0,2025-08-23T05:39:12Z,pytorch
161351,closed,[inductor] FlexibleLayout for ExternKernelChoice for mms,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* #161350
* __->__ #161351

# why

- if we only use ExternKernelChoice we're not doing any codegen
- if we're not doing any codegen, we can use a FlexibleLayout
  here, and provide deeper passes more chances to change it

# what

- if all the kernel template choices (KTC) are with a ExternKernelChoice
  template, we switch to a FlexibleLayout before generating the choice
- add a test to make sure that works as intended (FlexibleLayout for
  only extern, and FixedLayout if Triton is involved)
    
- caveats:
    - because CPP, CUTLASS, and CK are not using
       V.choices.get_mm_configs yet, we turn off the optimization
       if either of those backends are in use. This will be relaxed
       once they support this too
    - because Triton templates are still using their own calls
       (not a single call) to get_mm_configs, it's also turned
       off there. The next diff unifies Triton + ATEN to a single
       call to get_mm_configs and that in turn allows the optimization
       there too

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520584](https://our.internmc.facebook.com/intern/diff/D81520584)",2025-08-23 02:58:20+00:00,2025-09-12T21:10:48Z,,False,20,8,38,116,43,4,28,2025-09-12 21:10:48+00:00,56,1487,False,True,False,False,False,False,4,19,2549,113276,67165,46111,1,29,5.0,19.0,2025-09-02T20:28:21Z,pytorch
161350,closed,[inductor] leverage template stacking in V.choices.get_mm_configs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* __->__ #161350
* #161351

# why

- now everything is in place to just gather templates and run
  the V.choices.get_mm_configs once per op
- enables any overrides inside V.choices.get_mm_configs to
  have a full view of the options for an op, not just for
  one template

# what

- replace multiple calls to V.choices.get_mm_configs with
  calls to gather the active templates, and then using those
  in a single call

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520571](https://our.internmc.facebook.com/intern/diff/D81520571)",2025-08-23 02:58:16+00:00,2025-09-12T21:10:49Z,,False,16,0,36,110,136,4,16,2025-09-12 21:10:48+00:00,65,896,False,False,False,False,False,False,4,15,1958,113422,67238,46184,1,29,4.0,15.0,2025-09-02T20:28:26Z,pytorch
161349,closed,[inductor] V.choices.get_mm_configs override point,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* #161350
* #161351
* #162238
* __->__ #161349
* #161348
* #161347

# why

- enable us to override the default configs, or fall back to them
  through subclassing InductorChoices

# what

- override (private) function
- default implementationt takes the kernel template choice (ktc)
  generator for every template and just executes the generator
- future overrides can decide to replace those generators, or filter
  out choices

- the 2nd expensive step (maybe_append_choices, choice_or_none) is
  handled outside this function, in the main V.choices.get_mm_configs
  this means that any overriding benefits from not generating expensive
  templates that aren't going to be used

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520570](https://our.internmc.facebook.com/intern/diff/D81520570)",2025-08-23 02:58:12+00:00,2025-09-09T17:17:10Z,,False,11,0,32,94,40,1,11,2025-09-09 17:17:08+00:00,50,1157,False,False,False,False,False,False,1,10,1470,112946,66991,45955,1,29,3.0,10.0,2025-09-02T20:28:16Z,pytorch
161348,closed,[inductor][ez] V.choices.get_mm_configs returns list of ChoiceCallers,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* #161350
* #161351
* #162238
* #161349
* __->__ #161348
* #161347

\# why

- every callsite just executes the generator on the spot
- previous pr adds the ability to add an override before expensive
  generators are executed, so we don't need this generator anymore

\# what

- rather than yielding the ChoiceCaller, just return the list of all
  valid ChoiceCallers

\# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520574](https://our.internmc.facebook.com/intern/diff/D81520574)",2025-08-23 02:58:08+00:00,2025-09-09T17:17:09Z,,False,14,2,30,11,8,2,16,2025-09-09 17:17:07+00:00,69,846,False,False,False,False,False,False,2,13,2999,125936,74067,51869,1,29,3.0,13.0,2025-09-02T20:28:11Z,pytorch
161347,closed,[inductor] add kernel template choice (ktc),coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161468
* #162293
* #161350
* #161351
* #162238
* #161349
* #161348
* __->__ #161347

# why

- gather everything up to make choices, without running
  potentially expensive generators
- enables overrides where we toss the entire list of configs
  from inductor, without having to enumrate it (expensive)

# what

- add a holding class that just gets all the components necessary
  to generate a ChoiceCaller
- use that class to generate ChoiceCallers
- this does not (yet) add the override function, but just prepares
  the scene

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520569](https://our.internmc.facebook.com/intern/diff/D81520569)",2025-08-23 02:58:04+00:00,2025-09-09T17:17:08Z,,False,15,0,30,111,10,2,15,2025-09-09 17:17:07+00:00,43,978,False,False,False,False,False,False,2,14,1862,125719,74007,51712,1,29,4.0,14.0,2025-09-02T20:28:05Z,pytorch
161346,closed,[inductor] V.choice.get_mm_configs takes a stack of templates,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* __->__ #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- enables us to just gather relevant templates and get all
  choices at once
- that in turns allows us to make op wide override decisions

# what

- V.choice.get_mm_configs takes a stack of templates
- all callsites just provide a stack of size 1 right now
  but do not merge everything yet (other features pending)

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520583](https://our.internmc.facebook.com/intern/diff/D81520583)",2025-08-23 02:58:00+00:00,2025-09-05T18:03:03Z,,False,8,2,29,75,55,4,10,2025-09-05 18:03:03+00:00,61,928,False,False,True,False,False,False,4,7,886,112755,66858,45897,1,28,3.0,7.0,2025-09-02T15:59:39Z,pytorch
161345,closed,[inductor][ez] return choicecallers directly,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* __->__ #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- remove repeat patterns
- we have everything to make the choicecallers
  - templates
  - input_nodes
  - layouts
  - all the kwargs

# what

- yield a choicecaller directly from V.choices.get_mm_configs

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520577](https://our.internmc.facebook.com/intern/diff/D81520577)",2025-08-23 02:57:56+00:00,2025-09-05T18:03:02Z,,False,8,4,29,185,229,7,12,2025-09-05 18:03:01+00:00,44,816,False,False,False,False,False,False,7,7,886,113001,66948,46053,1,28,3.0,7.0,2025-08-30T01:10:47Z,pytorch
161344,closed,[inductor] move max-autotune logic inside V.choices.get_mm_configs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* __->__ #161344
* #161343
* #161342
* #161341
* #161340
* #162075

# why

- heuristics providers know decide whether to (or which choices to add)
  in the max-autotune case
- enables an eventual override point to gracefully fallback to the
  standard behavior

# what

- max-autotune is determined inside V.choices.get_mm_configs
  because it's mm only right now, we can just do
  `config.max_autotune or config.max_autotune_gemm`
  a TODO indicates that this can change in the future when this
  expands to more templates

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520573](https://our.internmc.facebook.com/intern/diff/D81520573)",2025-08-23 02:57:52+00:00,2025-09-05T18:03:01Z,,False,8,4,29,174,34,11,12,2025-09-05 18:03:00+00:00,66,1061,False,False,False,False,False,False,11,7,886,112684,66860,45824,1,28,3.0,7.0,2025-08-30T01:05:51Z,pytorch
161343,closed,[inductor][ez] pass template rather than template.uid,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* __->__ #161343
* #161342
* #161341
* #161340
* #162075

# why

- simpler interface
- enables future of extracting more things out of the template e.g. a
  hash

# what

V.choices.get_mm_configs now takes the whole template rather than just
the template.uid

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520576](https://our.internmc.facebook.com/intern/diff/D81520576)",2025-08-23 02:57:48+00:00,2025-09-05T18:03:00Z,,False,8,0,27,41,31,4,8,2025-09-05 18:02:59+00:00,53,806,False,False,False,False,False,False,4,7,886,112454,66680,45774,1,26,3.0,7.0,2025-08-30T01:04:37Z,pytorch
161342,closed,[inductor][aten] treat like a template in GEMMs,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* __->__ #161342
* #161341
* #161340
* #162075

# why

- central point to analyze and override all generated choices

# what

- add a pseudo heuristic for aten that just yields a single, empty
  kwargs
- add a pseudo heuristic with the bias_addmm logic for it
- add an addmm specific heuristic that yields a single choice, but
  also expands it with alpha and beta kwargs

- replace all the aten.bind calls with V.choices.get_mm_configs
  using the now matching API for aten

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @Lucaskabela

Differential Revision: [D81520580](https://our.internmc.facebook.com/intern/diff/D81520580)",2025-08-23 02:57:44+00:00,2025-09-05T18:02:59Z,,False,8,4,27,229,116,7,12,2025-09-05 18:02:58+00:00,47,1045,False,False,False,False,False,False,7,7,886,112670,66839,45831,1,26,3.0,7.0,2025-08-30T01:04:06Z,pytorch
161341,closed,[inductor][ez] add template/externchoice uid,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* __->__ #161341
* #161340
* #162075

# why

- to have a central registry of templates/externkernelchoice
  to match them to heuristics etc, they need unique names
- mm is both the triton template name and the aten_mm name

# what

- add a uid() to KernelTemplate/ExternKernelChoice that returns name
- override in ExternKernel to prepend ""aten::""
- override in TritonTemplate to prepend ""triton::""

This id is just use to find template heuristics, so it has no other
impact

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520579](https://our.internmc.facebook.com/intern/diff/D81520579)",2025-08-23 02:57:41+00:00,2025-09-05T18:02:58Z,,False,8,4,26,139,74,8,12,2025-09-05 18:02:57+00:00,44,1042,False,False,False,False,False,False,8,7,886,112383,66672,45711,1,25,4.0,7.0,2025-08-30T00:58:30Z,pytorch
161340,closed,[inductor] move scaled_mm input nodes logic,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #162238
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* __->__ #161340
* #162075

# why

- a step towards a unified interface for all choices, where any
  adjustment to nodes (e.g. unsqueezing) happens as part of
  choice specific preprocessing, behind a common point

# what

- move the unsqueeze logic for triton nodes for scaled_mm inside
  the new hookup for adjusting the kernel inputs for template
  heuristics

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v -k ""scale""
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520582](https://our.internmc.facebook.com/intern/diff/D81520582)",2025-08-23 02:57:37+00:00,2025-09-05T18:02:58Z,,False,8,0,26,44,41,2,8,2025-09-05 18:02:57+00:00,43,951,False,False,False,False,False,False,2,7,886,112163,66531,45632,1,26,4.0,7.0,2025-08-30T00:57:07Z,pytorch
161339,closed,[inductor][ez] add hook for heuristics to adjust kernel input nodes,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* __->__ #161339
* #161338
* #161336
* #161126
* #161125
* #161124
* #161123

# why

- some templates e.g. scale_mm need to unsqueeze/squeeze the nodes
  for codegen and heuristics

- unified place where we can just adjust them for the template

# what

- inside get_mm_configs, return not the passed in kernel inputs,
  but allow the template heuristic to adjust them if necessary

- the default implementation right now just passes them back

this diff just adds the functionality, but does not exercise it
other than the default (passthrough)

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520572](https://our.internmc.facebook.com/intern/diff/D81520572)",2025-08-23 02:57:33+00:00,2025-09-03T18:24:31Z,,False,11,0,20,17,1,2,11,2025-09-03 18:23:26+00:00,67,1123,False,False,False,False,False,False,2,10,2152,48422,30613,17809,1,20,5.0,10.0,2025-08-29T20:56:32Z,pytorch
161338,closed,[inductor][choices][ez] pass through layout and input_nodes,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #161339
* __->__ #161338
* #161336
* #161126
* #161125
* #161124
* #161123

# why

- params already available in get_mm_configs
- simplifies the code
- adds a possibility to edit the nodes/layout in
  a centralized place

# what

- add layout and input_nodes into extra_kwargs
- no other modifications

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py -v
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520575](https://our.internmc.facebook.com/intern/diff/D81520575)",2025-08-23 02:57:29+00:00,2025-09-03T01:04:06Z,,False,4,0,20,3,22,4,4,2025-09-03 01:04:05+00:00,59,881,False,False,False,False,False,False,4,3,364,48429,30599,17830,1,20,4.0,3.0,2025-08-30T00:54:42Z,pytorch
161337,closed,[inductor][ez] aten pseudo template heuristic,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161469
* #161468
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #161339
* #161338
* __->__ #161337
* #161336
* #161126
* #161125
* #161124
* #161123
* #161098
* #161097
* #161026

# why

- enable aten externkernelchoice to go through the same flow as
  templates

# what

- a single choice heuristic, that yields an empty kwarg

this pr does not use the heuristic yet, it just exposes it

# testing

n/a

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov",2025-08-23 02:57:25+00:00,2025-08-27T07:16:45Z,,False,1,2,14,30,0,1,3,2025-08-27 07:16:45+00:00,45,756,False,False,False,False,False,False,1,0,0,25965,13697,12268,1,14,2.0,0.0,2025-08-27T01:16:14Z,pytorch
161336,closed,[inductor][ez] ExternChoice with maybe_append_choice,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162017
* #161534
* #161350
* #161351
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #161339
* #161338
* __->__ #161336
* #161126
* #161125
* #161124
* #161123

# why

- make the API for ExternChoice the same as KernelTemplate
- make it possible to use the same retrieval point as templates

# what

- add a maybe_append_choice to ExternChoice that under the hood
  invokes self.bind

This pr does not actuate the new path, but just exposes it

# testing

```
python3 -bb -m pytest test/inductor/test_max_autotune.py
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov

Differential Revision: [D81520578](https://our.internmc.facebook.com/intern/diff/D81520578)",2025-08-23 02:57:21+00:00,2025-09-03T01:04:06Z,,False,4,4,20,26,5,2,8,2025-09-03 01:04:04+00:00,52,935,False,False,False,False,False,False,2,3,364,48395,30614,17781,1,20,3.0,3.0,2025-08-27T01:15:26Z,pytorch
161335,closed,[Cutlass-EVT] Fix buffer size issues,mlazos,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161335
* #161398



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-23 02:04:34+00:00,2025-09-25T02:11:01Z,,False,9,1,4,16,15,1,10,2025-08-25 22:08:34+00:00,36,307,False,True,False,False,False,False,1,8,3730,31560,24389,7171,1,4,4.0,8.0,2025-08-23T02:49:32Z,pytorch
161334,closed,[dtensor] Add propagate_tensor_meta function that skips cache if _are_we_tracing,azahed98,"Fixes an issue where the log softmax handler checked the tensor metadata cache without checking for tracing or symints.

Probably best to merge this after #160798, but not strictly blocking.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-23 01:18:29+00:00,2025-08-29T04:51:15Z,,False,3,2,3,40,1,3,5,2025-08-26 18:47:01+00:00,80,268,False,True,False,False,False,False,3,2,551,43,41,2,1,3,4.0,3.0,2025-08-25T20:14:54Z,pytorch
161333,closed,[MPS] Fix index_copy for strided indices,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161333

By passing strides to strided variant of the tensor

Fixes https://github.com/pytorch/pytorch/issues/160993",2025-08-23 01:05:51+00:00,2025-09-23T02:08:33Z,,False,3,0,1,5,3,3,3,2025-08-23 14:39:00+00:00,40,201,False,True,False,False,False,False,3,2,511,8,5,3,1,1,4.0,3.0,2025-08-23T01:25:42Z,pytorch
161332,closed,Add meta for add.Scalar,angelayi,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161332

Fixes https://github.com/pytorch/pytorch/issues/161076

",2025-08-23 00:49:52+00:00,2025-08-26T02:29:44Z,,False,6,0,3,12,0,1,6,2025-08-26 02:26:54+00:00,23,150,False,True,False,False,False,False,1,5,1916,16,14,2,1,3,3.0,5.0,2025-08-23T17:20:15Z,pytorch
161331,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-08-23 00:26:04+00:00,2025-09-23T02:08:31Z,,False,3,0,1,1,1,1,3,2025-08-23 04:21:06+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-08-23T00:26:05Z,pytorch
161329,closed,Fix accidental copy in pushPyOutToStack,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* __->__ #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301

`auto` forces a copy. Confirmed this did something noticable with perf.",2025-08-23 00:08:22+00:00,2025-08-30T06:55:52Z,,False,6,0,5,1,1,1,6,2025-08-30 06:55:52+00:00,39,255,False,True,False,False,False,False,1,5,1013,32519,21417,11102,1,5,6.0,5.0,2025-08-23T00:14:57Z,pytorch
161328,closed,Avoid double hash lookup in torch._library.simple_registry,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* __->__ #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301

Not a huge cost, but free win is free.",2025-08-23 00:01:18+00:00,2025-08-30T06:55:51Z,,False,6,0,5,5,3,2,6,2025-08-30 06:55:51+00:00,58,222,False,False,False,False,False,False,2,5,576,32524,21420,11104,1,5,3.0,5.0,2025-08-23T16:58:48Z,pytorch
161325,closed,[TEST][DO NOT MERGE] test bc-linter with the new config,izaitsevfb,"Testing the new bc-lint config as close to prod as reasonable.

This PR introduces four changes that test the [new bc-linter config ](https://github.com/pytorch/pytorch/pull/161319), testing the permutations of two factors:
* the change is excluded based on the config
* the change is a BC-violation


Changes tests:

- Included + violation: tools/vscode_settings.py
    - Changed deep_update(d, u) → deep_update(d, u, merge_lists) and updated the internal call to pass True.
    - This introduces a new required positional parameter, which should trigger a violation.
    - File: tools/vscode_settings.py
    - File: tools/vscode_settings.py

- Included + no violation: tools/vscode_settings.py
    - Added harmless public helper add_one(x: int) -> int.
    - Additions don’t trigger violations per stronghold’s checks.

- Excluded + “violation”: tools/test/linter_test_case.py
    - Modified method signature lint_test(self, path, args, mock_stdout) → lint_test(self, path, args, mock_stdout, required_flag).
    - This would be a violation if included, but **/test/** is excluded, so bc-linter should ignore it.

- Excluded + no violation: tools/test/test_upload_stats_lib.py
    - Added a top-of-file comment only (no API change).
    - As an excluded path with a harmless diff, bc-linter should report nothing.
    
    
---

The expectation is that only one change triggers BC-linter, namely ""Included + violation: tools/vscode_settings.py""",2025-08-22 23:33:47+00:00,2025-09-17T01:49:08Z,,False,1,0,1,16,6,4,1,2025-09-17 01:49:08+00:00,55,1446,False,False,False,False,False,False,4,0,0,22,16,6,1,1,,,,pytorch
161323,closed,[ONNX] Refactor torchscript based exporter,justinchuby,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161546
* #161449
* __->__ #161323

Refactor torchscript based exporter logic to move them to a single (private) location for better code management. Original public module and method apis are preserved.

- Updated module paths in `torch/csrc/autograd/python_function.cpp` accordingly
- Removed `check_onnx_broadcast` from `torch/autograd/_functions/utils.py` because it is private&unused

@albanD / @soulitzer could you review changes in `torch/csrc/autograd/python_function.cpp` and
`torch/autograd/_functions/utils.py`? Thanks!

## BC Breaking
- **Deprecated members in `torch.onnx.verification` are removed**


cc @titaiwangms @albanD

Differential Revision: [D81236421](https://our.internmc.facebook.com/intern/diff/D81236421)",2025-08-22 23:10:42+00:00,2025-09-02T16:11:39Z,,False,11,4,19,19034,18923,63,15,2025-09-02 16:10:33+00:00,42,809,False,False,False,False,False,True,63,10,1912,53481,28639,24842,1,19,7.0,11.0,2025-08-25T22:55:38Z,pytorch
161322,closed,[ONNX] Remove unused _onnx_supported_ops,justinchuby,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161323
* __->__ #161322

Signed-off-by: Justin Chu <justinchuby@users.noreply.github.com>",2025-08-22 23:10:37+00:00,2025-09-23T02:08:21Z,,False,3,0,1,0,98,1,3,2025-08-23 02:42:28+00:00,40,168,False,False,False,False,False,False,1,2,621,98,0,98,1,1,3.0,2.0,2025-08-22T23:29:46Z,pytorch
161321,closed,Update NVSHMEM to 3.3.24 and fix download link,tinglvv,"https://github.com/pytorch/pytorch/issues/159779

Update NVSHMEM 3.3.24 for [PyTorch CUDA13 Binary Cannot Be Built with SM_75 with NVSHMEM](https://github.com/pytorch/pytorch/issues/160980) 
Enabled back sm_75 for NVSHMEM
Fixed the NVSHMEM download link for the issue with 3.3.20 download in issue - [[CD] nvshem-3.3.9 wheels for aarch64 is not manylinux2_28 compliant](https://github.com/pytorch/pytorch/issues/160425)

Todo: Should also enable back build ARM with NVSHMEM since it is compatible with manylinux2_28

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @atalman @malfet @ptrblck @eqy @nWEIdia 


",2025-08-22 22:43:12+00:00,2025-08-26T13:27:25Z,,False,13,0,5,10,15,4,13,2025-08-26 13:26:22+00:00,46,637,False,True,False,False,False,False,4,12,2939,27,11,16,1,5,5.0,12.0,2025-08-25T15:25:30Z,pytorch
161320,open,Add stable parallel_for,mikaylagawarecki,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161320

",2025-08-22 22:38:10+00:00,2025-08-23T17:25:53Z,,False,2,2,3,247,37,9,4,,23,94,False,False,False,False,False,False,9,0,0,316,263,53,1,3,1.0,0.0,2025-08-23T17:25:53Z,pytorch
161319,closed,Add initial bc-linter configuration,izaitsevfb,"Preparation for https://github.com/pytorch/test-infra/pull/7016

Currently merging this PR is a noop change for PyTorch repo (bc-linter is not looking at the config yet).",2025-08-22 22:30:11+00:00,2025-09-22T02:16:27Z,,False,3,0,1,15,0,1,3,2025-08-22 22:54:29+00:00,35,170,False,False,False,False,False,False,1,2,799,15,15,0,1,1,4.0,3.0,2025-08-22T22:47:18Z,pytorch
161317,closed,Improve assert perf in _python_dispatch._correct_storage_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* __->__ #161317
* #161315
* #161308
* #161304
* #161292
* #161301

This assertion was expensive because of is_traceable_wrapper_subclass. Finding a cheap check to run first that's likely to let us skip the rest seems to improve things significantly.",2025-08-22 22:23:45+00:00,2025-08-30T06:55:49Z,,False,16,0,6,10,1,1,16,2025-08-30 06:55:49+00:00,65,366,False,False,False,False,True,False,1,15,3936,32529,21426,11103,1,6,5.0,15.0,2025-08-22T23:10:20Z,pytorch
161316,closed,Use -compress-mode=size for CUDA 13 build for binary size reduction,tinglvv,"https://github.com/pytorch/pytorch/issues/159779

CUDA 13 added the support for --compress-mode flag for nvcc across all drivers of CUDA 13.X toolkits, enabling the possibility to use --compress-mode=size for significant size reduction (~71% less for CUDA Math APIs for example). https://developer.nvidia.com/blog/whats-new-and-important-in-cuda-toolkit-13-0/

Why we have to add for CUDA 13 only, quote from @ptrblck : Any usage of --compress-mode=size/balance will drop the support of older CUDA drivers and will bump the min. driver requirement to CUDA 12.4. https://github.com/pytorch/pytorch/pull/157791#issuecomment-3058027353

Default for CUDA 13 will be --compress-mode=balance which gives smaller binaries than LZ4 speed mode used in previous CUDA versions.

Related - https://github.com/pytorch/pytorch/pull/157791 

cc @ptrblck @nWEIdia @atalman @malfet",2025-08-22 22:06:34+00:00,2025-08-24T03:29:35Z,,False,4,2,2,5,1,1,6,2025-08-24 03:28:33+00:00,67,864,False,False,False,False,False,False,1,3,1498,10,7,3,1,2,4.0,3.0,2025-08-22T22:15:11Z,pytorch
161315,closed,Fix pybind enum efficiency issue in return_and_correct_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* #161317
* __->__ #161315
* #161308
* #161304
* #161292
* #161301

Scanning a list of pybind enums with `in` is slow. See NOTE in code for full explanation.

This is a significant optimization; will be updating the torchdispatch/return_and_correct_aliasing portion of this stack with benchmark and results soonish.",2025-08-22 22:02:37+00:00,2025-08-30T06:55:48Z,,False,8,2,5,17,2,1,10,2025-08-30 06:55:48+00:00,63,431,False,True,False,False,False,False,1,7,336,32537,21433,11104,1,5,4.0,7.0,2025-08-22T22:18:51Z,pytorch
161314,closed,[export] serialization support for triton_kernel_wrapper_functional,dolpm,"Summary: att

Test Plan:
buck2 test mode/opt //caffe2/test:test_export -- test_triton_hop

Rollback Plan:

Differential Revision: D80827767


",2025-08-22 21:39:03+00:00,2025-08-28T17:43:54Z,,False,22,0,1,219,1,2,22,2025-08-28 17:42:51+00:00,67,142,False,False,False,False,False,False,2,1,476,220,219,1,1,1,2.0,1.0,2025-08-27T23:01:20Z,pytorch
161313,closed,echo variables,yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161313
* #161192

",2025-08-22 21:16:12+00:00,2025-09-23T02:08:28Z,,False,2,0,8,3,0,1,2,2025-08-23 07:26:46+00:00,14,104,False,False,False,False,False,False,1,0,0,7478,4347,3131,1,7,,,,pytorch
161312,closed,Bump onnxscript to 0.4.0 in CI,justinchuby,"Use onnxscript apis for torch 2.9.


cc @titaiwangms",2025-08-22 21:13:44+00:00,2025-08-23T00:10:35Z,,False,6,0,3,4,4,3,6,2025-08-22 23:23:12+00:00,30,52,False,False,False,False,False,False,3,5,1693,8,4,4,1,3,4.0,5.0,2025-08-22T21:24:00Z,pytorch
161311,closed,[DeviceMesh] Clarifying flatten use case,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161311

Since we are in the middle of big refactoring and simplying the bookkeeping for device mesh. We found an interesting bug inside DeviceMesh flatten implementation. Here is the finding:
1. In unit test, we assume users can call `dp_cp_mesh._flatten()` many times but no backend will be created (aka cached).
2. From the implementation of slicing, we actually throw exception erroring out doing the `_flatten` more than once. But there is bug which was partially fixed in https://github.com/pytorch/pytorch/pull/160709 but it does not fixed the check for the case when we call the `_flatten` twice.

What's more important question to ask is, what behavior we want for `_flatten`? Do we allow calling `_flatten` multiple times (with same mesh_name)? I think we should, why?
1. We allow slicing for the same mesh_name or name_list multiple times, and we cache the PG behinds. Although we will return a new device mesh object everytime, when we compare them they are all the same (according to __eq__).
2. We actually cached the flattened mesh today inside `root_to_flatten_mapping` and actually do the early return but that  line will never be reached if we error out before that.

Also we should allow a no-op for flatten a 1D mesh into itself's mesh_dim_name, I added a unit test for it.

cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-08-22 21:12:28+00:00,2025-09-10T07:47:57Z,,False,4,8,6,24,7,2,12,2025-09-10 07:46:53+00:00,40,1473,False,True,False,False,False,True,2,3,629,111594,73466,38128,1,6,4.0,3.0,2025-08-22T21:22:48Z,pytorch
161309,closed,[SymmMem] Allow handle->buffer to point to tensor's data_ptr,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161309

This PR is intended to guard against a case where a user passes `x[1]` to our ops.
If we use storage ptr to search in the `symm_mem` map, `x[1]` would end up with a hit, returning the same handle as `x[0]`. But `handle->buffer` points to `x[0]` address instead of `x[1]`. Since most of our ops today feed these buffer addresses into the kernel, so I think it'd be better for them to be distinctive.

Using `data_ptr` as key would create more entries in the `symm_mem` map (hopefully not by much), but I think that can be compensated by reducing the handle creation cost (later PRs).

(Only changed for NVSHMEM backend.)

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 20:50:29+00:00,2025-09-08T21:52:31Z,,False,5,1,1,32,18,7,6,2025-09-08 21:52:30+00:00,60,791,False,False,False,False,False,False,7,4,1130,50,32,18,1,1,3.0,4.0,2025-08-22T22:12:58Z,pytorch
161308,closed,Optimize _python_dispatch.return_and_correct_aliasing.get_write_alias,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* __->__ #161308
* #161304
* #161292
* #161301

- Empty containers are Falsey
- Hoist cheap checks first
- Microbenchmarked single-element set access method

Benchmark code:
```
import timeit

to_test = [
    ('list(x)', 'x = set([3])'),
    ('x[0]', 'x = [3]'),
    ('list(x)[0]', 'x = set([3])'),
    ('next(iter(x))', 'x = set([3])'),
]

for (stmt, setup) in to_test:
    res = timeit.timeit(stmt=stmt, setup=setup)
    print(f""Time for `{stmt}`: {res}"")
```

Result with Python 3.13 on Mac (with excess digits manually trimmed; directionally matches result on Linux)
```
Time for `list(x)`: 0.03418
Time for `x[0]`: 0.00852
Time for `list(x)[0]`: 0.03561
Time for `next(iter(x))`: 0.02278
```

FWIW, I was surprised by this result, so I guess I'm glad I wrote the benchmark!",2025-08-22 20:37:39+00:00,2025-08-30T06:55:47Z,,False,10,1,6,5,5,1,11,2025-08-30 06:55:47+00:00,69,914,False,False,False,False,False,False,1,9,1075,32528,21421,11107,1,6,3.0,9.0,2025-08-22T22:20:06Z,pytorch
161307,closed,Add rule for typechecking maintainers,lolpack,"Allow the following people merge rights on type checking configs:
  - @lolpack
  - @maggiemoss
  - @ndmitchell
  - @kinto0
",2025-08-22 20:31:28+00:00,2025-09-25T00:15:37Z,,False,3,0,1,15,0,1,3,2025-09-25 00:14:34+00:00,37,123,False,False,False,False,False,False,1,2,500,15,15,0,1,1,3.0,3.0,2025-08-22T20:36:21Z,pytorch
161306,closed,[export] Update unflattening dynamo.disable,angelayi,"Summary:
Doing inline disabling causes recompiles with the reason ""Cache line
invalidated because L['___stack0'] got deallocated""

Test Plan:
CI

Rollback Plan:

Differential Revision: D80816956


",2025-08-22 20:24:39+00:00,2025-08-27T00:28:21Z,,False,8,0,1,12,5,1,8,2025-08-27 00:27:18+00:00,43,197,False,False,False,False,False,False,1,4,1436,17,12,5,1,1,3.0,4.0,2025-08-26T16:52:47Z,pytorch
161305,closed,[cuBLASLt][FP8] `cuBLASLt` appears to support float8 rowwise-scaling on H100,eqy,"Following #157905 I think the macro around
```
  TORCH_INTERNAL_ASSERT(use_rowwise == false, ""rowwise scaled_gemm not supported with blaslt"");
```
was never updated and this would cause `float8` tests to fail. Also it appears the `Lt` accepts two inputs with `e4m3` and `e5m2` dtypes simultaneously, so removing that check here as well...

CC @lw 

cc @ptrblck @msaroufim @jerryzh168 @csarofeen @xwang233 @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 20:22:57+00:00,2025-09-05T16:56:14Z,,False,20,4,5,27,12,3,24,2025-09-05 16:55:12+00:00,76,603,False,False,False,False,False,False,3,19,4886,45,30,15,3,5,7.0,19.0,2025-08-23T17:11:25Z,pytorch
161304,closed,"Use `is`, not ==, to check exact type matches in _python_dispatch",swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* __->__ #161304
* #161292
* #161301

`is` checks object identity and is more efficient. Google seems to confirm it is the correct way to do an exact type check.",2025-08-22 20:17:12+00:00,2025-08-30T06:55:47Z,,False,9,1,6,2,2,1,10,2025-08-30 06:55:46+00:00,65,307,False,False,False,False,False,False,1,8,446,32522,21418,11104,1,6,4.0,9.0,2025-08-22T22:19:16Z,pytorch
161303,closed,Move inductor workflows to Python 3.9 -> 3.10,atalman,Related to: https://github.com/pytorch/pytorch/issues/161167,2025-08-22 20:07:35+00:00,2025-09-10T14:41:40Z,,False,2,0,3,94,94,11,2,2025-09-10 14:41:39+00:00,45,60,False,False,False,False,False,False,11,1,34,188,94,94,1,3,1.0,1.0,2025-09-10T14:41:39Z,pytorch
161301,closed,Fix forced copying def_property_readonly for FunctionSchema & friends,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* __->__ #161301

This took me a bit to figure out and I'm pretty sure I've looked at
this code before. Pybind uses
`return_value_policy::reference_internal` for `def_property`, which
[causes the owning object to be kept alive for the lifespan of the
return
value](https://pybind11.readthedocs.io/en/stable/advanced/functions.html),
allowing the getter to safely avoid copying the property
value. However, lambdas act like they return `auto`, not
`decltype(auto)`, so our lambdas themselves were forcing copies!

Testing: observed std::vector<Argument> copying disappear in Linux
perf profile of someOpInfo._schema.arguments/returns (in
_python_dispatch.correct_storage_aliasing).

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-22 19:41:58+00:00,2025-08-30T06:55:45Z,,False,13,3,6,10,16,1,16,2025-08-30 06:55:45+00:00,69,895,False,True,False,True,False,False,1,12,1544,32555,21444,11111,1,6,5.0,13.0,2025-08-23T17:14:09Z,pytorch
161300,closed,Fix non-const reference arguments in torch/csrc/jit/python/init.cpp,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161695
* #161694
* #161693
* #161692
* #161635
* #161634
* #161633
* #161622
* #161596
* #161595
* #161591
* #161590
* #161588
* #161586
* #161466
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* __->__ #161300
* #161286

Shouldn't be any generated code impact, just fixing bad practice.

cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-22 19:41:54+00:00,2025-08-29T19:02:39Z,,False,11,1,4,25,21,1,12,2025-08-29 19:01:35+00:00,67,468,False,True,False,False,False,False,1,10,986,29056,18683,10373,1,4,5.0,10.0,2025-08-23T17:02:12Z,pytorch
161299,open,[CUDA] Compare major version of the runtime device arch against the built version of the pytorch binary,nWEIdia,"Fixes misleading warning messages when running on sm12x devices using binaries built with sm120. 
PyTorch binary built with sm120 is compatible with e.g. sm121, so no need for the warning of incompatibility. 

Also allow the 'matched_cuda_warn' message to show when e.g. the user is running a binary built with only sm90 on sm12x, so that the user would be prompted to get a build which supports e.g. sm120. 

cc @ptrblck @tinglvv @atalman @malfet @eqy ",2025-08-22 19:31:18+00:00,2025-09-25T19:31:11Z,,False,18,5,5,3,6,1,23,,103,453,False,True,False,False,False,False,1,16,3701,25,11,14,1,5,7.0,17.0,2025-08-22T19:44:43Z,pytorch
161298,closed,[inductor] add MSVC language pack check.,xuhancn,"Check MSVC's language pack: https://github.com/pytorch/pytorch/issues/157673#issuecomment-3051682766

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 19:28:50+00:00,2025-08-23T07:07:55Z,,False,3,0,7,192,1,1,3,2025-08-23 07:06:52+00:00,40,409,False,False,False,False,False,False,1,2,493,237,214,23,1,7,3.0,2.0,2025-08-22T20:03:07Z,pytorch
161297,closed,Get Inductor periodic CI green,zou3519,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161297

I'll file hi-pri issues for the things that need looking into.

Test Plan:
- wait for CI

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-22 19:11:40+00:00,2025-09-25T02:11:04Z,,False,3,1,4,5,5,2,4,2025-08-26 00:49:52+00:00,30,354,False,False,False,False,False,False,2,2,786,11407,7054,4353,1,4,3.0,2.0,2025-08-22T20:55:27Z,pytorch
161294,closed,[CUDAGraph] Add getter for cuda graph exec,galv,"This is far simpler than #155164 since we never destroy the cudaGraphExec_t.

The request comes from TRT-LLM specifically. The motivation is that some power users would like to mutate specific kernel parameters via APIs like `cudaGraphExec*SetParams` after a cuda graph has been instantiated. For example, a common request has been to be able to change the sequence length of attention kernels, after having captured a graph for the largest possible sequence length. It turns out that the host overhead you eliminate via cuda graphs in LLM inference ends up causing an increase in computation time when you size your kernels to the maximum possible sequence length (which I believe is done in both TRT-LLM and vLLM). Attention is the most problematic kernel because its computation time is quadratic in the sequence length, rather than linear.

This can work if your attention kernel can work for arbitrary shapes (this is not the case for all attention implementations! Many of them specialize with templates), and you have a persistent kernel that allocates only as many blocks as you have SM's (so you don't have to figure out how many blocks to allocate for a specific sequence length). Using a conditional SWITCH node is a better generic approach to this problem, but that requires more infrastructure work.

Note that this requires knowledge of the exact location of the value in your kernel's parameter buffer to mutate. It won't work with arbitrary stream capture code whose kernels you don't know before hand. So I expect this code path to be rarely used.

Testing:

```
pytest -s -k raw_graph_exec test/test_cuda.py
```



cc @mcarilli @ezyang @eellison @penguinwu @BoyuanFeng",2025-08-22 18:42:47+00:00,2025-08-25T20:58:44Z,,False,11,5,2,56,0,6,16,2025-08-25 20:57:41+00:00,42,1686,False,False,False,False,False,False,6,10,6578,112,84,28,1,2,6.0,10.0,2025-08-22T18:51:08Z,pytorch
161293,closed,type misc init and tools for dynamo,Lucaskabela,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames",2025-08-22 18:41:54+00:00,2025-08-26T17:39:56Z,,False,4,2,3,208,189,4,6,2025-08-26 17:38:52+00:00,35,179,False,True,False,False,False,False,4,3,541,433,226,207,1,3,4.0,3.0,2025-08-22T21:07:45Z,pytorch
161292,closed,Stop accessing func._schema in _python_dispatch.correct_storage_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161455
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* __->__ #161292
* #161301

func._schema is a pybind, accessing the arguments/returns is expensive, we have no reason to do it anyway, and even though #161301 makes accessing the arguments/returns less expensive, this still seems to improve performance.",2025-08-22 18:36:47+00:00,2025-08-30T06:55:46Z,,False,10,0,7,3,6,1,10,2025-08-30 06:55:46+00:00,72,409,False,False,False,False,True,False,1,9,695,32588,21459,11129,1,7,5.0,10.0,2025-08-26T00:02:44Z,pytorch
161291,open,Use overwrite module params path for FakeTensor in nn.Module._apply,mikaylagawarecki,"Context:

Three paths:
- set_data: silent incorrectness (preserves references)
- swap_tensors: weakref issues (preserves references)
- overwrite params: loses references

In order to fix https://github.com/pytorch/pytorch/issues/148977 we opted FakeTensor into the swap_tensors path https://github.com/pytorch/pytorch/pull/152539 if the `param` was a FakeTensor
- I received reports that this broke some tracing mechanisms due to weakrefs https://github.com/pytorch/pytorch/pull/159180
- Separately there is a case where the `param_applied` (i.e.` f(param)` is a `FakeTensor` but `param` is not) see Sherlock's diff [D80102876](https://www.internalfb.com/diff/D80102876)
  - For that case `swap_tensors` simply does not work due to weakrefs --> so the only solution is to use the path that loses the references
    ```
    out_param = Parameter(param_applied, param.requires_grad)
    self._parameters[key] = out_param
    ```

According to Sherlock, preserving the references should not matter for FakeTensor, so I think it makes sense to make this narrowly BC-breaking change

Follow-ups

- ask Richard
- swap_tensors / weakrefs deepdive + Sam



Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161291


",2025-08-22 18:31:17+00:00,2025-08-22T20:21:30Z,,False,1,0,1,18,16,1,1,,67,1244,False,True,False,False,False,False,1,0,0,34,18,16,1,1,,,,pytorch
161288,open,[inductor] use argsort_sym to avoid size=1 recompiles,pianpwk,"Differential Revision: D80823877




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 18:00:52+00:00,2025-08-26T23:00:57Z,,False,3,0,1,18,3,2,3,,53,238,False,False,False,False,False,False,2,0,0,21,18,3,1,1,,,,pytorch
161286,closed,Replace manual cache in _python_dispatch.get_alias_info with functools.cache,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161466
* #161455
* #161438
* #161433
* #161432
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* __->__ #161286

In addition to being more code, the manual cache was doing an extra dictionary lookup on each cache hit.",2025-08-22 17:41:17+00:00,2025-08-27T00:18:57Z,,False,3,2,4,3,8,1,5,2025-08-27 00:17:54+00:00,76,338,False,False,False,False,False,False,1,2,493,11526,6932,4594,1,4,3.0,2.0,2025-08-22T23:59:37Z,pytorch
161285,closed,Inline is_read_only_alias_match in _correct_storage_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* __->__ #161285
* #161284
* #161240
* #161235
* #161234
* #161231

Drives down the overhead of return_and_correct_storage_aliasing slightly. Hopefully you'll agree it doesn't compromise readability.",2025-08-22 17:41:13+00:00,2025-09-25T02:10:41Z,,False,3,0,3,5,7,1,3,2025-08-25 18:35:30+00:00,60,375,False,False,False,False,False,False,1,2,493,16,7,9,1,3,3.0,2.0,2025-08-22T23:58:12Z,pytorch
161284,closed,Remove unnecessary len() call in _correct_storage_aliasing.is_read_only_alias_match,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* #161285
* __->__ #161284
* #161240
* #161235
* #161234
* #161231

Containers are truthy iff they're non-empty.",2025-08-22 17:41:09+00:00,2025-09-25T02:10:38Z,,False,2,2,3,1,1,1,4,2025-08-25 18:35:29+00:00,83,288,False,False,False,False,False,False,1,1,48,6,3,3,1,3,4.0,1.0,2025-08-22T18:13:39Z,pytorch
161280,open,[WIP][inductor][ROCm] MI350 reduction heuristics improvements,naromero77amd,"Improvements to reduction kernel heuristics for MI350.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 16:59:22+00:00,2025-09-23T14:21:24Z,,False,9,0,15,93,41,4,9,,61,356,False,False,False,False,True,False,4,3,1224,149715,101285,48430,3,15,2.0,3.0,2025-09-23T13:41:50Z,pytorch
161279,open,"Back out ""Enable output padding when only outermost dim is dynamic""",kflu,"Summary:
Original commit changeset: 9f5a12b7fd0e

Original Phabricator Diff: D79146886

Test Plan:
OOM is gone after back-out with the repro script: P1914070645

Rollback Plan:

Differential Revision: D80812202




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 16:53:14+00:00,2025-08-24T15:00:31Z,,False,5,0,1,19,102,2,5,,67,416,False,False,False,False,False,False,2,2,737,121,19,102,1,1,3.0,2.0,2025-08-22T17:57:24Z,pytorch
161278,open,[Inductor] Try to run `test_flex_attention` in parallel,anmyachev,"@pytorchbot label ""topic: not user facing""

@pytorchbot label ""ciflow/inductor""
",2025-08-22 16:31:20+00:00,2025-08-25T13:38:41Z,,False,8,0,1,0,1,1,8,,55,80,False,False,False,False,False,False,1,6,760,1,0,1,1,1,2.0,6.0,2025-08-22T16:32:00Z,pytorch
161277,closed,[ROCm] Enable unit tests in test_matmul_cuda.py,rraminen,"This PR is to enable 162 test_addmm_baddmm_dtype_overload_float32* and 162 test_mm_bmm_dtype_overload_float32* tests on ROCm. 

This enables 324 tests in total.

cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-22 16:17:46+00:00,2025-09-02T21:06:50Z,,False,3,0,1,5,4,1,3,2025-09-02 21:06:50+00:00,47,278,False,False,False,False,False,False,1,1,59,9,5,4,1,1,1.0,1.0,2025-09-02T21:06:50Z,pytorch
161274,open,[Don't merge] add Windows mmap functions.,xuhancn,"Fixes #ISSUE_NUMBER
",2025-08-22 14:46:43+00:00,2025-08-22T22:46:34Z,,False,4,0,1,256,0,1,4,,41,20,False,True,False,False,False,False,1,3,461,256,256,0,1,1,2.0,3.0,2025-08-22T18:39:47Z,pytorch
161273,closed,[inductor] Add get page_size support for Windows.,xuhancn,"`resource` can't work on Windows, as it is a Unix specific package as seen in https://docs.python.org/2/library/resource.html

Use Windows system API to get page_size.

Local tested:
<img width=""467"" height=""433"" alt=""image"" src=""https://github.com/user-attachments/assets/47a39060-3aea-46c3-bd8e-35a39413c51f"" />


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 14:31:32+00:00,2025-08-22T18:37:20Z,,False,3,0,4,37,2,1,3,2025-08-22 18:36:17+00:00,49,623,False,False,False,True,False,False,1,2,493,59,47,12,1,4,3.0,2.0,2025-08-22T18:24:59Z,pytorch
161272,closed,[WIP][device_mesh] Implement  on top of CuTe layout bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161272
* #161271
* #161270



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta",2025-08-22 14:28:43+00:00,2025-09-22T02:16:23Z,,False,2,0,1,124,6,2,2,2025-08-22 14:31:28+00:00,63,184,False,False,False,False,False,False,2,0,0,130,124,6,1,1,,,,pytorch
161271,closed,[DeviceMesh] Simplifying internal bookkeeping with CuTe layout,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* (to be filled)



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta",2025-08-22 14:28:39+00:00,2025-09-22T02:16:19Z,,False,2,0,1,533,527,3,2,2025-08-22 14:31:17+00:00,62,164,False,False,False,False,False,False,3,0,0,1060,533,527,1,1,,,,pytorch
161270,closed,[DeviceMesh] Introduce CuTe layout into devicemesh code base for internal bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161272
* #161271
* __->__ #161270



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta",2025-08-22 14:28:36+00:00,2025-09-22T02:15:30Z,,False,2,0,1,483,1,2,2,2025-08-22 14:31:10+00:00,85,184,False,False,False,False,False,False,2,0,0,484,483,1,1,1,,,,pytorch
161269,closed,[DeviceMesh] Simplifying internal bookkeeping with CuTe layout,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161269
* #161268



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta",2025-08-22 14:23:48+00:00,2025-08-26T14:05:21Z,,False,3,0,1,533,527,3,3,2025-08-22 14:26:25+00:00,62,174,False,False,False,False,False,False,3,1,2067,1060,533,527,1,1,1.0,1.0,2025-08-26T14:05:21Z,pytorch
161268,closed,[DeviceMesh] Introduce CuTe layout into devicemesh code base for internal bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161269
* __->__ #161268



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta",2025-08-22 14:23:44+00:00,2025-09-22T02:15:26Z,,False,2,0,1,483,1,2,2,2025-08-22 14:26:12+00:00,85,174,False,False,False,False,False,False,2,0,0,484,483,1,1,1,,,,pytorch
161267,closed,[MPS] Fix `index_copy` for scalars,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161267
* #161206

By `squeezing the input` when copying into scalar tensor from a 1d one 
And enable `test_index_copy_scalars_mps`

Fixes https://github.com/pytorch/pytorch/issues/160737",2025-08-22 14:10:55+00:00,2025-09-22T02:16:05Z,,False,3,0,1,5,4,2,3,2025-08-22 21:45:38+00:00,34,272,False,True,False,False,False,False,2,2,493,9,5,4,1,1,5.0,2.0,2025-08-22T14:46:25Z,pytorch
161266,open,BackwardHookFunction implementation for full_backward_hook in pytorch 2.0 format,PyDevC,Fixes #99556,2025-08-22 14:03:06+00:00,2025-09-04T16:55:43Z,,False,6,8,3,12,4,2,14,,80,12,False,True,False,False,False,False,2,5,2070,24,16,8,1,3,2.0,6.0,2025-08-22T14:03:58Z,pytorch
161264,closed,[inductor] fix march=native pass to Windows CC.,xuhancn,"fix march=native pass to Windows CC.

<img width=""593"" height=""218"" alt=""image"" src=""https://github.com/user-attachments/assets/1caedffa-d9be-43d9-9ce2-590c055980cd"" />


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 12:53:43+00:00,2025-08-22T18:39:56Z,,False,3,4,1,17,14,1,7,2025-08-22 18:38:53+00:00,47,478,False,True,False,False,False,False,1,2,493,31,17,14,1,1,3.0,2.0,2025-08-22T18:21:33Z,pytorch
161263,closed,[ONNX] Fix index_put_ usage,ironsided777,"Summary:
It's hard to understand how it's working in most of our models, but in general it looks like `aten::copy_` is replaced incorrectly.
There are two schemas for `aten::copy_`:
1. `aten::copy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)`
2. `aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)`

According to the logic in the comments we don't need one of the parameters for `aten::index_put_`.

It seems logic has been inferred from ordinary `aten::copy` where there could be a third parameter which is `non_blocking` flag.

Depending on the execution environment the sliced copying can be replaced either by first schema or by second schema with explicitly setting default parameter to `False`.

If first schema is selected it will lead to the crash (which is easily to catch in our prod env). In case of the second schema selection, there is no crash, but the third parameter is treated as `accumulate` parameter of the `index_put_` function which doesn't make sense.

So, in any case usage of the third parameter must be removed from the `aten::copy_` replacement.


For more details and check this post:
https://fb.workplace.com/groups/1405155842844877/permalink/25337687649165028/

Test Plan:

The test fails in production envirounment only.
In the test env `non_blocking` flag is mapped as `False` to the `acumulate` flag, which doesn't cause test to fail, but has no sense in terms of flags mapping.

The export works without errors, before the fix it was failing with accessing by index out of bounds vector, like this:
```
   1095     _C._jit_onnx_log(""Torch IR graph at exception: "", graph)
File ~/.bento/kernels/bento_kernel_gaia_ml/1578/bento_kernel_gaia_ml_binary-inplace#link-tree/torch/onnx/utils.py:636, in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)
    629 _C._jit_pass_lower_all_tuples(graph)
    630 # in _jit_pass_onnx, symbolic functions are called for each node for conversion.
    631 # However, there are nodes that cannot be converted without additional context.
    632 # For example, the number of outputs from split (and whether it is static or dynamic) is unknown
    633 # until the point where it is unpacked by listUnpack node.
    634 # This pass does a preprocess, and prepares the nodes such that enough context can be received
    635 # by the symbolic function.
--> 636 _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)
    637 _C._jit_pass_onnx_preprocess(graph)
    639 # onnx does not support tuples, so try to remove them
RuntimeError: vector::_M_range_check: __n (which is 2) >= this->size() (which is 2)
```

The test script:
```
import torch as th
import tempfile

class CopyTest(th.nn.Module):
    def forward(
        self,
        input_th: th.Tensor
    ):
        to_fill = th.ones((3, 3))
        to_fill[:, 0] = input_th[:, 0]
        return to_fill

m = CopyTest()

test_tensor = th.zeros((3, 3))

with tempfile.NamedTemporaryFile() as f:
    th.onnx.export(
            m,
            (test_tensor,),
            f,
            export_params=True,
            opset_version=17,
            do_constant_folding=True,
            input_names=[""input""],
            output_names=[""features""],
            dynamo=False,
        )
```

The exported model test:
```
import torch
import onnx
import onnxruntime

model_name = '/home/ironsided/test_model.onnx'
onnx_model = onnx.load(model_name)
onnx.checker.check_model(onnx_model)

example_inputs = (torch.zeros(3, 3),)

onnx_inputs = [tensor.numpy(force=True) for tensor in example_inputs]
print(f""Input length: {len(onnx_inputs)}"")
print(f""Sample input: {onnx_inputs}"")

ort_session = onnxruntime.InferenceSession(
    model_name, providers=[""CPUExecutionProvider""]
)

onnxruntime_input = {input_arg.name: input_value for input_arg, input_value in zip(ort_session.get_inputs(), onnx_inputs)}

# ONNX Runtime returns a list of outputs
onnxruntime_outputs = ort_session.run(None, onnxruntime_input)[0]

print(onnxruntime_outputs)
```

The produced result is correct:
```
Input length: 1
Sample input: [array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]], dtype=float32)]
[[0. 1. 1.]
 [0. 1. 1.]
 [0. 1. 1.]]
```

Rollback Plan:

Differential Revision: D80797028




cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-22 12:00:55+00:00,2025-08-28T11:36:54Z,,False,10,5,1,1,2,1,15,2025-08-27 18:53:16+00:00,27,4371,False,True,True,False,False,False,1,2,504,3,1,2,1,1,4.0,4.0,2025-08-22T21:04:58Z,pytorch
161262,open,Fix: corrected typo in header file documentation,groovitz,Changed typo: alocator -> allocator in the CachingHostAllocator.h header file,2025-08-22 11:20:58+00:00,2025-08-26T17:34:55Z,,False,10,0,1,1,1,1,10,,48,77,False,True,False,True,False,False,1,8,1958,2,1,1,1,1,3.0,8.0,2025-08-22T11:29:09Z,pytorch
161261,closed,port distributed tensor parallel test files for Intel GPU,wincent8,"In this pr, we port test/distributed/parallel 4 test files and test/distributed/debug 1 test file for Intel GPU
We could enable Intel GPU with following methods and try the best to keep the original code styles:


1. Use torch.accelerator for general gpu
2. Skip the case if running on xpu which has known issues  

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @bdhirsh @tianyu-l @XilunWu",2025-08-22 10:46:00+00:00,2025-09-03T15:04:41Z,,False,9,5,14,7,3,4,14,2025-09-03 15:03:36+00:00,57,420,False,True,False,False,False,False,4,5,1410,348,176,172,1,14,4.0,7.0,2025-09-02T03:01:53Z,pytorch
161260,open,Fix spelling of word 'dimension' and grammatical correction,BalajiR2205,"Fix typo of the word dimension in file:
aten/src/ATen/native/mps/operations/ReduceOps.mm

Also fixed a grammatical error along with a type (is->are)
",2025-08-22 10:04:38+00:00,2025-08-26T13:27:45Z,,False,3,0,1,3,3,1,3,,59,149,False,True,False,False,False,False,1,1,90,6,3,3,1,1,1.0,1.0,2025-08-22T10:11:53Z,pytorch
161259,closed,[Test] Adding a testcase for constant_pad_nd,can-gaa-hou,"Fixes #161066

This PR adds a simple testcase for constant_pad_nd on MPS as mentioned in https://github.com/pytorch/pytorch/pull/161149#issuecomment-3211701274

cc @malfet ",2025-08-22 08:59:09+00:00,2025-08-23T01:01:55Z,,False,4,1,2,6,0,1,5,2025-08-23 01:00:53+00:00,44,172,False,True,False,False,False,False,1,2,788,14,10,4,2,2,2.0,2.0,2025-08-22T23:53:09Z,pytorch
161258,closed,Brister/break tensorbox,blaine-rister,"Fixes #ISSUE_NUMBER


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 08:31:45+00:00,2025-09-25T02:10:30Z,,False,2,0,2,2,1,2,2,2025-08-25 21:38:40+00:00,23,223,False,True,False,False,False,False,2,0,0,3,2,1,1,2,,,,pytorch
161257,closed,[CD] [aarch64] Add CUDA 13.0 sbsa nightly build,tinglvv,"https://github.com/pytorch/pytorch/issues/159779

CUDA SBSA build for CUDA 13.0
1. Supported archs: sm_80 to sm_120. Including support for Thor (sm_110), SPARK (sm_121), GB300 (sm_103).
""This release adds support of SM110 GPUs for arm64-sbsa on Linux."" from 13.0 release notes https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html
2. Use -compress-mode=size for binary size reduction, 13.0 wheel is 2.18 GB, when compared with 12.9 3.28 GB, that is 1.1 GB of savings and ~33.5% smaller.
3. Refactored the libs_to_copy list with common libs, and version_specific_libs. 

TODO: add the other CUDA archs in the existing support matrix of x86 to SBSA build as well

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ptrblck @atalman @nWEIdia @malfet 

",2025-08-22 08:10:55+00:00,2025-08-27T14:39:14Z,,False,12,3,10,394,34,5,15,2025-08-27 14:38:11+00:00,47,789,False,False,False,True,False,True,5,11,5406,522,441,81,1,10,6.0,12.0,2025-08-22T17:40:31Z,pytorch
161255,closed,[CD] Enable triton xpu Windows build for Python 3.14,chuanqi129,Follow #159869,2025-08-22 07:52:15+00:00,2025-08-22T18:40:36Z,,False,3,0,1,6,12,2,3,2025-08-22 18:39:34+00:00,52,14,False,False,False,False,False,False,2,2,811,18,6,12,1,1,2.0,2.0,2025-08-22T18:37:16Z,pytorch
161254,open,Add `zero_grad` param for `optimizer.step`,zeshengzong,"Fixes part of #158638

## Test Result

```bash
pytest test/test_optim.py

..................................................ssssssssssssssssssssssss..............ssssssssssssss.....................s.............s.............s......................................................................................................................................................................s........................s.........................................................................................................................................................
================ 1583 passed, 242 skipped in 2423.77s (0:40:23) =========================
```

",2025-08-22 07:29:20+00:00,2025-09-17T18:22:33Z,,False,9,2,3,178,20,16,11,,42,667,False,True,False,False,False,False,16,8,1829,216,187,29,1,3,4.0,10.0,2025-09-16T19:12:32Z,pytorch
161253,open,[WIP] Test DTensor Ops With Compile And Dynamic Shapes,azahed98,"Fixes #127772

Adds compile and dynamic shapes for DTensor Ops tests.

TODO: Add xfails for compile and dynamic shapes.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 07:25:10+00:00,2025-08-25T08:55:43Z,,False,2,0,5,116,6,1,2,,54,197,False,True,False,False,False,False,1,1,35,278,194,84,1,5,1.0,1.0,2025-08-25T08:55:43Z,pytorch
161248,open,Clarrifying input output angle unit in the docs for trigonometric fun…,arkadip-maitra,"…ctions

Fixes #[160995](https://github.com/pytorch/pytorch/issues/160995)

Modified the docs to clarify that input tensor  values for torch.sin, torch.cos and torch.tan should be in radians and the output tensor  values for torch.acos, torch.asin and torch.atan is in radians.",2025-08-22 06:45:53+00:00,2025-09-15T04:52:25Z,,False,5,0,2,6,6,1,5,,70,277,False,True,False,True,False,False,1,4,295,18,9,9,1,2,1.0,4.0,2025-08-22T06:47:31Z,pytorch
161247,open,Reuse AcceleratorConfig in XPUCachingAllocator,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161247

# Motivation
Due to https://github.com/pytorch/pytorch/pull/160999 ...
",2025-08-22 06:13:06+00:00,2025-08-22T07:45:59Z,,False,2,0,1,1,2,1,2,,46,165,False,False,False,False,False,False,1,0,0,3,1,2,1,1,,,,pytorch
161246,open,[WIP] [2/N] Enabled 9 test functions of TestCommon on Intel GPU,daisyden,"This is the 2nd PR to enable test_ops.py to Intel GPU, following PR https://github.com/pytorch/pytorch/pull/159944. This time enabled 9 test functions and skipped some cases on Intel GPU in op list because the following reasons:
1. Unsupported ops, such as dot, vdot, geqrf
2. Known issue of Intel GPU

The method to enable test includes:

1. Use @onlyOn to replace @onlyCUDA
2. Remove @skipXPU


cc @gujinghui @EikanWang @fengyuan14 @guangyey",2025-08-22 05:54:49+00:00,2025-09-01T18:01:05Z,,False,1,0,17,302,31,6,1,,63,443,False,False,False,False,False,False,6,0,22,393773,251446,142327,1,17,1.0,1.0,2025-08-22T07:26:37Z,pytorch
161245,closed,[Inductor-FX] Support Tensorbox outputs,blaine-rister,"# Problem
The FX converter previously supported graph outputs which were `StorageBox`, but not `TensorBox`. The latter seems to show up in certain cases when the output is a slice/view of the input.

# Fix
This PR generalizes the code to handle `MutableBox` instead of `StorageBox` specifically.

# Test
Added a CI test exposing the issue. The test case was found by intentionally breaking `TensorBox(ReinterpretView` support in https://github.com/pytorch/pytorch/pull/161258.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 05:42:45+00:00,2025-09-22T02:16:26Z,,False,3,0,2,13,1,2,3,2025-08-23 02:04:16+00:00,39,679,False,True,False,False,False,False,2,2,4441,14,13,1,1,2,3.0,2.0,2025-08-22T23:16:33Z,pytorch
161243,open,Fix Assertion for Rowwise Scaling,petrex,"This pull request updates the assertion logic for rowwise scaling support in the `scaled_gemm` function, clarifying compatibility with CUDA versions.

forward-fix : https://github.com/pytorch/pytorch/pull/151360
fix https://github.com/pytorch/ao/issues/2843

Compatibility check improvement:

* In `aten/src/ATen/cuda/CUDABlas.cpp`, the assertion for rowwise scaling now explicitly checks for CUDA version 12.9 or above, providing a clearer error message about support limitations for `scaled_gemm` with rowwise scaling.
",2025-08-22 03:43:11+00:00,2025-08-27T00:34:14Z,,False,5,0,1,3,2,1,5,,33,521,False,True,False,False,True,False,1,4,570,5,3,2,1,1,3.0,4.0,2025-08-22T03:44:52Z,pytorch
161242,closed,Disable background threads for XPU host allocator,guangyey,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161242

# Motivation
https://github.com/pytorch/pytorch/pull/160505 enables background threads for XPU host allocator. However, it will hang on Windows during program exit. Now disable it until we narrow down the issue.
",2025-08-22 03:36:11+00:00,2025-09-22T02:16:16Z,,False,3,0,2,6,0,1,3,2025-08-22 08:40:16+00:00,49,306,False,False,False,False,False,False,1,2,493,10,8,2,1,2,3.0,2.0,2025-08-22T03:51:37Z,pytorch
161241,closed,Contiguous subgraph decomposition,exclamaforte,"## Summary

Adds a subgraph decomposition for addmm and mm that performs well on large `K` compared to `M` and `N`, and functions well as an alternative to `split-k` on AMD (transposed only), which does not support AMD currently.

## Background

On AMD (MI300x), for a matmul A * B, if B is non-contiguous, the resulting matmul is quite a bit slower. 
For example:
```
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float16, size=[1024, 178176], stride=[178176, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float16, size=[178176, 6144], stride=[1, 178176]))
  ))
```
is a lot slower than:
```
  args[0]: TensorBox(StorageBox(
    InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float16, size=[1024, 178176], stride=[178176, 1]))
  ))
  args[1]: TensorBox(StorageBox(
    InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float16, size=[178176, 6144], stride=[6144, 1]))
  ))
```
This PR adds a subgraph decomposition to test out whether making B contiguous is faster than just using the normal kernels.

## Data

I ran this on unique non-contiguous shapes from torchbench/huggingface and got these speedups:
```
Parsed 420 unique shapes from benchmark output
addmm improvements when best:
  addmm_16448x512x2048: +0.14%
  addmm_128x2048x2048: +0.01%
  addmm_128x768x1000: +0.75%
  addmm_12672x3072x768: +1.08%
  addmm_512x768x32000: +0.62%
  addmm_12608x384x384: +0.00%
  addmm_4160x1024x4096: +0.90%
  addmm_16x768x2: +0.56%
  addmm_12608x3072x768: +0.09%
  addmm_64x4096x1000: +2.77%
  addmm_256x1024x512: +1.99%
  addmm_30x256x256: +1.12%
  addmm_100480x128x384: +0.91%
  addmm_6400x2048x512: +0.25%
  addmm_61568x1024x256: +0.08%
  addmm_1x768x768: +0.93%
  addmm_12544x384x384: +0.19%
  addmm_128x512x1000: +0.77%
  addmm_2048x128x128: +1.32%
  addmm_128x3072x1000: +0.24%
  addmm_7936x512x2048: +0.07%
  addmm_8192x512x2048: +0.33%
  addmm_64x1024x1000: +1.43%
  addmm_128x2304x1000: +0.01%
  addmm_32768x256x2: +0.75%
  addmm_64x384x1152: +0.79%
  addmm_64x640x1000: +0.01%
  addmm_100480x128x128: +0.87%
  addmm_1152x3072x768: +1.13%
  addmm_8192x256x2048: +1.40%
  addmm_4096x128x768: +0.01%
  addmm_128x2560x1000: +0.01%
  addmm_12544x2048x512: +0.43%
  addmm_200704x24x96: +0.14%
  addmm_8448x512x2048: +0.96%
  addmm_50176x256x1024: +0.62%
  addmm_4160x4096x1024: +0.22%
  addmm_4096x768x768: +0.32%
  addmm_220x2048x512: +0.56%
  addmm_8x2048x1000: +1.12%
  addmm_256x197951x512: +26.99%
  addmm_401536x64x192: +0.60%
  addmm_2040x2048x512: +0.47%
  addmm_512x1024x256: +1.32%
  addmm_128x4096x1000: +1.67%
  addmm_12672x768x768: +0.34%
  addmm_128x368x1000: +0.77%
  addmm_96x1280x1000: +0.01%
  addmm_12544x512x2048: +0.41%
  addmm_6272x320x1280: +0.76%
  addmm_12544x3072x768: +0.09%
  addmm_64x384x1000: +0.39%
mm improvements when best:
  mm_200704x128x512: +1.29%
  mm_663552x16x16: +0.80%
  mm_4096x768x768: +0.51%
  mm_131072x64x31: +0.24%
  mm_12544x1152x384: +0.11%
  mm_128x2048x2: +0.46%
  mm_262144x16x23: +0.62%
  mm_50176x576x192: +0.37%
  mm_131072x16x31: +0.26%
================================================================================
BENCHMARK ANALYSIS RESULTS
================================================================================

Operation: addmm
----------------------------------------
Total shapes analyzed: 247
Average Subgraph placement: 3.38
Median Subgraph placement: 2.0
Subgraph is best choice: 52/247 shapes (21.1%)
Average improvement when best: 1.15%
Median improvement when best: 0.58%
Largest improvement when best: +26.99%

Operation: bmm
----------------------------------------
Total shapes analyzed: 85
Average Subgraph placement: 24.00
Median Subgraph placement: 21.0
Subgraph is best choice: 0/85 shapes (0.0%)
Average improvement when best: N/A (never best)
Median improvement when best: N/A (never best)
Largest improvement when best: N/A (never best)

Operation: mm
----------------------------------------
Total shapes analyzed: 88
Average Subgraph placement: 15.08
Median Subgraph placement: 4.0
Subgraph is best choice: 9/88 shapes (10.2%)
Average improvement when best: 0.52%
Median improvement when best: 0.46%
Largest improvement when best: +1.29%

```

## Results

The largest shape gain, `256,197951,512`, seemed to be driven by a case where the extern kernel is way faster than the best triton configs on the recursive autotune:
```
addmm,Extern,extern_kernels.addmm,256,197951,512,0.38024500012397766
addmm,Triton,256,197951,512,32,256,16,2,2,4,2.005444049835205
addmm,Triton,256,197951,512,32,128,32,2,4,8,2.04189395904541
addmm,Triton,256,197951,512,64,128,16,2,4,8,2.1911399364471436
addmm,Triton,256,197951,512,64,128,32,2,4,8,2.496040105819702
addmm,Triton,256,197951,512,64,128,64,2,8,16,2.9306790828704834
addmm,Triton,256,197951,512,64,64,32,2,4,8,3.0347819328308105
...
```
Compared to the non-transposed autotune:
```
addmm,Subgraph,contiguous_addmm_1384,256,197951,512,0.5024129748344421
addmm,Extern,extern_kernels.addmm,256,197951,512,0.6881489753723145
addmm,Triton,256,197951,512,32,256,16,2,2,4,2.5115010738372803
addmm,Triton,256,197951,512,32,128,32,2,4,8,2.5167479515075684
addmm,Triton,256,197951,512,64,128,16,2,4,8,2.9507460594177246
addmm,Triton,256,197951,512,64,256,64,2,8,4,2.9673290252685547
addmm,Triton,256,197951,512,64,128,64,2,8,16,3.3906331062316895
addmm,Triton,256,197951,512,64,128,32,2,4,8,3.496859073638916
```



It seems to perform really well for high values of `K` vs `N` and `M`. 
Testing this hypothesis with some custom shapes:
```
Parsed 64 unique shapes from benchmark output
addmm improvements when best:
  addmm_128x16384x128: +0.18%
  addmm_128x262144x256: +38.24%
  addmm_128x200000x512: +14.76%
  addmm_256x800000x128: +0.06%
  addmm_131072x128x256: +0.27%
  addmm_128x256x131072: +0.25%
  addmm_2048x200000x64: +12.45%
mm improvements when best:
  mm_128x16384x128: +0.18%
  mm_128x262144x256: +38.05%
  mm_128x200000x512: +9.47%
  mm_256x800000x128: +0.99%
  mm_512x6400000x256: +3.17%
  mm_524288x64x64: +0.29%
  mm_2048x200000x64: +11.19%
  mm_8192x1000000x256: +34.14%
  mm_128x4096x100000: +0.40%
  mm_128x3072x150000: +0.27%
================================================================================
BENCHMARK ANALYSIS RESULTS
================================================================================

Operation: addmm
----------------------------------------
Total shapes analyzed: 33
Average Subgraph placement: 4.39
Median Subgraph placement: 2.0
Subgraph is best choice: 7/33 shapes (21.2%)
Average improvement when best: 9.46%
Median improvement when best: 0.27%
Largest improvement when best: +38.24%

Operation: mm
----------------------------------------
Total shapes analyzed: 30
Average Subgraph placement: 7.63
Median Subgraph placement: 2.0
Subgraph is best choice: 10/30 shapes (33.3%)
Average improvement when best: 9.81%
Median improvement when best: 2.08%
Largest improvement when best: +38.05%

```
## Conclusion
Contiguous Subgraph Decompositionseems worthwhile for `mm` and `addmm`, but not `bmm`, and has a very large improvment on low `M`, low `N`, and high `K` shapes.

Data gathering scripts:
https://gist.github.com/exclamaforte/4a896c064d301b27bf5ca0a4f8fc3866

## Test Plan:
New unit tests.

Differential Revision: D80771648




cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 03:25:19+00:00,2025-09-04T04:45:03Z,,False,22,3,4,339,0,5,25,2025-09-04 04:44:01+00:00,33,7555,False,True,False,False,True,False,5,15,3002,527,433,94,1,4,5.0,16.0,2025-09-02T13:30:33Z,pytorch
161240,closed,Improve efficiency of _python_dispatch.return_and_correct_aliasing,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* #161285
* #161284
* __->__ #161240
* #161235
* #161234
* #161231

get_write_alias() call count reduction explained briefly in code comment.

We don't need to check write_aliases against None in the final outs_to_return calculation because we just did that check.",2025-08-22 03:17:41+00:00,2025-09-25T02:10:35Z,,False,2,3,4,8,12,1,5,2025-08-25 18:35:27+00:00,66,440,False,False,False,False,True,False,1,1,48,22,10,12,1,4,3.0,1.0,2025-08-22T23:52:26Z,pytorch
161239,closed,Optimzie `zero_grad` description,zeshengzong,"Optimize [zero_grad doc](https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html) format and description.

## Test Result

### Before 

<img width=""996"" height=""534"" alt=""image"" src=""https://github.com/user-attachments/assets/e1db973c-57e8-4525-90e7-0500cde2263d"" />


### After

<img width=""890"" height=""496"" alt=""image"" src=""https://github.com/user-attachments/assets/5579c4fb-a857-4030-9303-34770083d1a5"" />
",2025-08-22 03:12:01+00:00,2025-08-22T06:19:32Z,,False,3,0,1,7,5,1,3,2025-08-22 06:18:28+00:00,32,439,False,False,False,True,False,False,1,2,499,12,7,5,1,1,2.0,3.0,2025-08-22T03:21:41Z,pytorch
161238,closed,"[nccl symm mem] don't use arg for mempool, correctly use symmetric registration in hooks",ngimel,"Per title


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 03:08:08+00:00,2025-09-25T02:10:46Z,,False,15,1,4,78,69,9,16,2025-08-25 03:09:36+00:00,88,88,False,False,False,False,False,False,9,14,2964,287,148,139,1,4,5.0,15.0,2025-08-22T04:26:13Z,pytorch
161237,open,Add prefix_kernel_name for gemm template,CaoE,"Profiling doesn't distinguish between different template kernels, making analysis difficult:
<img width=""577"" height=""113"" alt=""image"" src=""https://github.com/user-attachments/assets/62875981-e02d-463c-b9ae-a9b98d45a938"" />

Therefore, adding a kernel prefix name to the gemm template helps distinguish different template kernels.
Python wrapper:
<img width=""574"" height=""109"" alt=""image"" src=""https://github.com/user-attachments/assets/b83f5fe4-9879-47ce-b606-ab9235480d9f"" />

CPP wrapper:
<img width=""614"" height=""109"" alt=""image"" src=""https://github.com/user-attachments/assets/655d58e7-5991-41e3-85d4-8a8c31a9ceb4"" />


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-22 03:00:37+00:00,2025-08-25T04:05:29Z,,False,1,0,1,19,4,3,1,,40,826,False,True,False,False,False,False,3,0,0,23,19,4,1,1,,,,pytorch
161236,closed,[FP8][cuBLAS][SM100] cuBLAS doesn't support rowwise-scaling on `sm110` or `sm120` either,eqy,"See also #160693

cc @ptrblck @msaroufim @jerryzh168 @csarofeen @xwang233",2025-08-22 02:54:59+00:00,2025-08-26T20:41:16Z,,False,6,0,1,1,1,1,6,2025-08-26 20:40:13+00:00,88,73,False,False,False,False,False,False,1,5,962,2,1,1,1,1,3.0,5.0,2025-08-22T18:53:22Z,pytorch
161235,closed,Minor cleanup of DeviceMesh.__eq__,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* #161285
* #161284
* #161240
* __->__ #161235
* #161234
* #161231

`self is other` means the same thing as `id(self) == id(other)`, but it's one operator instead of 3.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 02:45:07+00:00,2025-09-25T02:10:32Z,,False,2,0,2,9,10,1,2,2025-08-25 18:35:26+00:00,34,422,False,False,False,False,False,False,1,1,48,21,11,10,1,2,4.0,1.0,2025-08-22T23:50:35Z,pytorch
161234,closed,Use comparison key in OpSchema to avoid duplicate work between `__hash__` and `__eq__`,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* #161285
* #161284
* #161240
* #161235
* __->__ #161234
* #161231

The performance cost of `dict` lookups keyed by `OpSchema` is a
significant minority of DTensor overhead. With this change we shave a
net ~1% off the total running time of the benchmark from #160580, as
measured by using cProfile and comparing cumulative time spent in
propagate + OpSchema's `__post_init__`. (`__post_init__` grew from
2.5% to 6.4% (+3.9%) and propagate shrank from 12.5% to 7.8% (-4.7%)).

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 02:45:03+00:00,2025-09-25T02:10:25Z,,False,2,9,2,12,33,1,11,2025-08-25 18:35:25+00:00,86,728,False,False,False,False,False,False,1,1,48,45,12,33,1,2,5.0,1.0,2025-08-22T22:34:48Z,pytorch
161233,closed,[WIP] Build fbgemm for cuda mxfp8 kernels,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161233
* #161230


Need to add the torch ops to fbgemm before we can update this
",2025-08-22 02:12:46+00:00,2025-09-23T00:11:57Z,,False,1,0,1,53,14,2,1,2025-09-23 00:11:57+00:00,41,167,False,False,False,False,False,False,2,0,0,67,53,14,1,1,,,,pytorch
161232,closed,[SymmMem] Make sure CUDA runtime is initialized before NVSHMEM init,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161232
* #161214

Previously, without calling `torch.empty` before NVSHMEM init, we see error below:
```
src/host/init/init.cu:nvshmemi_check_state_and_init:1117: nvshmem initialization failed, exiting 
src/host/util/cs.cpp:21: non-zero status: 16: Device or resource busy, exiting... mutex destroy failed
```
Fixing it by calling a `cudaFree(nullptr)` to make sure CUDA runtime is initialized before NVSHMEM init.
Removing all `torch.empty(1)` calls from tests. 

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 01:20:38+00:00,2025-09-02T22:54:35Z,,False,6,3,4,8,10,3,9,2025-09-02 22:53:31+00:00,67,627,False,True,False,False,False,False,3,5,2000,67260,51560,15700,1,4,3.0,5.0,2025-08-22T01:32:19Z,pytorch
161231,closed,Fix OpSchema equality check,swolchok,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161329
* #161328
* #161317
* #161315
* #161308
* #161304
* #161292
* #161301
* #161300
* #161286
* #161285
* #161284
* #161240
* #161235
* #161234
* __->__ #161231

`__eq__` didn't compare lists of DTensorSpec, but `__hash__` did (and
it looks like attention was paid to hash, so I made comparison follow
suit).

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 01:18:15+00:00,2025-09-25T02:10:19Z,,False,3,0,1,27,4,2,3,2025-08-25 18:35:24+00:00,27,468,False,True,False,False,False,False,2,2,1250,31,27,4,1,1,4.0,3.0,2025-08-22T22:31:11Z,pytorch
161230,closed,[submodule] Update fbgemm package version,drisspg,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161233
* __->__ #161230

",2025-08-22 01:11:56+00:00,2025-09-23T00:12:00Z,,False,2,0,1,1,1,1,2,2025-09-23 00:12:00+00:00,41,104,False,False,False,False,False,False,1,1,92,2,1,1,1,1,1.0,1.0,2025-08-22T05:29:10Z,pytorch
161229,closed,Add test_dtensor_dynamic_higher_rank,azahed98,"Testing to see if the higher rank errors with dtensor or reproducible outside my local system.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-22 00:57:22+00:00,2025-08-28T18:22:09Z,,False,1,0,1,12,0,1,1,2025-08-28 18:22:01+00:00,36,172,False,False,False,False,False,False,1,0,0,12,12,0,1,1,,,,pytorch
161227,closed,[vllm hash update] update the pinned vllm hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned vllm hash.",2025-08-22 00:26:36+00:00,2025-09-23T02:08:30Z,,False,9,0,1,1,1,1,9,2025-08-23 04:25:18+00:00,46,156,False,False,False,False,False,False,1,8,2069,2,1,1,1,1,3.0,8.0,2025-08-22T00:26:37Z,pytorch
161226,closed,[audio hash update] update the pinned audio hash,pytorchupdatebot,"This PR is auto-generated nightly by [this action](https://github.com/pytorch/pytorch/blob/main/.github/workflows/nightly.yml).
Update the pinned audio hash.",2025-08-22 00:26:35+00:00,2025-09-22T02:16:09Z,,False,3,0,1,1,1,1,3,2025-08-22 04:22:11+00:00,48,157,False,False,False,False,False,False,1,2,493,2,1,1,1,1,2.0,2.0,2025-08-22T00:26:36Z,pytorch
161225,closed,update triton pin for vLLM,davidberard98,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161225

",2025-08-22 00:10:54+00:00,2025-09-11T13:13:45Z,,False,5,0,2,1,1,1,5,2025-09-11 13:13:45+00:00,26,94,False,False,False,False,False,False,1,3,491,991,926,65,1,2,2.0,3.0,2025-08-22T03:35:57Z,pytorch
161224,open,[device_mesh] Implement  `_unflatten` on top of CuTe layout bookkeeping,fduwjj,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #163358
* __->__ #161224
* #163213
* #163367
* #163288
* #163212



cc @H-Huang @awgu @wanchaol @fegin @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @dcci",2025-08-21 23:50:58+00:00,2025-09-25T00:35:55Z,,False,4,15,11,188,0,2,19,,71,239,False,False,False,False,False,False,2,3,77,156477,106479,49998,1,11,5.0,3.0,2025-08-21T23:58:23Z,pytorch
161223,closed,[benchmark] Add torchscript jit.trace to benchmark option,yiming0416,"For comparing NativeRT and TorchScript. We add `torchscript-jit-trace` as an option in the benchmark. With this option, we can run trace a model and run inference with the traced module using TorchScript interpreter 

```
python ./benchmarks/dynamo/huggingface.py --performance --inference --torchscript-jit-trace

python ./benchmarks/dynamo/timm_models.py --performance --inference --torchscript-jit-trace

python ./benchmarks/dynamo/torchbench.py --performance --inference --torchscript-jit-trace
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-21 23:45:26+00:00,2025-09-22T02:16:08Z,,False,3,0,2,49,1,1,3,2025-08-22 21:38:31+00:00,57,674,False,False,False,False,False,False,1,2,561,54,51,3,1,2,3.0,3.0,2025-08-22T19:54:30Z,pytorch
161222,closed,[ONNX] Remove enable_fake_mode and exporter_legacy,justinchuby,"Remove enable_fake_mode and exporter_legacy entirely. Even though this is bc breaking, `enable_fake_mode` is no longer compatible with the latest version of transformers, and so it is no longer useful.



cc @titaiwangms",2025-08-21 23:35:54+00:00,2025-08-22T22:17:10Z,,False,8,1,3,42,844,10,9,2025-08-22 22:15:30+00:00,50,220,False,False,False,False,False,False,10,7,2221,886,42,844,1,3,3.0,8.0,2025-08-22T16:43:56Z,pytorch
161221,closed,[ROCm] revamp HIPCachingAllocatorMasqueradingAsCUDA,naromero77amd,"HIPAllocatorMasqueradingAsCUDA and HIPCachingAllocatorMasqueradingAsCUDA are now proper complete wrappers of HIPAllocator and HIPCachingAllocator, respectively. HIPAllocatorMasqueradingAsCUDA now subclasses HIPAllocator instead of Allocator. This fixes usability of hipify replacing c10::cuda::CUDACachingAllocator::get() where callers expect a CUDAAllocator to be returned but instead were getting a very thin Allocator shim instead.

This also fixes using cudagraph trees with torch compile. The hip:0 device was not being replaced by the cuda:0 device in all methods.



cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang",2025-08-21 23:21:19+00:00,2025-08-22T18:14:18Z,,False,3,0,1,685,8,4,3,2025-08-22 18:13:14+00:00,51,675,False,True,False,False,False,False,4,2,493,693,685,8,1,1,2.0,2.0,2025-08-21T23:25:01Z,pytorch
161220,closed,[hop] make materialize_as_graph disable pre-existing dispatch modes,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161220

For materializing_as_subgraph, we just want to trace a graph. The handling of different modes should register their own logic. 

",2025-08-21 23:13:29+00:00,2025-08-26T18:53:45Z,,False,6,0,4,10,0,1,6,2025-08-26 18:52:41+00:00,67,223,False,False,False,False,False,False,1,5,1497,38,24,14,1,4,3.0,6.0,2025-08-22T21:18:22Z,pytorch
161217,closed,[NVSHMEM] Make device-side ptr filling lazy,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161232
* #161214
* __->__ #161217

NVSHMEM kernels will more likely get peer pointers via:
```
__device__ void *nvshmem_ptr(const void *dest, int pe)
```
Thus deferring the ptr filling (containing cudaMalloc and cudaMemcpy) to the `get_..._dev()` functions to reduce rendezvous cost.
On the other hand, not sure how much we care about CUDA Graphing a program at first run, because a cudaMalloc inside a graph capture would probably not work. Altho, it only impacts cases where one uses NVSHMEM SymmMem with regular CUDA kernels, so probably a small chance?

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-21 22:35:27+00:00,2025-09-08T21:52:08Z,,False,6,0,3,29,25,1,6,2025-09-08 21:52:08+00:00,43,713,False,False,False,False,False,False,1,5,1040,83,43,40,1,3,3.0,5.0,2025-08-21T22:51:01Z,pytorch
161216,closed,Add dependency-groups.dev to pyproject.toml,lakshayg,"[PEP 735](https://peps.python.org/pep-0735) introduces the
[dependency-groups] table for a number of use-cases one of
which includes specifying development dependencies for projects.
",2025-08-21 22:34:28+00:00,2025-09-04T16:52:42Z,,False,4,0,1,36,3,3,4,2025-09-04 16:51:39+00:00,43,183,False,False,False,False,False,False,3,3,627,39,36,3,1,1,3.0,3.0,2025-09-04T15:17:10Z,pytorch
161215,closed,[dynamo] Remove extra if statement in builder _wrap,azahed98,"Removes a redundant if statement. Does not impact logic so no test changes needed.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-08-21 22:31:33+00:00,2025-08-22T08:57:12Z,,False,3,2,4,4,7,1,5,2025-08-22 08:56:08+00:00,51,262,False,False,False,False,False,False,1,2,500,21,9,12,1,4,3.0,3.0,2025-08-21T22:42:46Z,pytorch
161214,closed,[SymmMem] Add device guard before alloc,kwen2501,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161232
* __->__ #161214



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-21 22:20:39+00:00,2025-09-02T18:54:52Z,,False,7,7,4,16,0,2,14,2025-09-02 18:53:47+00:00,39,182,False,False,False,False,False,False,2,6,1613,67312,51597,15715,1,4,3.0,7.0,2025-08-21T23:05:02Z,pytorch
161213,open,[dynamo ] Enabling batch_isend_irecv to compile,konradha,"This is a first stab at enabling compilation for operators involving pointwise communications.

I've taken advantage of the traceable functional collectives paradigm a little here and adapted the function call args to `batch_isend_irecv` to not return any work objects. Rather, the graph now contains tensors that are registered with the work objects associated to coalesced groups.

Additionally, `isend` and `irecv` compile as well, maybe we should take them out of the _functional_collectives here. Happy to adapt that.

This feature has been tested on NCCL, RCCL and Gloo backends.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-21 22:17:34+00:00,2025-09-06T06:21:25Z,,False,5,11,4,793,2,11,16,,47,850,False,False,True,False,False,False,11,3,335,825,808,17,1,4,4.0,3.0,2025-08-23T17:45:48Z,pytorch
161212,closed,Bump uv from 0.8.4 to 0.8.6 in /.ci/lumen_cli,dependabot[bot],"Bumps [uv](https://github.com/astral-sh/uv) from 0.8.4 to 0.8.6.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/astral-sh/uv/releases"">uv's releases</a>.</em></p>
<blockquote>
<h2>0.8.6</h2>
<h2>Release Notes</h2>
<p>This release contains hardening measures to address differentials in behavior between uv and Python's built-in ZIP parsers (<a href=""https://github.com/astral-sh/uv/security/advisories/GHSA-8qf3-x8v5-2pj8"">CVE-2025-54368</a>).</p>
<p>Prior to this release, attackers could construct ZIP files that would be extracted differently by pip, uv, and other tools. As a result, ZIPs could be constructed that would be considered harmless by (e.g.) scanners, but contain a malicious payload when extracted by uv. As of v0.8.6, uv now applies additional checks to reject such ZIPs.</p>
<p>Thanks to a triage effort with the <a href=""https://devguide.python.org/developer-workflow/psrt/"">Python Security Response Team</a> and PyPI maintainers, we were able to determine that these differentials <strong>were not exploited</strong> via PyPI during the time they were present. The PyPI team has also implemented similar checks and now guards against these parsing differentials on upload.</p>
<p>Although the practical risk of exploitation is low, we take the <em>hypothetical</em> risk of parser differentials very seriously. Out of an abundance of caution, we have assigned this advisory a CVE identifier and have given it a &quot;moderate&quot; severity suggestion.</p>
<p>These changes have been validated against the top 15,000 PyPI packages; however, it's plausible that a non-malicious ZIP could be falsely rejected with this additional hardening. As an escape hatch, users who do encounter breaking changes can enable <code>UV_INSECURE_NO_ZIP_VALIDATION</code> to restore the previous behavior. If you encounter such a rejection, please file an issue in uv and to the upstream package.</p>
<p>For additional information, please refer to the following blog posts:</p>
<ul>
<li><a href=""https://astral.sh/blog/uv-security-advisory-cve-2025-54368"">Astral: uv security advisory: ZIP payload obfuscation</a></li>
<li><a href=""https://blog.pypi.org/posts/2025-08-07-wheel-archive-confusion-attacks/"">PyPI: Preventing ZIP parser confusion attacks on Python package installers</a></li>
</ul>
<h3>Security</h3>
<ul>
<li>Harden ZIP streaming to reject repeated entries and other malformed ZIP files (<a href=""https://redirect.github.com/astral-sh/uv/pull/15136"">#15136</a>)</li>
</ul>
<h3>Python</h3>
<ul>
<li>Add CPython 3.13.6</li>
</ul>
<h3>Configuration</h3>
<ul>
<li>Add support for per-project build-time environment variables (<a href=""https://redirect.github.com/astral-sh/uv/pull/15095"">#15095</a>)</li>
</ul>
<h3>Bug fixes</h3>
<ul>
<li>Avoid invalid simplification with conflict markers  (<a href=""https://redirect.github.com/astral-sh/uv/pull/15041"">#15041</a>)</li>
<li>Respect <code>UV_HTTP_RETRIES</code> in <code>uv publish</code> (<a href=""https://redirect.github.com/astral-sh/uv/pull/15106"">#15106</a>)</li>
<li>Support <code>UV_NO_EDITABLE</code> where <code>--no-editable</code> is supported (<a href=""https://redirect.github.com/astral-sh/uv/pull/15107"">#15107</a>)</li>
<li>Upgrade <code>cargo-dist</code> to add <code>UV_INSTALLER_URL</code> to PowerShell installer (<a href=""https://redirect.github.com/astral-sh/uv/pull/15114"">#15114</a>)</li>
<li>Upgrade <code>h2</code> again to avoid <code>too_many_internal_resets</code> errors (<a href=""https://redirect.github.com/astral-sh/uv/pull/15111"">#15111</a>)</li>
<li>Consider <code>pythonw</code> when copying entry points in uv run (<a href=""https://redirect.github.com/astral-sh/uv/pull/15134"">#15134</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Ensure symlink warning is shown (<a href=""https://redirect.github.com/astral-sh/uv/pull/15126"">#15126</a>)</li>
</ul>
<h2>Install uv 0.8.6</h2>
<h3>Install prebuilt binaries via shell script</h3>
<pre lang=""sh""><code>curl --proto '=https' --tlsv1.2 -LsSf https://github.com/astral-sh/uv/releases/download/0.8.6/uv-installer.sh | sh
</code></pre>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/astral-sh/uv/blob/main/CHANGELOG.md"">uv's changelog</a>.</em></p>
<blockquote>
<h2>0.8.6</h2>
<p>This release contains hardening measures to address differentials in behavior between uv and Python's built-in ZIP parser (<a href=""https://github.com/astral-sh/uv/security/advisories/GHSA-8qf3-x8v5-2pj8"">CVE-2025-54368</a>).</p>
<p>Prior to this release, attackers could construct ZIP files that would be extracted differently by pip, uv, and other tools. As a result, ZIPs could be constructed that would be considered harmless by (e.g.) scanners, but contain a malicious payload when extracted by uv. As of v0.8.6, uv now applies additional checks to reject such ZIPs.</p>
<p>Thanks to a triage effort with the <a href=""https://devguide.python.org/developer-workflow/psrt/"">Python Security Response Team</a> and PyPI maintainers, we were able to determine that these differentials <strong>were not exploited</strong> via PyPI during the time they were present. The PyPI team has also implemented similar checks and now guards against these parsing differentials on upload.</p>
<p>Although the practical risk of exploitation is low, we take the <em>hypothetical</em> risk of parser differentials very seriously. Out of an abundance of caution, we have assigned this advisory a CVE identifier and have given it a &quot;moderate&quot; severity suggestion.</p>
<p>These changes have been validated against the top 15,000 PyPI packages; however, it's plausible that a non-malicious ZIP could be falsely rejected with this additional hardening. As an escape hatch, users who do encounter breaking changes can enable <code>UV_INSECURE_NO_ZIP_VALIDATION</code> to restore the previous behavior. If you encounter such a rejection, please file an issue in uv and to the upstream package.</p>
<p>For additional information, please refer to the following blog posts:</p>
<ul>
<li><a href=""https://astral.sh/blog/uv-security-advisory-cve-2025-54368"">Astral: uv security advisory: ZIP payload obfuscation</a></li>
<li><a href=""https://blog.pypi.org/posts/2025-08-07-wheel-archive-confusion-attacks/"">PyPI: Preventing ZIP parser confusion attacks on Python package installers</a></li>
</ul>
<h3>Security</h3>
<ul>
<li>Harden ZIP streaming to reject repeated entries and other malformed ZIP files (<a href=""https://redirect.github.com/astral-sh/uv/pull/15136"">#15136</a>)</li>
</ul>
<h3>Python</h3>
<ul>
<li>Add CPython 3.13.6</li>
</ul>
<h3>Configuration</h3>
<ul>
<li>Add support for per-project build-time environment variables (<a href=""https://redirect.github.com/astral-sh/uv/pull/15095"">#15095</a>)</li>
</ul>
<h3>Bug fixes</h3>
<ul>
<li>Avoid invalid simplification with conflict markers  (<a href=""https://redirect.github.com/astral-sh/uv/pull/15041"">#15041</a>)</li>
<li>Respect <code>UV_HTTP_RETRIES</code> in <code>uv publish</code> (<a href=""https://redirect.github.com/astral-sh/uv/pull/15106"">#15106</a>)</li>
<li>Support <code>UV_NO_EDITABLE</code> where <code>--no-editable</code> is supported (<a href=""https://redirect.github.com/astral-sh/uv/pull/15107"">#15107</a>)</li>
<li>Upgrade <code>cargo-dist</code> to add <code>UV_INSTALLER_URL</code> to PowerShell installer (<a href=""https://redirect.github.com/astral-sh/uv/pull/15114"">#15114</a>)</li>
<li>Upgrade <code>h2</code> again to avoid <code>too_many_internal_resets</code> errors (<a href=""https://redirect.github.com/astral-sh/uv/pull/15111"">#15111</a>)</li>
<li>Consider <code>pythonw</code> when copying entry points in uv run (<a href=""https://redirect.github.com/astral-sh/uv/pull/15134"">#15134</a>)</li>
</ul>
<h3>Documentation</h3>
<ul>
<li>Ensure symlink warning is shown (<a href=""https://redirect.github.com/astral-sh/uv/pull/15126"">#15126</a>)</li>
</ul>
<h2>0.8.5</h2>
<h3>Enhancements</h3>
<ul>
<li>Enable <code>uv run</code> with a GitHub Gist (<a href=""https://redirect.github.com/astral-sh/uv/pull/15058"">#15058</a>)</li>
<li>Improve HTTP response caching log messages (<a href=""https://redirect.github.com/astral-sh/uv/pull/15067"">#15067</a>)</li>
<li>Show wheel tag hints in install plan (<a href=""https://redirect.github.com/astral-sh/uv/pull/15066"">#15066</a>)</li>
<li>Support installing additional executables in <code>uv tool install</code> (<a href=""https://redirect.github.com/astral-sh/uv/pull/14014"">#14014</a>)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/astral-sh/uv/commit/329a6b446a8619bd055cce1dd402a0cd0de6e7bb""><code>329a6b4</code></a> Bump version to v0.8.6 (<a href=""https://redirect.github.com/astral-sh/uv/issues/15137"">#15137</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/abc68fc7c1b77149c8c809b48fb959fdd480c814""><code>abc68fc</code></a> Consider pythonw when copying entrypoints in uv run (<a href=""https://redirect.github.com/astral-sh/uv/issues/15134"">#15134</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/7f1eaf48c193e045ca2c62c4581048765c55505f""><code>7f1eaf4</code></a> Harden ZIP streaming to reject repeated entries and other malformed ZIP files...</li>
<li><a href=""https://github.com/astral-sh/uv/commit/038bf563669906bc8889b17b615f775411f887d1""><code>038bf56</code></a> Sync latest Python releases (<a href=""https://redirect.github.com/astral-sh/uv/issues/15135"">#15135</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/84d57f2ee9bb6f452e3be74c658b2e0096b1dc30""><code>84d57f2</code></a> Ensure symlink warning is shown (<a href=""https://redirect.github.com/astral-sh/uv/issues/15126"">#15126</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/9c634d9b1399e92e270699ea378c82711da4b56e""><code>9c634d9</code></a> Upgrade h2 again (<a href=""https://redirect.github.com/astral-sh/uv/issues/15111"">#15111</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/278295ef02f4e517d61896ad597d952dd3c01939""><code>278295e</code></a> Add test cases for <code>find_uv_bin</code> (<a href=""https://redirect.github.com/astral-sh/uv/issues/15110"">#15110</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/aec90f0a3c83e99bb99dfb8651710cab35d2c4bd""><code>aec90f0</code></a> Fix warnings when running tests with a subset of features (<a href=""https://redirect.github.com/astral-sh/uv/issues/15120"">#15120</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/3c1844ca4a8aa4742db27493fed4f5a421bd59ff""><code>3c1844c</code></a> Add support for per-project build-time environment variables (<a href=""https://redirect.github.com/astral-sh/uv/issues/15095"">#15095</a>)</li>
<li><a href=""https://github.com/astral-sh/uv/commit/fb518380ab2c95b8177aa1ddb30b1731e40ae6c3""><code>fb51838</code></a> chore(ci): address linting findings in sync-python-releases.yml (<a href=""https://redirect.github.com/astral-sh/uv/issues/15117"">#15117</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/astral-sh/uv/compare/0.8.4...0.8.6"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=uv&package-manager=pip&previous-version=0.8.4&new-version=0.8.6)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/pytorch/pytorch/network/alerts).

</details>",2025-08-21 22:02:53+00:00,2025-08-21T22:54:42Z,2025-08-21T22:54:34Z,True,1,0,1,1,1,1,1,2025-08-21 22:54:34+00:00,45,13310,False,True,True,True,True,False,1,0,0,2,1,1,1,1,,,,pytorch
161210,closed,[cuDNN] head dim > 128 works on H100 again in cuDNN SDPA? ,eqy,"reference: https://github.com/pytorch/torchtitan/pull/1610

9.10 only for now, we would want to hold off on upgrading to either cuDNN frontend 1.14+/cuDNN 9.11+ due to some head-dim > 128 handling issues


cc @csarofeen @ptrblck @xwang233",2025-08-21 21:50:19+00:00,2025-08-22T21:23:00Z,,False,3,0,2,24,14,2,3,2025-08-22 21:21:57+00:00,58,238,False,False,False,False,False,False,2,2,498,44,27,17,1,2,3.0,2.0,2025-08-22T17:43:11Z,pytorch
161209,closed,[Profiler] Add GC Events to Python Stack Tracer,sraikund16,"Summary:
Adds Python Garbage Collection to Kineto Traces and Profiler FunctionEvents. Create custom cpp callback in profiler_python.cpp. Then define a python function with cpp and register that callback for all python garbage collection. We don't worry about thread safety in this case because we are only doing init/teardown for main thread while holding GIL.

Currently we are hiding this behind experimental config because python tracing tends to be unstable especially when adding any new feature. If this is found to not add too much overhead we can set this to on by default. NOTE: To enable this you need both with_stack=True and the experimental config on!

Test Plan:
Ran trace with GC induced and saw it on trace

Also added a test

Rollback Plan:

Differential Revision: D80491146


",2025-08-21 21:48:09+00:00,2025-08-22T22:12:32Z,,False,14,1,1,228,3,9,15,2025-08-22 22:11:29+00:00,47,794,False,False,True,False,False,False,9,2,498,231,228,3,1,1,3.0,2.0,2025-08-21T21:56:59Z,pytorch
161208,closed,[inductor][addmm] remove inp(unexpanded) path,coconutruben,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161351
* #161350
* #161349
* #161348
* #161347
* #161346
* #161345
* #161344
* #161343
* #161342
* #161341
* #161340
* #161339
* #161338
* #161337
* #161336
* #161126
* #161125
* #161124
* #161123
* #161098
* #161097
* #161026
* __->__ #161208

# why

- simplifies the code a lot
- unnecessary for performance (see below)
- matches other mm family kernels in logic now

# what

- if we're not autotuning (only ATen), we use a flexible layout
- in any case, we use inp_expanded (the expanded bias view) for
  the aten kernel

# performance analysis

## results

this is on H100

```
================================================================================
BENCHMARK SUMMARY (MERGED)
================================================================================
Config   Dim Type     (M, N, K)            DType        Bias     Runtime (inp_expanded) ms   Runtime (inp) ms
----------------------------------------------------------------------------------------------------------------------------
0        large_M      (65536, 8192, 8192)  float16      full     14.531                      14.562
1        large_M      (65536, 8192, 8192)  float16      row      14.682                      14.675
2        large_M      (65536, 8192, 8192)  float16      column   14.754                      14.740
3        large_M      (65536, 8192, 8192)  bfloat16     full     15.172                      15.148
4        large_M      (65536, 8192, 8192)  bfloat16     row      15.072                      15.085
5        large_M      (65536, 8192, 8192)  bfloat16     column   15.082                      15.114
6        large_M      (65536, 8192, 8192)  float32      full     185.726                     186.308
7        large_M      (65536, 8192, 8192)  float32      row      185.042                     185.864
8        large_M      (65536, 8192, 8192)  float32      column   185.221                     185.991
9        large_M      (32768, 4096, 4096)  float16      full     2.025                       2.036
10       large_M      (32768, 4096, 4096)  float16      row      2.029                       2.033
11       large_M      (32768, 4096, 4096)  float16      column   2.036                       2.047
12       large_M      (32768, 4096, 4096)  bfloat16     full     1.966                       1.981
13       large_M      (32768, 4096, 4096)  bfloat16     row      1.963                       1.979
14       large_M      (32768, 4096, 4096)  bfloat16     column   1.973                       1.981
15       large_M      (32768, 4096, 4096)  float32      full     24.096                      24.180
16       large_M      (32768, 4096, 4096)  float32      row      23.951                      24.033
17       large_M      (32768, 4096, 4096)  float32      column   23.996                      24.061
18       large_M      (16384, 2048, 2048)  float16      full     0.297                       0.298
19       large_M      (16384, 2048, 2048)  float16      row      0.298                       0.299
20       large_M      (16384, 2048, 2048)  float16      column   0.301                       0.300
21       large_M      (16384, 2048, 2048)  bfloat16     full     0.293                       0.293
22       large_M      (16384, 2048, 2048)  bfloat16     row      0.290                       0.291
23       large_M      (16384, 2048, 2048)  bfloat16     column   0.292                       0.293
24       large_M      (16384, 2048, 2048)  float32      full     3.077                       3.073
25       large_M      (16384, 2048, 2048)  float32      row      3.034                       3.033
26       large_M      (16384, 2048, 2048)  float32      column   3.040                       3.038
27       large_K      (8192, 8192, 65536)  float16      full     14.278                      14.297
28       large_K      (8192, 8192, 65536)  float16      row      14.325                      14.283
29       large_K      (8192, 8192, 65536)  float16      column   14.179                      14.302
30       large_K      (8192, 8192, 65536)  bfloat16     full     13.616                      13.628
31       large_K      (8192, 8192, 65536)  bfloat16     row      13.584                      13.642
32       large_K      (8192, 8192, 65536)  bfloat16     column   13.594                      13.694
33       large_K      (8192, 8192, 65536)  float32      full     175.933                     176.153
34       large_K      (8192, 8192, 65536)  float32      row      175.504                     175.877
35       large_K      (8192, 8192, 65536)  float32      column   175.432                     175.992
36       large_K      (4096, 4096, 32768)  float16      full     1.726                       1.724
37       large_K      (4096, 4096, 32768)  float16      row      1.731                       1.735
38       large_K      (4096, 4096, 32768)  float16      column   1.733                       1.737
39       large_K      (4096, 4096, 32768)  bfloat16     full     1.662                       1.658
40       large_K      (4096, 4096, 32768)  bfloat16     row      1.664                       1.655
41       large_K      (4096, 4096, 32768)  bfloat16     column   1.660                       1.667
42       large_K      (4096, 4096, 32768)  float32      full     22.263                      22.305
43       large_K      (4096, 4096, 32768)  float32      row      22.257                      22.322
44       large_K      (4096, 4096, 32768)  float32      column   22.247                      22.337
45       large_K      (2048, 2048, 16384)  float16      full     0.236                       0.236
46       large_K      (2048, 2048, 16384)  float16      row      0.238                       0.239
47       large_K      (2048, 2048, 16384)  float16      column   0.238                       0.239
48       large_K      (2048, 2048, 16384)  bfloat16     full     0.219                       0.219
49       large_K      (2048, 2048, 16384)  bfloat16     row      0.221                       0.222
50       large_K      (2048, 2048, 16384)  bfloat16     column   0.222                       0.222
51       large_K      (2048, 2048, 16384)  float32      full     2.786                       2.789
52       large_K      (2048, 2048, 16384)  float32      row      2.790                       2.782
53       large_K      (2048, 2048, 16384)  float32      column   2.791                       2.791
54       large_N      (8192, 65536, 8192)  float16      full     14.692                      14.723
55       large_N      (8192, 65536, 8192)  float16      row      14.721                      14.637
56       large_N      (8192, 65536, 8192)  float16      column   14.743                      14.737
57       large_N      (8192, 65536, 8192)  bfloat16     full     15.156                      15.128
58       large_N      (8192, 65536, 8192)  bfloat16     row      15.152                      15.124
59       large_N      (8192, 65536, 8192)  bfloat16     column   15.112                      15.090
60       large_N      (8192, 65536, 8192)  float32      full     179.127                     179.313
61       large_N      (8192, 65536, 8192)  float32      row      178.445                     178.961
62       large_N      (8192, 65536, 8192)  float32      column   178.693                     178.694
63       large_N      (4096, 32768, 4096)  float16      full     2.035                       2.037
64       large_N      (4096, 32768, 4096)  float16      row      2.042                       2.037
65       large_N      (4096, 32768, 4096)  float16      column   2.053                       2.046
66       large_N      (4096, 32768, 4096)  bfloat16     full     1.992                       1.997
67       large_N      (4096, 32768, 4096)  bfloat16     row      1.997                       1.987
68       large_N      (4096, 32768, 4096)  bfloat16     column   2.005                       2.001
69       large_N      (4096, 32768, 4096)  float32      full     23.126                      23.077
70       large_N      (4096, 32768, 4096)  float32      row      23.002                      22.956
71       large_N      (4096, 32768, 4096)  float32      column   23.012                      22.969
72       large_N      (2048, 16384, 2048)  float16      full     0.314                       0.314
73       large_N      (2048, 16384, 2048)  float16      row      0.311                       0.311
74       large_N      (2048, 16384, 2048)  float16      column   0.314                       0.314
75       large_N      (2048, 16384, 2048)  bfloat16     full     0.306                       0.305
76       large_N      (2048, 16384, 2048)  bfloat16     row      0.302                       0.302
77       large_N      (2048, 16384, 2048)  bfloat16     column   0.305                       0.303
78       large_N      (2048, 16384, 2048)  float32      full     2.975                       2.971
79       large_N      (2048, 16384, 2048)  float32      row      2.927                       2.925
80       large_N      (2048, 16384, 2048)  float32      column   2.934                       2.936
81       large_all    (16384, 16384, 16384) float16      full     14.062                      14.096
82       large_all    (16384, 16384, 16384) float16      row      14.058                      14.078
83       large_all    (16384, 16384, 16384) float16      column   14.107                      14.120
84       large_all    (16384, 16384, 16384) bfloat16     full     13.504                      13.460
85       large_all    (16384, 16384, 16384) bfloat16     row      13.495                      13.499
86       large_all    (16384, 16384, 16384) bfloat16     column   13.509                      13.461
87       large_all    (16384, 16384, 16384) float32      full     177.279                     177.242
88       large_all    (16384, 16384, 16384) float32      row      176.896                     176.651
89       large_all    (16384, 16384, 16384) float32      column   176.830                     176.451
```

## script

```
""""""
Torch addmm benchmarking script covering different input configurations.
Tests 30 different combinations of:
- Input types: large M, large K, large N, large everything
- Data types: float16, bfloat16, float32
- Bias types: full, row, column
""""""

import itertools

import torch
import torch.utils.benchmark as benchmark

def create_test_configurations():
    """"""Create 30 different test configurations for torch.addmm""""""

    # Define dimension configurations
    # Large M: many rows in input matrix
    # Large K: many columns in input/rows in weight
    # Large N: many columns in weight matrix
    # Large everything: all dimensions large
    dim_configs = [
        # Large M configurations
        {""M"": 65536, ""K"": 8192, ""N"": 8192, ""type"": ""large_M""},
        {""M"": 32768, ""K"": 4096, ""N"": 4096, ""type"": ""large_M""},
        {""M"": 16384, ""K"": 2048, ""N"": 2048, ""type"": ""large_M""},
        # Large K configurations
        {""M"": 8192, ""K"": 65536, ""N"": 8192, ""type"": ""large_K""},
        {""M"": 4096, ""K"": 32768, ""N"": 4096, ""type"": ""large_K""},
        {""M"": 2048, ""K"": 16384, ""N"": 2048, ""type"": ""large_K""},
        # Large N configurations
        {""M"": 8192, ""K"": 8192, ""N"": 65536, ""type"": ""large_N""},
        {""M"": 4096, ""K"": 4096, ""N"": 32768, ""type"": ""large_N""},
        {""M"": 2048, ""K"": 2048, ""N"": 16384, ""type"": ""large_N""},
        # Large everything configurations
        {""M"": 16384, ""K"": 16384, ""N"": 16384, ""type"": ""large_all""},
    ]

    # Data types to test
    dtypes = [torch.float16, torch.bfloat16, torch.float32]

    # Bias configurations
    bias_configs = [""full"", ""row"", ""column""]

    # Generate all combinations
    configurations = []
    config_id = 0

    for dim_config, dtype, bias_type in itertools.product(
        dim_configs, dtypes, bias_configs
    ):
        config = {
            ""id"": config_id,
            ""M"": dim_config[""M""],
            ""K"": dim_config[""K""],
            ""N"": dim_config[""N""],
            ""dim_type"": dim_config[""type""],
            ""dtype"": dtype,
            ""bias_type"": bias_type,
        }
        configurations.append(config)
        config_id += 1

    return configurations

def create_tensors(config):
    """"""Create input tensors for a given configuration""""""
    M, K, N = config[""M""], config[""K""], config[""N""]
    dtype = config[""dtype""]
    bias_type = config[""bias_type""]

    # Create input tensor (M x K)
    input_tensor = torch.randn(M, K, dtype=dtype, device=""cuda"")

    # Create weight tensor (K x N)
    weight_tensor = torch.randn(K, N, dtype=dtype, device=""cuda"")

    # Create bias tensor based on bias type
    if bias_type == ""full"":
        bias_tensor = torch.randn(M, N, dtype=dtype, device=""cuda"")
    elif bias_type == ""row"":
        bias_tensor = torch.randn(M, 1, dtype=dtype, device=""cuda"")
    elif bias_type == ""column"":
        bias_tensor = torch.randn(1, N, dtype=dtype, device=""cuda"")

    return input_tensor, weight_tensor, bias_tensor

def benchmark_addmm(config, use_compile=False):
    """"""Benchmark torch.addmm for a given configuration""""""
    input_tensor, weight_tensor, bias_tensor = create_tensors(config)

    # Define the operation
    def addmm_op():
        return torch.addmm(bias_tensor, input_tensor, weight_tensor)

    # Optionally compile the operation
    if use_compile:
        addmm_op = torch.compile(addmm_op)
        # Warmup for compiled version
        for _ in range(3):
            _ = addmm_op()
        torch.cuda.synchronize()

    # Benchmark using torch.utils.benchmark
    timer = benchmark.Timer(
        stmt=""addmm_op()"",
        globals={""addmm_op"": addmm_op},
        description=f""addmm_{config['dim_type']}_{config['dtype']}_{config['bias_type']}_compiled_{use_compile}"",
    )

    # Run benchmark
    measurement = timer.blocked_autorange(min_run_time=1.0)

    return measurement

def print_tensor_info(config, input_tensor, weight_tensor, bias_tensor):
    """"""Print information about input tensors""""""
    print(f""\\nConfiguration {config['id']}:"")
    print(f""  Dimension type: {config['dim_type']}"")
    print(f""  Data type: {config['dtype']}"")
    print(f""  Bias type: {config['bias_type']}"")
    print(f""  Input tensor shape: {input_tensor.shape} ({input_tensor.dtype})"")
    print(f""  Weight tensor shape: {weight_tensor.shape} ({weight_tensor.dtype})"")
    print(f""  Bias tensor shape: {bias_tensor.shape} ({bias_tensor.dtype})"")

def main():
    """"""Main benchmarking function""""""
    print(""Torch addmm Benchmarking Script (Compiled Only)"")
    print(""="" * 50)

    # Check CUDA availability
    if not torch.cuda.is_available():
        print(""CUDA not available. This benchmark requires CUDA."")
        return

    print(f""Using device: {torch.cuda.get_device_name()}"")
    print(f""PyTorch version: {torch.__version__}"")

    # Create test configurations
    configurations = create_test_configurations()
    print(f""\\nTesting {len(configurations)} configurations..."")

    results = []

    for config in configurations:
        print(f""\\n{'='*60}"")

        # Create tensors and print info
        input_tensor, weight_tensor, bias_tensor = create_tensors(config)
        print_tensor_info(config, input_tensor, weight_tensor, bias_tensor)

        # Benchmark with compilation only
        print(""\\nBenchmarking with torch.compile:"")
        try:
            measurement = benchmark_addmm(config, use_compile=True)
            runtime_ms = measurement.mean * 1000
            print(f""  Runtime: {runtime_ms:.3f} ms"")
            results.append(
                {
                    ""config"": config,
                    ""runtime_ms"": runtime_ms,
                }
            )
        except Exception as e:
            print(f""  Error: {e}"")

        # Clear cache
        torch.cuda.empty_cache()

    # Print summary
    print(f""\\n\\n{'='*80}"")
    print(""BENCHMARK SUMMARY"")
    print(f""{'='*80}"")

    print(
        f""{'Config':<8} {'Dim Type':<12} {'(M, N, K)':<20} {'DType':<12} {'Bias':<8} {'Runtime (ms)':<15}""
    )
    print(""-"" * 90)

    for result in results:
        config = result[""config""]
        dtype_str = str(config[""dtype""]).split(""."")[-1]
        dimensions_str = f""({config['M']}, {config['N']}, {config['K']})""
        print(
            f""{config['id']:<8} {config['dim_type']:<12} {dimensions_str:<20} {dtype_str:<12} {config['bias_type']:<8} {result['runtime_ms']:<15.3f}""
        )

if __name__ == ""__main__"":
    main()
```

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @Lucaskabela",2025-08-21 21:44:47+00:00,2025-09-12T00:42:13Z,,False,2,0,5,15,35,3,2,2025-09-12 00:42:13+00:00,45,16704,False,False,False,False,False,False,3,1,90,13395,8180,5215,1,5,2.0,1.0,2025-08-21T22:11:34Z,pytorch
161206,closed,[MPS] Fix index_select for scalar_types,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161267
* __->__ #161206

By copy-n-pasting logic from `index_select_out_cpu` (and `_cuda`), where essentially the resizing is done inside the op,  which also fixes faulty logic for scalars",2025-08-21 21:28:04+00:00,2025-09-22T02:16:01Z,,False,3,0,2,7,41,2,3,2025-08-22 16:45:37+00:00,39,267,False,True,False,False,False,False,2,2,493,3490,2766,724,1,2,3.0,2.0,2025-08-22T14:44:04Z,pytorch
161205,closed,Avoid making node a successor/predecessor of itself,eellison,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161205

This fixes an assertion we were running into in the memory planning about not having an acyclic graph. The repro is very long so hard to make local test of, but fixes repro I am looking at.



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 21:24:47+00:00,2025-09-21T02:15:45Z,,False,3,2,2,6,0,1,5,2025-08-22 00:30:32+00:00,51,488,False,True,False,False,False,False,1,2,493,8,7,1,1,2,5.0,2.0,2025-08-21T22:04:37Z,pytorch
161204,closed,[tgif] fix getattr_recursive with ModuleList,liangwang,"Summary: This change updates `getattr_recursive`  to handle qualnames with ModuleList that contain digit indices, for example, `op_instances.1.value_model.feature_weights`

Test Plan:
TBA

Rollback Plan:

Reviewed By: jiayisuse

Differential Revision: D80503985




cc @ezyang @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv",2025-08-21 21:21:25+00:00,2025-08-25T10:09:52Z,,False,10,0,1,6,1,1,10,2025-08-25 10:08:50+00:00,44,322,False,True,True,False,False,False,1,2,484,7,6,1,1,1,3.0,2.0,2025-08-22T17:51:31Z,pytorch
161203,closed,[export] Allow tempfile._TemporaryFileWrapper in package_pt2,yiming0416,"Summary:
We use tempfile.NamedTemporaryFile to create a temporary pt2 file in `test_nativert.py`

However, it is not recognized as an allowed file format and a warning will be thrown.

Test Plan:
CI

Rollback Plan:

Differential Revision: D80740916


",2025-08-21 21:16:46+00:00,2025-08-22T04:11:41Z,,False,4,0,1,2,1,2,4,2025-08-22 04:10:39+00:00,60,251,False,False,False,False,False,False,2,1,476,3,2,1,1,1,2.0,1.0,2025-08-21T22:23:27Z,pytorch
161202,closed,remove old while_loop_schema_gen test,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161202

Fixes https://github.com/pytorch/pytorch/issues/141202.

This test is flaky for mysterious reasons and we have created a new way of creating schemas for hops. So delete the test.
",2025-08-21 21:04:14+00:00,2025-09-22T02:15:58Z,,False,3,0,1,0,16,1,3,2025-08-22 18:22:32+00:00,37,273,False,True,False,False,False,False,1,2,493,16,0,16,1,1,3.0,2.0,2025-08-21T21:20:05Z,pytorch
161201,closed,[BE] Enable `test_index_put_accumulate_duplicate_indices` on MPS,malfet,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161206
* __->__ #161201

By changing dtype to float if device is MPS

Note: for some reason test runs much longer on MPS than on CPU
```
% python ../test/test_indexing.py -v -k test_index_put_accumulate_duplicate_indices_mps
test_index_put_accumulate_duplicate_indices_mps (__main__.TestIndexingMPS.test_index_put_accumulate_duplicate_indices_mps) ... ok

----------------------------------------------------------------------
Ran 1 test in 9.139s

OK
```",2025-08-21 21:03:55+00:00,2025-09-21T02:15:53Z,,False,3,0,1,2,2,1,3,2025-08-21 22:05:44+00:00,64,534,False,False,False,False,False,False,1,2,781,4,2,2,1,1,3.0,2.0,2025-08-21T21:28:48Z,pytorch
161200,closed,Update pyrefly config for better codenav,lolpack,"This fixes behavior in codenav by switching from `replace_imports_with_any` to `ignore-missing-imports`
",2025-08-21 20:57:18+00:00,2025-08-22T23:06:14Z,,False,7,3,5,16,5,1,10,2025-08-22 23:05:11+00:00,40,104,False,True,False,False,False,False,1,5,1611,31,21,10,1,5,4.0,5.0,2025-08-22T14:12:35Z,pytorch
161199,closed,[dynamo] auto lift unbacked symbol in tensor's storage_offset,ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161199
* #161198

```python
import torch

torch._dynamo.config.capture_scalar_outputs = True

class M(torch.nn.Module):
    def forward(self, idx, x):
        u0 = idx.item()
        x0 = x.select(0, u0)
        def fn():
            return x0.sin()
        return torch.cond(x0.sum() > 0, fn, fn)

m = M()
out = torch.compile(m, fullgraph=True)(torch.tensor(0, dtype=torch.int64, device=""cuda""), torch.randn(3, 3, device=""cuda""))
print(out)

```

Before the PR, we didn't track the storage_offset symbol of a tensor. After https://github.com/pytorch/pytorch/pull/157605, we create an unbacked_symint for stroage_offset for the result of select. So when we try to lift the free basic symbols of x0  during speculating fn, we found a free symbol that's not bound to a proxy. 

This PR tracks the symbols of storage_offset and associated it with a proxy using torch.ops.aten.storage_offest.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-21 20:45:40+00:00,2025-08-26T17:08:02Z,,False,6,0,6,21,1,2,6,2025-08-26 17:06:58+00:00,61,1190,False,False,False,False,False,False,2,5,1594,19498,11618,7880,1,6,3.0,5.0,2025-08-21T21:19:06Z,pytorch
161198,closed,[dynamo] lift backed symint output of item(),ydwu4,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161199
* __->__ #161198


Before the change in this PR, we have an error for the following code
```python
import torch

torch._dynamo.config.capture_scalar_outputs = True

class M(torch.nn.Module):
    def forward(self, idx, x):
        u0 = idx.item()
        x0 = x.select(0, u0)
        def fn():
            return x0.sin()
        return torch.cond(x0.sum() > 0, fn, fn)

m = M()
out = torch.compile(m, fullgraph=True)(torch.tensor(0, dtype=torch.int64), torch.randn(3, 3))
```

The error is caused when speculate fn, and tries to lift symbol of x0.storage_offset() but found the symbols doesn't have a source associated with it.

What really happens is that, when input tensor is a scalar tensor of int type and resides on CPU, we have a short cut that creates a norm symint when .item() is called see https://github.com/pytorch/pytorch/pull/126245. 

However, previously, we only track the unbacked symint output of an operation because we believe all the backed symint must have a source associated with it and has already bee lifted as input at the top-level. Now this invariant no longer holds, so we end up an error saying the symbol doesn't have source (because only input and symbols derided from inputs have source and result of .item() doesn't have a source).

In this PR, we start to also track the normal symint with the proxy that created it (i.e. in this case the proxy .item()).

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-21 20:39:04+00:00,2025-08-26T17:06:58Z,,False,3,1,6,84,44,3,4,2025-08-26 17:06:57+00:00,44,1693,False,False,False,False,False,False,3,2,96,19511,11627,7884,1,6,2.0,2.0,2025-08-21T21:17:21Z,pytorch
161197,closed,[inductor] remove Windows unsupported build options.,xuhancn,"Changes:
1. Math related build option is not supported by msvc, skip them on Windows.
2. Move all math related build option to `_get_ffast_math_flags` function.


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 20:37:39+00:00,2025-08-22T08:25:22Z,,False,3,0,1,23,19,1,3,2025-08-22 06:23:46+00:00,52,470,False,False,False,False,False,False,1,2,493,42,23,19,1,1,3.0,2.0,2025-08-22T04:59:16Z,pytorch
161196,closed,[inductor] Enable max compatible to msvc for oneAPI headers.,xuhancn,"Enable max compatible to msvc for oneAPI headers.

The key context is `The /permissive- option is compatible with almost all of the header files from the latest Windows Kits` from https://learn.microsoft.com/en-us/cpp/build/reference/permissive-standards-conformance?view=msvc-170


cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 20:18:22+00:00,2025-08-22T08:25:29Z,,False,3,0,2,3,0,1,3,2025-08-22 06:23:28+00:00,60,590,False,False,False,False,False,False,1,2,493,5,4,1,1,2,3.0,2.0,2025-08-22T04:59:46Z,pytorch
161195,closed,[aoti-fx] Output OpOverload fallbacks,angelayi,"Updates the inductor-wrapper-fxir code to use the kernel.op_overload when generating extern kernel calls. This way we can keep the IR consistent with using ATen ops.

TODO: we're also inserting torch.empty_strided calls -- need to turn this into aten too


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 20:14:00+00:00,2025-09-25T02:10:49Z,,False,8,0,1,12,13,3,8,2025-08-25 17:03:09+00:00,37,458,False,False,False,False,False,False,3,4,676,25,12,13,1,1,3.0,5.0,2025-08-22T22:57:01Z,pytorch
161194,closed,[HSDP][Experiment] HSDP mixed precision test implementation,anshul-si,"pytest test/distributed/_composable/fsdp/test_fully_shard_mixed_precision.py -k test_compute_dtype

Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161194



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-21 20:03:09+00:00,2025-09-21T02:15:42Z,,False,1,0,1,40,10,1,1,2025-08-21 20:04:14+00:00,59,272,False,False,False,False,False,False,1,0,0,50,40,10,1,1,,,,pytorch
161192,closed,enable more tests,yangw-dev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161313
* __->__ #161192

Enable more vllm test against pytorch main, add schedule to run the test every 12 hours.

",2025-08-21 19:49:10+00:00,2025-09-23T02:08:24Z,,False,6,7,20,174,14,7,13,2025-08-23 06:01:25+00:00,17,194,False,False,False,False,False,False,7,5,1564,9466,5739,3727,1,20,3.0,6.0,2025-08-22T21:18:04Z,pytorch
161190,open,[rfc][dynamo][export] How to get input and output mapping for export,anijain2305,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161190


Test file - https://www.internalfb.com/phabricator/paste/view/P1913463170


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela",2025-08-21 19:18:08+00:00,2025-08-22T19:32:27Z,,False,2,1,4,26,1,2,3,,68,341,False,False,False,False,False,False,2,0,0,53,39,14,1,4,,,,pytorch
161185,closed,"[export] Remove unused Model, tensor_paths, constant_paths",yiming0416,"Summary: 
Removed `Model`, it's not being used anywhere so it's safe.

Removed `tensor_paths` and `constant_paths` fields in `ExportedProgram`
- BC: when the current deserializer load a previously serialized EP (that comes with empty `tensor_paths` and `constant_paths`), it will just ignore those two fields
- FC: when the old deserializer load a newly serialized EP (that doesn't come with `tensor_paths` and `constant_paths`, it will also ignore those two fields in `_dict_to_dataclass()`

Differential Revision: D80725094


",2025-08-21 18:31:37+00:00,2025-08-22T01:08:08Z,,False,4,0,1,5,125,4,4,2025-08-22 01:07:05+00:00,58,528,False,False,False,False,False,False,4,1,476,130,5,125,1,1,2.0,1.0,2025-08-21T20:42:31Z,pytorch
161184,closed,Use ROCm MI325 runners for trunk.yml,jithunnair-amd,cc @jeffdaily @sunway513 @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd,2025-08-21 18:27:41+00:00,2025-08-22T16:34:45Z,,False,4,0,1,3,3,1,4,2025-08-22 16:18:58+00:00,36,100,False,False,False,False,False,False,1,3,563,6,3,3,1,1,3.0,3.0,2025-08-22T02:38:11Z,pytorch
161183,closed,"Support NUMA Binding for Callable Entrypoints, Take 2",pdesupinski,"# Context
In #160163, we added support for NUMA binding for `Callable` entrypoints to `elastic_launch`. This requires special consideration, because they go through a different path to spawn subprocesses compared to `str` entrypoints, a path which does not provide a straightforward way to utilize `numactl` CLI. See #160006 for a full description of the challenges.

Although #160163 worked in initial local experiments, we ran into some linker errors in other environments when we tried to call `numactl`. This appeared to be due to interactions with how the `LD_PRELOAD` environment variable was being set.

# This PR
On further thought, the most straightforward, foolproof solution here is to use [the trick that @d4l3k suggested.](https://github.com/pytorch/pytorch/issues/160006#issuecomment-3162018836)

Specifically, for each local rank `i`:
1. The parent process sets its own CPU affinity to what local rank `i`'s should be.
2. Then, the parent spawns the subprocess for local rank `i`.
3. Finally, the parent resets its own CPU affinity to what it was originally.

There were other solutions that would work just for `Callable` entrypoints, but I believe this is the simplest one that can work for *both* `str` and `Callable`, and it's pretty simple.

This required a bit of refactoring:
1. Turn all the `_get_.*_numactl_options` into functions which return a set of logical CPUs to bind to, rather than options like `--cpunodebind=0`.
2. Instead of wrapping commands with `numactl`, use `os.sched_setaffinity` to bind to the CPUs from (1.).
3. Put this all inside a context manager which encapsulates applying and restoring the bindings in the parent process.
4. Use the context manager for both `str` and `Callable` paths

# Test Plan
## Automated
`$ pytest test/test_numa_binding.py`

## Manual
See [doc.](https://docs.google.com/document/d/1vxD-OKYBTT27jbBwtW9iz9g0tNM0u-i0tiTJg_ieQA8/edit?tab=t.0) Meta only, but TLDR tried out every combination of `str`, `Callable`, binding disabled, and binding enabled on the same model and saw 2x SM utilization for binding enabled.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta ",2025-08-21 18:22:22+00:00,2025-09-23T02:08:29Z,,False,8,3,1,353,489,6,11,2025-08-23 07:23:25+00:00,53,2164,False,False,False,True,False,True,6,1,480,842,353,489,1,1,2.0,2.0,2025-08-22T17:18:24Z,pytorch
161182,closed,Move non inductor workflows to Python 3.9 -> 3.10,atalman,"Related to: https://github.com/pytorch/pytorch/issues/161167 
",2025-08-21 18:02:30+00:00,2025-08-27T02:33:31Z,,False,13,0,8,60,60,6,13,2025-08-27 02:32:28+00:00,49,62,False,False,False,False,False,False,6,12,6349,228,114,114,1,8,6.0,14.0,2025-08-21T18:31:45Z,pytorch
161181,closed,[ROCm] Unroll loads in global_reduce,amd-hhashemi,cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd,2025-08-21 17:39:10+00:00,2025-08-25T16:01:05Z,,False,6,0,4,18,0,1,6,2025-08-25 15:45:52+00:00,36,116,False,False,False,False,False,False,1,5,1514,44,31,13,2,4,4.0,5.0,2025-08-21T20:26:10Z,pytorch
161180,closed,[ROCm] No-fence global reduce,amd-hhashemi,"This change removes need for fences in global_reduce by converting the stores to reduce_buffer[] into atomics+return. This is crucial for perf in architectures with split caches (e.g. MI300), where fences are inherently costly.


cc @jeffdaily @sunway513 @jithunnair-amd @pruthvistony @ROCmSupport @dllehr-amd @jataylo @hongxiayang @naromero77amd",2025-08-21 17:38:24+00:00,2025-08-26T20:45:06Z,,False,10,3,10,46,0,2,13,2025-08-26 20:44:03+00:00,29,346,False,False,False,False,False,False,2,9,3290,178,112,66,2,10,4.0,9.0,2025-08-21T19:12:10Z,pytorch
161178,closed,shape guards,avikchaudhuri,"Summary: This PR introduces shape guards to export. Previously only value ranges,  equalities, and specializations would be tracked for symbolic expressions, and we had a forward hook to check them. Instead now we create a function to check shape guards and call it in the exported program.

Test Plan:
updated several tests

Rollback Plan:

Differential Revision: D80713603


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @ezyang @msaroufim @SherlockNoMad @EikanWang @jgong5 @wenzhe-nrv @voznesenskym @penguinwu @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-21 17:29:27+00:00,2025-09-08T22:45:15Z,,False,25,18,1,617,123,29,43,2025-09-08 22:44:12+00:00,12,699,False,False,False,False,False,False,29,3,568,740,617,123,1,1,6.0,3.0,2025-09-05T19:08:18Z,pytorch
161177,closed,[cuDNN][convolution] remove redundant conv3d 64bit test,eqy,"turns out it's the same as
```
    @onlyCUDA
    @largeTensorTest(""40GB"")
    @largeTensorTest(""24GB"", ""cpu"")
    @tf32_on_and_off(0.005)
    def test_conv3d_64bit_indexing(self, device):
        x = torch.rand(1, 32, 512, 512, 256)
        m = torch.nn.Conv3d(32, 1, kernel_size=1, padding=0, stride=1, bias=False)
        yref = m(x)
        y = m.to(device=device)(x.to(device=device))
        self.assertEqual(yref, y)
 ```

cc @csarofeen @ptrblck @xwang233 @msaroufim @jerryzh168 @zasdfgbnm",2025-08-21 17:13:16+00:00,2025-08-25T15:02:10Z,,False,15,0,1,0,11,1,15,2025-08-25 15:01:07+00:00,55,495,False,False,False,False,False,False,1,14,2909,11,0,11,1,1,3.0,14.0,2025-08-21T17:16:51Z,pytorch
161176,closed,[Autograd Lab]: Enforce 2d inputs,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161176
* #161175
* #161174
* #161173
* #161172

Summary:

Throw an error if Q/K/V are not 2d tensors

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",2025-08-21 17:05:30+00:00,2025-09-25T13:56:45Z,,False,3,0,1,4,0,1,3,2025-09-25 13:56:45+00:00,33,239,False,False,False,False,False,False,1,0,0,4,4,0,1,1,,,,pytorch
161175,closed,[Autograd Lab] Move lab_attn to explicit grad,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161176
* __->__ #161175
* #161174
* #161173
* #161172

Summary:

Move the `lab_attn` op defined previously from implicitly- to
explicitly-defined gradient operator.

Test Plan:
```
python test/test_ops.py -vv -k lab_attn
```
Reviewers:

Subscribers:

Tasks:

Tags:",2025-08-21 17:05:27+00:00,2025-09-25T13:56:44Z,,False,2,0,1,7,7,3,2,2025-09-25 13:56:44+00:00,45,343,False,False,False,False,False,False,3,0,0,14,7,7,1,1,,,,pytorch
161174,closed,[Autograd lab]: C++ implicit autograd fn,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161176
* #161175
* __->__ #161174
* #161173
* #161172

Summary:

Added a c++ native function called `lab_attn` to `native_functions.yaml`,
with corresponding OpInfo entry in common_methods_invocations.

Some stubs/parts of Step4 (custom formula in derivatives.yaml) are in this
commit, but disabled/non-functional.

Test Plan:

Reviewers:

Subscribers:

Tasks:

Tags:",2025-08-21 17:05:23+00:00,2025-09-25T13:56:44Z,,False,4,0,1,71,0,4,4,2025-09-25 13:56:43+00:00,40,446,False,False,False,False,False,False,4,0,0,71,71,0,1,1,,,,pytorch
161173,closed,[Autograd Lab]: Add python Attn + test,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161176
* #161175
* #161174
* __->__ #161173
* #161172

Summary:

Add python version of the attention mechanism described in the lab.
Both implicit & explicit autograd demonstrated.",2025-08-21 17:05:19+00:00,2025-09-25T13:56:43Z,,False,2,0,1,91,0,1,2,2025-09-25 13:56:43+00:00,38,259,False,False,False,False,False,False,1,0,0,91,91,0,1,1,,,,pytorch
161172,closed,[Autograd Lab]: Add attn grad derivation,slayton58,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161176
* #161175
* #161174
* #161173
* __->__ #161172

Summary:

Add a markdown with derivation (in terms of using the existing grad form
of matmul) for the lab's attention operator.",2025-08-21 17:05:16+00:00,2025-09-25T13:56:42Z,,False,2,0,1,45,0,1,2,2025-09-25 13:56:42+00:00,40,261,False,False,False,False,False,False,1,0,0,45,45,0,1,1,,,,pytorch
161171,closed,Clear custom autograd Function ctx.to_save earlier,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161171

Fixes https://github.com/pytorch/pytorch/issues/161186


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 16:59:18+00:00,2025-09-02T03:27:37Z,,False,17,0,4,35,3,3,17,2025-09-02 03:26:34+00:00,50,352,False,True,False,False,False,False,3,16,6243,292385,176957,115428,1,4,3.0,17.0,2025-08-29T19:01:12Z,pytorch
161170,closed,[FlexAttention] fixing learnable bias assertion error in inductor,liangel-02,"Users encountered unexpected behaviour when using FlexAttention with learnable biases, including assertion errors (#157677)

We traced the root cause to the registration of subgraph buffers—this caused inconsistencies in the naming and ultimately incorrect retrieval later on. This problem only arose if the model was compiled as a whole (ie using @torch.compile) since only then would there be naming conflicts. 

In this PR, we register the buffers with the base graph to solve this issue. 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Chillee @drisspg @yanboliang @BoyuanFeng",2025-08-21 16:02:15+00:00,2025-09-23T02:08:19Z,,False,3,0,3,51,8,3,3,2025-08-23 06:24:24+00:00,65,737,False,True,False,False,False,False,3,2,493,65,54,11,1,3,3.0,2.0,2025-08-21T22:27:12Z,pytorch
161169,closed,SDP Backend function fix,ahkush,"

The issue cannot be reproduced using the original repro code provided in the issue description.

However, the underlying issue mentioned by the maintainer (missing functions in `builder.py` and `trace_rules.py`) was never addressed and can still be reproduced with this test case:

```python
import torch
from torch.nn.attention import _cur_sdpa_kernel_backends

@torch.compile(fullgraph=True)
def test_function_that_triggers_error():
    return _cur_sdpa_kernel_backends()

print(""Calling torch.compile function..."")
try:
    result = test_function_that_triggers_error()
    print(f""Success: {result}"")
except Exception as e:
    print(f""ERROR: {e}"")
    print(f""Error type: {type(e)}"")
```

The original repro likely no longer triggers the issue due to code path changes in the SDPA implementation, while the direct call to `_cur_sdpa_kernel_backends()` exposes the underlying problem where certain torch._C functions returning non-Tensor values aren't properly handled by dynamo tracing.


I have implemented the changes by adding the missing functions to both `builder.py` and `trace_rules.py` to properly handle these cases during compilation.

@guilhermeleobas 

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @chenyang78 @kadeng @chauhang @amjames @Lucaskabela @mlazos",2025-08-21 15:52:55+00:00,2025-09-19T20:21:05Z,,False,13,4,6,45,0,3,17,2025-09-19 20:20:02+00:00,24,1349,False,True,False,False,False,False,3,10,1343,117,81,36,1,6,4.0,11.0,2025-08-21T16:13:39Z,pytorch
161168,closed,Quick fix to headers in stable/tensor_inl.h,janeyx99,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161168

",2025-08-21 15:50:07+00:00,2025-09-21T02:15:39Z,,False,16,2,2,1,2,1,18,2025-08-22 01:27:46+00:00,43,94,False,True,False,False,False,False,1,15,4005,286,194,92,1,2,4.0,15.0,2025-08-21T15:53:56Z,pytorch
161166,closed,forward fix of #152198,jagadish-amd,"torch._inductor.virtualized.OpsValue objects instance does not have shape attribute. This breaks the fp8 test on ROCm. Add the OpsValue class in todo list.


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 15:06:03+00:00,2025-08-21T21:10:52Z,,False,5,0,1,1,1,1,5,2025-08-21 21:09:51+00:00,22,359,False,True,False,False,False,False,1,4,549,2,1,1,1,1,3.0,4.0,2025-08-21T15:06:29Z,pytorch
161165,closed,[Dependabot] Update(deps): Bump transformers from 4.54.0 to 4.55.3 in /.ci/docker/ci_commit_pins,dependabot[bot],"[//]: # (dependabot-start)
⚠️  **Dependabot is rebasing this PR** ⚠️ 

Rebasing might not happen immediately, so don't worry if this takes some time.

Note: if you make any changes to this PR yourself, they will take precedence over the rebase.

---

[//]: # (dependabot-end)

Bumps [transformers](https://github.com/huggingface/transformers) from 4.54.0 to 4.55.3.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/huggingface/transformers/releases"">transformers's releases</a>.</em></p>
<blockquote>
<h2>Patch release v4.55.3</h2>
<h1>Patch release 4.55.3</h1>
<p>Focused on stabilizing FlashAttention-2 on Ascend NPU, improving FSDP behavior for generic-task models, fixing MXFP4 integration for GPT-OSS</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>FlashAttention-2 / Ascend NPU – Fix “unavailable” runtime error (<a href=""https://redirect.github.com/huggingface/transformers/issues/40151"">#40151</a>) by <a href=""https://github.com/FightingZhen""><code>@​FightingZhen</code></a></li>
<li>FlashAttention kwargs – Revert FA kwargs preparation to resolve regression (<a href=""https://redirect.github.com/huggingface/transformers/issues/40161"">#40161</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>FSDP (generic-task models) – Fix sharding/runtime issues (<a href=""https://redirect.github.com/huggingface/transformers/issues/40191"">#40191</a>) by <a href=""https://github.com/Cyrilvallez""><code>@​Cyrilvallez</code></a></li>
<li>GPT-OSS / MXFP4 – Ensure swiglu_limit is correctly passed through (<a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a>) by <a href=""https://github.com/returnL""><code>@​returnL</code></a></li>
<li>Mamba – Fix cache handling to prevent stale/incorrect state (<a href=""https://redirect.github.com/huggingface/transformers/issues/40203"">#40203</a>) by <a href=""https://github.com/manueldeprada""><code>@​manueldeprada</code></a></li>
<li>Misc – Minor follow-up fix addressing <a href=""https://redirect.github.com/huggingface/transformers/issues/40262"">#40262</a> by <a href=""https://github.com/ArthurZucker""><code>@​ArthurZucker</code></a></li>
</ul>
<h2>Patch release 4.55.2: for FA2 users!</h2>
<h1>Patch release 4.55.2!</h1>
<h2>only affects <code>FA2</code> generations!</h2>
<p>😢 Well sorry everyone, sometimes shit can happen...
4.55.1 was broken because of 🥁 git merge conflict.
I cherry-picked <a href=""https://redirect.github.com/huggingface/transformers/pull/40002"">huggingface/transformers#40002</a> without having <a href=""https://redirect.github.com/huggingface/transformers/pull/40029"">huggingface/transformers#40029</a> , thus <code>from ..modeling_flash_attention_utils import prepare_fa_kwargs_from_position_ids</code> is missing, and since this is a slow test, nothing caught it.</p>
<p>Will work to remediate and write the post-mortem when yanking the release.</p>
<h1>Patch release 4.55.1:</h1>
<p>Mostly focused around stabalizing the Mxfp4 for GPTOSS model!</p>
<h2>Bug Fixes &amp; Improvements</h2>
<ul>
<li>Idefics2, Idefics3, SmolVLM – Fix tensor device issue (<a href=""https://redirect.github.com/huggingface/transformers/issues/39975"">#39975</a>) by <a href=""https://github.com/qgallouedec""><code>@​qgallouedec</code></a></li>
<li>Merge conflicts – Fix merge conflicts from previous changes by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>MXFP4 / CPU device_map – Default to dequantize when CPU is in device_map (<a href=""https://redirect.github.com/huggingface/transformers/issues/39993"">#39993</a>) by <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>GPT Big Code – Fix attention scaling (<a href=""https://redirect.github.com/huggingface/transformers/issues/40041"">#40041</a>) by <a href=""https://github.com/vasqu""><code>@​vasqu</code></a></li>
<li>Windows compatibility – Resolve Triton version check compatibility (<a href=""https://redirect.github.com/huggingface/transformers/issues/39986"">#39986</a>) by <a href=""https://github.com/Tsumugii24""><code>@​Tsumugii24</code></a> <a href=""https://github.com/MekkCyber""><code>@​MekkCyber</code></a></li>
<li>Gemma3n model – Add missing None default values for get_placeholder_mask (<a href=""https://redirect.github.com/huggingface/transformers/issues/39991"">#39991</a>, <a href=""https://redirect.github.com/huggingface/transformers/issues/40024"">#40024</a>) by <a href=""https://github.com/Znerual""><code>@​Znerual</code></a></li>
<li>Fuyu model – Fix broken image inference (<a href=""https://redirect.github.com/huggingface/transformers/issues/39915"">#39915</a>) by <a href=""https://github.com/Isotr0py""><code>@​Isotr0py</code></a></li>
<li>PerceptionLM – Fix missing video inputs (<a href=""https://redirect.github.com/huggingface/transformers/issues/39971"">#39971</a>) by <a href=""https://github.com/shuminghu""><code>@​shuminghu</code></a></li>
<li>Idefics – Fix device mismatch (<a href=""https://redirect.github.com/huggingface/transformers/issues/39981"">#39981</a>) by <a href=""https://github.com/zucchini-nlp""><code>@​zucchini-nlp</code></a></li>
<li>Triton kernels – Remove triton_kernels dependency in favor of included kernels (<a href=""https://redirect.github.com/huggingface/transformers/issues/39926"">#39926</a>) by <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>GPT-OSS MXFP4 – Enable on older hardware (sm75+) (<a href=""https://redirect.github.com/huggingface/transformers/issues/39940"">#39940</a>) by <a href=""https://github.com/matthewdouglas""><code>@​matthewdouglas</code></a> <a href=""https://github.com/SunMarc""><code>@​SunMarc</code></a></li>
<li>MXFP4 quantizer – Allow CPU inference with dequantize option (<a href=""https://redirect.github.com/huggingface/transformers/issues/39953"">#39953</a>) by <a href=""https://github.com/returnL""><code>@​returnL</code></a></li>
</ul>
<h2>CI &amp; Build</h2>
<ul>
<li>CI stability – Post-GPT-OSS fixes for green CI (<a href=""https://redirect.github.com/huggingface/transformers/issues/39929"">#39929</a>) by <a href=""https://github.com/gante""><code>@​gante</code></a> <a href=""https://github.com/LysandreJik""><code>@​LysandreJik</code></a></li>
</ul>
<h2>v4.55.0: New openai GPT OSS model!</h2>
<h2>Welcome GPT OSS, the new open-source model family from OpenAI!</h2>
<!-- raw HTML omitted -->
<p>For more detailed information about this model, we recommend reading the following blogpost: <a href=""https://huggingface.co/blog/welcome-openai-gpt-oss"">https://huggingface.co/blog/welcome-openai-gpt-oss</a></p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/huggingface/transformers/commit/170b2708cb1977690a87753bbe55280974388513""><code>170b270</code></a> Fixes <a href=""https://redirect.github.com/huggingface/transformers/issues/40262"">#40262</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/7dbc054e2a0c3cafd3ea22db0566db700b3a8cbf""><code>7dbc054</code></a> v4.55.3</li>
<li><a href=""https://github.com/huggingface/transformers/commit/c097a43898550846beeb42ddaba2fffa9513929e""><code>c097a43</code></a> [bugfix] fix flash-attention2 unavailable error for Ascend NPU (<a href=""https://redirect.github.com/huggingface/transformers/issues/40151"">#40151</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/663cbb0d046e4a22919e7c822478b1f4f090c626""><code>663cbb0</code></a> [FA2] Fix it finally - revert fa kwargs preparation (<a href=""https://redirect.github.com/huggingface/transformers/issues/40161"">#40161</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/c7bd5350f090a3d064ba892d492080cce4d03326""><code>c7bd535</code></a> Fix fsdp for generic-task models <a href=""https://redirect.github.com/huggingface/transformers/issues/40191"">#40191</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/e75d67ec39b4f8bc03dbeea9016fff16c2375c5a""><code>e75d67e</code></a> Fix GPT-OSS swiglu_limit not passed in for MXFP4 <a href=""https://redirect.github.com/huggingface/transformers/issues/40197"">#40197</a></li>
<li><a href=""https://github.com/huggingface/transformers/commit/d7f67d2006aedd69999809f71657acb32b7d7e14""><code>d7f67d2</code></a> Fix mamba caches (<a href=""https://redirect.github.com/huggingface/transformers/issues/40203"">#40203</a>)</li>
<li><a href=""https://github.com/huggingface/transformers/commit/acf295aec3383b2e68ff8e4b6891c3c18fd078fa""><code>acf295a</code></a> v4.55.2</li>
<li><a href=""https://github.com/huggingface/transformers/commit/aaa3169aa225a540c5a43e3bd1d0d7ea50880c65""><code>aaa3169</code></a> qfix bad cherry-pick</li>
<li><a href=""https://github.com/huggingface/transformers/commit/ea2eee0bc8920a880db73a12c87ad39b41d3e834""><code>ea2eee0</code></a> v4.55.1</li>
<li>Additional commits viewable in <a href=""https://github.com/huggingface/transformers/compare/v4.54.0...v4.55.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=transformers&package-manager=pip&previous-version=4.54.0&new-version=4.55.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)


</details>

cc @seemethere @malfet @pytorch/pytorch-dev-infra @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 14:54:09+00:00,2025-09-19T17:59:08Z,,False,5,0,1,1,1,1,5,2025-09-19 17:58:58+00:00,96,11184,False,True,False,True,True,False,1,1,66,2,1,1,1,1,1.0,1.0,2025-09-19T17:58:58Z,pytorch
161164,closed,[BE] Improve torch.inference_mode docs and error message,soulitzer,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #161171
* __->__ #161164

cc @stas00 ",2025-08-21 14:35:18+00:00,2025-08-26T21:01:54Z,,False,3,1,2,27,16,2,4,2025-08-26 20:58:59+00:00,56,115,False,False,False,True,True,False,2,2,1384,45,28,17,1,2,5.0,3.0,2025-08-21T15:08:07Z,pytorch
161161,open,reshape or view ops validate evenly and unevenly sharded dtensor,dayanandav,"For view/reshape ops validate evenly or unevenly sharded dtensor before getting to runtime dispatch, thrown more specific error before getting to runtime dispatch as implemented here https://github.com/pytorch/pytorch/pull/149764

Fixes #161147 


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @tianyu-l @XilunWu",2025-08-21 13:27:31+00:00,2025-09-21T06:01:04Z,,False,2,9,8,18,8,2,11,,64,343,False,True,False,False,False,False,2,1,87,112162,69020,43142,2,8,5.0,2.0,2025-08-21T13:28:28Z,pytorch
161160,closed,Set proper device type before create tensor in distributed environment,dayanandav,"Distributed mesh device type must use to create tensor, this resolve create tensor on cpu instead create hardware/mesh device

Fixes #161154 


cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @tianyu-l @XilunWu",2025-08-21 12:21:59+00:00,2025-08-26T05:19:33Z,,False,4,0,1,1,1,1,4,2025-08-26 05:19:33+00:00,70,239,False,True,False,False,False,False,1,2,127,2,1,1,1,1,1.0,2.0,2025-08-21T13:13:32Z,pytorch
161159,closed,[bucketing] allow convert_element_type after fsdp reduce_scatter,IvanKobzarev,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161159



cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 11:22:11+00:00,2025-09-22T02:15:54Z,,False,6,1,1,233,205,3,7,2025-08-22 06:41:52+00:00,64,371,False,False,False,False,False,False,3,5,1373,438,233,205,1,1,4.0,5.0,2025-08-21T14:51:24Z,pytorch
161158,open,[ZENDNN] Add ZenDNN optimization pass,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* #161512
* #161495
* __->__ #161158
* #161157
* #161156
* #161155

-- Add fake tensor support through meta registrations for
   zendnn_linear and zendnn_weight_prepack_for_linear ops
-- Add shim file and required infra for AOT-Inductor support
-- Add optimize in joint_graph_passes to apply zendnn's
   optimization passes on the inductor generated graph
-- Register graph replacement patterns to replace aten.mm,
   aten.addmm and aten.linear with zendnn_linear wherever
   feasible
-- Add amd zen4 detection function
-- Add python binding for is_amd_cpu()
-- Add check for ZenDNN availability (torch._C.has_zendnn),
   user configuration (USE_ZENDNN env var), & CPU compatibility
   (torch._C._cpu._is_amd_cpu()) and AVX512 support
-- Register graph replacement patterns to add weight prepack op
   into aot inductor graph
-- Enable weight prepack optimization through zendnn's optimize
   api when inductor config for weight_prepack is True with
   inductor's freezing path

Co-authored-by: Mrigank Srivastava <mrigank.srivastava@amd.com>
Co-authored-by: Charan Ponnada <charan.ponnada@amd.com>
Co-authored-by: Harshal Adhav <harshal.adhav@amd.com>
Change-Id: I6c9627d1bfc5e87c50bae3d3de27be0d51442c1d

[RFC](https://github.com/pytorch/pytorch/issues/150296)

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @jerryzh168 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @Lucaskabela",2025-08-21 10:55:27+00:00,2025-09-25T15:14:15Z,,False,7,18,11,416,0,15,25,,37,1620,False,False,False,False,False,False,15,5,1940,144887,94711,50176,1,11,4.0,6.0,2025-08-22T17:07:34Z,pytorch
161157,open,[ZENDNN] Add Prepack op for ZenDNN linear,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* #161512
* #161495
* #161158
* __->__ #161157
* #161156
* #161155

-- Implement zendnn_weight_prepack_for_linear op to
   prepack weights into zendnn optimized blocked format.
-- Update zendnn_linear op to support prepacked weights.

Co-authored-by: Chinmay Kulkarni <Chinmay.Kulkarni@amd.com>
Change-Id: Ie04309b7f85d2e8e1321c4d811313efc0205603c

[RFC](https://github.com/pytorch/pytorch/issues/150296)",2025-08-21 10:55:13+00:00,2025-09-16T14:34:50Z,,False,4,0,11,106,6,7,4,,41,490,False,False,False,False,False,False,7,2,82,144404,94314,50090,1,11,1.0,2.0,2025-08-21T16:19:01Z,pytorch
161156,open,[ZENDNN] Add ZenDNN linear op support,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* #161512
* #161495
* #161158
* #161157
* __->__ #161156
* #161155

----

- Implements linear operations using ZenDNN
- Ensures compatibility with ZenDNN-specific tensor attributes and memory layouts

Change-Id: Ic7313a887528ac8071e0c126392f862ee16b1700
Co-authored-by: Harshal Adhav <harshal.adhav@amd.com>
Co-authored-by: Chinmay Kulkarni <Chinmay.Kulkarni@amd.com>

[RFC](https://github.com/pytorch/pytorch/issues/150296)",2025-08-21 10:54:58+00:00,2025-09-16T14:20:23Z,,False,3,0,11,242,11,3,3,,37,510,False,False,False,False,False,False,3,2,82,144536,94440,50096,1,11,1.0,2.0,2025-08-21T16:16:07Z,pytorch
161155,open,[ZENDNN] Add ZenDNN as an optional third-party lib,naveenthangudu,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* #162954
* #161512
* #161495
* #161158
* #161157
* #161156
* __->__ #161155

----

- Add ZenDNN as a third-party library and link it to ATen
- Add ZenDNN(caffe2::zendnn) as a caffe2 dependency
- Introduce USE_ZENDNN CMake ENV option
    - default OFF
    - user-enableable, valid only on x86_64 hosts
- Provide torch._C.has_zendnn() to query availability from Python
- Define AT_ZENDNN_ENABLED macro to gate C/C++ code
- Add ZenDNN as deperdency to ATen library
- Add ZenDNN as a submodule
- Extend the CMake summary to report ZenDNN build status
- Add USE_ZENDNN  setting and its status to build settings string in __confg__.show()
- Add variable substitutions for bazel build.
- Modify zendnn inductor perf builds with ""zen"" keyword.
- USE_ZENDNN set to 1 for *zen* build enviroments.

Co-authored-by: Dinesh Mareedu <Dinesh.Mareedu@amd.com>
Co-authored-by: Aakar Dwivedi <aakar.dwivedi@amd.com>
 Co-authored-by: Arijit Mukhopadhyay<arijit.mukhopadhyay@amd.com>
Change-Id: I8fdd93e11384d3550557f163faf34a7b8f18a6a9

[RFC](https://github.com/pytorch/pytorch/issues/150296)",2025-08-21 10:54:45+00:00,2025-09-16T14:46:28Z,,False,2,12,11,190,4,26,14,,50,1150,False,False,False,False,False,False,26,0,206,144444,94371,50073,1,11,4.0,2.0,2025-08-26T16:44:48Z,pytorch
161152,closed,Update torch-xpu-ops commit pin,yucai-intel,"Update the torch-xpu-ops commit to [8b58040ee32689487f660462f655085f31506dab](https://github.com/intel/torch-xpu-ops/commit/8b58040ee32689487f660462f655085f31506dab), includes:

- Add vectorization path on maxpool forward channel last 
- Add FlightRecorder support for ProcessGroupXCCL
- Fix random build failure on codegen
- Suppress dllexport warning on Windows
- Make torch-xpu-ops build depend on ATen XPU",2025-08-21 09:34:56+00:00,2025-08-30T07:20:30Z,,False,11,3,8,1,1,1,14,2025-08-30 07:19:28+00:00,31,409,False,True,False,False,False,False,1,6,2097,33079,22162,10917,2,7,3.0,6.0,2025-08-21T21:42:43Z,pytorch
161149,closed,Fix constant_pad_nd_mps bug when pad is empty,can-gaa-hou,"Fixes #161066

There is a size check here, which causes the error. 
https://github.com/pytorch/pytorch/blob/8ce81bcee1da294a34af0a90dc16483055e8c5a4/aten/src/ATen/native/mps/operations/Pad.mm#L39-L40

If the argument `pad` is empty, it will return the cloned tensor on CPU.

https://github.com/pytorch/pytorch/blob/8ce81bcee1da294a34af0a90dc16483055e8c5a4/aten/src/ATen/native/PadNd.cpp#L43-L64

Therefore, this PR fixes the empty padding argument error by checking the size first and returning a cloned tensor immediately if the padding size is 0.
",2025-08-21 07:52:34+00:00,2025-08-21T21:01:02Z,,False,6,0,1,3,0,1,6,2025-08-21 20:45:30+00:00,45,549,False,True,False,False,False,False,1,3,905,3,3,0,1,1,2.0,3.0,2025-08-21T18:42:10Z,pytorch
161148,closed,[CPU][Inductor] Improve performance of A16W8 GEMM template,Xia-Weiwen,"**Summary**
This PR improves the performance of A16W8 GEMM template by
- Removing the config with block_n=48 & block_m=16 as it is not very efficient.
- Using AMX microkernel when M >= 5 so that we use AMX instead of AVX512 for M=5~31.
- Converting int8 values to bf16 with intrinsics instead of `at::vec::convert` as the latter does not have optimized implementation for this case.

We saw up to >10% performance gain in various cases of running Llama-3.1-8b-instruct.

**Test plan**
Already covered by UT.

cc @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 07:46:13+00:00,2025-08-31T09:57:36Z,,False,6,5,8,92,20,2,11,2025-08-31 09:56:32+00:00,58,754,False,False,False,False,True,False,2,5,1161,34209,22135,12074,1,8,5.0,5.0,2025-08-26T01:38:11Z,pytorch
161146,closed,[inductor] add libraries_dirs for level_zero,xuhancn,"Changes:
1. change set `include_dirs` to append value.
2. add append `libraries_dirs` for level_zero.

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 07:21:11+00:00,2025-08-21T19:55:38Z,,False,3,0,1,2,1,1,3,2025-08-21 19:55:25+00:00,44,410,False,False,False,False,False,False,1,2,493,3,2,1,1,1,3.0,2.0,2025-08-21T17:29:25Z,pytorch
161145,closed,Add profiler analysis flag to combine multiple profiles into one,exclamaforte,"Combine multiple profiles into one:
```
python profile_analysis.py --combine <file1> <file2> ... <out>
```
This only works well if they have different pids, like from different programs in a distributed run.

<img width=""1521"" height=""465"" alt=""combining_multiple_profiles"" src=""https://github.com/user-attachments/assets/aba7112b-e9a9-4075-b82b-a4e4408384da"" />


cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben @mlazos",2025-08-21 07:14:05+00:00,2025-09-21T02:15:47Z,,False,6,0,6,171,0,2,6,2025-08-21 21:37:01+00:00,64,574,False,False,False,False,False,False,2,5,1857,219,195,24,1,6,3.0,6.0,2025-08-21T18:00:12Z,pytorch
161144,closed,[Inductor][CPP] Optimize config selecting for micro gemm when number of mxn blocks can not occupy all the threads,CaoE,"If number of mxn blocks can not occupy all the threads, use smaller register block size will get better performance since the computing size per thread is smaller.
It may get ~20% performance improvement for the real case `m1_n512_k4096`.

cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 06:59:26+00:00,2025-08-30T05:54:56Z,,False,5,4,1,7,2,1,9,2025-08-30 05:53:52+00:00,113,441,False,False,False,False,True,False,1,4,631,9,7,2,1,1,4.0,4.0,2025-08-25T06:51:41Z,pytorch
161143,closed,"[nativert] Introduce PassRegistrar, use static registration for graph passes",SherlockNoMad,"Summary:
Introduce PassRegistrar and REGISTER_GRAPH_PASS macro. 
Switch Graph passes to use REGISTER_GRAPH_PASS. 
Remove register_fuse_list_unpack_passes(), register_base_passes(), register_base_passes(), register_variadic_op_passes()

Graph Passes are now registered when program initializes.

Test Plan:
CI

Rollback Plan:

Differential Revision: D80682852


",2025-08-21 06:55:37+00:00,2025-09-13T03:42:47Z,,False,6,1,1,56,61,4,7,2025-09-13 03:42:47+00:00,76,361,False,False,False,False,False,False,4,2,156,117,56,61,1,1,3.0,2.0,2025-08-21T16:42:19Z,pytorch
161142,closed,[Fix XPU CI][Inductor UT] Fix test cases broken by community.,etaf,"Stack from [ghstack](https://github.com/ezyang/ghstack) (oldest at bottom):
* __->__ #161142

Fixes #161384, Fixes #161162, Fixes #160946, Fixes #160947, Fixes #160948



cc @voznesenskym @penguinwu @EikanWang @jgong5 @Guobing-Chen @XiaobingSuper @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 06:35:20+00:00,2025-09-03T03:18:51Z,,False,23,0,4,24,0,6,23,2025-09-03 03:18:51+00:00,61,372,False,True,False,False,False,False,6,22,6426,36882,25092,11790,1,4,4.0,22.0,2025-08-30T00:43:20Z,pytorch
161141,open,functorch: add vmap batching rule for efficient SDPA (forward + backward),PrithviElancherran,"## Summary
Implements a vmap batching rule for `aten::_scaled_dot_product_efficient_attention`
(and backward), removing the warning and slow fallback when using `vmap`.

## Approach
- Move the vmapped dim into the heads dimension (-3), call the efficient primitive once,
  and return output/grads with bdim at -3.
- Registered via `functorch._C.register_batch_rule(...)` in `functorch/_src`.

## Tests
- `test_vmap_sdpa_efficient_matches_loop`: vmap result ≈ manual loop.
- `test_vmap_sdpa_efficient_backward_matches_loop`: grads match loop.
- Tests skip gracefully if the efficient op isn't available on a given device.

## Motivation
Fixes #117016 — ""performance drop because batching rule not implemented"" for efficient SDPA. :contentReference[oaicite:1]{index=1}
",2025-08-21 06:01:26+00:00,2025-09-01T17:19:54Z,,False,3,0,3,185,0,3,3,,73,767,False,True,False,False,False,False,3,0,0,185,185,0,1,3,,,,pytorch
161140,closed,Debug #160583,huydhn,"No need to review
",2025-08-21 05:28:44+00:00,2025-09-22T02:15:51Z,,False,1,0,83,939,72,19,1,2025-08-22 16:59:01+00:00,13,18,False,True,False,False,False,False,19,0,0,27597,19736,7861,1,30,,,,pytorch
161137,closed,[BE] Remove the default TORCH_CUDA_ARCH_LIST in CI Docker image,huydhn,"This doesn't make sense to have this default to Maxwell, which is too old.  All other places in CI/CD needs to overwrite this value.  IMO, it makes more sense to not set this at all and let CI/CD jobs set it for their own use cases instead.  This is partly responsible for the build failure in https://github.com/pytorch/pytorch/issues/160988",2025-08-21 03:54:10+00:00,2025-08-22T06:04:18Z,,False,3,0,2,0,3,2,3,2025-08-22 06:03:15+00:00,63,342,False,False,False,True,False,False,2,2,810,3540,2736,804,1,2,3.0,2.0,2025-08-21T15:21:23Z,pytorch
161136,closed,[dist] expose unsafe_get_ptr for dist.ProcessGroupNCCL.NCCLConfig,youkaichao,"expose the pointer so that we can create the `ncclConfig_t` object from pytorch and use it elsewhere. this is useful to control the nccl communicator parameters for multiple nccl communicators.

cc @H-Huang @awgu @wanchaol @fegin @fduwjj @wz337 @wconstab @d4l3k @pragupta",2025-08-21 03:49:32+00:00,2025-08-21T10:58:08Z,,False,4,3,6,6,0,2,7,2025-08-21 10:47:06+00:00,65,271,False,False,False,False,False,False,2,3,665,44,25,19,1,6,3.0,4.0,2025-08-21T04:38:48Z,pytorch
161135,open,Update custom_class_detail.h,bumi001,"[quote=""pya, post:1, topic:222401""]
Without the above specialization, I ran into the following error using g+±13 with –std=c++17 option:

```
error: invalid use of incomplete type ‘struct torch::detail::WrapMethod<void (MyClass::*)() const noexcept>’
  110 |   return WrapMethod<Func>(std::move(f));
```
[/quote]

Fixes #ISSUE_NUMBER
",2025-08-21 03:29:51+00:00,2025-08-23T17:30:35Z,,False,5,1,1,11,0,1,6,,28,334,False,True,False,False,False,False,1,2,177,11,11,0,1,1,2.0,3.0,2025-08-22T18:55:12Z,pytorch
161133,closed,[inductor] disable min/max macro on Windows.,xuhancn,"Disable min/max macro on Windows.



cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @iremyux @Blackhex @jgong5 @mingfeima @XiaobingSuper @sanchitintel @ashokei @jingxu10 @voznesenskym @penguinwu @EikanWang @Guobing-Chen @zhuhaozhe @blzheng @wenzhe-nrv @jiayisunx @ipiszy @chenyang78 @kadeng @muchulee8 @amjames @chauhang @aakhundov @coconutruben",2025-08-21 03:06:57+00:00,2025-08-21T19:54:04Z,,False,3,0,2,13,0,1,3,2025-08-21 19:53:00+00:00,44,344,False,False,False,False,False,False,1,2,836,17,15,2,2,2,3.0,3.0,2025-08-21T17:24:47Z,pytorch
161131,closed,Add debugging facilities in jit code,cyyever,"Assertions are added to jit Graph destructor.
cc @EikanWang @jgong5 @wenzhe-nrv @sanchitintel",2025-08-21 02:53:13+00:00,2025-08-31T14:15:48Z,,False,6,0,1,6,3,1,6,2025-08-31 14:15:43+00:00,36,93,False,True,False,False,False,False,1,5,629,9,6,3,1,1,4.0,6.0,2025-08-21T05:02:59Z,pytorch
161130,open,Fix autograd assertion error in CUDA backend when privateuse1 is registered,matthewdcong,"Fixes #161129 .

When a PrivateUse1 backend is registered, the `at::isAccelerator()` and `at::getAccelerator()` functions will always return the PrivateUse1 accelerator even if the underlying tensors and models are using the existing CUDA backend. This results in the autograd assertion error in the issue above.

This PR addresses the issue by using the streams associated with the inputs (in part, reverting to the behavior of PyTorch 2.7.1) instead of the `isAccelerator()` and `getAccelerator` functions which can return the wrong backend and consequently the wrong stream.

",2025-08-21 02:30:21+00:00,2025-08-26T18:21:29Z,,False,4,0,1,7,9,2,4,,75,579,False,True,False,False,False,False,2,2,319,16,7,9,1,1,2.0,3.0,2025-08-21T02:34:18Z,pytorch
